{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236677f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b532072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 64\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "#             nn.BatchNorm1d(hidden_size),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d74d705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 57706, Validation Loss: 79300, 544714.2953250317\n",
      "Epoch 101, Training Loss: 16212, Validation Loss: 57488, 476321.5964711749\n",
      "Epoch 201, Training Loss: 14242, Validation Loss: 55986, 643777.5222900927\n",
      "Epoch 301, Training Loss: 12794, Validation Loss: 57778, 400984.1288185901\n",
      "Epoch 401, Training Loss: 15117, Validation Loss: 56509, 402572.34697657\n",
      "Epoch 501, Training Loss: 14301, Validation Loss: 57406, 604568.3877441789\n",
      "Epoch 601, Training Loss: 13659, Validation Loss: 55992, 446902.3396072545\n",
      "Epoch 701, Training Loss: 13177, Validation Loss: 56970, 497078.768496703\n",
      "Epoch 801, Training Loss: 12967, Validation Loss: 55756, 442566.4118235769\n",
      "Epoch 901, Training Loss: 14782, Validation Loss: 56865, 513505.21458876773\n",
      "Epoch 1001, Training Loss: 15867, Validation Loss: 56409, 701768.3883504705\n",
      "Epoch 1101, Training Loss: 14697, Validation Loss: 57228, 533387.6120425524\n",
      "Epoch 1201, Training Loss: 13867, Validation Loss: 57215, 489579.2670355222\n",
      "Epoch 1301, Training Loss: 14549, Validation Loss: 58247, 533457.5533504632\n",
      "Epoch 1401, Training Loss: 13315, Validation Loss: 57808, 488931.51795593183\n",
      "Epoch 1501, Training Loss: 12358, Validation Loss: 56931, 349268.7254188605\n",
      "Epoch 1601, Training Loss: 15201, Validation Loss: 57229, 427684.0355832665\n",
      "Epoch 1701, Training Loss: 12976, Validation Loss: 57615, 365099.13183613727\n",
      "Epoch 1801, Training Loss: 14557, Validation Loss: 58156, 507036.03542129183\n",
      "Epoch 1901, Training Loss: 12634, Validation Loss: 56450, 511487.7296650171\n",
      "Epoch 2001, Training Loss: 13839, Validation Loss: 57194, 455648.5779853303\n",
      "Epoch 2101, Training Loss: 13076, Validation Loss: 57330, 513169.0920059937\n",
      "Epoch 2201, Training Loss: 12761, Validation Loss: 55905, 544735.7854147532\n",
      "Epoch 2301, Training Loss: 12066, Validation Loss: 57056, 433791.30896195874\n",
      "Epoch 2401, Training Loss: 12586, Validation Loss: 56794, 458384.09231779835\n",
      "Epoch 2501, Training Loss: 14823, Validation Loss: 57395, 636428.1303522794\n",
      "Epoch 2601, Training Loss: 11211, Validation Loss: 56941, 419309.4852219438\n",
      "Epoch 2701, Training Loss: 13258, Validation Loss: 56708, 527865.8620329963\n",
      "Epoch 2801, Training Loss: 15490, Validation Loss: 58374, 573110.1335784316\n",
      "Epoch 2901, Training Loss: 11376, Validation Loss: 57255, 414621.29742051626\n",
      "Epoch 3001, Training Loss: 11781, Validation Loss: 58144, 356994.3324944917\n",
      "Epoch 3101, Training Loss: 13141, Validation Loss: 56454, 429918.7841850586\n",
      "Epoch 3201, Training Loss: 13337, Validation Loss: 57817, 521978.0484671181\n",
      "Epoch 3301, Training Loss: 13287, Validation Loss: 58817, 511431.6679535805\n",
      "Epoch 3401, Training Loss: 13968, Validation Loss: 58544, 670609.8227286811\n",
      "Epoch 3501, Training Loss: 12877, Validation Loss: 57775, 431859.54456511035\n",
      "Epoch 3601, Training Loss: 11997, Validation Loss: 57035, 471400.8459222652\n",
      "Epoch 3701, Training Loss: 12801, Validation Loss: 57196, 424672.48298502516\n",
      "Epoch 3801, Training Loss: 14207, Validation Loss: 57266, 508301.84765427356\n",
      "Epoch 3901, Training Loss: 12660, Validation Loss: 58523, 570487.0405335432\n",
      "Epoch 4001, Training Loss: 13541, Validation Loss: 57753, 502511.3623383286\n",
      "Epoch 4101, Training Loss: 11518, Validation Loss: 57089, 477969.6786203006\n",
      "Epoch 4201, Training Loss: 11646, Validation Loss: 58549, 355457.83127616363\n",
      "Epoch 4301, Training Loss: 13715, Validation Loss: 57126, 480498.7875235284\n",
      "Epoch 4401, Training Loss: 13905, Validation Loss: 57887, 530166.4580048245\n",
      "Epoch 4501, Training Loss: 12187, Validation Loss: 57262, 453596.89744402736\n",
      "Epoch 4601, Training Loss: 14137, Validation Loss: 58164, 551991.5611796785\n",
      "Epoch 4701, Training Loss: 11376, Validation Loss: 57401, 410375.0320485167\n",
      "Epoch 4801, Training Loss: 14611, Validation Loss: 58973, 456421.6156753416\n",
      "Epoch 4901, Training Loss: 13218, Validation Loss: 57453, 549287.3688752303\n",
      "Epoch 5001, Training Loss: 12515, Validation Loss: 57005, 624446.6224262982\n",
      "Epoch 5101, Training Loss: 11569, Validation Loss: 58056, 401518.28418689047\n",
      "Epoch 5201, Training Loss: 13676, Validation Loss: 57861, 530982.588471683\n",
      "Epoch 5301, Training Loss: 11530, Validation Loss: 57723, 456626.5121938423\n",
      "Epoch 5401, Training Loss: 11808, Validation Loss: 58430, 480450.7741754233\n",
      "Epoch 5501, Training Loss: 13082, Validation Loss: 58573, 541352.4291698047\n",
      "Epoch 5601, Training Loss: 11755, Validation Loss: 57766, 412050.100426309\n",
      "Epoch 5701, Training Loss: 11541, Validation Loss: 58399, 393144.2206146212\n",
      "Epoch 5801, Training Loss: 12778, Validation Loss: 58168, 456023.51418274967\n",
      "Epoch 5901, Training Loss: 12818, Validation Loss: 58144, 498925.6069770843\n",
      "Epoch 6001, Training Loss: 12166, Validation Loss: 58120, 411991.3183606604\n",
      "Epoch 6101, Training Loss: 11744, Validation Loss: 57801, 391398.2173375109\n",
      "Epoch 6201, Training Loss: 10911, Validation Loss: 58710, 416419.8244765089\n",
      "Epoch 6301, Training Loss: 10731, Validation Loss: 59001, 391595.736341066\n",
      "Epoch 6401, Training Loss: 13168, Validation Loss: 58419, 470455.86234037334\n",
      "Epoch 6501, Training Loss: 12998, Validation Loss: 57243, 553875.5751046214\n",
      "Epoch 6601, Training Loss: 10744, Validation Loss: 58029, 404898.85381311475\n",
      "Epoch 6701, Training Loss: 11208, Validation Loss: 58022, 310565.7778217523\n",
      "Epoch 6801, Training Loss: 12089, Validation Loss: 58005, 424369.05056370434\n",
      "Epoch 6901, Training Loss: 12814, Validation Loss: 58378, 530797.0789954066\n",
      "Epoch 7001, Training Loss: 12810, Validation Loss: 57365, 469944.8562044146\n",
      "Epoch 7101, Training Loss: 10943, Validation Loss: 58766, 397291.98605922563\n",
      "Epoch 7201, Training Loss: 12958, Validation Loss: 57537, 533192.5253212827\n",
      "Epoch 7301, Training Loss: 16918, Validation Loss: 57726, 510276.29082991276\n",
      "Epoch 7401, Training Loss: 11382, Validation Loss: 58635, 408405.3497702326\n",
      "Epoch 7501, Training Loss: 11751, Validation Loss: 57382, 401447.95791301056\n",
      "Epoch 7601, Training Loss: 12952, Validation Loss: 59117, 541197.4778345524\n",
      "Epoch 7701, Training Loss: 15971, Validation Loss: 57650, 544284.7575213098\n",
      "Epoch 7801, Training Loss: 11216, Validation Loss: 58250, 403452.16829026694\n",
      "Epoch 7901, Training Loss: 11799, Validation Loss: 58322, 449893.3608454207\n",
      "Epoch 8001, Training Loss: 10387, Validation Loss: 57173, 399311.84221151733\n",
      "Epoch 8101, Training Loss: 14084, Validation Loss: 59083, 598684.474484788\n",
      "Epoch 8201, Training Loss: 11696, Validation Loss: 58122, 441526.3228095635\n",
      "Epoch 8301, Training Loss: 11999, Validation Loss: 58436, 544776.0498658406\n",
      "Epoch 8401, Training Loss: 11291, Validation Loss: 58607, 401621.2112281845\n",
      "Epoch 8501, Training Loss: 12284, Validation Loss: 58109, 492503.51541209937\n",
      "Epoch 8601, Training Loss: 10462, Validation Loss: 58854, 378458.453225606\n",
      "Epoch 8701, Training Loss: 12309, Validation Loss: 58175, 463572.9825860291\n",
      "Epoch 8801, Training Loss: 14993, Validation Loss: 59147, 627495.3011456614\n",
      "Epoch 8901, Training Loss: 11894, Validation Loss: 57278, 465609.3167892037\n",
      "Epoch 9001, Training Loss: 10652, Validation Loss: 58393, 416485.8358065103\n",
      "Epoch 9101, Training Loss: 14069, Validation Loss: 59262, 599333.144926027\n",
      "Epoch 9201, Training Loss: 12360, Validation Loss: 58701, 508775.20686267444\n",
      "Epoch 9301, Training Loss: 11178, Validation Loss: 59849, 407425.72880484664\n",
      "Epoch 9401, Training Loss: 10405, Validation Loss: 58714, 353990.6124918781\n",
      "Epoch 9501, Training Loss: 13060, Validation Loss: 58809, 477200.8833081342\n",
      "Epoch 9601, Training Loss: 11376, Validation Loss: 59007, 574164.2781055036\n",
      "Epoch 9701, Training Loss: 11553, Validation Loss: 58501, 511017.8184772112\n",
      "Epoch 9801, Training Loss: 10892, Validation Loss: 57728, 361728.56294442544\n",
      "Epoch 9901, Training Loss: 10349, Validation Loss: 57796, 361486.31993273925\n",
      "Epoch 10001, Training Loss: 9990, Validation Loss: 58471, 320316.06200405606\n",
      "Epoch 10101, Training Loss: 12161, Validation Loss: 59523, 469554.08187794714\n",
      "Epoch 10201, Training Loss: 10274, Validation Loss: 58715, 386239.9845031251\n",
      "Epoch 10301, Training Loss: 12228, Validation Loss: 58209, 459235.2514685492\n",
      "Epoch 10401, Training Loss: 11432, Validation Loss: 60355, 460081.64908462315\n",
      "Epoch 10501, Training Loss: 12390, Validation Loss: 58953, 446374.79355110397\n",
      "Epoch 10601, Training Loss: 11869, Validation Loss: 60552, 526457.8901861474\n",
      "Epoch 10701, Training Loss: 13219, Validation Loss: 58297, 518220.6450247772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10801, Training Loss: 11140, Validation Loss: 58486, 440584.0532189442\n",
      "Epoch 10901, Training Loss: 11930, Validation Loss: 58169, 487688.8872809497\n",
      "Epoch 11001, Training Loss: 12761, Validation Loss: 59594, 547426.3470896835\n",
      "Epoch 11101, Training Loss: 15153, Validation Loss: 58876, 625252.2991966812\n",
      "Epoch 11201, Training Loss: 12712, Validation Loss: 59523, 596305.6738303278\n",
      "Epoch 11301, Training Loss: 9127, Validation Loss: 58475, 347448.5598052243\n",
      "Epoch 11401, Training Loss: 10578, Validation Loss: 59373, 434635.12319088104\n",
      "Epoch 11501, Training Loss: 11197, Validation Loss: 58740, 481319.6078210422\n",
      "Epoch 11601, Training Loss: 12098, Validation Loss: 59429, 445155.86370701075\n",
      "Epoch 11701, Training Loss: 14873, Validation Loss: 58293, 610805.9292221079\n",
      "Epoch 11801, Training Loss: 11369, Validation Loss: 57806, 420796.34126098716\n",
      "Epoch 11901, Training Loss: 10614, Validation Loss: 58673, 419675.6768562656\n",
      "Epoch 12001, Training Loss: 10962, Validation Loss: 57780, 431982.3175126927\n",
      "Epoch 12101, Training Loss: 11490, Validation Loss: 59288, 418715.22249666136\n",
      "Epoch 12201, Training Loss: 11605, Validation Loss: 58721, 437053.87087041436\n",
      "Epoch 12301, Training Loss: 12697, Validation Loss: 59517, 531347.7596004421\n",
      "Epoch 12401, Training Loss: 10961, Validation Loss: 58842, 386559.33718035696\n",
      "Epoch 12501, Training Loss: 11639, Validation Loss: 59152, 472977.34867039917\n",
      "Epoch 12601, Training Loss: 12205, Validation Loss: 59428, 510046.55312049633\n",
      "Epoch 12701, Training Loss: 9547, Validation Loss: 59884, 360196.7420802909\n",
      "Epoch 12801, Training Loss: 10573, Validation Loss: 59066, 535116.3709775049\n",
      "Epoch 12901, Training Loss: 13121, Validation Loss: 58259, 486505.862048312\n",
      "Epoch 13001, Training Loss: 10887, Validation Loss: 58063, 427335.3296245274\n",
      "Epoch 13101, Training Loss: 10019, Validation Loss: 59285, 373546.6856320947\n",
      "Epoch 13201, Training Loss: 9845, Validation Loss: 60259, 378201.1229079835\n",
      "Epoch 13301, Training Loss: 12855, Validation Loss: 58247, 470983.25606315845\n",
      "Epoch 13401, Training Loss: 11665, Validation Loss: 59639, 494683.70517746115\n",
      "Epoch 13501, Training Loss: 11812, Validation Loss: 59903, 434375.3043050279\n",
      "Epoch 13601, Training Loss: 10901, Validation Loss: 58317, 458996.6198506433\n",
      "Epoch 13701, Training Loss: 10520, Validation Loss: 58223, 422931.0414304604\n",
      "Epoch 13801, Training Loss: 11083, Validation Loss: 59025, 510377.26621480286\n",
      "Epoch 13901, Training Loss: 14138, Validation Loss: 58885, 545861.5207764702\n",
      "Epoch 14001, Training Loss: 12497, Validation Loss: 58928, 428308.10699966847\n",
      "Epoch 14101, Training Loss: 12312, Validation Loss: 58705, 495023.2793027576\n",
      "Epoch 14201, Training Loss: 9963, Validation Loss: 59839, 444952.20976665104\n",
      "Epoch 14301, Training Loss: 10527, Validation Loss: 58608, 470517.5538108349\n",
      "Epoch 14401, Training Loss: 11116, Validation Loss: 60108, 410741.67981735413\n",
      "Epoch 14501, Training Loss: 14134, Validation Loss: 59727, 655067.955067779\n",
      "Epoch 14601, Training Loss: 10473, Validation Loss: 58695, 416659.02570316364\n",
      "Epoch 14701, Training Loss: 12123, Validation Loss: 59054, 545517.4172854314\n",
      "Epoch 14801, Training Loss: 10365, Validation Loss: 58864, 426989.48719211016\n",
      "Epoch 14901, Training Loss: 10683, Validation Loss: 58017, 441721.0286937176\n",
      "Epoch 15001, Training Loss: 10643, Validation Loss: 58579, 408830.6461724566\n",
      "Epoch 15101, Training Loss: 9980, Validation Loss: 58581, 378280.9783106618\n",
      "Epoch 15201, Training Loss: 9575, Validation Loss: 59270, 384394.9068787816\n",
      "Epoch 15301, Training Loss: 11326, Validation Loss: 59191, 512567.384921751\n",
      "Epoch 15401, Training Loss: 10702, Validation Loss: 59237, 444706.6686588413\n",
      "Epoch 15501, Training Loss: 10558, Validation Loss: 59068, 373078.0428787429\n",
      "Epoch 15601, Training Loss: 11442, Validation Loss: 60276, 491599.4299872151\n",
      "Epoch 15701, Training Loss: 9104, Validation Loss: 59228, 332307.99951707374\n",
      "Epoch 15801, Training Loss: 12237, Validation Loss: 59412, 539257.6203857347\n",
      "Epoch 15901, Training Loss: 9664, Validation Loss: 59094, 381380.26831750915\n",
      "Epoch 16001, Training Loss: 10901, Validation Loss: 59434, 467869.74299111933\n",
      "Epoch 16101, Training Loss: 12426, Validation Loss: 59031, 447816.2404651396\n",
      "Epoch 16201, Training Loss: 10036, Validation Loss: 59499, 378837.1395668147\n",
      "Epoch 16301, Training Loss: 12436, Validation Loss: 59555, 547988.9229118364\n",
      "Epoch 16401, Training Loss: 10203, Validation Loss: 58989, 501780.2934509708\n",
      "Epoch 16501, Training Loss: 11363, Validation Loss: 59339, 506863.2258277937\n",
      "Epoch 16601, Training Loss: 11654, Validation Loss: 58997, 459089.96963430877\n",
      "Epoch 16701, Training Loss: 10107, Validation Loss: 58806, 386984.3152269085\n",
      "Epoch 16801, Training Loss: 10551, Validation Loss: 60033, 462288.43885111035\n",
      "Epoch 16901, Training Loss: 9547, Validation Loss: 58740, 416212.86806652485\n",
      "Epoch 17001, Training Loss: 9751, Validation Loss: 58960, 458773.7235070424\n",
      "Epoch 17101, Training Loss: 11298, Validation Loss: 58686, 499025.3102780304\n",
      "Epoch 17201, Training Loss: 8904, Validation Loss: 58859, 422812.6797022934\n",
      "Epoch 17301, Training Loss: 11724, Validation Loss: 60115, 425253.6547731973\n",
      "Epoch 17401, Training Loss: 9421, Validation Loss: 59334, 408493.3438523589\n",
      "Epoch 17501, Training Loss: 9880, Validation Loss: 59733, 362098.79260426364\n",
      "Epoch 17601, Training Loss: 12883, Validation Loss: 58750, 464506.4185017461\n",
      "Epoch 17701, Training Loss: 10239, Validation Loss: 58990, 434885.82841996115\n",
      "Epoch 17801, Training Loss: 10758, Validation Loss: 58786, 386854.7454610029\n",
      "Epoch 17901, Training Loss: 9908, Validation Loss: 59743, 395180.51036948414\n",
      "Epoch 18001, Training Loss: 11186, Validation Loss: 59342, 471620.63133186253\n",
      "Epoch 18101, Training Loss: 10199, Validation Loss: 59862, 363844.63350146025\n",
      "Epoch 18201, Training Loss: 12724, Validation Loss: 59732, 472404.7032286359\n",
      "Epoch 18301, Training Loss: 13110, Validation Loss: 60216, 442126.51803612366\n",
      "Epoch 18401, Training Loss: 10446, Validation Loss: 59595, 404436.00915719836\n",
      "Epoch 18501, Training Loss: 11181, Validation Loss: 59164, 433019.9619586317\n",
      "Epoch 18601, Training Loss: 10292, Validation Loss: 58327, 423530.28705173184\n",
      "Epoch 18701, Training Loss: 9352, Validation Loss: 58414, 408181.30697856983\n",
      "Epoch 18801, Training Loss: 9982, Validation Loss: 58444, 340117.23013433977\n",
      "Epoch 18901, Training Loss: 8828, Validation Loss: 59152, 411386.36211515765\n",
      "Epoch 19001, Training Loss: 9946, Validation Loss: 58649, 452399.8435406765\n",
      "Epoch 19101, Training Loss: 10687, Validation Loss: 58953, 393209.9225498163\n",
      "Epoch 19201, Training Loss: 13617, Validation Loss: 59773, 578064.4290421157\n",
      "Epoch 19301, Training Loss: 9615, Validation Loss: 59236, 378693.0539852402\n",
      "Epoch 19401, Training Loss: 12521, Validation Loss: 59221, 511376.7811561944\n",
      "Epoch 19501, Training Loss: 12744, Validation Loss: 58954, 536017.9802539049\n",
      "Epoch 19601, Training Loss: 10153, Validation Loss: 59354, 408175.9777123233\n",
      "Epoch 19701, Training Loss: 10563, Validation Loss: 58733, 381384.005929536\n",
      "Epoch 19801, Training Loss: 11305, Validation Loss: 59434, 517290.46108123456\n",
      "Epoch 19901, Training Loss: 10551, Validation Loss: 59662, 477984.382899213\n",
      "Epoch 20001, Training Loss: 9237, Validation Loss: 59428, 412479.69016917795\n",
      "Epoch 20101, Training Loss: 9103, Validation Loss: 59627, 373681.424043018\n",
      "Epoch 20201, Training Loss: 9664, Validation Loss: 59508, 395571.84296620975\n",
      "Epoch 20301, Training Loss: 9038, Validation Loss: 60490, 363131.5478500559\n",
      "Epoch 20401, Training Loss: 11331, Validation Loss: 59696, 487713.50673674094\n",
      "Epoch 20501, Training Loss: 8590, Validation Loss: 58938, 370473.0978566329\n",
      "Epoch 20601, Training Loss: 10492, Validation Loss: 58751, 472830.03747137095\n",
      "Epoch 20701, Training Loss: 10088, Validation Loss: 59570, 478154.3706742417\n",
      "Epoch 20801, Training Loss: 10061, Validation Loss: 60214, 415930.5373841118\n",
      "Epoch 20901, Training Loss: 11169, Validation Loss: 60080, 470345.2908945138\n",
      "Epoch 21001, Training Loss: 11106, Validation Loss: 58757, 417206.3066654649\n",
      "Epoch 21101, Training Loss: 8722, Validation Loss: 59673, 341804.35907609266\n",
      "Epoch 21201, Training Loss: 11358, Validation Loss: 59310, 516719.23941614816\n",
      "Epoch 21301, Training Loss: 9610, Validation Loss: 58746, 465531.9413766223\n",
      "Epoch 21401, Training Loss: 9535, Validation Loss: 59708, 386104.42827932606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21501, Training Loss: 12326, Validation Loss: 60747, 489845.1981148251\n",
      "Epoch 21601, Training Loss: 10153, Validation Loss: 59698, 441840.6408679184\n",
      "Epoch 21701, Training Loss: 11535, Validation Loss: 59020, 454822.20476757485\n",
      "Epoch 21801, Training Loss: 10962, Validation Loss: 59142, 476324.2692109762\n",
      "Epoch 21901, Training Loss: 10687, Validation Loss: 58927, 474093.5994671737\n",
      "Epoch 22001, Training Loss: 10314, Validation Loss: 59753, 471797.1942511977\n",
      "Epoch 22101, Training Loss: 9191, Validation Loss: 59092, 444658.20340678794\n",
      "Epoch 22201, Training Loss: 10444, Validation Loss: 58882, 457487.22902190534\n",
      "Epoch 22301, Training Loss: 11089, Validation Loss: 58646, 365373.7722407513\n",
      "Epoch 22401, Training Loss: 10853, Validation Loss: 59347, 370101.1887741421\n",
      "Epoch 22501, Training Loss: 9234, Validation Loss: 58912, 389877.0750379846\n",
      "Epoch 22601, Training Loss: 8737, Validation Loss: 59295, 351387.2318175598\n",
      "Epoch 22701, Training Loss: 9240, Validation Loss: 59321, 369841.2672113124\n",
      "Epoch 22801, Training Loss: 10624, Validation Loss: 59587, 504573.4504372589\n",
      "Epoch 22901, Training Loss: 9495, Validation Loss: 59637, 450140.5339152638\n",
      "Epoch 23001, Training Loss: 10172, Validation Loss: 59245, 402551.60266596655\n",
      "Epoch 23101, Training Loss: 8422, Validation Loss: 59752, 314915.6409038483\n",
      "Epoch 23201, Training Loss: 9165, Validation Loss: 58779, 397523.8799381411\n",
      "Epoch 23301, Training Loss: 8749, Validation Loss: 59290, 432285.4062100297\n",
      "Epoch 23401, Training Loss: 9092, Validation Loss: 61084, 369857.76390752976\n",
      "Epoch 23501, Training Loss: 8540, Validation Loss: 59492, 408706.4048545511\n",
      "Epoch 23601, Training Loss: 11103, Validation Loss: 59302, 502097.12945063424\n",
      "Epoch 23701, Training Loss: 10690, Validation Loss: 60392, 499841.90220493625\n",
      "Epoch 23801, Training Loss: 10357, Validation Loss: 58552, 454896.5353554669\n",
      "Epoch 23901, Training Loss: 10614, Validation Loss: 59668, 441972.070309026\n",
      "Epoch 24001, Training Loss: 10050, Validation Loss: 60430, 435367.12734228314\n",
      "Epoch 24101, Training Loss: 9874, Validation Loss: 59762, 357281.56997128273\n",
      "Epoch 24201, Training Loss: 9736, Validation Loss: 59432, 412605.6153560298\n",
      "Epoch 24301, Training Loss: 9463, Validation Loss: 60261, 503472.4000157686\n",
      "Epoch 24401, Training Loss: 11547, Validation Loss: 59483, 463012.86269557086\n",
      "Epoch 24501, Training Loss: 8732, Validation Loss: 60519, 325741.52522590966\n",
      "Epoch 24601, Training Loss: 9698, Validation Loss: 59023, 376240.93480608065\n",
      "Epoch 24701, Training Loss: 8602, Validation Loss: 60049, 393731.8948547343\n",
      "Epoch 24801, Training Loss: 12950, Validation Loss: 58804, 533241.0048436783\n",
      "Epoch 24901, Training Loss: 8486, Validation Loss: 59387, 351592.2449520731\n",
      "Epoch 25001, Training Loss: 8985, Validation Loss: 59242, 314062.2558563333\n",
      "Epoch 25101, Training Loss: 14024, Validation Loss: 58494, 588630.1287486354\n",
      "Epoch 25201, Training Loss: 11088, Validation Loss: 59770, 448296.6153685148\n",
      "Epoch 25301, Training Loss: 9962, Validation Loss: 59231, 393824.5758519631\n",
      "Epoch 25401, Training Loss: 8448, Validation Loss: 58426, 432773.46632238774\n",
      "Epoch 25501, Training Loss: 8853, Validation Loss: 58704, 405358.4854190231\n",
      "Epoch 25601, Training Loss: 8939, Validation Loss: 58738, 362807.97367126896\n",
      "Epoch 25701, Training Loss: 11002, Validation Loss: 60341, 476916.48619734263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m     grad_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_gradient_norm(model)\n\u001b[0;32m---> 32\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     36\u001b[0m grad_norms\u001b[38;5;241m.\u001b[39mappend(grad_norm \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:370\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    366\u001b[0m         (param\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_cuda) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mis_xla \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_xla)\n\u001b[1;32m    367\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=True, params and state_steps must be CUDA or XLA tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    373\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=9e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999999, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d48a51cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHcCAYAAAA3PbXpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAACBKUlEQVR4nO3dd3hTZfsH8O9J2qa7pZS2tBQKBQplVaBsQbAyZMjGgUJRQK2g8iKKiiDiD9T3VRxVBEEQFRBERGWoCKKAgAxB9lRWSwvdO8n9+yPNoemAlqYkKd/PdeVqcs7JyfMkpzl37mccRUQERERERLcxja0LQERERGRrDIiIiIjotseAiIiIiG57DIiIiIjotseAiIiIiG57DIiIiIjotseAiIiIiG57DIiIiIjotseAiIiIiG57DIiIqpHdu3ejU6dO8PDwgKIo2L9/P2bMmAFFUWxdNNx111246667bF2MGxo9ejTCwsJu6rn2XsfK1M0awsLCMHr0aJu9PtH1MCAiqqBDhw5h5MiRCAkJgU6nQ3BwMB566CEcOnTIpuUqKCjAsGHDcPXqVbzzzjtYunQp6tWrV+q2//d//4c1a9aUWL59+3bMmDEDqampVVtYqrZ4DJGjYkBEVAGrV69G69atsWnTJsTGxuLDDz/Eo48+is2bN6N169b45ptvbFa2U6dO4Z9//sHkyZMxbtw4jBw5EjVq1MDLL7+MnJwci22vFxC9+uqrPJnRTbveMXTs2DEsWLDg1heKqBycbF0AIkdx6tQpPPzww2jQoAG2bt2KWrVqqeuefvpp3HnnnXj44Ydx4MABNGjQ4JaVKysrCx4eHrh8+TIAwNfX12K9k5MTnJz4r062p9PpbF0EojIxQ0RUTm+99Rays7Mxf/58i2AIAPz9/fHxxx8jKysLb775JgBg1apVUBQFv/76a4l9ffzxx1AUBX///be67OjRoxg6dCj8/Pzg6uqKtm3bYu3atRbPW7x4sbrPJ598EgEBAahTpw5Gjx6Nbt26AQCGDRsGRVHUvizF+xApioKsrCwsWbIEiqJAURSMHj0aM2bMwHPPPQcAqF+/vrru7Nmz6nM///xztGnTBm5ubvDz88P999+Pc+fOlajf/PnzER4eDjc3N7Rr1w6//fZbud9nRVHw1FNPYeXKlYiMjISbmxs6duyIgwcPqu9dw4YN4erqirvuusuifGYrV65Uy+nv74+RI0fiwoULJbZbs2YNmjdvDldXVzRv3rzMDJ/RaMTcuXPRrFkzuLq6IjAwEOPHj0dKSkq561Xcjd7Lp556Cp6ensjOzi7x3AceeABBQUEwGAwAgG+//RZ9+/ZFcHAwdDodwsPD8dprr6nry7JlyxYoioItW7ZYLD979iwURcHixYvVZQcOHMDo0aPRoEEDuLq6IigoCGPGjMGVK1fUbW50DJXWh+j06dMYNmwY/Pz84O7ujg4dOuCHH34otZxfffUVXn/9ddSpUweurq64++67cfLkyevWkai8+LORqJy+++47hIWF4c477yx1fdeuXREWFqZ+mfft2xeenp746quv1GDFbMWKFWjWrBmaN28OwNQvqXPnzggJCcELL7wADw8PfPXVVxg4cCC+/vprDBo0yOL5Tz75JGrVqoVXXnkFWVlZ6Nq1K0JCQvB///d/mDhxIqKjoxEYGFhqOZcuXYrHHnsM7dq1w7hx4wAA4eHh8PDwwPHjx7Fs2TK888478Pf3BwA1+Hv99dcxbdo0DB8+HI899hiSkpLw/vvvo2vXrti3b5+amVq4cCHGjx+PTp064ZlnnsHp06cxYMAA+Pn5ITQ0tFzv9W+//Ya1a9ciLi4OADB79mz069cPU6ZMwYcffognn3wSKSkpePPNNzFmzBj88ssv6nMXL16M2NhYREdHY/bs2UhMTMS7776Lbdu2WZTzxx9/xJAhQxAZGYnZs2fjypUriI2NRZ06dUqUZ/z48ep+J06ciDNnzuCDDz7Avn37sG3bNjg7O5erXmbleS9HjBiB+Ph4/PDDDxg2bJj63OzsbHz33XcYPXo0tFqtWmdPT09MmjQJnp6e+OWXX/DKK68gPT0db731VoXKVpaffvoJp0+fRmxsLIKCgnDo0CHMnz8fhw4dwh9//AFFUTB48ODrHkPFJSYmolOnTsjOzsbEiRNRs2ZNLFmyBAMGDMCqVatKHPdz5syBRqPB5MmTkZaWhjfffBMPPfQQdu7caZU60m1OiOiGUlNTBYDcd999191uwIABAkDS09NFROSBBx6QgIAA0ev16jaXLl0SjUYjM2fOVJfdfffd0qJFC8nNzVWXGY1G6dSpkzRq1Ehd9umnnwoA6dKli8U+RUQ2b94sAGTlypUWy6dPny7F/9U9PDxk1KhRJcr/1ltvCQA5c+aMxfKzZ8+KVquV119/3WL5wYMHxcnJSV2en58vAQEBEhUVJXl5eep28+fPFwDSrVu3Eq9ZHADR6XQWZfj4448FgAQFBanvrYjI1KlTLcprfv3mzZtLTk6Out33338vAOSVV15Rl0VFRUnt2rUlNTVVXfbjjz8KAKlXr5667LfffhMA8sUXX1iUc8OGDSWWd+vW7YZ1LO97aTQaJSQkRIYMGWKx3VdffSUAZOvWreqy7OzsEq8zfvx4cXd3tzimRo0aZVE38zGzefNmi+eeOXNGAMinn3563ddYtmxZibKUdQyJiNSrV8/iuHvmmWcEgPz222/qsoyMDKlfv76EhYWJwWCwKGfTpk0tjqt3331XAMjBgwdLvBZRRbHJjKgcMjIyAABeXl7X3c68Pj09HQAwYsQIXL582aJJYtWqVTAajRgxYgQA4OrVq/jll18wfPhwZGRkIDk5GcnJybhy5Qp69eqFEydOlGjuGTt2rJoduBVWr14No9GI4cOHq+VLTk5GUFAQGjVqhM2bNwMA/vzzT1y+fBmPP/44XFxc1OePHj0aPj4+5X69u+++22J4ePv27QEAQ4YMsfgMzMtPnz5t8fpPPvkkXF1d1e369u2LJk2aqNm7S5cuYf/+/Rg1apRFue655x5ERkZalGXlypXw8fHBPffcY1H3Nm3awNPTU617eZX3vVQUBcOGDcO6deuQmZmpPn/FihUICQlBly5d1GVubm7qffMxdOeddyI7OxtHjx6tUPnKUvQ1cnNzkZycjA4dOgAA9u7de1P7XLduHdq1a2dRF09PT4wbNw5nz57F4cOHLbaPjY21OK7M2Vrz509UGWwyIyoH80nYHBiVpXjg1Lt3b/j4+GDFihW4++67AZhOaFFRUWjcuDEA4OTJkxARTJs2DdOmTSt1v5cvX0ZISIj6uH79+pWrUAWdOHECIoJGjRqVut7cZPTPP/8AQIntnJ2dK9TRvG7duhaPzUFL8SY383JzXx7z60dERJTYZ5MmTfD7779ft5zm5xY9wZ84cQJpaWkICAgotazmzuzlVd73EjAF1HPnzsXatWvx4IMPIjMzE+vWrcP48eMt+oUdOnQIL7/8Mn755Rc1GDdLS0urUPnKcvXqVbz66qtYvnx5iTrf7Gv8888/alBbVNOmTdX15mZloORxUaNGDQCoVF8uIjMGRETl4OPjg9q1a+PAgQPX3e7AgQMICQmBt7c3ANOomoEDB+Kbb77Bhx9+iMTERGzbtg3/93//pz7HaDQCACZPnoxevXqVut+GDRtaPC76a/1WMBqNUBQF69evLzUz5enpadXXKyv7VdZyEbHq6xdlNBoREBCAL774otT1ZfWPud7+yvtedujQAWFhYfjqq6/w4IMP4rvvvkNOTo6aXQSA1NRUdOvWDd7e3pg5cybCw8Ph6uqKvXv34vnnn1ePr9KUNWFnaZ2xhw8fju3bt+O5555DVFQUPD09YTQa0bt37+u+hjXZ4vOn2wcDIqJy6tevHxYsWIDff//dIsVv9ttvv+Hs2bMYP368xfIRI0ZgyZIl2LRpE44cOQIRsTihmTMnzs7OiImJqdpKFCrrRFjW8vDwcIgI6tevr2a2SmOeCPLEiRPo0aOHurygoABnzpxBq1atKlHqGzO//rFjxyxe37zMvL5oOYs7duyYxePw8HD8/PPP6Ny5s1UC0fK+l2bDhw/Hu+++i/T0dKxYsQJhYWFqUxVgGoF15coVrF69Gl27dlWXnzlz5ob7NmdYis8ZZM6gmaWkpGDTpk149dVX8corr6jLS3v/KjIrer169Uq83wDUZr6yJhYlqgrsQ0RUTs899xzc3Nwwfvx4i6HGgKk54fHHH4e7u7s67NgsJiYGfn5+WLFiBVasWIF27dpZNHkFBATgrrvuwscff4xLly6VeN2kpCSr18XDw6PUifM8PDwAlDxBDh48GFqtFq+++mqJX+Mior4fbdu2Ra1atTBv3jzk5+er2yxevPiWTPbYtm1bBAQEYN68ecjLy1OXr1+/HkeOHEHfvn0BALVr10ZUVBSWLFli0dzz008/lei3Mnz4cBgMBrz22mslXk+v11e4XuV9L81GjBiBvLw8LFmyBBs2bMDw4cMt1puzJkX3lZ+fjw8//PCGZalXrx60Wi22bt1qsbz4c0t7DQCYO3duiX2WdQyV5t5778WuXbuwY8cOdVlWVhbmz5+PsLCwEv25iKoSM0RE5dSoUSMsWbIEDz30EFq0aIFHH30U9evXx9mzZ7Fw4UIkJydj2bJlCA8Pt3ies7MzBg8ejOXLlyMrKwv//e9/S+w7Pj4eXbp0QYsWLTB27Fg0aNAAiYmJ2LFjB86fP4+//vrLqnVp06YNfv75Z7z99tsIDg5G/fr10b59e7Rp0wYA8NJLL+H++++Hs7Mz+vfvj/DwcMyaNQtTp07F2bNnMXDgQHh5eeHMmTP45ptvMG7cOEyePBnOzs6YNWsWxo8fjx49emDEiBE4c+YMPv3001syWaWzszPeeOMNxMbGolu3bnjggQfUYfdhYWF49tln1W1nz56Nvn37okuXLhgzZgyuXr2K999/H82aNbPoxNytWzeMHz8es2fPxv79+9GzZ084OzvjxIkTWLlyJd59910MHTq03GUs73tp1rp1azRs2BAvvfQS8vLyLLKLANCpUyfUqFEDo0aNwsSJE6EoCpYuXVquZiQfHx8MGzYM77//PhRFQXh4OL7//vsSfYS8vb3RtWtXvPnmmygoKEBISAh+/PHHUrNQZR1D5kCpqBdeeAHLli1Dnz59MHHiRPj5+WHJkiU4c+YMvv76a2g0/M1Ot5ANRrYRObQDBw7IAw88ILVr1xZnZ2cJCgqSBx544LpDf3/66ScBIIqiyLlz50rd5tSpU/LII49IUFCQODs7S0hIiPTr109WrVqlbmMedr979+4Sz6/IsPujR49K165dxc3NTQBYDIV+7bXXJCQkRDQaTYnh019//bV06dJFPDw8xMPDQ5o0aSJxcXFy7Ngxi/1/+OGHUr9+fdHpdNK2bVvZunVruYaki5iG3cfFxVksMw8Df+utt8pV5xUrVsgdd9whOp1O/Pz85KGHHpLz58+XeK2vv/5amjZtKjqdTiIjI2X16tUlhqabzZ8/X9q0aSNubm7i5eUlLVq0kClTpsjFixfVbcpbR/Nrl+e9FBF56aWXBIA0bNiw1H1t27ZNOnToIG5ubhIcHCxTpkyRjRs3lhhSX1rdkpKSZMiQIeLu7i41atSQ8ePHy99//11i2P358+dl0KBB4uvrKz4+PjJs2DC5ePGiAJDp06db7LOsY6j4sHsR03E/dOhQ8fX1FVdXV2nXrp18//33FtuU9TmXNj0A0c1SRNgbjYiIiG5vzEcSERHRbY8BEREREd32GBARERHRbY8BEREREd32GBARERHRbY8BEREREd32ODFjGYxGIy5evAgvL68KTUVPREREtiMiyMjIQHBwcIUm92RAVIaLFy+WuLI2EREROYZz586hTp065d6eAVEZvLy8AJjeUPOVy4mIiMi+paenIzQ0VD2PlxcDojKYm8m8vb0ZEBERETmYinZ3YadqIiIiuu0xICIiIqLbHgMiIiIiuu0xICIiIqLbHgMiIiIiuu0xICIiIqLbXrUIiAYNGoQaNWpg6NChFsu///57REREoFGjRvjkk09sVDoiIiKyd9UiIHr66afx2WefWSzT6/WYNGkSfvnlF+zbtw9vvfUWrly5YqMSEhERkT2rFgHRXXfdVWJGyl27dqFZs2YICQmBp6cn+vTpgx9//NFGJSQiIiJ7ZvOAaOvWrejfvz+Cg4OhKArWrFlTYpv4+HiEhYXB1dUV7du3x65du26434sXLyIkJER9HBISggsXLliz6ERERFRN2DwgysrKQqtWrRAfH1/q+hUrVmDSpEmYPn069u7di1atWqFXr164fPnyLS4pERERVVc2D4j69OmDWbNmYdCgQaWuf/vttzF27FjExsYiMjIS8+bNg7u7OxYtWnTd/QYHB1tkhC5cuIDg4OAyt8/Ly0N6errFjYiIiG4PNg+Iric/Px979uxBTEyMukyj0SAmJgY7duy47nPbtWuHv//+GxcuXEBmZibWr1+PXr16lbn97Nmz4ePjo95CQ0OtVg9yXGk5BTiWkIHE9FyIiK2LQ0REVcSur3afnJwMg8GAwMBAi+WBgYE4evSo+jgmJgZ//fUXsrKyUKdOHaxcuRIdO3bE//73P3Tv3h1GoxFTpkxBzZo1y3ytqVOnYtKkSerj9PR0uwmKDEbBd39dhIuTBlGhvqjt41rhq/haU77eiMOX0rH3nxQcvJCGfIMROq0GOmcNXLQauDgV3rTaa/edNNBZrCu5vc5JA0+dM7zdnODmrK3yOhqNgqTMPFxIzcGFlByLvxcL72fk6dXta7g7IyLIC02CvNEkyAtNanujcaAn3F3s+t+IiIjKoVp8k//888+lLh8wYAAGDBhQrn3odDrodDprFstqtp5IwjMr9quPa3npEBXqi6hQX7Sq44uWoT7wdnWuste/nJ6Lvf+mYO+/qWoQlKc3VtnrAYCzVoGPmzO8XZ3h7Wa6mR47wcd8X11mfmxa5+XqDK1GQb7eiEtppsDmfGGAczG1MPBJzcGl1FzkG25cD193Z6TnFCAluwB/nL6KP05fVdcpClDPz71EoFTXzx1aje2CViIiqhi7Doj8/f2h1WqRmJhosTwxMRFBQUFV8prx8fGIj4+HwWCokv3fjPScAgCAi1YDgwiSMvLw0+FE/HT42vsSXssDUaE1EBXqg1ahvmgS5A0Xp4q3iBYYjDhSmP3Z+28q9v6bgvMpOSW283V3xh2hvrijbg14uTohX2803Qymv3nF7+sNFuuLb2N+nJmnh8EoKDAIkjPzkZyZf1PvmafOCVn5etyolUujALV93BDs64oQXzeE1HBDiK974V9XBPu6wd3FCbkFBpy8nImjCRk4eind9DchA8mZeTh7JRtnr2Rj46Frn4ebsxaNAz3RJMjbFCzVNgVMfh4uJcqgNxiRU2BAToEBuflGZBfokZNf+LjAgJx8I7Lz9ab7hY/N62q4uyAiyBONAr1Qz88dTlq7bgUnIrJbdh0Qubi4oE2bNti0aRMGDhwIADAajdi0aROeeuqpKnnNuLg4xMXFIT09HT4+PlXyGhXlpDGd5KLq+mJJbDscupiG/edSsf9cKv46n4pzV3NwKikLp5Ky8PXe8wAAFycNmgV7W2SS6tV0L9EMlZyZh73/pGDPvynY908qDlxIRW6BZdZEUYCIQC/cUbcGWtf1Ret6NdDA36NKmrREBFn5BqTnFCAtp0D9m5ZTgPRcvbosPacA6blF1uWY1uUUmALZzMKmLldnDYJ93RDi64Y6NdzUoCfYx/Q3yNu1XEGEq7MWzUN80DzE8phIzszDsYQMHCkMko4lZOB4YgZyCgz463wa/jqfZrF9LS8dPFy0hYGNKcApMFinb5KLkwbhtTzRONATjQO9Cm+eCK3hDg2zVURE12XzgCgzMxMnT55UH585cwb79++Hn58f6tati0mTJmHUqFFo27Yt2rVrh7lz5yIrKwuxsbE2LPWtZW56MRgFbi5atA3zQ9swP3V9cmYeDpxPxf5zpkDpr3OpSMspwL5/U7Hv31R1O193Z7Sq44vmId64kJKDvf+m4t+r2SVez8fNGXfU9UXrujXQum4NtAr1gVcVNskVpSgKPHVO8NQ5IdjXrcLPz9cbkZ5rCph83Jzh5+FSpX2R/D118G+oQ+eG/uoyg1Fw9koWjl7KwLGEdBxJyMDRhHScu5qDpIw8JJWxL0UxZZbcXbRwddbCzVkLt8L77i6Fj521cC287+qsQUJaHo4nZuDE5QzkFpiye0cuWY6QdHPWomGAJxoFeiLCHCgFeSHYxn3RiIjsiSI2HjqzZcsWdO/evcTyUaNGYfHixQCADz74AG+99RYSEhIQFRWF9957D+3bt6+S8hRtMjt+/DjS0tLg7e1dJa9VXpuOJOLRJX+iVagvvo3rfMPtRQT/XMm2yCIdupiO/FL6/SgK0CjAUw1+WtfzRQN/T2YUqkBmnh4nEjNgMIop4HG5Fui4Omuhc9LcdIBiNArOpWTjeGImjidmFN4ycSops9TPHTA1KzYMMAVJjQI9UaeGO2p6uqCGuwtqerjAx825yo8Dg1GQnJmHi6k5SEjLxaW0XFxKy8GltFz1safOSW1ybFLY/BjkfWuDuTy9AaeTsize25x8A9xctPBw0cLNxQnuxe6bbpb33Vy08NBp4e5sun8zzdpEFWEwmk7xt1OfRnMLT0XP3zYPiOzVzb6hVWHLscsY/eluNAv2xg8T77ypfeTrjTiakI6/zqXi8KV0BHq7onXdGoiq61ulHbLJtvQGI/65mo0ThSfxY4kZOJGYgdNJWdAbr/+vr9UoqOFuyrL5ebigpocOfh4uqOFhCpj8zH89TfdruLvAuUjzo95gRFJmninISTUFOglpubiUnotLhQFQYkae+oVdET5uzqbgqLATe5MgU+bLQ1e5pLfeYMTZK9k4nmhq/jxx2fT37JXsmyrnjThrFbg5a+Gpc4KXqzM8XZ3g5eqkPvZydYKXzqlwuTM8dU7wdr322Lytq7PWKuURERiMAoMIRABnrcZmJ1IRQUp2wbURoEUGRuiNgiAfHWr7uCHQ2xW1fVwR5OOKIG/XSh8Dt4KIqH0ms/MMpr/5emTm6ZGVZ0BWvqkfYZ7egNwCI/L0BuQVGJGr/jUir8CAPL0RuYV/84osK/o8c5O8l85JHYhicXO3HKRS/Obt6uRwfRNv9vxt/0cOqX2IKvOF7OKkQcs6vmhZx9dKpSJH4KQ19SsKr+WJ3s2vLc/XG3H2Spaa7TiRmIGE9FxczcrH1cx8ZBR2bq9ox3ZvVyfU9NQht8CAxPRclOeQ1WoUBHrpEOTjitq+bqjtbTq51fZxQ5CPK9JzCnAkIR1HL5maHk8nZSEtpwA7z1zFzjNXLfZVr6Z7YaDkjaa1vRARVPqIP3NGzRT0ZKp9v04nZZU58tDb1UltbowI9IK3mxOy8019wbLzr53EsvMNyM7XF/41rS++znySKjAICgx6pOfqgbTccr/PxbloNWow5azVwFgY1BiMAqNRYBTAIFJiuUFM64xFgqCiFAXwLWx6NgfENT2vBcR+njr4FwmK/dxdyn3y1BuMSEjPtZzqIjUHF1JzcSElGxdTc9U+gRXh5epUGCC5IchbhyAfN4uAqbaPK3zcnMuVYTQHLrkFBvXzvHb/2uCH4seBKdDRIzPPtF1WXuGyfIP6tyoC7OvJyNMjI0+PC6klB8nciKfOSQ2aPHVaaBQFWo3ppigKtIrp/9i8XKNRoDXfVxRozOuLLddqgIfa10OYv0cV1LjiGBA5ACet6R+3oBxDxInKw8VJo3a8Lk2e3oDU7AJcyczH1ax8XMnKMwVLWfm4Uhg0Xc3OV5elZOdDBEjPLTy5F3LSKBa/4GsXBjrXHruhlpfuhlmI7k0CLMp26nIWjiZcG+139FI6Lmfk4Z8r2fintBF/QV5oEugFvVFwPDEDJy9nlnmydXfRolGgFxoHeCIiyEt9nwK9dVZrpiswGC0Cp6w8PTJyzbcCZOSaTqAZuQXIzDO9pxm5emRarNOrgwfyDUb1s7AmESAl2zTlxKmkrHI9x9fd+Vr20MMFNT11qOnhAoNRLLI8CeUMmGt56UyDIdRRoG5w0ipILGxSTUg3/U1MyzWd9HP1yMjNxPHEzDL36eqsQVBh4O2pKwxsC64FNdfu68tVxsowN6d66sx/neChK+w/6KSFrrA5XeesKXysgc7J1IdQ52RaZ25yV+8X28YoYjFApfiAFcubXl1nPr4yCwO6mwmmbqRHk0AGRPbKHofdOxXpVE10K+ictAj01iLQ27Vc2xuMgtTsawGTq7MWwT6uqOl542DnZsoWGeyNyGDLVPgV84i/BFNndvOov5wCA/4qHGxQlIuTBg1rmYKeoh3OQ3zdqrzvlLNWAx83DXzcKtdcbTQKMvPNwZIpgCowSJm/yDUaQKsUW64p/AVfZLmm8Nd/Tr5BDYivBcf5uFr4+EpWyaA4NbsAqdkFOF2OAMpFq0HtwikvgosFPSG+pgxhRZoDM3ILkFgYICWY+6GlX7tvzoLmFhjV6TLKy9y8ae4LZh4AYXnfFNh46Jzg4eJk+qvTWt4vHDRiDoRuVZOkv2fF59nTG4zq6F7zLTtPfy3LKAKDETAWyT4ajeZmVxTJQpqXW2YqjUZByE0Mnqkq7ENUBnvqQ7T/XCoGxm9DnRpu+P35HjYtC5EjMY/4O1aYSdIqCiKCTNMS1OW8TVZlMApSzEFxkcyi+b6i4Nq0F75uqOPrBn9P3S0fwGFuzjUHTTkFBnVkp7s60MHp2qCHwoDHmceKw2AfomrMnCHSW2m+GqLbhVajqH2o7m1R29bFqda0GsU0DYWnDgi88fa24uqsRb2aHqhX0z6aach+MOR1AOaU6o1GBREREdHNYUBUTHx8PCIjIxEdHW3roqicteaAiJ2qiYiIqgIDomLi4uJw+PBh7N6929ZFUWnNw+7ZZEZERFQl2IfIAZj6EAkKmCGi6kwEKMgGsq8A2VdNf3NSAEUDuNWwvOm8TJPkEBFZCQMiB6BLP4PduidQA5nA666Akw5wKvyr1Vk+Vv+WZ5uif0tZ5lxsudalepyERABDvunkm58NFOQABVmmv/mFfwuyi6zPBpzdAFdfwM238G+Na/ed3ezzfTEUAPmZpjrlZ1+7X5Bd+nJFc53P383y+HAu9ti83vw+iJj2qwY2V6/dt1h2BchOKfx7BTDkla9uGqeSQVJ5bs5uABRTXZUif2+WiOl9LsguctzkXLuvzy17XdG/UExlc3Y3/XVxv3bf4m8py1zcTe+/poyEvwhg1JuOeUOB5X1DAWAsKOV+fuF2xe6Xtm2Z2xW+jnk7RSnyXVPacVb8++g630VOrqb62+I7SZ9vOrbzMgr/Zhb+D2UC+jKO3zIHc18n669oAK1z4fe3i6muZd53KfyudzH9b9j6+0jEdOzrc4GC3Gv39bmm96ggx/RXnwOEdQU8atq2vIUYEDkA18v74a0UXrDTfKK2lTKDqWJfbOoJ063s5eqJtcjzjUbTSVGfZ/oytfibZ/oysvh7ne0sTjpFg59sQKw4z5TWpViw5Gs6+RZfZv6raApPSPnXTiT6/JLLStyKLDdvb65PaQGOscB6dSz3e1F4TOhzTOW7qX24AG5+gHtNwN0PEKMpU5STYgqmDHmmE29WkulmFUrhSaRYsGRert7XAAoKP0O99Y+lyjD/fynaIkFJYZBSbSnXvkOc3Qu/a4p85zi7Fa53K7bO7do6oGRwY/E4C8jPuLbuZo/rW0a5FhyZAyWNk+mY1WgLj2HzX40pkDbfNy9Xt1OKbastDHZySgY2+rxrwU95f9gAwOh1gMeNr9F5KzAgKsYeJ2bUaE0f015jQ7R6ehW0xvxrkbb6N6/kMkNpy/OvHbylPad4JK8vdjkB8zqk3fo3oiponABnj2K/ykv59a3PBXJTC0/MqYX3U00nQ0M+kHXZdLNHGmfAxQNw8TTVx8XDVGcX883d9Bi4/nFQkFvK8ZJjCljMDHmWX4ZaXWFgUxNwr2H6qwY7hQGPu59lAOTief1fuAU5psDIHCRd95Z67X7B9SYKlGu/4m82wFE0146lEpmdwhNxiWWFJ2tzvczBe/FsUn4py8wZKDP1f7M8ZdUWZh8KMwpaF9PjEveLblPsvsa5cFk57mucTO9x0WNHPZkWOZbKs74gB9cyK3Ltvci5ep0KVwGtDtB5mo5XnZfpb9Es6bU3u+RzSz2+iy0TgymwNf/YU3/wFRT5UVj4w7Do/yCkYsdCVVM0xX4YF/tB7OJu6xKqGBAVExcXh7i4OHViJ3ug0Zpms9VDC71PXWidrHMhx3IxNy+VGjgV/9LKLXxc5GRaUNb94r8sCp+naAvTwLpS/uosU8MWf0vZ3qIJwgMWzQ7m4EdbiZmCzc1CaoBULFgq/jcnxXQfMJXPfIIxn4DUujhbLlPv64otd75WFxfPwvulBD5OLpU5Am78Hhj1xY6HXNMXn7ufqUzWTt87uwE+IaZbRZiPUxGoAZBI4cmk8K+6zlhsnVw76YjRdDO//+bshNb51jdVGI2m/6GigZO5bGUGJ85lN685AnMzpVrvHMvvnrKWqc2XuYXPLfwrci2gsQhwPIos8yqyrvBvZb47rM3cPFk0e64GU3mm9ebjVgzX7hsN145t83JjkfVFb+blQMnMfmmBjtrVwo7epxtgQOQAtIUnNGcYoDcIbunFnBXlWp8ksqQopi9OnReAUFuXxjYU5doJV1f6ddHsRnU8jjWaa5m+24WimIJ8JxfA1T5+tNqc1sl0g/1kWxyRA/9MuH1onEwRkBYGTs5IRERUBRgQOQCt9lqGiBd4JSIisj4GRA7AIkNk4FxERERE1saAyBFoTJ3SnNhkRkREVCUYEBVjj9cyMw1ZLQyIePkOIiIiq2NAVIw9XssMhfMQOSkGXuCViIioCjAgcgRqhsjIJjMiIqIqwIDIEah9iPRsMiMiIqoCDIgcQZEMEYfdExERWR8DIkegvdapuoB9iIiIiKyOAZEj0Fybh4gZIiIiIutjQOQICvsQOcOAAk7MSEREZHUMiIqxy3mICq8WrFEEBoPBxoUhIiKqfhgQFWOX8xBptOpdgz7fhgUhIiKqnhgQOYLCJjMAMOj1NiwIERFR9cSAyBEUdqoGAGGGiIiIyOoYEDmCIgERM0RERETWx4DIEWg0MBZ+VEZDgY0LQ0REVP0wIHIQBpg6VrNTNRERkfUxIHIQBsUUEAkzRERERFbHgMhBGBRTPyKjgX2IiIiIrI0BkYMwmjNEbDIjIiKyOgZEDsLIDBEREVGVYUBUjF1eugOAEeY+RAyIiIiIrI0BUTF2eekOXMsQsVM1ERGR9TEgchBGDTNEREREVYUBkYNQM0RGdqomIiKyNgZEDkLUJjNmiIiIiKyNAZGDEPP1zNiHiIiIyOoYEDkIdR4iIzNERERE1saAyEGIxtl0h01mREREVseAyEEIM0RERERVhgGRgzD3IVLYh4iIiMjqGBA5Co152D0zRERERNbGgMhBqBkiBkRERERWx4DIUZiH3TMgIiIisrpqHRD997//RbNmzdC8eXN8/vnnti5OpajzEBnZh4iIiMjanGxdgKpy8OBBfPnll9izZw9EBN27d0e/fv3g6+tr66LdHLXJzGDjghAREVU/1TZDdOTIEXTs2BGurq5wc3NDq1atsGHDBlsX6+YVzkPEPkRERETWZ7cB0datW9G/f38EBwdDURSsWbOmxDbx8fEICwuDq6sr2rdvj127dqnrmjdvji1btiA1NRUpKSnYsmULLly4cAtrYGXmDJEwICIiIrI2uw2IsrKy0KpVK8THx5e6fsWKFZg0aRKmT5+OvXv3olWrVujVqxcuX74MAIiMjMTEiRPRo0cPDB48GB06dIBWq72VVbAubeFM1cwQERERWZ3dBkR9+vTBrFmzMGjQoFLXv/322xg7dixiY2MRGRmJefPmwd3dHYsWLVK3GT9+PPbu3YvNmzfD2dkZjRo1KvP18vLykJ6ebnGzKxpTMKdhhoiIiMjq7DYgup78/Hzs2bMHMTEx6jKNRoOYmBjs2LFDXWbOFh07dgy7du1Cr169ytzn7Nmz4ePjo95CQ0OrrgI3QdGyDxEREVFVcchRZsnJyTAYDAgMDLRYHhgYiKNHj6qP77vvPqSlpcHDwwOffvopnJzKru7UqVMxadIk9XF6erp9BUXsVE1ERFRlHDIgKq+i2aIb0el00Ol0VViaytFoTR+VRjjsnoiIyNocssnM398fWq0WiYmJFssTExMRFBRUqX3Hx8cjMjIS0dHRldqP1ZmbzNiHiIiIyOocMiBycXFBmzZtsGnTJnWZ0WjEpk2b0LFjx0rtOy4uDocPH8bu3bsrW0yrUgozRFoGRERERFZnt01mmZmZOHnypPr4zJkz2L9/P/z8/FC3bl1MmjQJo0aNQtu2bdGuXTvMnTsXWVlZiI2NtWGpq861TtVsMiMiIrI2uw2I/vzzT3Tv3l19bO7wPGrUKCxevBgjRoxAUlISXnnlFSQkJCAqKgobNmwo0dG6ouLj4xEfHw+Dwb4CD01hQKQFM0RERETWpoiI2LoQ9ig9PR0+Pj5IS0uDt7e3rYuDhJ/fQ9Dv0/Cj0gk9p6+3dXGIiIjs0s2evx2yD9HtSKNhHyIiIqKqwoDIQShOhU1mDIiIiIisjgFRMfY67N7ch0gD++rbREREVB0wICrGXofda9QMEQMiIiIia2NA5CC06igzA9gPnoiIyLoYEDkIc5OZEwwwMh4iIiKyKgZExdhtH6LCC9M6wQC90Wjj0hAREVUvDIiKsdc+RFonFwCFAZGBKSIiIiJrYkDkIDRF+hDp2WZGRERkVQyIHIS2cJSZMwzQG9hkRkREZE0MiByEOuweBhiYISIiIrIqBkTF2GunamgKM0SKAQUMiIiIiKyKAVEx9tqpGhotAEALIwzsVE1ERGRVDIgchToPkZ7D7omIiKyMAZGj0JjnITJylBkREZGVMSByFJoiEzOyyYyIiMiqGBA5Cg1nqiYiIqoqDIgcRZFrmbHJjIiIyLoYEBVjv8PuTRkiZ8UAAydmJCIisioGRMXY77B7J/Vugb7AhgUhIiKqfhgQOYoiAZHRoLdhQYiIiKofBkSOorAPEQAYC/JtWBAiIqLqhwGRoyiSITIwQ0RERGRVDIgcRdEmM2aIiIiIrIoBkaNQFBgKPy5miIiIiKyLAZEDMcCUJRIDM0RERETWxICoGLudhwiAQTEFRAYOuyciIrIqBkTF2O08RACMihYAIAYGRERERNbEgMiBmDNERgZEREREVsWAyIEYUZgh0rNTNRERkTUxIHIgBjaZERERVQkGRA5EzE1mRmaIiIiIrIkBkQMxmidnZIaIiIjIqhgQORDzKDN2qiYiIrIuBkQOxKiYM0RsMiMiIrImBkQORNQMEQMiIiIia2JA5ECksA+RYuSlO4iIiKyJAVEx9nzpDvMoMzEabFwSIiKi6oUBUTF2fekOjjIjIiKqEgyIHInGnCFiHyIiIiJrYkDkQIQZIiIioirBgMiBmPsQKexDREREZFUMiByImiEyMkNERERkTQyIHIiiBkTsQ0RERGRNDIgciGjNTWYMiIiIiKyJAZEjUZghIiIiqgoMiByJ1hkAoAgDIiIiImtiQORINGwyIyIiqgoMiByJOUPEgIiIiMiqGBA5EEVjutq9RjgPERERkTVV64DonXfeQbNmzRAZGYmJEydCRGxdpEpRmCEiIiKqEtU2IEpKSsIHH3yAPXv24ODBg9izZw/++OMPWxercsx9iNipmoiIyKqcbF2AqqTX65GbmwsAKCgoQEBAgI1LVDnmDJGGAREREZFV2W2GaOvWrejfvz+Cg4OhKArWrFlTYpv4+HiEhYXB1dUV7du3x65du9R1tWrVwuTJk1G3bl0EBwcjJiYG4eHht7AG1qcUTszIPkRERETWZbcBUVZWFlq1aoX4+PhS169YsQKTJk3C9OnTsXfvXrRq1Qq9evXC5cuXAQApKSn4/vvvcfbsWVy4cAHbt2/H1q1bb2UVrI4ZIiIioqphtwFRnz59MGvWLAwaNKjU9W+//TbGjh2L2NhYREZGYt68eXB3d8eiRYsAAD///DMaNmwIPz8/uLm5oW/fvtftQ5SXl4f09HSLm73RqJ2qmSEiIiKyJrsNiK4nPz8fe/bsQUxMjLpMo9EgJiYGO3bsAACEhoZi+/btyM3NhcFgwJYtWxAREVHmPmfPng0fHx/1FhoaWuX1qChzhkjLDBEREZFVOWRAlJycDIPBgMDAQIvlgYGBSEhIAAB06NAB9957L+644w60bNkS4eHhGDBgQJn7nDp1KtLS0tTbuXPnqrQON0PjZOpDpAUDIiIiImuq1qPMXn/9dbz++uvl2lan00Gn01VxiSrnWh8iNpkRERFZk0NmiPz9/aHVapGYmGixPDExEUFBQZXad3x8PCIjIxEdHV2p/VQFjdYFAKBlQERERGRVDhkQubi4oE2bNti0aZO6zGg0YtOmTejYsWOl9h0XF4fDhw9j9+7dlS2m1WnMw+7ZZEZERGRVdttklpmZiZMnT6qPz5w5g/3798PPzw9169bFpEmTMGrUKLRt2xbt2rXD3LlzkZWVhdjYWBuWumppnExNZk7MEBEREVmV3QZEf/75J7p3764+njRpEgBg1KhRWLx4MUaMGIGkpCS88sorSEhIQFRUFDZs2FCio3VFxcfHIz4+HgaD/QUd5mH3GhggIlAUxcYlIiIiqh4UcfQrnlaR9PR0+Pj4IC0tDd7e3rYuDgAg48D38Fr9EP4yNkDzGXuh1TAgIiIiKupmz98O2YfodqU1N5nBgAKD0calISIiqj4YEDkQrZNplJkTDDAYmdgjIiKyFgZExdj3sHtTly8nGKBnQERERGQ1DIiKsedh90UzRHo2mREREVkNAyIHol66QzGyyYyIiMiKGBA5Eo2pU7Uz9ChgQERERGQ1DIiKsec+RNCYL+5qhMHAgIiIiMhaGBAVY899iKAtmiFiHyIiIiJrYUDkSDRaAIUZIjaZERERWQ0DIkei9iEyQM8mMyIiIqthQORI1D5EBujZZEZERGQ1DIiKsetO1YV9iJwUI+chIiIisiIGRMXYdafqwj5EAGDQF9iwIERERNULAyJHUtiHCAAM+nwbFoSIiKh6YUDkSAr7EAGA0cAMERERkbUwIHIk2iIZogIGRERERNbCgMiRKNc+LqNBb8OCEBERVS8MiIqx61FmigI9TM1mxgL2ISIiIrIWBkTF2PUoMwAGxTTSzMAMERERkdUwIHIwBpgCIqOBGSIiIiJrYUDkYIyFGSIj5yEiIiKyGgZEDsagmPoQCYfdExERWQ0DIgdjVJvM2IeIiIjIWioUEL355pvIyclRH2/btg15eXnq44yMDDz55JPWKx2VwAwRERGR9VUoIJo6dSoyMjLUx3369MGFCxfUx9nZ2fj444+tVzoqQQr7EDEgIiIisp4KBUQict3H1YFdz0OEohkiNpkRERFZC/sQFWPv8xAZNWwyIyIisjYGRA6GTWZERETW53TjTSx98skn8PT0BADo9XosXrwY/v7+AGDRv4iqhrGwyQxGNpkRERFZS4UCorp162LBggXq46CgICxdurTENlR1pLDJjMPuiYiIrKdCAdHZs2erqBhUXmLOELHJjIiIyGrYh8jBmDNEwiYzIiIiq6lQQLRjxw58//33Fss+++wz1K9fHwEBARg3bpzFRI1kfeaASGGGiIiIyGoqFBDNnDkThw4dUh8fPHgQjz76KGJiYvDCCy/gu+++w+zZs61eSLpGHWVmNNi4JERERNVHhQKi/fv34+6771YfL1++HO3bt8eCBQswadIkvPfee/jqq6+sXki6RjTOAADFyAwRERGRtVQoIEpJSUFgYKD6+Ndff0WfPn3Ux9HR0Th37pz1SkclaUwZIg67JyIisp4KBUSBgYE4c+YMACA/Px979+5Fhw4d1PUZGRlwdna2bgnJgjlDxICIiIjIeioUEN1777144YUX8Ntvv2Hq1Klwd3fHnXfeqa4/cOAAwsPDrV7IW8ner2UGc6dqBkRERERWU6GA6LXXXoOTkxO6deuGBQsWYP78+XBxcVHXL1q0CD179rR6IW8le7+WGQMiIiIi66vQxIz+/v7YunUr0tLS4OnpCa1Wa7F+5cqV8PLysmoBqZjCgAjCgIiIiMhaKhQQjRkzplzbLVq06KYKQ+WgNY8yY0BERERkLRUKiBYvXox69erhjjvugIhUVZnoethkRkREZHUVCoieeOIJLFu2DGfOnEFsbCxGjhwJPz+/qioblUIxZ4jYZEZERGQ1FepUHR8fj0uXLmHKlCn47rvvEBoaiuHDh2Pjxo3MGN0qzBARERFZXYUv7qrT6fDAAw/gp59+wuHDh9GsWTM8+eSTCAsLQ2ZmZlWUkYpQtKaASMMMERERkdVU6mr3Go0GiqJARGAw8Npat4KiMQdEfL+JiIispcIBUV5eHpYtW4Z77rkHjRs3xsGDB/HBBx/g33//haenZ1WUkYpQnEzzPmnYZEZERGQ1FepU/eSTT2L58uUIDQ3FmDFjsGzZMvj7+1dV2agU1zJEDIiIiIispUIB0bx581C3bl00aNAAv/76K3799ddSt1u9erVVCkclKU6FnarZZEZERGQ1FQqIHnnkESiKUlVloXJQCi/uqmWGiIiIyGoqPDGjozh27BhGjBhh8XjZsmUYOHCg7QplBRonU0DETtVERETWU6GAyJFERERg//79AIDMzEyEhYXhnnvusW2hrEBTOOxeC2aIiIiIrKVSw+4dxdq1a3H33XfDw8PD1kWpNPNM1cwQERERWY/dBkRbt25F//79ERwcDEVRsGbNmhLbxMfHIywsDK6urmjfvj127dpV6r6++uori+YzR6bVmvsQMSAiIiKyFrsNiLKystCqVSvEx8eXun7FihWYNGkSpk+fjr1796JVq1bo1asXLl++bLFdeno6tm/fjnvvvfdWFLvKKeY+RGwyIyIishq77UPUp08f9OnTp8z1b7/9NsaOHYvY2FgApikBfvjhByxatAgvvPCCut23336Lnj17wtXV9bqvl5eXh7y8PPVxenp6JWtQNcydqp2YISIiIrIau80QXU9+fj727NmDmJgYdZlGo0FMTAx27NhhsW15m8tmz54NHx8f9RYaGmr1cluDVls4UzUMvKAuERGRlThkQJScnAyDwYDAwECL5YGBgUhISFAfp6WlYdeuXejVq9cN9zl16lSkpaWpt3Pnzlm93NZgHmXmDAOMjIeIiIiswm6bzKzBx8cHiYmJ5dpWp9NBp9NVcYkqT+tc2KkaBhQYjNBqtDYuERERkeNzyAyRv78/tFptiWAnMTERQUFBldp3fHw8IiMjER0dXan9VBWtkyloc4IBBqaIiIiIrMIhAyIXFxe0adMGmzZtUpcZjUZs2rQJHTt2rNS+4+LicPjwYezevbuyxawS2sJrmTnBAL2BAREREZE12G2TWWZmJk6ePKk+PnPmDPbv3w8/Pz/UrVsXkyZNwqhRo9C2bVu0a9cOc+fORVZWljrqrLrSFHaqdlIM0BuNNi4NERFR9WC3AdGff/6J7t27q48nTZoEABg1ahQWL16MESNGICkpCa+88goSEhIQFRWFDRs2lOhoXVHx8fGIj4+HwWCfw9o1aobIyCYzIiIiK1GEY7dLlZ6eDh8fH6SlpcHb29vWxbnm8lHgw/a4Kp7IefYkQnzdbF0iIiIiu3Gz52+H7EN0W9MUyRCxDxEREZFVMCByNNprnaoL2IeIiIjIKhgQFWPvw+7NGSIth90TERFZDQOiYux92D00pokZnQsnZiQiIqLKY0DkaAozRBpF7HYkHBERkaNhQORotNdmSjDo821YECIiouqDAVExjtKHCAAMBXobFoSIiKj6YEBUjKP0IQIAg4EZIiIiImtgQORoLDJEBTYsCBERUfXBgMjRaDQwFH5sYmBAREREZA0MiByQsfBjM+oZEBEREVkDA6Ji7L5TNQCDYmo2MzAgIiIisgoGRMXYfadqAAZoAQBGNpkRERFZBQMiB2TOEDEgIiIisg4GRA7IqBRmiNhkRkREZBUMiByQuclMDJyYkYiIyBoYEDkgI5vMiIiIrIoBUTGOMMrMHBBxHiIiIiLrYEBUjCOMMjP3IWJAREREZB0MiByQmiEysg8RERGRNTAgckCiKcwQcZQZERGRVTAgckDMEBEREVkXAyIHJIV9iMA+RERERFbBgMgBGTXOAJghIiIishYGRI6IGSIiIiKrYkBUjEPMQ6RhHyIiIiJrYkBUjCPMQySFTWYKL91BRERkFQyIHJG5yYwZIiIiIqtgQOSARGvKEMHIPkRERETWwIDIEakZIoNty0FERFRNMCByQGofImaIiIiIrIIBkSPSmkaZQdiHiIiIyBoYEDmiwmH3HGVGRERkHQyIHJE5IBL2ISIiIrIGBkSOiH2IiIiIrIoBkQNSCvsQKRxlRkREZBUMiByROUPETtVERERWwYCoGEe4lpk5Q6RhkxkREZFVMCAqxhGuZWYedq9hp2oiIiKrYEDkgBR1lBmbzIiIiKyBAZED0ji5mP4yQ0RERGQVDIgckcbcZMYMERERkTUwIHJAGifTKDNmiIiIiKyDAZEDUkeZMUNERERkFQyIHJBGywwRERGRNTEgckDmgEgLZoiIiIisgQGRA2IfIiIiIutiQOSAzBkiJwZEREREVsGAyAEp5j5EbDIjIiKyCgZEDkjrxAwRERGRNVXrgOjMmTPo3r07IiMj0aJFC2RlZdm6SFah9iGCASJi49IQERE5PidbF6AqjR49GrNmzcKdd96Jq1evQqfT2bpIVqEtbDJzhgEGo8BJq9i4RERERI6t2gZEhw4dgrOzM+68804AgJ+fn41LZD1aZ/OwewP0RoGT1sYFIiIicnB222S2detW9O/fH8HBwVAUBWvWrCmxTXx8PMLCwuDq6or27dtj165d6roTJ07A09MT/fv3R+vWrfF///d/t7D0Vcs8ysxZMWWIiIiIqHLsNiDKyspCq1atEB8fX+r6FStWYNKkSZg+fTr27t2LVq1aoVevXrh8+TIAQK/X47fffsOHH36IHTt24KeffsJPP/10K6tQZbTOpqvda2GE3sCAiIiIqLLsNiDq06cPZs2ahUGDBpW6/u2338bYsWMRGxuLyMhIzJs3D+7u7li0aBEAICQkBG3btkVoaCh0Oh3uvfde7N+/v8zXy8vLQ3p6usXNXmkLr2XmBD30RqONS0NEROT47DYgup78/Hzs2bMHMTEx6jKNRoOYmBjs2LEDABAdHY3Lly8jJSUFRqMRW7duRdOmTcvc5+zZs+Hj46PeQkNDq7weN0vjZMoQOcEIPZvMiIiIKs0hA6Lk5GQYDAYEBgZaLA8MDERCQgIAwMnJCf/3f/+Hrl27omXLlmjUqBH69etX5j6nTp2KtLQ09Xbu3LkqrUOlaMwZIgMDIiIiIiuotqPMAFOzW58+fcq1rU6nc5xh+UUDIgObzIiIiCrLITNE/v7+0Gq1SExMtFiemJiIoKCgSu07Pj4ekZGRiI6OrtR+qpT5WmbMEBEREVmFQwZELi4uaNOmDTZt2qQuMxqN2LRpEzp27FipfcfFxeHw4cPYvXt3ZYtZdQozRM6KAQZmiIiIiCrNbpvMMjMzcfLkSfXxmTNnsH//fvj5+aFu3bqYNGkSRo0ahbZt26Jdu3aYO3cusrKyEBsba8NS3yKaax9bgb7AhgUhIiKqHuw2IPrzzz/RvXt39fGkSZMAAKNGjcLixYsxYsQIJCUl4ZVXXkFCQgKioqKwYcOGEh2tKyo+Ph7x8fEwGOz4wqlFAiIjAyIiIqJKU4RXBy1Veno6fHx8kJaWBm9vb1sXx1JBDvC6qa/UvpF/446G9jtFABER0a10s+dvh+xDdNuzyBDpbVgQIiKi6oEBkSOyCIjybFgQIiKi6oEBUTEOMexeUWAo/OgM7ENERERUaQyIinGIYfcA9IX94Y0GNpkRERFVFgMiB2VUtAAAgz7fxiUhIiJyfAyIHJQBpoBI2KmaiIio0hgQFeMQfYgAGMwZIgP7EBEREVUWA6JiHKUPkUEx9SESBkRERESVxoDIQRkLm8w4UzUREVHlMSByUEZzhsjIPkRERESVxYDIQZlHmRnZZEZERFRpDIiKcZxO1YUZIjaZERERVRoDomIcpVO1FGaIYGRAREREVFkMiByUuQ8RZ6omIiKqPAZEDkrMF3hlHyIiIqJKY0DkoDjKjIiIyHoYEDkocx8idqomIiKqPAZExTjKKDOjucnMaLBtQYiIiKoBBkTFOMooMyjmPkS82j0REVFlMSByUOZO1cIMERERUaUxIHJQ5oBI4TxERERElcaAyEGpw+45yoyIiKjSGBA5KgZEREREVsOAyEExQ0RERGQ9DIgcldqHiAERERFRZTEgKsZR5iFSAyJhQERERFRZDIiKcZh5iDTOpr/MEBEREVUaAyJHpWWTGRERkbUwIHJUhRkiBkRERESVx4DIFvKzK70LpTBDpGEfIiIiokpjQHQrGQ3IXfsf5L3RCMYrZyq3L2aIiIiIrIYB0S1UIAr2798DnSETZ374b6X2xQwRERGR9TAguoWctRpcbvYoAKD26VXIzbh60/tStKYMkUZ4cVciIqLKYkB0i/Xs/wBOKXXhjlzs/WbuTe9H4SgzIiIiq2FAdIu5ujghrdU4AED9U58jKTXzpvbDDBEREZH1MCCygah7xyJF8UVt5Qo2ff3xTe1D7UMEZoiIiIgqiwGRDWhcXJEVNQYA0Oyfz3DsUnrF98EMERERkdUwICrmVl3LrE7MU8hXXNBCcxarvvmqws83N5lpOcqMiIio0pxsXQB7ExcXh7i4OKSnp8PHx6fqXsijJvKajYDL30sRfelLbDk2AHdFBJT76cwQERGVT2pqKlJSUiAiti4KWYGiKKhRowZ8fX2tul8GRDbk1W0i8PdSxGj2Ysx3m9Cl4Qg4acuXtNM4mT46ZoiIiMqWkJCA3NxceHh4QFEUWxeHrEBEkJqaitzcXAQFBVltv2wys6VajVEQ3hMaRdAj9Wus+PNcuZ+qZojADBERUVmys7MZDFUziqLAw8MD2dmVvwxWUQyIbMy5ywQAwDDtr/hk4x5k5BaU63nmgMiJTWZERESVxoDI1sLuhAS2gJuSjz55G/DRllPleprGiRkiIiIia2FAZGuKAqWTKUs0ymkjlvx+AudTbpwG1DqZM0TsQ0RERNfXpk0bfPxx+ee927ZtGwICApCWllaFpbIvDIjsQbNBEK/aCFRS0cv4O97ccOyGT9E4uQAAtDBw5AQRUTUREBBw3dubb755U/vduHEjHn744XJvHx0djYMHD8Lb2/umXs8RcZSZPXBygdJuHLDpVTzmtB73/nUnYjuH4Y66Ncp8iqZwpmonGGAwCpy07DBIROToDh48qN7/9ttv8cYbb2D79u3qMg8PD/W+iMBgMMDJ6cancn9//wqVw8XFBYGBgRV6jqNjhshetBkNOLsjUvMPOmoOY9YPR66b+dE6F07MqBihNzJDRERUHQQGBqo3b29vKIqiPj558iQaNGiATZs2ISYmBnXq1MHOnTtx5swZPPLII4iMjERYWBh69uyJX3/91WK/xZvMAgIC8Pnnn2PUqFGoV68e2rdvjw0bNqjrizeZLV++HA0bNsQvv/yCzp07IywsDCNGjEBiYqL6HL1ejxdffBENGzZEREQEZs6ciaeeegqPPPJIFb9r1sGAyF64+wF3jAQAPO68Dnv+ScH6vxPK3FyrNTWZOUPPgIiIqBxEBDkFBpvcrNm14bXXXsPLL7+M33//HZGRkcjKysLdd9+Nr7/+Gr/88gt69OiBhx9+GOfPn7/ufv773//ivvvuw+bNmxETE4MnnngCKSkpZW6fk5ODDz/8EPHx8Vi7di0uXLiA6dOnq+vff/99fP3113j33Xfx/fffIyMjA+vXr7davasam8zsSfvHgV0L0E3Zh3DlAmavd8PdTQOgc9KW2FTNEMEIg4EBERHRjeTqjej63h6bvPbWiW3g5lzyu/xmPP/887jrrrvUxzVq1EDz5s3Vxy+88ALWrVuHjRs34tFHHy1zP/fffz8GDx4MAHjxxRexYMEC7Nu3Dz169Ch1+4KCArz11luoX78+AGDMmDH43//+p67/5JNPMHHiRPTt2xcAMGfOHGzatOmm63mrMUNkT2qGA01MB9JTbj/i3NUcLNl+ttRNnQpHmZkyRMZbVUIiIrKxqKgoi8eZmZmYPn06OnfujIYNGyIsLAzHjx+/YYYoMjJSve/h4QEvLy8kJSWVub27u7saDAGm5r3k5GQAQHp6OpKSktC6dWt1vVarRcuWLStSNZuq1hmisLAweHt7Q6PRoEaNGti8ebOti3RjHeOAo99jALbiNQzB+7+cxNA2ofDzcLHYTL24K4zIZZMZEdENuTppsHViG5u9trW4u7tbPJ4xYwZ+/fVXzJgxA/Xr14erqyseffRRFBRcf6Lf4p2xFUW5btNeRbd3NNU6IAKA7du3w9PT09bFKL+6HYHg1tBe3ItJNX7Dyyl98e7Px/Hqfc0tt9OYPjpnGNiHiIioHBRFsVqzlT3ZvXs37r//frWpKjMzE+fOlf9SUNbg7e2NWrVqYd++fejYsSMAwGAw4ODBg2jWrNktLcvNYpOZvVEUU5YIwHDZAB3y8fnOf3HycqbldhpzhsgAvYFNZkREt6v69evjhx9+wMGDB/H333/jiSeegNEGXSkee+wxvPfee1i/fj1OnjyJl156CampqQ5zHTm7DYi2bt2K/v37Izg4GIqiYM2aNSW2iY+PR1hYGFxdXdG+fXvs2rXLYr2iKOjWrRuio6PxxRdf3KKSW0HkfYB3HbjkXsELdf6GwSiYs/6I5TaFGSInxciAiIjoNjZz5kz4+vqiX79+ePjhh3HXXXfZpO/OhAkTMGjQIDz11FO499574eHhge7du8PV1fWWl+VmKGKnDYDr16/Htm3b0KZNGwwePBjffPMNBg4cqK5fsWIFHnnkEcybNw/t27fH3LlzsXLlShw7dgwBAQEAgAsXLiAkJASXLl1CTEwMli1bVu6DJD09HT4+PkhLS7PNTJ3b3wd+fBn5fhGITJgOvRH44rH26NywcHKtnBTgjTAAwLGxZxAR4nfry0hEZOdOnz4NLy8vWxfjtmQ0GtG5c2fcd999eOGFF6y+/4yMDDRo0KDE8ps9f9tthqhPnz6YNWsWBg0aVOr6t99+G2PHjkVsbCwiIyMxb948uLu7Y9GiReo2ISEhAIDatWvj3nvvxd69e8t8vby8PKSnp1vcbKr1I4CLJ1yuHsP0SNN8RLN+OAKDub+Q5lr3L4Mh3xYlJCIiUp07dw5Lly7FqVOncPjwYTz33HP4999/1aH99s5uA6Lryc/Px549exATE6Mu02g0iImJwY4dOwAAWVlZyMjIAGDqYPbLL79ct2PX7Nmz4ePjo95CQ0OrthI34upjCooA3K9fC29XJxy5lI6v9xYOoyzsQwQARj0DIiIisi2NRoPly5ejZ8+e6NevH44cOYJVq1ahcePGti5auThkQJScnAyDwVDiOiuBgYFISDBlUxITE9GlSxe0atUKHTp0wCOPPILo6Ogy9zl16lSkpaWpt1vdQ79U7ccDigbOZ7dgentTp7T/bjyGrDy9RYZIX8Ar3hMRkW2FhITghx9+wKlTp3D69GmsW7dOHXHmCKrtsPsGDRrgr7/+Kvf2Op0OOp2uCkt0E2qEAU0HAIfXYGDuGrzrNwz/Xs3Gx1tPY1JMI3UzMVx/rgkiIiK6PofMEPn7+0Or1VpcVA4wZYWCgoIqte/4+HhERkZeN5t0S3V8CgCg/XslpnevCQCYv/UUEtLzUFAYzxoK2GRGRERUGQ4ZELm4uKBNmzYW10gxGo3YtGlTpdNzcXFxOHz4MHbv3l3ZYlpHaDRQpx1gyEePjLWIDquB3AIj3tp4DAaYJhgzMkNERERUKXYbEGVmZmL//v3Yv38/AODMmTPYv38//v33XwDApEmTsGDBAixZsgRHjhzBE088gaysLMTGxtqw1FWkcKJGZfdCTOtlGmL49d7zKCj8+IwG9iEiIiKqDLvtQ/Tnn3+ie/fu6uNJkyYBAEaNGoXFixdjxIgRSEpKwiuvvIKEhARERUVhw4YNJTpaV1R8fDzi4+NhMBgqtR+ratof8K0HpP6DllfWY2BUFNbsvwi9aAEFMHCUGRERUaXY7cSMtmbziRmL++MjYMMLQM1GuDDyV/T431b8rh2PWkoafo/5Fl263GXrEhIR2R1OzFh93TYTM1Ixd4wEdD7AlRMISfodj91ZH/rCPkQGPfsQERGRycCBA/Hyyy+rj9u0aYOPP/74us8JCAjAunXrKv3a1tqPLTAgchQ6L6DNKNP9HR/gibsawqiYr2dmR817RER000aOHIkRI0aUuu6PP/5AQEAADh06VKF9bty4EQ8//LA1iqd68803Lbq1mB08eBB33323VV/rVmFAVIzdDbsvqv14QNECZ7bC8+ph1PRyBwC0qeNp44IREZE1PPjgg/j1119x8eLFEuuWLVuGqKio6151oTT+/v5wd3e3VhGvKzAw0P7m9CsnBkTF2N2w+6J86gDNCq/ttiMeroUHnauG3cCIiKqDnj17ombNmli+fLnF8szMTKxduxZ9+vTB+PHj0bJlS9SrVw/dunXD6tWrr7vP4k1mp0+fxoABAxAaGoouXbpgy5YtJZ4zc+ZMdOjQAfXq1UPbtm0xZ84cFBSYumcsX74c//3vf3Ho0CEEBAQgICBALW/xJrPDhw9j8ODBqFu3LiIiIvCf//wHmZmZ6voJEybgkUceQXx8PJo3b46IiAg8//zz6mvdSnY7yozK0DEO+HuV6eZWeIV7I/sQERHdkAigz7HNazu5AYpy482cnDB8+HAsX74czz77LJTC53z33XcwGo0YOnQo1q5di6eeegpeXl74+eefERcXh7CwMLRu3fqG+zcajYiNjYW/vz/Wr1+PjIwMi/5GZp6ennjvvfcQFBSEI0eOYNKkSfDw8MCECRNw33334ciRI9i8eTNWrlwJAKV2Xs7KysKIESPQtm1bbNy4EcnJyXj22WcxdepUvP/+++p227ZtQ2BgIL755hucOXMG48aNQ/Pmza3ezHcjDIgcTUhroF5n4J9tQNZl0zIj+xAREd2QPge15re0yUsnjTsAOJev2erBBx9EfHw8tm/fjs6dOwMwNZf17dsXoaGhiIuLU7d97LHHsHnzZnz77bflCoh+/fVXnDhxAitWrFCv7PDSSy/h/vvvt9jOPNUNANStWxdPPvkk1qxZgwkTJsDNzQ0eHh7QarXXnepm9erVyMvLwwcffAAPDw8AwJw5czBy5EhMmzYNAQEBAABfX1/MmTMHWq0WjRo1QkxMDH777TcGRLZml/MQFdcxzhQQmXGmaiKiaqNRo0aIjo7Gl19+ic6dO+P06dP4448/8M0338BgMGDu3LlYu3YtLl26hPz8fOTn58PNza1c+z5x4gSCg4MtLnPVtm3bEtutWbMGCxYswNmzZ5GVlQWDwVDh6QuOHz+OZs2aqcEQALRr1w5GoxGnTp1SA6KIiAhotVp1m8DAQBw5cqRCr2UNDIiKiYuLQ1xcnDqPgV1q3BvwawBcPW16bORM1UREN+TkZsrU2Oi1K+Khhx7Ciy++iDfeeAPLly9HWFgYOnXqhPfffx8LFizAa6+9hqZNm8Ld3R3Tpk1Dfr71JujdvXs3nnjiCUyZMgXdu3eHt7c3vvnmG3z00UdWe42inJwsQxFFUWA0Gqvkta6HnaodkUYLdHjy2mP2ISIiujFFMTVb2eJWjv5DRQ0YMACKouDrr7/GV199hQcffBCKomDXrl3o3bs3hg0bhubNmyMsLAynTp0q934bNWqEixcvWlwc/c8//7TYZvfu3ahTpw6effZZREVFoUGDBjh//rzFNi4uLjcMWho3boxDhw4hKytLXbZr1y5oNBqEh4eXu8y3CgMiRxX1IODqa7pvg0iaiIiqjqenJwYOHIjXX38diYmJah+f+vXr49dff8WuXbtw/PhxTJ48GUlJSeXeb7du3RAeHo6nnnoKf//9N/744w/Mnj3bYpsGDRrgwoULaifnBQsWlJhsMTQ0FP/88w8OHjyIK1euIC8vr8RrDRkyBDqdDhMmTMCRI0fw+++/Y+rUqRg2bJjaXGZPGBA5KhcPoO//gHpdgPCSk2MREZFje/DBB5Gamoru3burfX4mTZqEFi1aYMSIERg4cCACAgLQp0+fcu9To9Fg8eLFyM3NRe/evdVRX0X17t0b48ePx9SpU9GjRw/s3r3bopM1APTr1w89evTA4MGD0bRpU3zzzTclXsvd3R0rVqxAamoqevXqhUcffRRdu3YtEYDZC17LrJiinaqPHz9uP9cyIyKiCuO1zKovXsusitn1xIxERERUJRgQERER0W2PARERERHd9hgQERER0W2PARERERHd9hgQFRMfH4/IyEhER0fbuihERFRJSgUnRCTHYe3PlsPuy3Czw/aIiMh+nD17Fm5ubtBo+Pu/OjEajcjJyUFYWFiJdRx2T0REVExgYCAyMjJscm0sqhpGoxEZGRkIDAy06n55cVciIqq23NzcEBISgsTERIgI2Cji2BRFgaIoCAkJgZtbxS6YeyMMiIiIqFpzc3MrtWmFqCg2mREREdFtjwERERER3fYYEBEREdFtjwFRMZyHiIiI6PbDeYjKkJaWBl9fX5w7d47zEBERETmI9PR0hIaGIjU1FT4+PuV+HkeZlSEjIwMAEBoaauOSEBERUUVlZGRUKCBihqgMRqMRFy9ehJeXl1WnBzdHrtU581Td68j6Ob7qXkfWz/FV9zpWZf1EBBkZGQgODq7QDOXMEJVBo9GgTp06VbZ/b2/vanmQF1Xd68j6Ob7qXkfWz/FV9zpWVf0qkhkyY6dqIiIiuu0xICIiIqLbHgOiW0yn02H69OnQ6XS2LkqVqe51ZP0cX3WvI+vn+Kp7He2xfuxUTURERLc9ZoiIiIjotseAiIiIiG57DIiIiIjotseAiIiIiG57DIiIyKquXr2q3r9dxmzo9XoAphnuyXHdLscrlY4B0S1iMBjw7rvvYtWqVeqXJ10zf/58PPLII3jnnXdw4cIFWxenSuzcuRPJycm2LkaVmT9/Ptq0aYOhQ4ciNjYWWVlZVr3sjT06f/48nn76acTExABAhS4T4AjOnz+P2bNnY9u2bbYuSpU5d+4cPv30UwColsdrUlISpk2bhq1bt9q6KFUiISEB3333HS5evFj5nQlVKaPRKGvXrpVWrVqJoijSoUMHuXjxoq2LZTdSU1PlnnvukbCwMJk4caLUqlVLunXrJmvXrhUREb1eb+MSVt57770nAQEB0qxZM2nYsKG88cYb6jFgMBhsXLrKy8rKkuHDh0tERITMmzdP5s6dK4GBgTJ27FgRMf0PVDf79u2Tnj17ilarFa1WK+3atZMrV67YulhWc/nyZXnkkUdEp9PJPffcIxs3bpTMzExbF8uq9u7dKz179hRFUeSJJ56QnJwcWxepSowaNUoURZGJEydKenq6rYtjNUeOHJH+/ftLjRo1pFWrVhIUFCRvvPFGpfbJgKiK5ebmyuuvvy6TJ0+WH3/8UbRaraxcudLWxbKZ4ifHlStXSkREhBw9elRERI4ePSoPPfSQhIeH26J4Vrdq1Spp27atLF++XJKSkmTu3LnSqVMnGTlypK2LZjXr16+XRo0ayY4dO9Rls2fPlsjISBuWqmqcPn1aunfvLoqiyCOPPCKpqamyePFiCQ0NtXXRrOrdd9+Vbt26yd9//23roljd+fPnZciQIaIoijz22GPqd091YzAYJC8vTyZNmiT333+/+Pj4yO7du21dLKswJxkeeeQROX78uJw6dUpmz54tHh4ekpube9P7ZUB0C/z9999y7tw5ERHp2bOnxMTESHJyso1LdeulpKRIWlqaxbL//e9/0qBBA4tle/bsEX9/f/nvf/8rIo6VRcnIyFCDvpycHBk2bJgMGjTIYpvJkyeLoijy66+/iohjZVAuXbok+/btk9TUVHXZihUrRKfTWfz6HDp0qCxdulR97Eh1LI25/Hv37pW5c+dKYmKius4cEJ08edJWxbMK8/9ZQkKChIeHy/fffy8iIr/99pssW7ZMjh07pp5sHPHzvHz5soiI/P777xIZGSnjxo1T1508edLiROqI9StNUlKS1K1bVwwGg9SpU0eeffZZ9f/Uket47NgxmTdvnkXW8ueff5amTZvKpUuXbnq/1avB2041a9YMderUAQC89tpr2LRpE3bt2mXjUt06Bw4cwIABA9ClSxcMHToUM2bMUNdlZWUhODgYx48fV5c1b94csbGxePvttwE4Rr+Mffv2oVOnTrj77ruRkpICAHBxccHp06fRpk2bUp8zbdq0W1nESjl+/DgGDBigfjadO3fGqVOnAADdunWDt7c37rvvPjz55JMICAjAzz//jHfeeQedOnXCqVOnHLZvxokTJwBc62x7xx134Omnn0ZAQAAMBgMAU0dqrVbrUB1yzWWdP38+nnjiCQDX+s8UFBRAr9fDyckJY8aMwf33348PPvgA3bt3xwsvvGCxrSNYsGAB2rRpgxdffBEA0LlzZ/Tr1w8nTpzAnDlz0L17dwwZMgRdu3bFM888A8Cx6vfpp5/ipZdewsaNG0usO378OFq0aAGNRoOJEydi6dKlyMnJAeA4dVyyZAmee+45fPHFF8jIyAAANG7cGI8++ig8PDwAAMeOHcOMGTPQtm1bnD179uZfrNKhGpWbOSKPjo6WwYMHS0pKim0LVMUyMzPlySeflODgYBk9erRs2bJFnn76aalTp44sWbJERExNSpGRkSWaEbdt2yZ+fn6yfv16EbHfXzNbtmyRjh07irOzszz88MNy4sQJEbnW92ny5MnSpEkTWbhwoYiIfPbZZ3LHHXfIyy+/LP7+/nLw4EGblb28Ll++LF26dJGRI0fK0aNH5c8//5QmTZpYNPudOnVKVq5cKcHBwfLuu+/KpUuX5MCBAxIVFSWDBw9Wf507ik8++UTq1KkjrVu3loSEBBEpeQyaH//111+iKIr8+++/pW5nry5fvizu7u6iKIps27ZNXb5z507p3r273H///TJo0CA5c+aMJCQkyEcffSSenp6yevVqEbHvehqNRpkxY4Z4e3tLeHi4aDQaee6559T1v/zyi7Rq1UrCw8Pl9ddflx9++EFmzpwpOp1Opk2b5hB9bY4fPy5RUVFSv3596devnzg7O8tjjz2mfgeJmL5vYmJi1Mfe3t4ycOBAad++vZqhtlfz5s2TkJAQiYiIkJEjR4qrq6uMHDnSog9ufn6+fPzxx+Lq6io9e/aUgQMHiq+vr0yZMuWmWmEYEN1C5pPk+vXrxcnJSbZu3aqus+cvl5uVl5cnL774onz33XfqsqSkJGnXrp3aHCYiEhUVJePGjVNPPCIiZ8+elY4dO8qHH354S8tcESkpKeLu7i5RUVFy9erVUrdJT0+XESNGSP369cXHx0dq1qwpS5culZ07d0pERIRD9Cf76KOPpEGDBhZ9LV544QXp1auXxXZTpkyRYcOGiYioHVRXr14t/v7+DhX8z5kzR9q2bSsDBgyQjh07yrx580Sk7KbbLVu2SFhYmNrE5CgWLVokXbt2laFDh1qcNPV6vXTo0EEURZH33ntPXZ6dnS0PP/ywdOzY0RbFLbdPPvlE3NzcpEWLFrJixQoRERk7dqzF8Wo0GuWDDz6QLVu2WDz3jTfekAYNGsjevXtvaZlvxqxZs6RTp05qs9HPP/8szZo1s2gKfPLJJ2XVqlUiYvo/9vLyEo1GI88++6xkZGTYpNzlsWHDBrnvvvtk3rx56v/dunXrxMXFRf755x+LbQ8fPixnz55VH69evVq8vb0tgvzysv+2iGpEq9UCAHr37o0GDRrg888/x+HDhzF37lysXr3axqWrvI8//hi9e/fGr7/+CsDUZPTss8+iT58+6jaHDh2Cq6sr+vfvr04/EBsbi507d2Lt2rXqdhqNBnv27EFoaOitrUQF+Pr64sknn4SzszN8fX3x1VdfYezYsXjjjTewY8cO5ObmwsvLC5999hlWrlyJdevWITk5GSNHjkSDBg1w7tw5NGnSxNbVsLBixQp8+eWXOHz4sLrMw8MDycnJaqr977//xo8//ojRo0dbDHVNSkpCfn4+ANNnDwCHDx+Gn58f8vLybmEtKiY5Odli7qTWrVsjNjYWc+fORd26dbF+/XpkZGRAo9FYNIuZ7wcEBODKlSvw9/e/5WW/Gea5ktLT0xEdHY3hw4dj586d+OuvvwCYvqdGjhwJwPSZmp/j5uaGRo0awdnZGampqTYpe1nOnTuHvXv3AgDCw8Px448/4sCBAxg+fDgMBgOysrLg6uqKzMxMAKbmolGjRqFbt24Arn2WcXFxOHPmjLqdvbh48SJOnz4NwDTnVX5+Pg4dOoTQ0FC12ejuu+/GmDFjsGPHDnz77bcATJ/fxIkTUbNmTbzzzjt47bXX4OzsjEaNGsHNzc1m9SnO/N1i1qxZM0yZMgWjR49Wu0z4+vrizjvvhJeXF4Brn1nTpk1Rr1499XGnTp2QkZGB3NzcihfkpkM4uinmLNGbb74piqKIVquV8PBwi2yRo9m+fbs0atRIGjRoIDNmzCg1FfvPP//IgAEDRFEUadasmURGRqpNLrm5uTJ69GipX7++zJ8/X06dOiUvv/yydOnSRe2MbksGg0HmzJkjDzzwgLz22muyfft2dd0///wjiqJISEiIREZGypgxYyQiIkLCw8NlxowZIlJ69m/69Olyxx13SGJiok2zg+bXXrRokQQFBUl4eLh06NBBvLy81OZKEZFWrVpJt27dJCoqSpydnaVTp07Srl07i8zI3LlzpWbNmvLOO+/IX3/9JfPmzZMGDRrIu+++a5O63cjevXula9euEhYWVuavyY8//ljatWsnn3/+uYiUniW6cuWKBAQEXHcbWzh06JCanS1t+op77rlHVq1aJUlJSdKpUycZNWqUiJiOiaysLOnatau0adPGopNq37595emnn1a3s7U///xT/V7p1q1bifUFBQUiIvLoo49K165dReT6n89nn30mPj4+FiMmbe3bb78VRVGkT58+Fsu7d+9eYrqAY8eOSf/+/WXEiBEiIvL0009L//795YsvvlAHQowdO1Zq1aplkVWxlYMHD0r//v3lrrvukvHjx8uRI0dK3e61114TLy8v6dq1q/znP/+R8+fPl7nPGTNmSLdu3SwGPpQXA6Jb7OrVq3L//feLVquVmJgY+emnn2xdpErR6/USGxsrkyZNKnW9+cvnxIkT8r///U+OHDkily5dkk2bNomLi4u8//77ImIa2fKf//xHmjdvrp6Ybd0EYTAYZMmSJVK3bl1p3bq1zJgxQ5o0aSIRERGyZ88edbuZM2fK2LFj1X4yWVlZ8tprr0lAQIDanl9QUCCnTp2SnTt3yvTp06VevXpqU4ytHT16VFq1aiULFiyQ/Px8ycvLk44dO0psbKy6TU5Ojnz33XfSrFkz2bt3r+j1eikoKJB+/fpJ3759JTk5WdLT02XChAnStGlTqV+/vkRGRsoXX3xhw5qVbseOHXLnnXeq/b6OHz9eYhvzcXvu3Dm57777ZPjw4ZKfn1/q/k6fPi1NmzaVuXPnVmm5y+vEiRMyevRoURRFgoOD1eZccwBjNBolPT1devToIYcOHRIR00hBT09P6d69u/q/vGHDBunQoYPUqVNHXnnlFenbt680bdpUfv/9d9tUrIgNGzZIu3bt1H4zHTp0kPHjx0teXp7FduY6z507V8LCwkoNDM3zR23dulXuuusueeaZZ6q+AhUwfvx46dq1q0RFRVm893PmzJE6depYdDUQMTWldejQQU6fPi3Z2dmSnZ0tIteO6cTERBkzZozN+0kdPnxYwsPD5YEHHpBPPvlEGjRoIB07dlSDeHMwu3LlSmnYsKG88847snDhQmnZsqX07t1bbda8fPmy/Prrr7J582bp27evhIWFyeLFi2+qTAyIbrGrV6/KlClTHDojVPSX4alTp6Rhw4ayZ88eycjIkGnTpsmMGTPk66+/vuF+Bg4cKH379rVYlpiYKPv27bN2kcutaN3+/fdf6d+/v8yePVv9Mjl27Jj06dNH4uLi1O2uXr0qFy5csHj+wYMHpWXLlmpfqZycHFm9erU0btxYWrduLT/88MOtqlIJxU8KM2bMkObNm1v8Yhw7dqxFJ1QRkQceeEA9WWRlZYmISHx8vNSpU0c96RoMBrlw4YJddxaPioqS4OBgizl2rjd3ydtvvy3t27eXNWvWiEjJDENBQYHUrVvXIqNmS1OmTJH+/fvLjBkzpF27dvLWW2+JiOXnnpaWJp07d5aCggLZt2+ftG3bVpydnaV27dpq53AR0zQLM2fOlAEDBsjjjz9e4uRrC5988okEBgbKs88+q3awjYuLkw4dOohI6RmgV199Vdq0aVMi45yQkCAPPPCAdOzYUdzc3OTpp58uMTXIrVQ06DYHBA888IC88sorMmjQIHn44YfV9QkJCeLm5iaffPKJxT42bNggdevWlb/++qvE/m2dvSxav8mTJ0v79u3VIPbMmTPy0EMPSePGjUs8p+ixu3v3bmnZsqU6rcehQ4ekV69eEhoaKo8++miljlEGRFQu+/btk9mzZ8v27dsteu+vX79e6tatKzt37pQmTZpIr169ZPDgwaIoijz//PNqur34P+KxY8ekSZMm6pe1PaTfz58/b/EL8/Tp07J06VKLUQ15eXlyzz33yNSpU0sts7meR48eFU9PT4sMyZUrV+T06dNVWIPr27t3r4wZM0YmTZok33zzjdqpcuHChVKzZk1ZtGiR5ObmyqxZs6RGjRoya9Ys2bp1q/oldt9998m9996r7s9oNMqECROkd+/eaoBkjy5cuGDxhbpo0SJp1aqVbNmyRVauXCnDhw+XsWPHyoIFC9Q0u8FgUD/L48ePS58+fdQmpeTkZLX5wfzemE9etnDlyhWLY/H333+XLVu2yOXLlyUuLk6io6PV49q83fLly6VevXrSuHFjcXV1lUceeUT+85//SEBAgHpcFK2TrU+kn3zyiXriv3DhgkUAW1BQINOnT5eoqKgSs2mby718+XLx9vYuNUM0d+5ceeutt2x6DH/yySdSs2ZN9QeUuZznz5+X6OhoSU5OljfeeEMaN25sEcg/8cQT0qhRI4sBD3/88YcoimIx2szWitdPxJT56t27t8V2u3btEk9PT1m0aJGImD5b8zFr/iz37t0riqKoPyqNRqP8+eefVplpnAERXdelS5dk+PDh4uPjIzExMdK8eXNp27at+ivy4sWL4urqKl26dLEIEpYtWyZNmjSx+PWSkpIiqampsnXrVrn33nvl3nvvVTMrtrRt2zbp2LGjhIWFSZ8+feTFF18sdTvzCSIqKuqGzV3/+9//pEuXLtdt675VUlJSZNy4ceLr6ysPPfSQDB06VPz8/OSDDz5Qtxk3bpwMGjRIfHx8pEmTJvLKK6/IgAEDJDw8XJ0O/6uvvlJnaF6wYIHcd999EhISYjGK0F4YDAZ57bXXpEGDBtKlSxcZPHiwRQYsIiJCatasKREREfLcc8/JgAEDJDQ0tMQXtNns2bOlbdu28tBDD4miKGo/Glsx169p06Zyzz33yLhx40o0F4mYprVo0aKFzJ8/X0SuBXD//vuvNG7cWF566SV1QskDBw5I7dq1ZeLEiSJi+x8pBoNBpk2bJv7+/qIoirRr185ivdFoVMs4efJkiY6OlqysrFLLvXbtWgkJCZH9+/erz7U1vV4vM2bMEC8vLwkODhZfX1+ZMGGCxTaHDh2Srl27Sk5OjuzevVt69uwpzz77rGzYsEHOnj0rGRkZUq9ePRk2bJhs2rRJUlNTZdSoUTJixIhSj4db6Ub1e/zxx6V///4WWbucnBwZO3asNG3atNR9pqSkyIQJE2TYsGFVksljQETXtXDhQunYsaOa2UhOTpbQ0FAZPHiw/Pvvv5KdnS1Dhw4VRVHUIa5m3bp1kwkTJojRaJSTJ0/Km2++KdHR0eLt7S2PP/64XVz7ae/evdKsWTN54YUX5M8//5T3339fXF1dZcaMGWoWoOgv5f3790tQUJCcOXPGYj+XL1+WH374QeLj46VLly4SFBSkzrVkqy9f8+v+8ssv0rRpU4umyO7du0tcXJzFL+bFixdL586dLYbIP/XUU9K7d2810zd37lwZOnSo3HHHHfLEE0/cVMfFqpaamir33XeftG/fXr744gtZvXq11KpVS+Li4tTsx5IlS+Sxxx5Tv4wNBoNs375dtFqt2jRmNBrFYDDI+fPnZdiwYaIoijRv3lwWLFhgs7qJmJqVe/ToIR06dJDPPvtM4uPjRavVymuvvaaW2/y5Xrp0SWJjY+XOO+9Un29eV3S2cRFTM+hHH31k8wA3OztbHnnkEXFzc5OoqCj56quv5JlnnpHevXuXKLM5a/D555+XmgEy/w+sWbNGfH197aYp98svvxRFUeSOO+5Q53Xq1q2b2n/LXI+FCxeqQXpeXp7ExMSIi4uLuLu7y86dO0VEZNOmTdKnTx9p2LChBAQESEREhM3nGLpR/URMWaNmzZrJunXrLJ77ww8/iL+/v/z5558iYrrSwzvvvCMvvviiBAQESPv27ausHxsDIrquli1byrPPPisiokbks2bNEq1WK2+//baImE4uWq1W7chm/pIaMWKEOvdHbm6u/Pzzz7Jw4UKbX0SxaF+PcePGSa9evSQnJ0f98jSPaDCPGipqwoQJarNR0UDn0qVL8tJLL0mrVq3kpZdesmkdk5KSLB5PnjxZOnfubNE35IEHHlADNrM+ffrIm2++KSKi/rp86aWXpEGDBhbNpPn5+TZtIrqRjRs3SqNGjSxGjk2ZMkXatm2rPjYYDBbvh1lMTIzcf//96uPMzEypX7++NG/e3G76CC1ZskSaNWumdogWEYmNjS0xL5TZ0qVLpWnTpuoPFls3f5XF3GS1detW6dOnj2zatEld98wzz8gdd9whIqWXf+HChRIZGSkHDhwodd+HDx++6ZFHVeG3334rMbJx2LBhavBjruPUqVNl+vTpMnnyZPHy8pI6depIeHi4+p1slpubKzt27LCLDu8iN66fiCkb1KRJE3nmmWcsvl92794tzZs3lx9//FFETOedoUOHSr9+/dSLflcVzkNEMBgMePPNN/HQQw/hnXfewZEjR9R1YWFh6iUavL29AQD//vsvQkJCsHnzZpw/fx5DhgxBr1698NZbb2Hnzp3QaDQ4d+4cLl68iAceeAAAoNPp1HkyXF1dbVLHl19+Gd7e3pg6dSouX74MAEhMTISPjw9cXV2RnZ0NwDTvTm5uLr799lskJiYCMF3OIDs7G1u2bMGQIUMAAPn5+YiPj0dOTg6CgoIwYcIE7N+/H7NmzbrldTQYDJg5cyZatGiBESNGYNasWeq8JU2aNEFCQgLmzJmDdevWoUuXLli1ahW+/vprLFiwQK0jAPz888/Iy8uDi4sLkpKScOzYMdx7772oWbOmuo2zszOcnJxuaf3KUnSuESmch+TUqVPIyclR52cpKCjAmTNn8Oyzz6qfsUajUee4Mj8vOTkZly9fhq+vL0QEBoMBHh4e2Lx5Mw4ePIjevXvfyqoBMM3jtGzZMqSnp6vLzpw5ox5zAJCdnY3Lly9j8uTJ6txeANRLi9x5551o2bIllixZAgDYunWrekkSe7Bv3z6MGDECcXFxyMrKQufOnbFu3Tr06NFDnTOpSZMmSExMRGZmpsWlfMzr/fz8cOnSJfj5+ZX6Gk2bNsWWLVsQEBBQ9RUqZv/+/ZgzZw62b9+OK1euAAC6dOmCTp06qceeXq+Hm5sbnJ2dkZWVpdYxISEBM2fOxPbt27Fw4ULs2bMHI0aMwI8//ojk5GQApvdAp9OhQ4cO6Ny58y2v34EDBzBr1iz88MMP+Oeff8pVPxGBq6srYmNj8dtvv+Gbb75R95eamoqTJ0+q/5/e3t5YsGABvvvuO/Tv379qK1Ol4RbZNb1eLwsXLpSQkBDp0KGDPPXUU9KoUSNp2LCh2pSwatUqURRFnnvuOVm/fr0MHjxY7rzzTnn++eclLCxMdu3aJSKmvkTR0dFSu3ZtGTJkiAQFBck999xTqQvtWUNBQYH83//9nyiKItHR0bJs2TJ1nXm2Wp1OJ3/88YeImGbI7tevn/Tr10/atm0rP//8s7r9H3/8IXfccYecOnVKXn31VfH39xcXFxe1X4KtnDp1Srp16ybR0dGydOlSmTp1qkRGRspLL70kIqZ6fvnllzJx4kTx9vaWcePGydatW+U///mPREdHqzPb/v777+Li4iJt27aVMWPGSO3ataV9+/Z2ecXz7du3S4cOHWT06NFqtsr8qzo7O1vq168vXbp0kQcffFA8PDwkMDBQoqKipFOnThaZh6I+//xzadOmjcWUCra0b98+cXV1lcaNG8vmzZvV5efOnRNXV1fp37+/jBo1Sjw8PKRWrVoSFRUl/fr1U+dyKZrB/PDDD6VWrVoSEREhiqJY9B+zlY0bN0rr1q3F1dVV3N3d1QxQaU3Mb731lkRFRZV5Ad0jR46Ih4eHOhTb1n2EjEajpKamSmxsrLi7u0vnzp2lTp060qlTpxKXlDAft2PGjJHu3buLyLVm+kuXLslPP/1k0Yl8zZo1EhUVZfOM5c6dO+Wuu+4SHx8f6dmzpzRo0ECaNGmiDvM3K61+5r5s6enp8sQTT0iNGjXk1VdfldWrV0uXLl0kNjbWJll2BkS3seTkZOncubN89NFH6kFbUFAgnp6e6rW3REzDju+55x6pWbOmDBo0SO0IXfwaSBcvXpRvv/1Wpk6dWqJd2FauXLki7du3l06dOqnLMjIy1C+crKwsiYmJkdDQUKlfv74oiiL/+c9/5N9//xUXFxc1bSsiMnLkSFEURXQ6nTRt2rTK07fl9emnn0qjRo0svmi7desmU6ZMEb1eb9EU2K9fP4uTxcyZM6Vr167qZ7px40aZPXu2PProo3bzGRaVl5cnS5YskRYtWkjbtm3F1dXVol9I0dE5K1eulPr168uXX34pV69elUOHDkn//v2lT58+cunSJdHr9bJ27VqZOXOmdOnSRWrUqCFz5861GNliC+bX/v3336VJkybSuHFjmTlzpsWJ5tixY/Lxxx9LaGiorFq1ShISEmTnzp3SrFkzGTt2rNrXJjs7W1atWiX16tUTNzc3efbZZ23+I+XQoUPi5eUlNWrUkOeee04SExPl448/llatWpW4BI75e+m7774TV1fXEqPIzNatWyeenp42nc6iuK1bt6qXAdHr9XLw4EEJDg6WsWPHqj84i3YMf+eddyQsLKzM/Zm3y87OLvN9uFWOHTsm3bt3l4kTJ6qj/o4dOyY6nU7tM2RWVv3Mn21ubq7MnDlT+vTpIyEhIfLYY4+VeSmkqsaA6DY3e/Zs9URq7jdy1113lRhFk5KSYtF2//vvv0udOnXsps36ej766CNp1aqVrF69Wp566imJjo6Wvn37qhmUzMxM2bx5s8ybN8/iZFG/fn21H1Fubq5MnjxZBgwYILt377ZJPcoybdo0ufvuu9Uv2YKCAunbt69s2LBB3SY3N1c6duyodgg2f0mNHTtW2rZtW6Kzqr0yGo3yySefyJtvvikXL16UFi1ayNixY0vd9tVXX5Vhw4ZJbm6uGijNnz9fWrRoIadPn5bc3FxZuXKldOnSRaZPn27zvm3FTZo0SRYuXKjOoWPOxpo/u0cffVQdtWP+350xY4a0bdtW7e9nvjDvlClT7KZ+ubm58t///tdimPsrr7wizZo1K/OCnL/88osEBASomdzi0tLSZO7cuaUOq7eVKVOmSMeOHS3+t1asWCFNmjQpdZSq+XMufq0ue7Vy5UqLizZfuHBBOnfuXGbG/Eb1y8rKKnPy01uFARFZyMnJkbCwsOumYw0Ggzz99NMWc9LYM/M/qoeHh4wcOVIWLlwoEyZMEFdXV5k0aVKpX8LmS1kcO3ZMXWavc+38/fff0rJlS+nVq5eMHTtWPDw8JDg4WBo2bChxcXHqHCU9e/aUzp07y44dO0Sv18tPP/0kHTt2tJgbxJ7o9XqZO3eurFy50qITd3JyshoULF26VNzd3S2aUszr7rvvPvXyMGazZs2SiIgIdSSdLYOEsupnvv/YY4/Jl19+KefOnZOQkBB1EINZhw4dZMqUKRbPeeqpp6Rdu3Ylmi1spaw6mpmXmTNAZX0eP/30k9SuXVsdeWQv9Hq9vPHGG/Lggw/K22+/LYcPH1bXTZs2TZo0aSIiliNV7733Xhk+fLjFDxgR08gsX19fu+r0fr36FbVlyxaJjo6WBg0ayJw5cyy+N+25fsUxICKLA/Snn36Shg0byvnz5y2aDQwGg6xbt07mz58v0dHRUqdOHbVJxdbt9eWxaNEi+eKLLyza4t977z2JiopSh1n/+++/cunSJVm1apVER0fL1KlTbVXcCjt27Ji8+eab0qhRI/nqq68kJSVFli5dKj169JD77rtPREz9LOrXry9hYWESHR0tnp6e8p///MfuAj2j0Shr166VVq1aiaIo0qFDB4vJMc0MBoPk5uZKvXr15Pnnn7d4vojpC9jJyUleffVV2bZtm0yfPl3q1q0rH3300S2rS2nKW7/IyEj1mlpxcXHSrVs3eeqpp9TM5syZM8XV1VUWLlwo+/btkxkzZkhYWFiJ6S9sobx1NPvyyy8lPDxczYIVl5SUJE5OTvLbb7+p+7elsvpfNmrUSJ3vasuWLaLVatUfJOYs3qeffioREREW10QUMQ039/LyKnOk3K10vfoVz/D89NNP0rJlS4mNjZXZs2dL165dJSIiosQoM3uqX1kYEJGIXOt7MXbsWOnXr1+p2yxfvlx69uwpL7300nUvdWCPsrKy1F8q5i/T/Px8qVmzpjqb9GeffSadO3cWX19feeWVV+x6aHlpxowZI4899pjFsjlz5kibNm3U4cb//POP/PDDD/LZZ5/Z7WeYm5srr7/+ukyePFl+/PFH0Wq1snLlyjK3N19UtrRJPuPi4qRdu3bSsGFD6dChg807ooqUr34nTpyQQYMGqY9Hjx4tWq1WAgMD1f59mZmZMnz4cImKipK6detK+/btZePGjbe0LmUp72do/jH2888/i4+PT4n5vcy2bdsmderUka+++qoqi11u5el/efr0aWnbtq2apTQHRPn5+eLm5lbiml1Lly6VBx980C4uj3K9+n366acicu2ckZaWVqLJ3d/fX70Kgfn59lS/sjAgIlVKSorUqVNHveBsdna2fP311+qoFVvPfGot5o6M+/btE51Op/YTunjxYpkjkOyd0WiUO+64Q/2yMhs8eLAMGjRI8vPz7TpVXdzff/+tNin07NlTYmJiyuxfkpaWJv7+/jJnzhwRMX1Rm7ctKCiQlJSUMk+0tnKj+v32229So0YNefTRR0Wn00mHDh2kXbt20qdPH4u65OXlyYULF8ocfWVLFfkMjx49Kl5eXuqEgsUzQJmZmXZ1KQqRsvtfmmf6zsvLk48++kicnZ0tLiCckJAgwcHB6uzhts52laWs+hW/+G3R8uv1ejEYDFKvXj11EkZ7rV9pGBCRauXKldKtWze5cOGCvPTSS+Lh4SENGza0aA92dOZ/7MTERBk1apQMHDjQ4TJBZZk7d660adNG3n//fTl58qS8+OKL0qRJE7toQqmMnTt3iqIo1x31NnPmTAkLC5NPP/1UunXrJjNmzLCrDrbXU1r9fv31V4mMjJRevXqpI6f++OMP8ff3lw8//NBWRb1pN/oMd+zYIY0bN7abDNDNKK3/5ZUrV6Rbt27SqVMnNXs3b948admypc1H+1VUefqXioh8/PHH0qFDB4vrqzkKBkQkIqYo3nxRVldXV2nevLnNp/C3NoPBIBMnTpSHH35YvLy8pEePHhaXs3B0+fn5MmTIEImIiJD69etLdHS0mu1zVOZfl9HR0TJ48GCLy4qYpaWlyTvvvCOKooibm5s8/PDDNhu2W1HF62e+nE1KSoqcPXu2xKibvn37ytKlSx3qV3d5PsOcnJwbBr32qKz+l0WX//PPPxITEyPBwcHSsmVLcXV1lf/9739iMBjs/nMsT//StLQ0WbZsmSxatEg6dOggQUFBEh8f71AZaTMGRKSaNm2a9OzZ024mpqsKq1atknHjxpXZebM6OHr0qF02odwMc5Zn/fr14uTkJFu3brVYn5SUJL169RJFUWTWrFkO16xbvH6lXYOq6LXJ7P0EWprrfYbm+pw+fVo6d+7skAH89fpfmuuXkZEhv//+uyxdutRu++6V5Ub9SzMyMuT555+XVq1aybRp0xyufkUpIoVza9Ntz2g0WkyLXx2JCBRFsXUx6CZERETgrrvuwtNPP40NGzagYcOGGDBgALZt22aTSxZYW9H6bdy4EXXr1sWQIUOq1TFbvI6hoaEYOnQoCgoK4OzsbOvi3bTU1FS0aNECn376KWJiYpCTk4P169ejefPmaNy4sa2LV2ml1W/dunVo0aIFGjdujCtXrlhc3sdRVe+zH1VIdQ+GAFSbE8vtxHxNrsceewwLFixAy5Yt8eGHH6JGjRoA4PDBUGn1i4+PV6+7VR2O2bLqGBgYCAAOHQwBpmsAhoeHIzIyEi+//DJq1aqF559/3tbFsprS6vfCCy+o66tDMASA1zIjIvt29epVuf/++0Wr1UpMTIxDNqtcT3Wvn0j1rmN1739Z3etXlH1cspqI6Drq1q2LzZs3484777R1UapEda8fUH3rqCgKmjVrhszMTMyePRutW7e2dZGsqrrXryj2ISIiIqqE6t7/srrXz4wBEREREd32qn/IR0RERHQDDIiIiIjotseAiIiIiG57DIiIiIjotseAiIiIiG57DIiIiIjotseAiIionBRFwZo1a2xdDCKqAgyIiMghjB49GoqilLj17t3b1kUjomqAl+4gIofRu3dvfPrppxbLdDqdjUpDRNUJM0RE5DB0Oh2CgoIsbuar3iuKgo8++gh9+vSBm5sbGjRogFWrVlk8/+DBg+jRowfc3NxQs2ZNjBs3DpmZmRbbLFq0CM2aNYNOp0Pt2rXx1FNPWaxPTk7GoEGD4O7ujkaNGmHt2rVVW2kiuiUYEBFRtTFt2jQMGTIEf/31Fx566CHcf//9OHLkCAAgKysLvXr1Qo0aNbB7926sXLkSP//8s0XA89FHHyEuLg7jxo3DwYMHsXbtWjRs2NDiNV599VUMHz4cBw4cwL333ouHHnoIV69evaX1JKIqIEREDmDUqFGi1WrFw8PD4vb666+LiAgAefzxxy2e0759e3niiSdERGT+/PlSo0YNyczMVNf/8MMPotFoJCEhQUREgoOD5aWXXiqzDADk5ZdfVh9nZmYKAFm/fr3V6klEtsE+RETkMLp3746PPvrIYpmfn596v2PHjhbrOnbsiP379wMAjhw5glatWsHDw0Nd37lzZxiNRhw7dgyKouDixYu4++67r1uGli1bqvc9PDzg7e2Ny5cv32yViMhOMCAiIofh4eFRognLWtzc3Mq1nbOzs8VjRVFgNBqrokhEdAuxDxERVRt//PFHicdNmzYFADRt2hR//fUXsrKy1PXbtm2DRqNBREQEvLy8EBYWhk2bNt3SMhORfWCGiIgcRl5eHhISEiyWOTk5wd/fHwCwcuVKtG3bFl26dMEXX3yBXbt2YeHChQCAhx56CNOnT8eoUaMwY8YMJCUlYcKECXj44YcRGBgIAJgxYwYef/xxBAQEoE+fPsjIyMC2bdswYcKEW1tRIrrlGBARkcPYsGEDateubbEsIiICR48eBWAaAbZ8+XI8+eSTqF27NpYtW4bIyEgAgLu7OzZu3Iinn34a0dHRcHd3x5AhQ/D222+r+xo1ahRyc3PxzjvvYPLkyfD398fQoUNvXQWJyGYUERFbF4KIqLIURcE333yDgQMH2rooROSA2IeIiIiIbnsMiIiIiOi2xz5ERFQtsPWfiCqDGSIiIiK67TEgIiIiotseAyIiIiK67TEgIiIiotseAyIiIiK67TEgIiIiotseAyIiIiK67TEgIiIiotseAyIiIiK67f0/qNvVolpR3V4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d54a04",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd72688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
