{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 48\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "#             nn.Dropout(0.03),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 240692, Validation Loss: 246775, 11.484246614509658\n",
      "Epoch 101, Training Loss: 57752, Validation Loss: 99171, 153034.5628056057\n",
      "Epoch 201, Training Loss: 52702, Validation Loss: 71112, 205421.57840483662\n",
      "Epoch 301, Training Loss: 52031, Validation Loss: 74352, 277257.5967465402\n",
      "Epoch 401, Training Loss: 53271, Validation Loss: 76641, 274749.9623102167\n",
      "Epoch 501, Training Loss: 52581, Validation Loss: 53912, 231613.9127814402\n",
      "Epoch 601, Training Loss: 50696, Validation Loss: 67203, 220317.9968240783\n",
      "Epoch 701, Training Loss: 52925, Validation Loss: 53046, 209632.0237298026\n",
      "Epoch 801, Training Loss: 52316, Validation Loss: 90476, 220085.34443614623\n",
      "Epoch 901, Training Loss: 51995, Validation Loss: 61053, 235606.84372133575\n",
      "Epoch 1001, Training Loss: 50858, Validation Loss: 56168, 161824.68324576318\n",
      "Epoch 1101, Training Loss: 48983, Validation Loss: 55330, 225663.9791615112\n",
      "Epoch 1201, Training Loss: 50966, Validation Loss: 52473, 241871.0537533313\n",
      "Epoch 1301, Training Loss: 48951, Validation Loss: 53092, 185610.91595754496\n",
      "Epoch 1401, Training Loss: 51289, Validation Loss: 99170, 221734.33183726008\n",
      "Epoch 1501, Training Loss: 53657, Validation Loss: 65948, 214846.52847808992\n",
      "Epoch 1601, Training Loss: 50469, Validation Loss: 51938, 194684.86319734354\n",
      "Epoch 1701, Training Loss: 49049, Validation Loss: 60866, 164672.53794824332\n",
      "Epoch 1801, Training Loss: 49621, Validation Loss: 55832, 254957.7250633646\n",
      "Epoch 1901, Training Loss: 49332, Validation Loss: 68924, 162292.41550608826\n",
      "Epoch 2001, Training Loss: 49423, Validation Loss: 55150, 213845.43769562824\n",
      "Epoch 2101, Training Loss: 52523, Validation Loss: 67203, 207664.47952467864\n",
      "Epoch 2201, Training Loss: 49519, Validation Loss: 75334, 184719.2182889022\n",
      "Epoch 2301, Training Loss: 48665, Validation Loss: 85354, 198336.8036959482\n",
      "Epoch 2401, Training Loss: 48259, Validation Loss: 60849, 206069.271093639\n",
      "Epoch 2501, Training Loss: 50090, Validation Loss: 67135, 174435.12240137396\n",
      "Epoch 2601, Training Loss: 49648, Validation Loss: 51881, 217812.1322889373\n",
      "Epoch 2701, Training Loss: 51002, Validation Loss: 68130, 190262.4412533102\n",
      "Epoch 2801, Training Loss: 51243, Validation Loss: 55700, 189293.60506158232\n",
      "Epoch 2901, Training Loss: 48428, Validation Loss: 53811, 174669.83407104574\n",
      "Epoch 3001, Training Loss: 49416, Validation Loss: 56811, 202610.36300723618\n",
      "Epoch 3101, Training Loss: 48805, Validation Loss: 71511, 177766.5231547846\n",
      "Epoch 3201, Training Loss: 48572, Validation Loss: 50431, 188676.79233278366\n",
      "Epoch 3301, Training Loss: 49701, Validation Loss: 56376, 170630.9051860138\n",
      "Epoch 3401, Training Loss: 50487, Validation Loss: 55188, 200911.12328523365\n",
      "Epoch 3501, Training Loss: 51946, Validation Loss: 136759, 282028.2000508259\n",
      "Epoch 3601, Training Loss: 50023, Validation Loss: 70590, 161752.08110540165\n",
      "Epoch 3701, Training Loss: 49949, Validation Loss: 52326, 221285.35030315805\n",
      "Epoch 3801, Training Loss: 48224, Validation Loss: 62318, 185883.91410589745\n",
      "Epoch 3901, Training Loss: 49310, Validation Loss: 60495, 189973.32618513214\n",
      "Epoch 4001, Training Loss: 48352, Validation Loss: 54592, 196687.72298297877\n",
      "Epoch 4101, Training Loss: 48577, Validation Loss: 53011, 202733.9757302728\n",
      "Epoch 4201, Training Loss: 46645, Validation Loss: 55119, 157937.63923664755\n",
      "Epoch 4301, Training Loss: 48497, Validation Loss: 52252, 210632.26710019028\n",
      "Epoch 4401, Training Loss: 46853, Validation Loss: 93610, 178113.87237162795\n",
      "Epoch 4501, Training Loss: 51940, Validation Loss: 55126, 177878.35927889214\n",
      "Epoch 4601, Training Loss: 47609, Validation Loss: 66927, 209659.02669413472\n",
      "Epoch 4701, Training Loss: 47584, Validation Loss: 60031, 168383.46441736835\n",
      "Epoch 4801, Training Loss: 47901, Validation Loss: 49590, 212738.38366905213\n",
      "Epoch 4901, Training Loss: 48970, Validation Loss: 53718, 200745.76001253884\n",
      "Epoch 5001, Training Loss: 49724, Validation Loss: 59894, 202492.63638710478\n",
      "Epoch 5101, Training Loss: 48506, Validation Loss: 57183, 217284.47613438938\n",
      "Epoch 5201, Training Loss: 48846, Validation Loss: 51321, 237896.3205042866\n",
      "Epoch 5301, Training Loss: 48489, Validation Loss: 131386, 253574.63035864852\n",
      "Epoch 5401, Training Loss: 45483, Validation Loss: 58598, 181982.8065021726\n",
      "Epoch 5501, Training Loss: 47295, Validation Loss: 77492, 195685.40001428887\n",
      "Epoch 5601, Training Loss: 48366, Validation Loss: 50518, 209194.8040635481\n",
      "Epoch 5701, Training Loss: 49010, Validation Loss: 54256, 193144.91201014628\n",
      "Epoch 5801, Training Loss: 47916, Validation Loss: 50530, 221977.62696866013\n",
      "Epoch 5901, Training Loss: 47573, Validation Loss: 50991, 256334.90400198693\n",
      "Epoch 6001, Training Loss: 45712, Validation Loss: 63861, 207458.6086937515\n",
      "Epoch 6101, Training Loss: 46651, Validation Loss: 96021, 228531.28589605945\n",
      "Epoch 6201, Training Loss: 47303, Validation Loss: 69196, 207982.92841226017\n",
      "Epoch 6301, Training Loss: 46865, Validation Loss: 50464, 192575.61248470147\n",
      "Epoch 6401, Training Loss: 47586, Validation Loss: 58367, 205444.5744294487\n",
      "Epoch 6501, Training Loss: 44781, Validation Loss: 50753, 179155.2161728659\n",
      "Epoch 6601, Training Loss: 47149, Validation Loss: 65974, 187969.53068873534\n",
      "Epoch 6701, Training Loss: 46337, Validation Loss: 54539, 192965.03630429786\n",
      "Epoch 6801, Training Loss: 48142, Validation Loss: 56892, 227995.98976376854\n",
      "Epoch 6901, Training Loss: 46614, Validation Loss: 74895, 231859.73999753993\n",
      "Epoch 7001, Training Loss: 49521, Validation Loss: 54197, 229069.00856709437\n",
      "Epoch 7101, Training Loss: 46218, Validation Loss: 66323, 230005.5815579923\n",
      "Epoch 7201, Training Loss: 46423, Validation Loss: 51303, 186251.98215210938\n",
      "Epoch 7301, Training Loss: 45523, Validation Loss: 50179, 184602.49577717052\n",
      "Epoch 7401, Training Loss: 43934, Validation Loss: 50649, 188205.73309781027\n",
      "Epoch 7501, Training Loss: 48055, Validation Loss: 58928, 250077.6992040832\n",
      "Epoch 7601, Training Loss: 46599, Validation Loss: 74967, 200074.3244060826\n",
      "Epoch 7701, Training Loss: 45344, Validation Loss: 75641, 195002.4792358637\n",
      "Epoch 7801, Training Loss: 46336, Validation Loss: 52746, 192486.71192212743\n",
      "Epoch 7901, Training Loss: 45159, Validation Loss: 55070, 180490.27801188102\n",
      "Epoch 8001, Training Loss: 46252, Validation Loss: 62251, 214264.64044662777\n",
      "Epoch 8101, Training Loss: 45260, Validation Loss: 58691, 210570.0701086281\n",
      "Epoch 8201, Training Loss: 45449, Validation Loss: 56764, 198597.03943993596\n",
      "Epoch 8301, Training Loss: 44952, Validation Loss: 53884, 184539.51801318885\n",
      "Epoch 8401, Training Loss: 47658, Validation Loss: 65577, 207034.83848579242\n",
      "Epoch 8501, Training Loss: 43837, Validation Loss: 61933, 209955.17905301045\n",
      "Epoch 8601, Training Loss: 43592, Validation Loss: 72132, 195818.35402097166\n",
      "Epoch 8701, Training Loss: 44349, Validation Loss: 49042, 253835.59175395736\n",
      "Epoch 8801, Training Loss: 46522, Validation Loss: 52679, 202671.4787279054\n",
      "Epoch 8901, Training Loss: 47039, Validation Loss: 69475, 252735.34564758473\n",
      "Epoch 9001, Training Loss: 45129, Validation Loss: 59344, 199256.0724326397\n",
      "Epoch 9101, Training Loss: 47385, Validation Loss: 56041, 177891.80893260558\n",
      "Epoch 9201, Training Loss: 43010, Validation Loss: 58607, 203555.01067237018\n",
      "Epoch 9301, Training Loss: 44345, Validation Loss: 54674, 221263.86990859325\n",
      "Epoch 9401, Training Loss: 45578, Validation Loss: 51669, 216935.0496653004\n",
      "Epoch 9501, Training Loss: 45059, Validation Loss: 57945, 186366.1799475398\n",
      "Epoch 9601, Training Loss: 45830, Validation Loss: 51140, 233971.75650733744\n",
      "Epoch 9701, Training Loss: 44147, Validation Loss: 66065, 239779.50144671137\n",
      "Epoch 9801, Training Loss: 46059, Validation Loss: 83435, 227836.6539937489\n",
      "Epoch 9901, Training Loss: 44570, Validation Loss: 52435, 223871.61958547847\n",
      "Epoch 10001, Training Loss: 44583, Validation Loss: 51000, 195974.42286293444\n",
      "Epoch 10101, Training Loss: 44845, Validation Loss: 101596, 169864.48695562236\n",
      "Epoch 10201, Training Loss: 44789, Validation Loss: 68545, 250331.05386749542\n",
      "Epoch 10301, Training Loss: 44383, Validation Loss: 68851, 224265.53680237173\n",
      "Epoch 10401, Training Loss: 45588, Validation Loss: 50813, 207743.79873292203\n",
      "Epoch 10501, Training Loss: 43403, Validation Loss: 57976, 189813.77398932714\n",
      "Epoch 10601, Training Loss: 44650, Validation Loss: 50410, 221608.12697547147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10701, Training Loss: 44733, Validation Loss: 108600, 229962.94344057926\n",
      "Epoch 10801, Training Loss: 44012, Validation Loss: 50592, 235186.3394286825\n",
      "Epoch 10901, Training Loss: 43930, Validation Loss: 54253, 193629.50587135006\n",
      "Epoch 11001, Training Loss: 47185, Validation Loss: 146685, 243007.38873447943\n",
      "Epoch 11101, Training Loss: 44586, Validation Loss: 50079, 282203.5016780222\n",
      "Epoch 11201, Training Loss: 43326, Validation Loss: 60495, 154976.8099915108\n",
      "Epoch 11301, Training Loss: 42919, Validation Loss: 51721, 223706.9180876914\n",
      "Epoch 11401, Training Loss: 45733, Validation Loss: 67736, 224050.7270845817\n",
      "Epoch 11501, Training Loss: 44402, Validation Loss: 50701, 223499.64999299226\n",
      "Epoch 11601, Training Loss: 44202, Validation Loss: 53435, 212897.05489301204\n",
      "Epoch 11701, Training Loss: 44398, Validation Loss: 51329, 206623.84044466124\n",
      "Epoch 11801, Training Loss: 44940, Validation Loss: 55124, 233021.4939041131\n",
      "Epoch 11901, Training Loss: 43994, Validation Loss: 62424, 197544.05427029045\n",
      "Epoch 12001, Training Loss: 43361, Validation Loss: 52765, 250385.7272077802\n",
      "Epoch 12101, Training Loss: 44772, Validation Loss: 54373, 265198.3886769885\n",
      "Epoch 12201, Training Loss: 43094, Validation Loss: 79070, 189489.76928130048\n",
      "Epoch 12301, Training Loss: 42947, Validation Loss: 48246, 193523.96792240845\n",
      "Epoch 12401, Training Loss: 43098, Validation Loss: 65334, 241180.48791136887\n",
      "Epoch 12501, Training Loss: 43208, Validation Loss: 57427, 190298.0399153328\n",
      "Epoch 12601, Training Loss: 45374, Validation Loss: 51996, 198232.64483099873\n",
      "Epoch 12701, Training Loss: 42374, Validation Loss: 51352, 200832.60372116612\n",
      "Epoch 12801, Training Loss: 44174, Validation Loss: 55738, 218587.2290023067\n",
      "Epoch 12901, Training Loss: 43334, Validation Loss: 61680, 277079.19655397633\n",
      "Epoch 13001, Training Loss: 42460, Validation Loss: 55040, 189466.41029454485\n",
      "Epoch 13101, Training Loss: 40944, Validation Loss: 51408, 216686.10402177167\n",
      "Epoch 13201, Training Loss: 42398, Validation Loss: 55867, 213455.58663814177\n",
      "Epoch 13301, Training Loss: 41396, Validation Loss: 57619, 193179.47581532772\n",
      "Epoch 13401, Training Loss: 43481, Validation Loss: 62803, 190092.5744771517\n",
      "Epoch 13501, Training Loss: 43618, Validation Loss: 49030, 240720.6318567601\n",
      "Epoch 13601, Training Loss: 42667, Validation Loss: 50840, 223017.9209823201\n",
      "Epoch 13701, Training Loss: 42829, Validation Loss: 49102, 182949.4245013478\n",
      "Epoch 13801, Training Loss: 43817, Validation Loss: 72477, 229283.74491148442\n",
      "Epoch 13901, Training Loss: 41248, Validation Loss: 49654, 209817.76462965927\n",
      "Epoch 14001, Training Loss: 43602, Validation Loss: 66493, 220103.07144988386\n",
      "Epoch 14101, Training Loss: 44641, Validation Loss: 56742, 238167.63905069386\n",
      "Epoch 14201, Training Loss: 42706, Validation Loss: 54880, 204234.51059419758\n",
      "Epoch 14301, Training Loss: 45365, Validation Loss: 54634, 218996.25076250068\n",
      "Epoch 14401, Training Loss: 41932, Validation Loss: 51211, 184814.99651677764\n",
      "Epoch 14501, Training Loss: 43071, Validation Loss: 80897, 217582.61389359235\n",
      "Epoch 14601, Training Loss: 44097, Validation Loss: 63091, 243512.649288467\n",
      "Epoch 14701, Training Loss: 41857, Validation Loss: 53458, 152027.243824511\n",
      "Epoch 14801, Training Loss: 43171, Validation Loss: 50589, 186888.49106477931\n",
      "Epoch 14901, Training Loss: 47518, Validation Loss: 52737, 223499.27364913796\n",
      "Epoch 15001, Training Loss: 44565, Validation Loss: 50361, 183452.18426578303\n",
      "Epoch 15101, Training Loss: 41476, Validation Loss: 53364, 174988.90020791054\n",
      "Epoch 15201, Training Loss: 42825, Validation Loss: 75347, 196112.22409437687\n",
      "Epoch 15301, Training Loss: 43248, Validation Loss: 65086, 176580.7520420705\n",
      "Epoch 15401, Training Loss: 43024, Validation Loss: 51650, 191369.43244740914\n",
      "Epoch 15501, Training Loss: 44250, Validation Loss: 61298, 175154.7444939837\n",
      "Epoch 15601, Training Loss: 42219, Validation Loss: 54414, 225919.81287918644\n",
      "Epoch 15701, Training Loss: 45108, Validation Loss: 61855, 257819.8969439759\n",
      "Epoch 15801, Training Loss: 43155, Validation Loss: 57221, 174332.19189972253\n",
      "Epoch 15901, Training Loss: 42559, Validation Loss: 52919, 208481.16037064325\n",
      "Epoch 16001, Training Loss: 42723, Validation Loss: 47491, 223106.8094647832\n",
      "Epoch 16101, Training Loss: 42939, Validation Loss: 97840, 222959.19250046147\n",
      "Epoch 16201, Training Loss: 44153, Validation Loss: 48149, 190523.38491402942\n",
      "Epoch 16301, Training Loss: 46400, Validation Loss: 58515, 197434.74713908168\n",
      "Epoch 16401, Training Loss: 45369, Validation Loss: 73744, 162128.62467260085\n",
      "Epoch 16501, Training Loss: 42961, Validation Loss: 54215, 204480.04583095908\n",
      "Epoch 16601, Training Loss: 43234, Validation Loss: 71820, 233449.2490027356\n",
      "Epoch 16701, Training Loss: 43637, Validation Loss: 78797, 208247.48853786345\n",
      "Epoch 16801, Training Loss: 44546, Validation Loss: 47465, 193678.90080391933\n",
      "Epoch 16901, Training Loss: 44588, Validation Loss: 48256, 199928.15430042337\n",
      "Epoch 17001, Training Loss: 43500, Validation Loss: 53790, 202417.17414069132\n",
      "Epoch 17101, Training Loss: 43633, Validation Loss: 57960, 201542.26819166695\n",
      "Epoch 17201, Training Loss: 44240, Validation Loss: 49478, 208720.53650708855\n",
      "Epoch 17301, Training Loss: 45021, Validation Loss: 48998, 180890.7917891783\n",
      "Epoch 17401, Training Loss: 44812, Validation Loss: 67113, 209568.9278863984\n",
      "Epoch 17501, Training Loss: 45374, Validation Loss: 107668, 212147.36360137336\n",
      "Epoch 17601, Training Loss: 42226, Validation Loss: 52175, 196527.35941645948\n",
      "Epoch 17701, Training Loss: 45700, Validation Loss: 58572, 257207.06108196513\n",
      "Epoch 17801, Training Loss: 44884, Validation Loss: 59575, 211803.2086139639\n",
      "Epoch 17901, Training Loss: 42942, Validation Loss: 66304, 232138.0081077238\n",
      "Epoch 18001, Training Loss: 42840, Validation Loss: 58030, 191592.83900075278\n",
      "Epoch 18101, Training Loss: 43503, Validation Loss: 52076, 203034.7890216013\n",
      "Epoch 18201, Training Loss: 41031, Validation Loss: 48339, 183482.94122263696\n",
      "Epoch 18301, Training Loss: 42408, Validation Loss: 59960, 213861.6863026688\n",
      "Epoch 18401, Training Loss: 43431, Validation Loss: 52661, 205257.23204892027\n",
      "Epoch 18501, Training Loss: 43149, Validation Loss: 49123, 247543.04101260193\n",
      "Epoch 18601, Training Loss: 42056, Validation Loss: 63949, 191903.47464589283\n",
      "Epoch 18701, Training Loss: 47509, Validation Loss: 95890, 184998.2358041629\n",
      "Epoch 18801, Training Loss: 43779, Validation Loss: 56398, 185199.71263875265\n",
      "Epoch 18901, Training Loss: 41896, Validation Loss: 50861, 214257.60721446222\n",
      "Epoch 19001, Training Loss: 42425, Validation Loss: 58292, 253614.07607756567\n",
      "Epoch 19101, Training Loss: 42597, Validation Loss: 58229, 195295.06648776063\n",
      "Epoch 19201, Training Loss: 42871, Validation Loss: 52137, 178798.07793631693\n",
      "Epoch 19301, Training Loss: 41364, Validation Loss: 74574, 220060.71779121758\n",
      "Epoch 19401, Training Loss: 41528, Validation Loss: 57089, 199074.96115338462\n",
      "Epoch 19501, Training Loss: 41951, Validation Loss: 55956, 173478.73030943287\n",
      "Epoch 19601, Training Loss: 42338, Validation Loss: 70287, 204201.25232330314\n",
      "Epoch 19701, Training Loss: 42912, Validation Loss: 71590, 182032.6075843821\n",
      "Epoch 19801, Training Loss: 42659, Validation Loss: 50090, 187791.72286231758\n",
      "Epoch 19901, Training Loss: 41019, Validation Loss: 53515, 183741.96391122392\n",
      "Epoch 20001, Training Loss: 42586, Validation Loss: 70663, 180137.88073305754\n",
      "Epoch 20101, Training Loss: 42894, Validation Loss: 63058, 205813.00420494945\n",
      "Epoch 20201, Training Loss: 41923, Validation Loss: 65671, 211926.25435934562\n",
      "Epoch 20301, Training Loss: 44322, Validation Loss: 49778, 191991.18041974062\n",
      "Epoch 20401, Training Loss: 41522, Validation Loss: 72094, 212381.21086546546\n",
      "Epoch 20501, Training Loss: 41007, Validation Loss: 50017, 215006.9974772385\n",
      "Epoch 20601, Training Loss: 41888, Validation Loss: 51943, 140061.67878818244\n",
      "Epoch 20701, Training Loss: 41145, Validation Loss: 53068, 186072.9680513259\n",
      "Epoch 20801, Training Loss: 41085, Validation Loss: 59318, 207324.54807913283\n",
      "Epoch 20901, Training Loss: 43639, Validation Loss: 51371, 181776.62115328407\n",
      "Epoch 21001, Training Loss: 41153, Validation Loss: 60992, 211831.96671739966\n",
      "Epoch 21101, Training Loss: 41136, Validation Loss: 51173, 200497.04273892482\n",
      "Epoch 21201, Training Loss: 42172, Validation Loss: 58828, 206875.8844940896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21301, Training Loss: 42879, Validation Loss: 52606, 218820.48884386788\n",
      "Epoch 21401, Training Loss: 42222, Validation Loss: 53998, 212618.57867517087\n",
      "Epoch 21501, Training Loss: 43135, Validation Loss: 54690, 181391.29618303955\n",
      "Epoch 21601, Training Loss: 40541, Validation Loss: 52798, 194042.14530987327\n",
      "Epoch 21701, Training Loss: 43405, Validation Loss: 56394, 158881.99625812226\n",
      "Epoch 21801, Training Loss: 43082, Validation Loss: 75574, 181351.1749964206\n",
      "Epoch 21901, Training Loss: 42179, Validation Loss: 73242, 196857.70386267235\n",
      "Epoch 22001, Training Loss: 40591, Validation Loss: 53774, 168912.42859219722\n",
      "Epoch 22101, Training Loss: 42029, Validation Loss: 60206, 219987.84137407923\n",
      "Epoch 22201, Training Loss: 43466, Validation Loss: 64145, 214929.15995188503\n",
      "Epoch 22301, Training Loss: 42279, Validation Loss: 54441, 189358.2437444815\n",
      "Epoch 22401, Training Loss: 43381, Validation Loss: 66542, 196807.72102234812\n",
      "Epoch 22501, Training Loss: 43243, Validation Loss: 57149, 204153.50007076922\n",
      "Epoch 22601, Training Loss: 41406, Validation Loss: 50125, 201271.12921564435\n",
      "Epoch 22701, Training Loss: 42126, Validation Loss: 47579, 193149.21957622538\n",
      "Epoch 22801, Training Loss: 40778, Validation Loss: 56785, 197971.08703779217\n",
      "Epoch 22901, Training Loss: 40110, Validation Loss: 54637, 190590.6756206208\n",
      "Epoch 23001, Training Loss: 40711, Validation Loss: 55362, 201245.18246768092\n",
      "Epoch 23101, Training Loss: 41404, Validation Loss: 68984, 185059.59250581366\n",
      "Epoch 23201, Training Loss: 43256, Validation Loss: 99565, 231550.73354149525\n",
      "Epoch 23301, Training Loss: 41130, Validation Loss: 53455, 220430.53522698837\n",
      "Epoch 23401, Training Loss: 43843, Validation Loss: 51380, 218357.44546999\n",
      "Epoch 23501, Training Loss: 42486, Validation Loss: 50374, 185483.21788560765\n",
      "Epoch 23601, Training Loss: 40165, Validation Loss: 57339, 180048.68753046225\n",
      "Epoch 23701, Training Loss: 41983, Validation Loss: 55173, 214054.45819848598\n",
      "Epoch 23801, Training Loss: 39840, Validation Loss: 50411, 220245.6131157437\n",
      "Epoch 23901, Training Loss: 41487, Validation Loss: 53086, 168998.1836533647\n",
      "Epoch 24001, Training Loss: 41189, Validation Loss: 49258, 170063.3056880751\n",
      "Epoch 24101, Training Loss: 40820, Validation Loss: 48913, 256776.04149753295\n",
      "Epoch 24201, Training Loss: 41738, Validation Loss: 50274, 211556.15504738723\n",
      "Epoch 24301, Training Loss: 41856, Validation Loss: 101701, 177752.15681573263\n",
      "Epoch 24401, Training Loss: 42604, Validation Loss: 50285, 211690.36013543888\n",
      "Epoch 24501, Training Loss: 40994, Validation Loss: 50327, 165128.78286386185\n",
      "Epoch 24601, Training Loss: 41779, Validation Loss: 55553, 231228.59466531524\n",
      "Epoch 24701, Training Loss: 41110, Validation Loss: 48828, 192428.43073111493\n",
      "Epoch 24801, Training Loss: 41510, Validation Loss: 54591, 173722.22338667803\n",
      "Epoch 24901, Training Loss: 45354, Validation Loss: 91375, 260490.07923654522\n",
      "Epoch 25001, Training Loss: 41494, Validation Loss: 51101, 226733.60154938838\n",
      "Epoch 25101, Training Loss: 41033, Validation Loss: 82239, 205558.37112373172\n",
      "Epoch 25201, Training Loss: 43002, Validation Loss: 59596, 196458.51550189627\n",
      "Epoch 25301, Training Loss: 42455, Validation Loss: 56371, 180193.07823718005\n",
      "Epoch 25401, Training Loss: 41592, Validation Loss: 54419, 215687.4565259194\n",
      "Epoch 25501, Training Loss: 42009, Validation Loss: 49554, 223568.0265249785\n",
      "Epoch 25601, Training Loss: 41060, Validation Loss: 48497, 179868.08518239288\n",
      "Epoch 25701, Training Loss: 41280, Validation Loss: 62196, 211998.86958921407\n",
      "Epoch 25801, Training Loss: 42240, Validation Loss: 79786, 234357.75174966748\n",
      "Epoch 25901, Training Loss: 40555, Validation Loss: 50845, 227951.20476939494\n",
      "Epoch 26001, Training Loss: 39875, Validation Loss: 56516, 180495.14312525094\n",
      "Epoch 26101, Training Loss: 39970, Validation Loss: 51607, 190345.6253043838\n",
      "Epoch 26201, Training Loss: 41253, Validation Loss: 57300, 204756.59657616168\n",
      "Epoch 26301, Training Loss: 39867, Validation Loss: 49947, 187899.78386905344\n",
      "Epoch 26401, Training Loss: 41633, Validation Loss: 54376, 239331.5866682932\n",
      "Epoch 26501, Training Loss: 40892, Validation Loss: 52269, 212216.55333698078\n",
      "Epoch 26601, Training Loss: 42969, Validation Loss: 62437, 202096.62024089383\n",
      "Epoch 26701, Training Loss: 41213, Validation Loss: 55389, 189923.8998575199\n",
      "Epoch 26801, Training Loss: 39127, Validation Loss: 61544, 163350.99040623338\n",
      "Epoch 26901, Training Loss: 39514, Validation Loss: 60320, 196307.65286860845\n",
      "Epoch 27001, Training Loss: 41720, Validation Loss: 56199, 176933.87270844445\n",
      "Epoch 27101, Training Loss: 43659, Validation Loss: 86894, 225938.95711404897\n",
      "Epoch 27201, Training Loss: 40053, Validation Loss: 52992, 165473.7819931259\n",
      "Epoch 27301, Training Loss: 40417, Validation Loss: 63143, 182684.68829232917\n",
      "Epoch 27401, Training Loss: 40793, Validation Loss: 72740, 209238.1105482187\n",
      "Epoch 27501, Training Loss: 39815, Validation Loss: 52315, 194387.3138875645\n",
      "Epoch 27601, Training Loss: 41122, Validation Loss: 51901, 185926.19080877813\n",
      "Epoch 27701, Training Loss: 41833, Validation Loss: 70867, 198889.95065389542\n",
      "Epoch 27801, Training Loss: 40592, Validation Loss: 58476, 213612.33852130306\n",
      "Epoch 27901, Training Loss: 42268, Validation Loss: 50414, 180801.82228747316\n",
      "Epoch 28001, Training Loss: 38927, Validation Loss: 62443, 177291.9456928507\n",
      "Epoch 28101, Training Loss: 41186, Validation Loss: 54917, 232837.21368781812\n",
      "Epoch 28201, Training Loss: 39438, Validation Loss: 68241, 202832.39106932524\n",
      "Epoch 28301, Training Loss: 42265, Validation Loss: 63460, 223678.9485926169\n",
      "Epoch 28401, Training Loss: 40261, Validation Loss: 51168, 202401.07966476062\n",
      "Epoch 28501, Training Loss: 39047, Validation Loss: 60023, 192344.97621506496\n",
      "Epoch 28601, Training Loss: 38629, Validation Loss: 57882, 192315.39547429842\n",
      "Epoch 28701, Training Loss: 38744, Validation Loss: 50108, 193290.35401712134\n",
      "Epoch 28801, Training Loss: 42099, Validation Loss: 88216, 213959.13183755137\n",
      "Epoch 28901, Training Loss: 40982, Validation Loss: 52695, 224988.90630966212\n",
      "Epoch 29001, Training Loss: 40949, Validation Loss: 52030, 172202.36504163165\n",
      "Epoch 29101, Training Loss: 40510, Validation Loss: 63300, 194256.48390743215\n",
      "Epoch 29201, Training Loss: 38444, Validation Loss: 49464, 196926.53937822906\n",
      "Epoch 29301, Training Loss: 40273, Validation Loss: 51504, 215995.2172343567\n",
      "Epoch 29401, Training Loss: 38344, Validation Loss: 51884, 201975.95278283747\n",
      "Epoch 29501, Training Loss: 40354, Validation Loss: 53691, 231572.1001003665\n",
      "Epoch 29601, Training Loss: 41828, Validation Loss: 61208, 186332.47312598163\n",
      "Epoch 29701, Training Loss: 42328, Validation Loss: 100159, 243520.69085606313\n",
      "Epoch 29801, Training Loss: 40435, Validation Loss: 78556, 196855.89443744146\n",
      "Epoch 29901, Training Loss: 39160, Validation Loss: 55366, 211048.5477692138\n",
      "Epoch 30001, Training Loss: 38437, Validation Loss: 64748, 183319.00030557564\n",
      "Epoch 30101, Training Loss: 38988, Validation Loss: 51567, 204522.36104262283\n",
      "Epoch 30201, Training Loss: 39593, Validation Loss: 48929, 194178.89416789336\n",
      "Epoch 30301, Training Loss: 39186, Validation Loss: 53034, 203452.85128743152\n",
      "Epoch 30401, Training Loss: 39990, Validation Loss: 52822, 187484.77003850855\n",
      "Epoch 30501, Training Loss: 38991, Validation Loss: 80806, 188764.29310045668\n",
      "Epoch 30601, Training Loss: 39441, Validation Loss: 50751, 197641.34340622826\n",
      "Epoch 30701, Training Loss: 39929, Validation Loss: 69269, 206632.7087260495\n",
      "Epoch 30801, Training Loss: 37773, Validation Loss: 63463, 186741.60808079736\n",
      "Epoch 30901, Training Loss: 40819, Validation Loss: 53756, 217913.9770289834\n",
      "Epoch 31001, Training Loss: 38643, Validation Loss: 58466, 215960.65515179562\n",
      "Epoch 31101, Training Loss: 39579, Validation Loss: 60765, 199513.7775012181\n",
      "Epoch 31201, Training Loss: 37461, Validation Loss: 50380, 200875.5072852112\n",
      "Epoch 31301, Training Loss: 38560, Validation Loss: 75247, 196570.3912885098\n",
      "Epoch 31401, Training Loss: 38792, Validation Loss: 52797, 203083.97962249463\n",
      "Epoch 31501, Training Loss: 39563, Validation Loss: 50627, 205044.49636428888\n",
      "Epoch 31601, Training Loss: 38256, Validation Loss: 50378, 237345.51801836232\n",
      "Epoch 31701, Training Loss: 39607, Validation Loss: 51457, 181087.10305676944\n",
      "Epoch 31801, Training Loss: 41468, Validation Loss: 53356, 222789.42001279234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31901, Training Loss: 39163, Validation Loss: 55113, 189550.5086297885\n",
      "Epoch 32001, Training Loss: 39015, Validation Loss: 49848, 190029.40317975904\n",
      "Epoch 32101, Training Loss: 40453, Validation Loss: 53536, 198113.61002740916\n",
      "Epoch 32201, Training Loss: 37465, Validation Loss: 57707, 242700.5461383691\n",
      "Epoch 32301, Training Loss: 37344, Validation Loss: 51654, 198106.20480008432\n",
      "Epoch 32401, Training Loss: 36950, Validation Loss: 51158, 192471.38519710457\n",
      "Epoch 32501, Training Loss: 39824, Validation Loss: 82910, 265541.16963347007\n",
      "Epoch 32601, Training Loss: 39232, Validation Loss: 53319, 224406.07843868367\n",
      "Epoch 32701, Training Loss: 37148, Validation Loss: 60198, 217024.16866326937\n",
      "Epoch 32801, Training Loss: 37803, Validation Loss: 49296, 232985.1017956593\n",
      "Epoch 32901, Training Loss: 40834, Validation Loss: 69723, 193205.70889528215\n",
      "Epoch 33001, Training Loss: 38824, Validation Loss: 53060, 188787.90726184958\n",
      "Epoch 33101, Training Loss: 38070, Validation Loss: 49809, 223553.56541968207\n",
      "Epoch 33201, Training Loss: 39020, Validation Loss: 55364, 218125.69796414813\n",
      "Epoch 33301, Training Loss: 38604, Validation Loss: 63028, 178537.333267665\n",
      "Epoch 33401, Training Loss: 37728, Validation Loss: 63782, 197072.24724707543\n",
      "Epoch 33501, Training Loss: 37782, Validation Loss: 61150, 181210.82258095045\n",
      "Epoch 33601, Training Loss: 38483, Validation Loss: 53702, 199363.6237403341\n",
      "Epoch 33701, Training Loss: 36939, Validation Loss: 72470, 181160.8867831648\n",
      "Epoch 33801, Training Loss: 39155, Validation Loss: 81895, 227057.72160466283\n",
      "Epoch 33901, Training Loss: 40146, Validation Loss: 55745, 196977.0940884198\n",
      "Epoch 34001, Training Loss: 39086, Validation Loss: 53547, 197738.6392854028\n",
      "Epoch 34101, Training Loss: 39289, Validation Loss: 68509, 209519.96121359497\n",
      "Epoch 34201, Training Loss: 38494, Validation Loss: 50764, 158844.43094832086\n",
      "Epoch 34301, Training Loss: 38575, Validation Loss: 59003, 213492.4155776886\n",
      "Epoch 34401, Training Loss: 38680, Validation Loss: 50603, 204149.79796665543\n",
      "Epoch 34501, Training Loss: 38398, Validation Loss: 73020, 214689.42315972387\n",
      "Epoch 34601, Training Loss: 38859, Validation Loss: 52574, 190127.056582896\n",
      "Epoch 34701, Training Loss: 39474, Validation Loss: 82761, 202066.4780711118\n",
      "Epoch 34801, Training Loss: 40072, Validation Loss: 54549, 201863.86410912007\n",
      "Epoch 34901, Training Loss: 37127, Validation Loss: 53001, 204785.02630380949\n",
      "Epoch 35001, Training Loss: 39410, Validation Loss: 57960, 224334.87068751166\n",
      "Epoch 35101, Training Loss: 37434, Validation Loss: 49603, 167747.6235151677\n",
      "Epoch 35201, Training Loss: 37023, Validation Loss: 57064, 210205.97610973616\n",
      "Epoch 35301, Training Loss: 37522, Validation Loss: 80115, 196287.06480980906\n",
      "Epoch 35401, Training Loss: 37243, Validation Loss: 61027, 162219.44492896192\n",
      "Epoch 35501, Training Loss: 37620, Validation Loss: 50302, 211625.15577082985\n",
      "Epoch 35601, Training Loss: 37333, Validation Loss: 53519, 184525.48185934965\n",
      "Epoch 35701, Training Loss: 37848, Validation Loss: 65261, 166086.6557446203\n",
      "Epoch 35801, Training Loss: 36833, Validation Loss: 71550, 208874.29266178553\n",
      "Epoch 35901, Training Loss: 39572, Validation Loss: 81716, 256601.88690516213\n",
      "Epoch 36001, Training Loss: 36505, Validation Loss: 50908, 208644.5567847438\n",
      "Epoch 36101, Training Loss: 37410, Validation Loss: 52221, 192358.78353923012\n",
      "Epoch 36201, Training Loss: 36438, Validation Loss: 49689, 201089.1139512098\n",
      "Epoch 36301, Training Loss: 36170, Validation Loss: 53709, 179442.55897692163\n",
      "Epoch 36401, Training Loss: 38027, Validation Loss: 92583, 208878.7082327865\n",
      "Epoch 36501, Training Loss: 38430, Validation Loss: 53111, 152337.98154539693\n",
      "Epoch 36601, Training Loss: 37496, Validation Loss: 51161, 188504.9733078743\n",
      "Epoch 36701, Training Loss: 37826, Validation Loss: 58476, 191906.06017685638\n",
      "Epoch 36801, Training Loss: 38748, Validation Loss: 82979, 258528.8501894363\n",
      "Epoch 36901, Training Loss: 36873, Validation Loss: 54132, 188105.1206930113\n",
      "Epoch 37001, Training Loss: 36597, Validation Loss: 53146, 202902.28715862674\n",
      "Epoch 37101, Training Loss: 37638, Validation Loss: 58970, 179548.4345700769\n",
      "Epoch 37201, Training Loss: 37429, Validation Loss: 57904, 174615.3205439344\n",
      "Epoch 37301, Training Loss: 38250, Validation Loss: 54386, 263814.68791883317\n",
      "Epoch 37401, Training Loss: 39022, Validation Loss: 86082, 208743.87196643333\n",
      "Epoch 37501, Training Loss: 37032, Validation Loss: 55559, 173499.6211937303\n",
      "Epoch 37601, Training Loss: 36865, Validation Loss: 48876, 214236.8381318402\n",
      "Epoch 37701, Training Loss: 38349, Validation Loss: 79709, 210031.76036066152\n",
      "Epoch 37801, Training Loss: 36825, Validation Loss: 52643, 180482.9476525304\n",
      "Epoch 37901, Training Loss: 39152, Validation Loss: 56634, 216490.08572802282\n",
      "Epoch 38001, Training Loss: 40085, Validation Loss: 55588, 195298.15111869792\n",
      "Epoch 38101, Training Loss: 36197, Validation Loss: 53558, 212910.760720233\n",
      "Epoch 38201, Training Loss: 37949, Validation Loss: 84409, 238347.20509255302\n",
      "Epoch 38301, Training Loss: 37707, Validation Loss: 70377, 236859.76888892215\n",
      "Epoch 38401, Training Loss: 38650, Validation Loss: 59902, 216182.98528153574\n",
      "Epoch 38501, Training Loss: 36660, Validation Loss: 80039, 204855.22466229057\n",
      "Epoch 38601, Training Loss: 37903, Validation Loss: 70107, 217529.27119283334\n",
      "Epoch 38701, Training Loss: 38556, Validation Loss: 66862, 248010.58602418285\n",
      "Epoch 38801, Training Loss: 36901, Validation Loss: 71067, 223547.8878988824\n",
      "Epoch 38901, Training Loss: 36655, Validation Loss: 49472, 173326.56552446933\n",
      "Epoch 39001, Training Loss: 36854, Validation Loss: 50821, 171196.77104178484\n",
      "Epoch 39101, Training Loss: 37563, Validation Loss: 52109, 191491.63032439977\n",
      "Epoch 39201, Training Loss: 38367, Validation Loss: 57184, 200674.00838494775\n",
      "Epoch 39301, Training Loss: 36511, Validation Loss: 53043, 194756.41580053885\n",
      "Epoch 39401, Training Loss: 37401, Validation Loss: 53144, 193521.90793446204\n",
      "Epoch 39501, Training Loss: 36856, Validation Loss: 53177, 235872.52370458352\n",
      "Epoch 39601, Training Loss: 36937, Validation Loss: 62000, 179145.37328090263\n",
      "Epoch 39701, Training Loss: 37407, Validation Loss: 52428, 150649.52357394143\n",
      "Epoch 39801, Training Loss: 38413, Validation Loss: 75625, 203951.2804441319\n",
      "Epoch 39901, Training Loss: 37425, Validation Loss: 55076, 240842.17300518378\n",
      "Epoch 40001, Training Loss: 37057, Validation Loss: 62936, 201342.35552938122\n",
      "Epoch 40101, Training Loss: 38073, Validation Loss: 50669, 198364.76863609534\n",
      "Epoch 40201, Training Loss: 36878, Validation Loss: 54915, 219516.09053221278\n",
      "Epoch 40301, Training Loss: 37978, Validation Loss: 52187, 231784.6518129372\n",
      "Epoch 40401, Training Loss: 37328, Validation Loss: 68741, 218790.5441418068\n",
      "Epoch 40501, Training Loss: 39568, Validation Loss: 55921, 206177.16202414292\n",
      "Epoch 40601, Training Loss: 36420, Validation Loss: 84286, 190238.16251480256\n",
      "Epoch 40701, Training Loss: 37498, Validation Loss: 56750, 190284.10909573393\n",
      "Epoch 40801, Training Loss: 36474, Validation Loss: 55514, 217855.6829408761\n",
      "Epoch 40901, Training Loss: 37739, Validation Loss: 69166, 194914.4349324391\n",
      "Epoch 41001, Training Loss: 35284, Validation Loss: 51253, 184942.02356534507\n",
      "Epoch 41101, Training Loss: 37947, Validation Loss: 60181, 241132.51775063164\n",
      "Epoch 41201, Training Loss: 35962, Validation Loss: 67500, 202314.96481209074\n",
      "Epoch 41301, Training Loss: 35922, Validation Loss: 49176, 193265.72370169082\n",
      "Epoch 41401, Training Loss: 35907, Validation Loss: 50413, 192825.1539975622\n",
      "Epoch 41501, Training Loss: 35521, Validation Loss: 56358, 197315.0112475278\n",
      "Epoch 41601, Training Loss: 37310, Validation Loss: 72576, 184183.30745111546\n",
      "Epoch 41701, Training Loss: 36483, Validation Loss: 51286, 196321.0617483124\n",
      "Epoch 41801, Training Loss: 38781, Validation Loss: 98600, 289067.4410299959\n",
      "Epoch 41901, Training Loss: 35044, Validation Loss: 54029, 159326.7785570698\n",
      "Epoch 42001, Training Loss: 36964, Validation Loss: 68506, 185000.32432493498\n",
      "Epoch 42101, Training Loss: 36684, Validation Loss: 49192, 206704.65405051535\n",
      "Epoch 42201, Training Loss: 38147, Validation Loss: 56359, 205297.74346595712\n",
      "Epoch 42301, Training Loss: 35600, Validation Loss: 55052, 191037.29207081345\n",
      "Epoch 42401, Training Loss: 37209, Validation Loss: 54370, 200911.37974327678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42501, Training Loss: 36743, Validation Loss: 53130, 195074.19021144425\n",
      "Epoch 42601, Training Loss: 35823, Validation Loss: 49707, 233555.4873520833\n",
      "Epoch 42701, Training Loss: 36792, Validation Loss: 49346, 242546.77031250333\n",
      "Epoch 42801, Training Loss: 36265, Validation Loss: 52174, 182979.92012849674\n",
      "Epoch 42901, Training Loss: 35987, Validation Loss: 80399, 195344.34261830014\n",
      "Epoch 43001, Training Loss: 39927, Validation Loss: 51773, 250387.6286066453\n",
      "Epoch 43101, Training Loss: 36488, Validation Loss: 48677, 175110.9149072142\n",
      "Epoch 43201, Training Loss: 37026, Validation Loss: 48959, 208842.17700282388\n",
      "Epoch 43301, Training Loss: 37224, Validation Loss: 48139, 213107.45904718002\n",
      "Epoch 43401, Training Loss: 37098, Validation Loss: 55932, 179506.36358052003\n",
      "Epoch 43501, Training Loss: 36112, Validation Loss: 49642, 194577.74130584928\n",
      "Epoch 43601, Training Loss: 34970, Validation Loss: 59880, 209248.76611381513\n",
      "Epoch 43701, Training Loss: 36745, Validation Loss: 62763, 219289.74498907334\n",
      "Epoch 43801, Training Loss: 38352, Validation Loss: 48639, 231347.57414876393\n",
      "Epoch 43901, Training Loss: 36116, Validation Loss: 72650, 211528.37779850015\n",
      "Epoch 44001, Training Loss: 36631, Validation Loss: 59348, 186508.31855627696\n",
      "Epoch 44101, Training Loss: 36261, Validation Loss: 59662, 166425.96577360798\n",
      "Epoch 44201, Training Loss: 36829, Validation Loss: 49426, 203214.63577491592\n",
      "Epoch 44301, Training Loss: 36131, Validation Loss: 57067, 189285.93002576695\n",
      "Epoch 44401, Training Loss: 35818, Validation Loss: 52102, 175103.79699503482\n",
      "Epoch 44501, Training Loss: 39474, Validation Loss: 51294, 264902.2454204962\n",
      "Epoch 44601, Training Loss: 38550, Validation Loss: 52107, 222916.57504816054\n",
      "Epoch 44701, Training Loss: 34864, Validation Loss: 56857, 197098.55591987105\n",
      "Epoch 44801, Training Loss: 35984, Validation Loss: 49542, 227246.11355124568\n",
      "Epoch 44901, Training Loss: 37118, Validation Loss: 55036, 199091.10670050862\n",
      "Epoch 45001, Training Loss: 36615, Validation Loss: 50751, 199045.74289573438\n",
      "Epoch 45101, Training Loss: 35915, Validation Loss: 50454, 178604.88453257084\n",
      "Epoch 45201, Training Loss: 36949, Validation Loss: 54359, 176122.18818610648\n",
      "Epoch 45301, Training Loss: 34401, Validation Loss: 49179, 194742.34647568836\n",
      "Epoch 45401, Training Loss: 36791, Validation Loss: 60062, 172854.0327438617\n",
      "Epoch 45501, Training Loss: 34215, Validation Loss: 55497, 247723.3371215373\n",
      "Epoch 45601, Training Loss: 37344, Validation Loss: 56708, 203326.08095447428\n",
      "Epoch 45701, Training Loss: 35745, Validation Loss: 59387, 211052.79120282494\n",
      "Epoch 45801, Training Loss: 38385, Validation Loss: 59755, 196501.35990905695\n",
      "Epoch 45901, Training Loss: 36614, Validation Loss: 54504, 176899.31542936203\n",
      "Epoch 46001, Training Loss: 37343, Validation Loss: 77966, 222584.807634889\n",
      "Epoch 46101, Training Loss: 37191, Validation Loss: 54507, 207302.5017673111\n",
      "Epoch 46201, Training Loss: 37764, Validation Loss: 53517, 168723.87176583352\n",
      "Epoch 46301, Training Loss: 38283, Validation Loss: 56278, 194556.3201844564\n",
      "Epoch 46401, Training Loss: 36015, Validation Loss: 49028, 197800.6687061925\n",
      "Epoch 46501, Training Loss: 36211, Validation Loss: 50461, 191745.04102713906\n",
      "Epoch 46601, Training Loss: 37930, Validation Loss: 61261, 218735.75923932376\n",
      "Epoch 46701, Training Loss: 37286, Validation Loss: 52992, 196778.04336949368\n",
      "Epoch 46801, Training Loss: 36195, Validation Loss: 56565, 179108.62875332905\n",
      "Epoch 46901, Training Loss: 37084, Validation Loss: 48518, 219138.99590442338\n",
      "Epoch 47001, Training Loss: 36442, Validation Loss: 70891, 181400.43903939854\n",
      "Epoch 47101, Training Loss: 39890, Validation Loss: 55990, 206614.38760768308\n",
      "Epoch 47201, Training Loss: 36555, Validation Loss: 48281, 182387.11506015164\n",
      "Epoch 47301, Training Loss: 37212, Validation Loss: 80967, 211161.60321627124\n",
      "Epoch 47401, Training Loss: 35657, Validation Loss: 63322, 242179.84384623216\n",
      "Epoch 47501, Training Loss: 36232, Validation Loss: 56751, 166871.0071131527\n",
      "Epoch 47601, Training Loss: 39159, Validation Loss: 54279, 241434.3062924297\n",
      "Epoch 47701, Training Loss: 39787, Validation Loss: 65254, 247626.56324874368\n",
      "Epoch 47801, Training Loss: 36449, Validation Loss: 50418, 187904.76870140372\n",
      "Epoch 47901, Training Loss: 36467, Validation Loss: 57467, 205240.50362688952\n",
      "Epoch 48001, Training Loss: 38094, Validation Loss: 48484, 228356.42259310817\n",
      "Epoch 48101, Training Loss: 35189, Validation Loss: 49964, 207928.5627239731\n",
      "Epoch 48201, Training Loss: 37379, Validation Loss: 62482, 251865.50814080017\n",
      "Epoch 48301, Training Loss: 36449, Validation Loss: 71312, 161284.3765619409\n",
      "Epoch 48401, Training Loss: 36766, Validation Loss: 56866, 184303.5687738536\n",
      "Epoch 48501, Training Loss: 37596, Validation Loss: 51965, 249877.85934256297\n",
      "Epoch 48601, Training Loss: 39125, Validation Loss: 52935, 190658.86131100266\n",
      "Epoch 48701, Training Loss: 34842, Validation Loss: 53048, 177148.09919118843\n",
      "Epoch 48801, Training Loss: 37698, Validation Loss: 47539, 203763.41790352456\n",
      "Epoch 48901, Training Loss: 36512, Validation Loss: 55127, 196794.98683814882\n",
      "Epoch 49001, Training Loss: 36169, Validation Loss: 67231, 201015.71099486155\n",
      "Epoch 49101, Training Loss: 36919, Validation Loss: 82018, 165398.50743528124\n",
      "Epoch 49201, Training Loss: 39614, Validation Loss: 65997, 204958.82229874158\n",
      "Epoch 49301, Training Loss: 36779, Validation Loss: 49773, 176802.86754478223\n",
      "Epoch 49401, Training Loss: 34967, Validation Loss: 52834, 200751.50556859156\n",
      "Epoch 49501, Training Loss: 37972, Validation Loss: 49673, 180151.8505114457\n",
      "Epoch 49601, Training Loss: 36404, Validation Loss: 61869, 193253.74359425358\n",
      "Epoch 49701, Training Loss: 35022, Validation Loss: 51498, 203866.05695256885\n",
      "Epoch 49801, Training Loss: 36403, Validation Loss: 50365, 211939.7859334956\n",
      "Epoch 49901, Training Loss: 36999, Validation Loss: 50317, 210950.7816335199\n",
      "Epoch 50001, Training Loss: 36038, Validation Loss: 57148, 205281.84817047697\n",
      "Epoch 50101, Training Loss: 37235, Validation Loss: 58307, 202223.06496592754\n",
      "Epoch 50201, Training Loss: 36596, Validation Loss: 61384, 187617.41935142744\n",
      "Epoch 50301, Training Loss: 37536, Validation Loss: 60733, 187601.73115567304\n",
      "Epoch 50401, Training Loss: 35342, Validation Loss: 64265, 221475.37876015567\n",
      "Epoch 50501, Training Loss: 37057, Validation Loss: 50198, 212778.55731324936\n",
      "Epoch 50601, Training Loss: 38138, Validation Loss: 51144, 226010.56802880947\n",
      "Epoch 50701, Training Loss: 36346, Validation Loss: 49850, 190852.16211768132\n",
      "Epoch 50801, Training Loss: 37476, Validation Loss: 50876, 205517.90057352776\n",
      "Epoch 50901, Training Loss: 37029, Validation Loss: 50943, 177733.84777322077\n",
      "Epoch 51001, Training Loss: 37812, Validation Loss: 66085, 209646.05634430167\n",
      "Epoch 51101, Training Loss: 35877, Validation Loss: 51122, 176993.63647801834\n",
      "Epoch 51201, Training Loss: 37937, Validation Loss: 59654, 196468.50763450572\n",
      "Epoch 51301, Training Loss: 36780, Validation Loss: 48895, 152412.35121076685\n",
      "Epoch 51401, Training Loss: 38952, Validation Loss: 47870, 146893.3383548331\n",
      "Epoch 51501, Training Loss: 36447, Validation Loss: 48068, 221263.00166163288\n",
      "Epoch 51601, Training Loss: 36708, Validation Loss: 53526, 179705.83090454977\n",
      "Epoch 51701, Training Loss: 35848, Validation Loss: 63277, 192793.83246656763\n",
      "Epoch 51801, Training Loss: 39139, Validation Loss: 49615, 171733.12755150514\n",
      "Epoch 51901, Training Loss: 36774, Validation Loss: 54752, 202057.9673489118\n",
      "Epoch 52001, Training Loss: 35930, Validation Loss: 48512, 167884.88260994057\n",
      "Epoch 52101, Training Loss: 35339, Validation Loss: 51337, 191083.04522524308\n",
      "Epoch 52201, Training Loss: 36779, Validation Loss: 48610, 164984.75666387446\n",
      "Epoch 52301, Training Loss: 36171, Validation Loss: 54650, 213651.05083513097\n",
      "Epoch 52401, Training Loss: 34789, Validation Loss: 50157, 252035.69651648062\n",
      "Epoch 52501, Training Loss: 37318, Validation Loss: 90366, 277268.2659591214\n",
      "Epoch 52601, Training Loss: 35571, Validation Loss: 57237, 230173.3208625043\n",
      "Epoch 52701, Training Loss: 36660, Validation Loss: 74928, 216474.07013941766\n",
      "Epoch 52801, Training Loss: 37117, Validation Loss: 48704, 244650.3036215199\n",
      "Epoch 52901, Training Loss: 35945, Validation Loss: 55695, 220468.85963550696\n",
      "Epoch 53001, Training Loss: 37696, Validation Loss: 55034, 203531.95589773866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53101, Training Loss: 37300, Validation Loss: 50696, 234721.26835054214\n",
      "Epoch 53201, Training Loss: 39021, Validation Loss: 72184, 200331.04483362057\n",
      "Epoch 53301, Training Loss: 35206, Validation Loss: 54801, 198060.07294781445\n",
      "Epoch 53401, Training Loss: 35299, Validation Loss: 55108, 216695.7263244261\n",
      "Epoch 53501, Training Loss: 36331, Validation Loss: 50771, 247655.02506031014\n",
      "Epoch 53601, Training Loss: 36795, Validation Loss: 48966, 221903.56285567232\n",
      "Epoch 53701, Training Loss: 36181, Validation Loss: 52945, 198673.32162501648\n",
      "Epoch 53801, Training Loss: 36094, Validation Loss: 46547, 233511.30308534202\n",
      "Epoch 53901, Training Loss: 34833, Validation Loss: 64202, 196051.72738958852\n",
      "Epoch 54001, Training Loss: 34978, Validation Loss: 55743, 196177.7976378568\n",
      "Epoch 54101, Training Loss: 33771, Validation Loss: 53167, 198937.63138023193\n",
      "Epoch 54201, Training Loss: 35755, Validation Loss: 49835, 208366.50051385214\n",
      "Epoch 54301, Training Loss: 35885, Validation Loss: 50118, 181551.12540626165\n",
      "Epoch 54401, Training Loss: 37504, Validation Loss: 63297, 238039.48690610364\n",
      "Epoch 54501, Training Loss: 36848, Validation Loss: 52605, 198683.02759649148\n",
      "Epoch 54601, Training Loss: 37009, Validation Loss: 57218, 234050.90101160947\n",
      "Epoch 54701, Training Loss: 35507, Validation Loss: 48137, 199992.98217260675\n",
      "Epoch 54801, Training Loss: 39330, Validation Loss: 56931, 197453.9852393541\n",
      "Epoch 54901, Training Loss: 36256, Validation Loss: 57892, 205370.19528900835\n",
      "Epoch 55001, Training Loss: 37706, Validation Loss: 52572, 188660.88350983695\n",
      "Epoch 55101, Training Loss: 37050, Validation Loss: 52044, 174620.7535837715\n",
      "Epoch 55201, Training Loss: 37435, Validation Loss: 54765, 173624.04346412283\n",
      "Epoch 55301, Training Loss: 34982, Validation Loss: 56601, 179045.58228990587\n",
      "Epoch 55401, Training Loss: 37106, Validation Loss: 47424, 199558.74608361442\n",
      "Epoch 55501, Training Loss: 36578, Validation Loss: 81417, 199252.80281141677\n",
      "Epoch 55601, Training Loss: 36698, Validation Loss: 47945, 195133.38455565952\n",
      "Epoch 55701, Training Loss: 35406, Validation Loss: 49772, 192479.3504790675\n",
      "Epoch 55801, Training Loss: 38252, Validation Loss: 135013, 270689.91843290307\n",
      "Epoch 55901, Training Loss: 36353, Validation Loss: 52445, 194235.00997399076\n",
      "Epoch 56001, Training Loss: 37124, Validation Loss: 49882, 191079.40234847122\n",
      "Epoch 56101, Training Loss: 37223, Validation Loss: 46699, 248214.1288583429\n",
      "Epoch 56201, Training Loss: 37130, Validation Loss: 49858, 197583.61845480299\n",
      "Epoch 56301, Training Loss: 34863, Validation Loss: 50316, 175835.86157031765\n",
      "Epoch 56401, Training Loss: 35586, Validation Loss: 58345, 194714.0010486258\n",
      "Epoch 56501, Training Loss: 36173, Validation Loss: 66562, 201805.24283179524\n",
      "Epoch 56601, Training Loss: 34440, Validation Loss: 63423, 196224.4912433431\n",
      "Epoch 56701, Training Loss: 36977, Validation Loss: 72182, 212699.69738054587\n",
      "Epoch 56801, Training Loss: 37019, Validation Loss: 55619, 218976.26954908643\n",
      "Epoch 56901, Training Loss: 34977, Validation Loss: 58365, 176453.56837558185\n",
      "Epoch 57001, Training Loss: 35833, Validation Loss: 50260, 192030.14186297773\n",
      "Epoch 57101, Training Loss: 34541, Validation Loss: 50159, 212770.46821514342\n",
      "Epoch 57201, Training Loss: 34725, Validation Loss: 50839, 207818.92161530664\n",
      "Epoch 57301, Training Loss: 37646, Validation Loss: 52092, 218338.82063781386\n",
      "Epoch 57401, Training Loss: 37266, Validation Loss: 56918, 205362.46378481694\n",
      "Epoch 57501, Training Loss: 35529, Validation Loss: 47720, 187052.79570708898\n",
      "Epoch 57601, Training Loss: 36594, Validation Loss: 47101, 194521.69861041274\n",
      "Epoch 57701, Training Loss: 35885, Validation Loss: 53272, 211430.88855664435\n",
      "Epoch 57801, Training Loss: 36186, Validation Loss: 53876, 221301.5942381951\n",
      "Epoch 57901, Training Loss: 36586, Validation Loss: 52514, 201283.29412360446\n",
      "Epoch 58001, Training Loss: 37970, Validation Loss: 49074, 249021.46105625606\n",
      "Epoch 58101, Training Loss: 36353, Validation Loss: 59368, 231377.39630129383\n",
      "Epoch 58201, Training Loss: 37032, Validation Loss: 51661, 209481.15708788068\n",
      "Epoch 58301, Training Loss: 34092, Validation Loss: 55101, 205311.0687573319\n",
      "Epoch 58401, Training Loss: 34831, Validation Loss: 49412, 184694.377273433\n",
      "Epoch 58501, Training Loss: 38587, Validation Loss: 50924, 188134.67126647735\n",
      "Epoch 58601, Training Loss: 36984, Validation Loss: 53253, 190169.97127759786\n",
      "Epoch 58701, Training Loss: 35718, Validation Loss: 60809, 188037.13567325138\n",
      "Epoch 58801, Training Loss: 34780, Validation Loss: 56578, 171574.7963466828\n",
      "Epoch 58901, Training Loss: 36163, Validation Loss: 55773, 218926.56110128225\n",
      "Epoch 59001, Training Loss: 37821, Validation Loss: 55318, 189250.90571521525\n",
      "Epoch 59101, Training Loss: 35005, Validation Loss: 46474, 168697.71811143187\n",
      "Epoch 59201, Training Loss: 35869, Validation Loss: 52052, 201265.39737320514\n",
      "Epoch 59301, Training Loss: 35805, Validation Loss: 48773, 161525.33085769173\n",
      "Epoch 59401, Training Loss: 35455, Validation Loss: 47256, 175630.35814009001\n",
      "Epoch 59501, Training Loss: 34547, Validation Loss: 49766, 175557.12701950714\n",
      "Epoch 59601, Training Loss: 35095, Validation Loss: 52111, 211924.61121997776\n",
      "Epoch 59701, Training Loss: 35828, Validation Loss: 47689, 195544.69324498952\n",
      "Epoch 59801, Training Loss: 36363, Validation Loss: 51365, 240761.7494423011\n",
      "Epoch 59901, Training Loss: 36451, Validation Loss: 60938, 204435.48099979348\n",
      "Epoch 60001, Training Loss: 41322, Validation Loss: 66467, 209042.52693844246\n",
      "Epoch 60101, Training Loss: 36640, Validation Loss: 49720, 191120.5055064417\n",
      "Epoch 60201, Training Loss: 35803, Validation Loss: 50166, 199569.7474344554\n",
      "Epoch 60301, Training Loss: 36600, Validation Loss: 50970, 183000.6339151248\n",
      "Epoch 60401, Training Loss: 37026, Validation Loss: 49049, 233832.16442554144\n",
      "Epoch 60501, Training Loss: 34985, Validation Loss: 51361, 159429.50072441043\n",
      "Epoch 60601, Training Loss: 34374, Validation Loss: 47491, 190749.03210863352\n",
      "Epoch 60701, Training Loss: 35460, Validation Loss: 47859, 199200.69859305382\n",
      "Epoch 60801, Training Loss: 35195, Validation Loss: 53641, 260180.53692943722\n",
      "Epoch 60901, Training Loss: 36370, Validation Loss: 51718, 240700.49726461945\n",
      "Epoch 61001, Training Loss: 34536, Validation Loss: 48357, 215157.10511791543\n",
      "Epoch 61101, Training Loss: 35321, Validation Loss: 54214, 182473.9729965036\n",
      "Epoch 61201, Training Loss: 36717, Validation Loss: 78829, 183127.81937530186\n",
      "Epoch 61301, Training Loss: 37718, Validation Loss: 48146, 220194.6497518093\n",
      "Epoch 61401, Training Loss: 36312, Validation Loss: 58200, 212607.33417559255\n",
      "Epoch 61501, Training Loss: 36569, Validation Loss: 65571, 223719.991647254\n",
      "Epoch 61601, Training Loss: 36996, Validation Loss: 50389, 213287.99594449546\n",
      "Epoch 61701, Training Loss: 36990, Validation Loss: 49897, 202337.397319718\n",
      "Epoch 61801, Training Loss: 34941, Validation Loss: 78685, 176101.0720156496\n",
      "Epoch 61901, Training Loss: 34294, Validation Loss: 51613, 178910.7202464809\n",
      "Epoch 62001, Training Loss: 34256, Validation Loss: 50089, 184585.80522122126\n",
      "Epoch 62101, Training Loss: 35270, Validation Loss: 52929, 202386.17385928423\n",
      "Epoch 62201, Training Loss: 34103, Validation Loss: 48652, 173601.98814449066\n",
      "Epoch 62301, Training Loss: 35515, Validation Loss: 61090, 221257.65004551565\n",
      "Epoch 62401, Training Loss: 35850, Validation Loss: 58580, 274021.9607070476\n",
      "Epoch 62501, Training Loss: 37064, Validation Loss: 73692, 181490.3568798796\n",
      "Epoch 62601, Training Loss: 35140, Validation Loss: 51967, 176455.22142405272\n",
      "Epoch 62701, Training Loss: 35642, Validation Loss: 50333, 178855.63908066068\n",
      "Epoch 62801, Training Loss: 33724, Validation Loss: 77605, 220647.7634919119\n",
      "Epoch 62901, Training Loss: 34829, Validation Loss: 56599, 207034.9674416378\n",
      "Epoch 63001, Training Loss: 37213, Validation Loss: 55033, 241729.93220322565\n",
      "Epoch 63101, Training Loss: 35572, Validation Loss: 71407, 197809.17260059586\n",
      "Epoch 63201, Training Loss: 35789, Validation Loss: 49008, 163749.61982272935\n",
      "Epoch 63301, Training Loss: 35255, Validation Loss: 49369, 196968.92393210484\n",
      "Epoch 63401, Training Loss: 35770, Validation Loss: 51043, 203550.82490421005\n",
      "Epoch 63501, Training Loss: 34370, Validation Loss: 68154, 179670.82148141533\n",
      "Epoch 63601, Training Loss: 36061, Validation Loss: 84435, 260100.27596696047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63701, Training Loss: 33714, Validation Loss: 49332, 175771.44943534804\n",
      "Epoch 63801, Training Loss: 35678, Validation Loss: 63406, 205725.79511012547\n",
      "Epoch 63901, Training Loss: 36042, Validation Loss: 49354, 161106.3901559714\n",
      "Epoch 64001, Training Loss: 33828, Validation Loss: 73969, 202296.68114008673\n",
      "Epoch 64101, Training Loss: 35894, Validation Loss: 60819, 218370.24326131624\n",
      "Epoch 64201, Training Loss: 36006, Validation Loss: 53855, 181567.42357295295\n",
      "Epoch 64301, Training Loss: 35496, Validation Loss: 48421, 195690.65113842717\n",
      "Epoch 64401, Training Loss: 34816, Validation Loss: 52423, 236176.4783582608\n",
      "Epoch 64501, Training Loss: 35773, Validation Loss: 52410, 215519.57148270184\n",
      "Epoch 64601, Training Loss: 36320, Validation Loss: 79813, 213447.05717312146\n",
      "Epoch 64701, Training Loss: 38020, Validation Loss: 55399, 219165.5287299334\n",
      "Epoch 64801, Training Loss: 38586, Validation Loss: 47499, 226851.6388218597\n",
      "Epoch 64901, Training Loss: 35832, Validation Loss: 57128, 196175.47989664602\n",
      "Epoch 65001, Training Loss: 35260, Validation Loss: 51847, 176368.9849499759\n",
      "Epoch 65101, Training Loss: 35261, Validation Loss: 50412, 163099.98437525448\n",
      "Epoch 65201, Training Loss: 34516, Validation Loss: 48416, 197906.3937921702\n",
      "Epoch 65301, Training Loss: 35402, Validation Loss: 57580, 178528.2346041349\n",
      "Epoch 65401, Training Loss: 35297, Validation Loss: 49770, 188607.74031196712\n",
      "Epoch 65501, Training Loss: 37345, Validation Loss: 47762, 213875.40519489872\n",
      "Epoch 65601, Training Loss: 34737, Validation Loss: 69805, 196492.5205892279\n",
      "Epoch 65701, Training Loss: 34313, Validation Loss: 49396, 175629.6058531741\n",
      "Epoch 65801, Training Loss: 35250, Validation Loss: 50051, 158878.38158782094\n",
      "Epoch 65901, Training Loss: 35360, Validation Loss: 55607, 184810.71605356628\n",
      "Epoch 66001, Training Loss: 34142, Validation Loss: 50932, 206753.5839665618\n",
      "Epoch 66101, Training Loss: 35630, Validation Loss: 72989, 216130.7349665664\n",
      "Epoch 66201, Training Loss: 34520, Validation Loss: 61423, 186702.14112226412\n",
      "Epoch 66301, Training Loss: 35598, Validation Loss: 52297, 196312.64709692146\n",
      "Epoch 66401, Training Loss: 34312, Validation Loss: 62640, 214720.31297922754\n",
      "Epoch 66501, Training Loss: 39382, Validation Loss: 51206, 228186.05803538382\n",
      "Epoch 66601, Training Loss: 33839, Validation Loss: 70502, 190295.3108649941\n",
      "Epoch 66701, Training Loss: 36229, Validation Loss: 51952, 217320.83349975804\n",
      "Epoch 66801, Training Loss: 34479, Validation Loss: 50314, 185866.51982373404\n",
      "Epoch 66901, Training Loss: 35212, Validation Loss: 49323, 216194.30649701788\n",
      "Epoch 67001, Training Loss: 35959, Validation Loss: 62038, 172612.47387549427\n",
      "Epoch 67101, Training Loss: 37255, Validation Loss: 52267, 207544.49801154423\n",
      "Epoch 67201, Training Loss: 34873, Validation Loss: 51777, 170926.41786610236\n",
      "Epoch 67301, Training Loss: 34484, Validation Loss: 48206, 164305.00494031617\n",
      "Epoch 67401, Training Loss: 45790, Validation Loss: 55632, 119588.40291087599\n",
      "Epoch 67501, Training Loss: 37841, Validation Loss: 54046, 181701.30167189438\n",
      "Epoch 67601, Training Loss: 34257, Validation Loss: 52359, 174603.15030955314\n",
      "Epoch 67701, Training Loss: 36102, Validation Loss: 57901, 209898.74886461566\n",
      "Epoch 67801, Training Loss: 34650, Validation Loss: 49739, 182051.495882437\n",
      "Epoch 67901, Training Loss: 35438, Validation Loss: 62276, 233135.74302290683\n",
      "Epoch 68001, Training Loss: 36956, Validation Loss: 79096, 233298.5677240733\n",
      "Epoch 68101, Training Loss: 39389, Validation Loss: 50922, 245955.46638425929\n",
      "Epoch 68201, Training Loss: 35512, Validation Loss: 49447, 180367.5137622664\n",
      "Epoch 68301, Training Loss: 36687, Validation Loss: 62256, 204358.12831955517\n",
      "Epoch 68401, Training Loss: 35871, Validation Loss: 52497, 194309.84308273694\n",
      "Epoch 68501, Training Loss: 35549, Validation Loss: 50160, 182119.42578346198\n",
      "Epoch 68601, Training Loss: 36038, Validation Loss: 49133, 190741.05653510883\n",
      "Epoch 68701, Training Loss: 34452, Validation Loss: 69912, 184082.34874240682\n",
      "Epoch 68801, Training Loss: 35359, Validation Loss: 49565, 262988.1716707189\n",
      "Epoch 68901, Training Loss: 35345, Validation Loss: 69033, 192962.59078531968\n",
      "Epoch 69001, Training Loss: 35918, Validation Loss: 79214, 202988.38865927936\n",
      "Epoch 69101, Training Loss: 35438, Validation Loss: 75896, 269226.2795220246\n",
      "Epoch 69201, Training Loss: 36299, Validation Loss: 48416, 233867.98882018845\n",
      "Epoch 69301, Training Loss: 35272, Validation Loss: 53873, 196331.53564637856\n",
      "Epoch 69401, Training Loss: 35628, Validation Loss: 51439, 182325.3790211886\n",
      "Epoch 69501, Training Loss: 36915, Validation Loss: 48831, 214626.32868032632\n",
      "Epoch 69601, Training Loss: 36162, Validation Loss: 50426, 198001.37695457507\n",
      "Epoch 69701, Training Loss: 34186, Validation Loss: 72448, 213764.79107054297\n",
      "Epoch 69801, Training Loss: 36241, Validation Loss: 61694, 197448.71464267283\n",
      "Epoch 69901, Training Loss: 34312, Validation Loss: 59540, 195554.4542321911\n",
      "Epoch 70001, Training Loss: 36077, Validation Loss: 48176, 215799.8719175224\n",
      "Epoch 70101, Training Loss: 34460, Validation Loss: 51252, 212077.39365284584\n",
      "Epoch 70201, Training Loss: 35374, Validation Loss: 48094, 177637.60790181844\n",
      "Epoch 70301, Training Loss: 35674, Validation Loss: 53520, 196649.3120556647\n",
      "Epoch 70401, Training Loss: 35827, Validation Loss: 49573, 204970.0717065483\n",
      "Epoch 70501, Training Loss: 34814, Validation Loss: 61226, 190851.48160180438\n",
      "Epoch 70601, Training Loss: 34386, Validation Loss: 53027, 178455.69066635438\n",
      "Epoch 70701, Training Loss: 37073, Validation Loss: 50544, 165025.75909922627\n",
      "Epoch 70801, Training Loss: 35481, Validation Loss: 52509, 185899.27105309188\n",
      "Epoch 70901, Training Loss: 34152, Validation Loss: 52804, 183387.71016737618\n",
      "Epoch 71001, Training Loss: 35510, Validation Loss: 66530, 163680.11989613372\n",
      "Epoch 71101, Training Loss: 39115, Validation Loss: 68465, 277999.3982324145\n",
      "Epoch 71201, Training Loss: 36978, Validation Loss: 49164, 202193.54854023698\n",
      "Epoch 71301, Training Loss: 33074, Validation Loss: 53221, 179430.4761796495\n",
      "Epoch 71401, Training Loss: 34055, Validation Loss: 53163, 189247.88079363047\n",
      "Epoch 71501, Training Loss: 34325, Validation Loss: 61695, 217198.6550531908\n",
      "Epoch 71601, Training Loss: 35305, Validation Loss: 52058, 209868.36307653703\n",
      "Epoch 71701, Training Loss: 34512, Validation Loss: 49201, 168848.509515642\n",
      "Epoch 71801, Training Loss: 50117, Validation Loss: 63558, 40218.978034768814\n",
      "Epoch 71901, Training Loss: 45613, Validation Loss: 56441, 51601.17587359145\n",
      "Epoch 72001, Training Loss: 41869, Validation Loss: 51708, 72718.18408087612\n",
      "Epoch 72101, Training Loss: 39497, Validation Loss: 56493, 112330.80274125379\n",
      "Epoch 72201, Training Loss: 38689, Validation Loss: 59620, 118021.44860273197\n",
      "Epoch 72301, Training Loss: 37250, Validation Loss: 50596, 132063.23651256348\n",
      "Epoch 72401, Training Loss: 35824, Validation Loss: 51886, 114047.33212967451\n",
      "Epoch 72501, Training Loss: 37353, Validation Loss: 50861, 141645.350834158\n",
      "Epoch 72601, Training Loss: 36629, Validation Loss: 49974, 182125.85946140543\n",
      "Epoch 72701, Training Loss: 34771, Validation Loss: 52962, 182213.33007802567\n",
      "Epoch 72801, Training Loss: 35663, Validation Loss: 68128, 169285.85302681933\n",
      "Epoch 72901, Training Loss: 36052, Validation Loss: 50343, 211366.35592446415\n",
      "Epoch 73001, Training Loss: 38229, Validation Loss: 62575, 229999.88711656572\n",
      "Epoch 73101, Training Loss: 36729, Validation Loss: 82475, 197931.93534854354\n",
      "Epoch 73201, Training Loss: 35992, Validation Loss: 49981, 196468.36839707277\n",
      "Epoch 73301, Training Loss: 37303, Validation Loss: 60862, 205888.9280043113\n",
      "Epoch 73401, Training Loss: 34937, Validation Loss: 51187, 200469.8613309342\n",
      "Epoch 73501, Training Loss: 35431, Validation Loss: 52001, 182793.09529194536\n",
      "Epoch 73601, Training Loss: 35748, Validation Loss: 54096, 204886.37771137792\n",
      "Epoch 73701, Training Loss: 35909, Validation Loss: 56008, 164905.018067144\n",
      "Epoch 73801, Training Loss: 34031, Validation Loss: 51405, 187217.95705414357\n",
      "Epoch 73901, Training Loss: 35959, Validation Loss: 49457, 192347.83942091707\n",
      "Epoch 74001, Training Loss: 38462, Validation Loss: 59039, 202785.29520077258\n",
      "Epoch 74101, Training Loss: 34787, Validation Loss: 58897, 212120.42056606442\n",
      "Epoch 74201, Training Loss: 38880, Validation Loss: 59638, 279483.5272527615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74301, Training Loss: 35229, Validation Loss: 87961, 188899.76251329193\n",
      "Epoch 74401, Training Loss: 36002, Validation Loss: 49424, 181575.80207440504\n",
      "Epoch 74501, Training Loss: 36065, Validation Loss: 54748, 229076.85754659332\n",
      "Epoch 74601, Training Loss: 34715, Validation Loss: 66702, 191233.6623499143\n",
      "Epoch 74701, Training Loss: 35710, Validation Loss: 51345, 212069.98909472112\n",
      "Epoch 74801, Training Loss: 34113, Validation Loss: 85377, 195611.9466261235\n",
      "Epoch 74901, Training Loss: 35737, Validation Loss: 89008, 192456.23038455963\n",
      "Epoch 75001, Training Loss: 35776, Validation Loss: 52226, 217772.17422725365\n",
      "Epoch 75101, Training Loss: 34942, Validation Loss: 50213, 182260.8109508294\n",
      "Epoch 75201, Training Loss: 35070, Validation Loss: 52435, 202657.81725905082\n",
      "Epoch 75301, Training Loss: 36244, Validation Loss: 67106, 188014.3927322485\n",
      "Epoch 75401, Training Loss: 33932, Validation Loss: 70121, 195829.02073790526\n",
      "Epoch 75501, Training Loss: 36595, Validation Loss: 86275, 229895.09453929003\n",
      "Epoch 75601, Training Loss: 35530, Validation Loss: 65215, 210324.25693550962\n",
      "Epoch 75701, Training Loss: 34243, Validation Loss: 55154, 207868.81919457283\n",
      "Epoch 75801, Training Loss: 34734, Validation Loss: 51226, 171151.51249276646\n",
      "Epoch 75901, Training Loss: 35361, Validation Loss: 51852, 237713.86853651574\n",
      "Epoch 76001, Training Loss: 34633, Validation Loss: 49105, 215605.58860409065\n",
      "Epoch 76101, Training Loss: 35137, Validation Loss: 49088, 177290.40531898497\n",
      "Epoch 76201, Training Loss: 33628, Validation Loss: 52773, 188362.68037176947\n",
      "Epoch 76301, Training Loss: 33767, Validation Loss: 52116, 201639.43327441855\n",
      "Epoch 76401, Training Loss: 36094, Validation Loss: 54861, 230247.91980613323\n",
      "Epoch 76501, Training Loss: 36831, Validation Loss: 55185, 199132.4527071976\n",
      "Epoch 76601, Training Loss: 35326, Validation Loss: 49481, 203521.3498977206\n",
      "Epoch 76701, Training Loss: 35538, Validation Loss: 80548, 243706.75497869423\n",
      "Epoch 76801, Training Loss: 35131, Validation Loss: 52896, 218454.78497446366\n",
      "Epoch 76901, Training Loss: 36845, Validation Loss: 87476, 270593.4354914291\n",
      "Epoch 77001, Training Loss: 37453, Validation Loss: 50897, 168027.81591194496\n",
      "Epoch 77101, Training Loss: 36383, Validation Loss: 52784, 194652.63850602365\n",
      "Epoch 77201, Training Loss: 34413, Validation Loss: 53794, 155788.14723222103\n",
      "Epoch 77301, Training Loss: 37852, Validation Loss: 64180, 220928.86209968527\n",
      "Epoch 77401, Training Loss: 35574, Validation Loss: 59945, 177104.9888125048\n",
      "Epoch 77501, Training Loss: 35436, Validation Loss: 56273, 183755.393823317\n",
      "Epoch 77601, Training Loss: 34979, Validation Loss: 47830, 202365.03256412983\n",
      "Epoch 77701, Training Loss: 35850, Validation Loss: 77574, 207356.249168092\n",
      "Epoch 77801, Training Loss: 33927, Validation Loss: 55951, 203578.33462616534\n",
      "Epoch 77901, Training Loss: 34958, Validation Loss: 58294, 213025.92899041294\n",
      "Epoch 78001, Training Loss: 36217, Validation Loss: 53929, 229592.28781637325\n",
      "Epoch 78101, Training Loss: 34567, Validation Loss: 67049, 201312.83042037324\n",
      "Epoch 78201, Training Loss: 35768, Validation Loss: 50133, 195081.76792920454\n",
      "Epoch 78301, Training Loss: 33968, Validation Loss: 54699, 148469.2417061832\n",
      "Epoch 78401, Training Loss: 34644, Validation Loss: 55854, 231971.91544778753\n",
      "Epoch 78501, Training Loss: 36608, Validation Loss: 52662, 207165.50721100732\n",
      "Epoch 78601, Training Loss: 34562, Validation Loss: 50457, 187334.20895313748\n",
      "Epoch 78701, Training Loss: 34919, Validation Loss: 53010, 209968.05889099083\n",
      "Epoch 78801, Training Loss: 34246, Validation Loss: 57798, 193936.83578877503\n",
      "Epoch 78901, Training Loss: 35860, Validation Loss: 49019, 202762.12039205854\n",
      "Epoch 79001, Training Loss: 34363, Validation Loss: 79429, 194430.63929592608\n",
      "Epoch 79101, Training Loss: 34713, Validation Loss: 51206, 152917.65912057617\n",
      "Epoch 79201, Training Loss: 32642, Validation Loss: 49908, 167809.9670657075\n",
      "Epoch 79301, Training Loss: 35863, Validation Loss: 61782, 239420.6467456255\n",
      "Epoch 79401, Training Loss: 33545, Validation Loss: 49378, 184058.36958811805\n",
      "Epoch 79501, Training Loss: 36068, Validation Loss: 51266, 158752.69181179578\n",
      "Epoch 79601, Training Loss: 35456, Validation Loss: 50403, 224756.34599927344\n",
      "Epoch 79701, Training Loss: 35217, Validation Loss: 51779, 220803.28001517348\n",
      "Epoch 79801, Training Loss: 34187, Validation Loss: 91271, 214065.6335614283\n",
      "Epoch 79901, Training Loss: 34438, Validation Loss: 55765, 165492.65676987966\n",
      "Epoch 80001, Training Loss: 33989, Validation Loss: 50869, 195915.49170514048\n",
      "Epoch 80101, Training Loss: 34957, Validation Loss: 49515, 178421.38835129188\n",
      "Epoch 80201, Training Loss: 36050, Validation Loss: 67614, 205905.4549699258\n",
      "Epoch 80301, Training Loss: 33923, Validation Loss: 48642, 205206.3535325552\n",
      "Epoch 80401, Training Loss: 34713, Validation Loss: 49905, 186065.49534104706\n",
      "Epoch 80501, Training Loss: 36508, Validation Loss: 51423, 224038.18374757122\n",
      "Epoch 80601, Training Loss: 35392, Validation Loss: 51805, 244331.7252320222\n",
      "Epoch 80701, Training Loss: 35428, Validation Loss: 61840, 181657.24532032144\n",
      "Epoch 80801, Training Loss: 34886, Validation Loss: 49359, 203562.2850191316\n",
      "Epoch 80901, Training Loss: 33852, Validation Loss: 58170, 184288.3775896414\n",
      "Epoch 81001, Training Loss: 35066, Validation Loss: 53884, 200206.32037800946\n",
      "Epoch 81101, Training Loss: 35764, Validation Loss: 60697, 210072.48593268858\n",
      "Epoch 81201, Training Loss: 34695, Validation Loss: 67500, 181908.30056336868\n",
      "Epoch 81301, Training Loss: 36827, Validation Loss: 50138, 243452.55061780938\n",
      "Epoch 81401, Training Loss: 36672, Validation Loss: 56130, 241084.44897291868\n",
      "Epoch 81501, Training Loss: 34194, Validation Loss: 57545, 185427.67652005007\n",
      "Epoch 81601, Training Loss: 35477, Validation Loss: 52731, 234399.36884772035\n",
      "Epoch 81701, Training Loss: 32958, Validation Loss: 54074, 179618.0778059043\n",
      "Epoch 81801, Training Loss: 35489, Validation Loss: 50605, 201888.10455223627\n",
      "Epoch 81901, Training Loss: 34198, Validation Loss: 87134, 204673.39544941732\n",
      "Epoch 82001, Training Loss: 35683, Validation Loss: 50804, 160953.86884392114\n",
      "Epoch 82101, Training Loss: 37108, Validation Loss: 59453, 231243.82294441134\n",
      "Epoch 82201, Training Loss: 36439, Validation Loss: 60549, 213199.0671212074\n",
      "Epoch 82301, Training Loss: 33432, Validation Loss: 55476, 222948.3450625645\n",
      "Epoch 82401, Training Loss: 35638, Validation Loss: 60736, 209239.4237008654\n",
      "Epoch 82501, Training Loss: 35907, Validation Loss: 54018, 174015.13153115302\n",
      "Epoch 82601, Training Loss: 35354, Validation Loss: 55199, 183543.24191356543\n",
      "Epoch 82701, Training Loss: 35253, Validation Loss: 55414, 195856.28074103533\n",
      "Epoch 82801, Training Loss: 34450, Validation Loss: 52414, 237931.40338375908\n",
      "Epoch 82901, Training Loss: 36565, Validation Loss: 75111, 211094.85526710682\n",
      "Epoch 83001, Training Loss: 35638, Validation Loss: 52895, 187082.12649156534\n",
      "Epoch 83101, Training Loss: 36476, Validation Loss: 55783, 237378.26821330158\n",
      "Epoch 83201, Training Loss: 35607, Validation Loss: 75632, 212172.6692705752\n",
      "Epoch 83301, Training Loss: 36133, Validation Loss: 50972, 202312.59096898147\n",
      "Epoch 83401, Training Loss: 34259, Validation Loss: 50156, 217435.55996564694\n",
      "Epoch 83501, Training Loss: 36823, Validation Loss: 52828, 212567.97573145968\n",
      "Epoch 83601, Training Loss: 36225, Validation Loss: 49739, 202021.23124816662\n",
      "Epoch 83701, Training Loss: 34319, Validation Loss: 51171, 218424.04458268793\n",
      "Epoch 83801, Training Loss: 35001, Validation Loss: 52869, 224565.16679037563\n",
      "Epoch 83901, Training Loss: 34689, Validation Loss: 50391, 193252.94744459342\n",
      "Epoch 84001, Training Loss: 33418, Validation Loss: 56750, 200910.07593147934\n",
      "Epoch 84101, Training Loss: 34887, Validation Loss: 51115, 231251.11703271265\n",
      "Epoch 84201, Training Loss: 34388, Validation Loss: 50767, 173875.4187333456\n",
      "Epoch 84301, Training Loss: 33348, Validation Loss: 73719, 172028.63574373213\n",
      "Epoch 84401, Training Loss: 34239, Validation Loss: 57018, 211934.87950155183\n",
      "Epoch 84501, Training Loss: 36032, Validation Loss: 57093, 155937.49126411218\n",
      "Epoch 84601, Training Loss: 34435, Validation Loss: 51173, 201991.0785902096\n",
      "Epoch 84701, Training Loss: 34986, Validation Loss: 62826, 205920.52118636752\n",
      "Epoch 84801, Training Loss: 35332, Validation Loss: 52217, 176434.65521850242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84901, Training Loss: 35310, Validation Loss: 54461, 260818.94733146136\n",
      "Epoch 85001, Training Loss: 34949, Validation Loss: 50169, 197598.0610684087\n",
      "Epoch 85101, Training Loss: 35325, Validation Loss: 55861, 198286.0704590968\n",
      "Epoch 85201, Training Loss: 33290, Validation Loss: 50870, 184647.1153730302\n",
      "Epoch 85301, Training Loss: 35365, Validation Loss: 61251, 185706.42007498676\n",
      "Epoch 85401, Training Loss: 34041, Validation Loss: 59331, 191721.3345840325\n",
      "Epoch 85501, Training Loss: 35906, Validation Loss: 53400, 223978.08735841908\n",
      "Epoch 85601, Training Loss: 34211, Validation Loss: 52307, 193464.95016932467\n",
      "Epoch 85701, Training Loss: 35277, Validation Loss: 49927, 211059.6985693467\n",
      "Epoch 85801, Training Loss: 36250, Validation Loss: 51501, 225863.26154748994\n",
      "Epoch 85901, Training Loss: 35668, Validation Loss: 75300, 223000.45646838986\n",
      "Epoch 86001, Training Loss: 35170, Validation Loss: 71724, 193753.01195548082\n",
      "Epoch 86101, Training Loss: 35331, Validation Loss: 50260, 177820.99847843254\n",
      "Epoch 86201, Training Loss: 35010, Validation Loss: 51314, 164597.5888843499\n",
      "Epoch 86301, Training Loss: 35215, Validation Loss: 53394, 189425.06023184964\n",
      "Epoch 86401, Training Loss: 37308, Validation Loss: 49753, 204670.8837849237\n",
      "Epoch 86501, Training Loss: 32861, Validation Loss: 50401, 193369.24374947112\n",
      "Epoch 86601, Training Loss: 35468, Validation Loss: 53306, 174226.12394116682\n",
      "Epoch 86701, Training Loss: 34479, Validation Loss: 53457, 184315.2671601498\n",
      "Epoch 86801, Training Loss: 37045, Validation Loss: 57470, 192788.39441622564\n",
      "Epoch 86901, Training Loss: 36402, Validation Loss: 59856, 255890.33910384154\n",
      "Epoch 87001, Training Loss: 33416, Validation Loss: 58402, 198831.94007575783\n",
      "Epoch 87101, Training Loss: 32981, Validation Loss: 50653, 179532.52666103598\n",
      "Epoch 87201, Training Loss: 34644, Validation Loss: 57065, 177965.551070728\n",
      "Epoch 87301, Training Loss: 34792, Validation Loss: 82873, 201833.84808702278\n",
      "Epoch 87401, Training Loss: 36510, Validation Loss: 49135, 200221.37955521862\n",
      "Epoch 87501, Training Loss: 32640, Validation Loss: 54670, 172812.2125365323\n",
      "Epoch 87601, Training Loss: 35259, Validation Loss: 55362, 172735.75346682256\n",
      "Epoch 87701, Training Loss: 34696, Validation Loss: 56556, 192677.84326913615\n",
      "Epoch 87801, Training Loss: 34094, Validation Loss: 53416, 195018.0814488812\n",
      "Epoch 87901, Training Loss: 35418, Validation Loss: 54693, 204343.62523120246\n",
      "Epoch 88001, Training Loss: 33275, Validation Loss: 49843, 182241.6111830998\n",
      "Epoch 88101, Training Loss: 36900, Validation Loss: 52007, 223282.74478674834\n",
      "Epoch 88201, Training Loss: 33989, Validation Loss: 51106, 177197.89811644133\n",
      "Epoch 88301, Training Loss: 34613, Validation Loss: 50347, 175280.0896471869\n",
      "Epoch 88401, Training Loss: 34997, Validation Loss: 60295, 211436.43728884772\n",
      "Epoch 88501, Training Loss: 34519, Validation Loss: 53241, 208805.6340757131\n",
      "Epoch 88601, Training Loss: 34843, Validation Loss: 62174, 205992.22418709286\n",
      "Epoch 88701, Training Loss: 34825, Validation Loss: 49735, 232313.19663458376\n",
      "Epoch 88801, Training Loss: 35180, Validation Loss: 50629, 193215.73812670916\n",
      "Epoch 88901, Training Loss: 37023, Validation Loss: 53023, 190427.8403281808\n",
      "Epoch 89001, Training Loss: 32878, Validation Loss: 53993, 168323.28609573576\n",
      "Epoch 89101, Training Loss: 34406, Validation Loss: 50183, 206107.50967484046\n",
      "Epoch 89201, Training Loss: 35249, Validation Loss: 51638, 196747.35377727484\n",
      "Epoch 89301, Training Loss: 35697, Validation Loss: 52033, 193935.430825104\n",
      "Epoch 89401, Training Loss: 34255, Validation Loss: 63635, 168394.6843070369\n",
      "Epoch 89501, Training Loss: 33731, Validation Loss: 72045, 240891.12225898882\n",
      "Epoch 89601, Training Loss: 34365, Validation Loss: 60140, 216086.15349612632\n",
      "Epoch 89701, Training Loss: 35431, Validation Loss: 58844, 217757.5811446456\n",
      "Epoch 89801, Training Loss: 33557, Validation Loss: 54547, 209128.150992015\n",
      "Epoch 89901, Training Loss: 34709, Validation Loss: 55199, 225684.04148080212\n",
      "Epoch 90001, Training Loss: 33890, Validation Loss: 51895, 195289.81152385336\n",
      "Epoch 90101, Training Loss: 36974, Validation Loss: 67113, 206729.6664882592\n",
      "Epoch 90201, Training Loss: 34214, Validation Loss: 50275, 218117.65199097487\n",
      "Epoch 90301, Training Loss: 38440, Validation Loss: 56055, 263957.5946531132\n",
      "Epoch 90401, Training Loss: 36006, Validation Loss: 51010, 197845.9547734511\n",
      "Epoch 90501, Training Loss: 35020, Validation Loss: 51028, 184051.96289264865\n",
      "Epoch 90601, Training Loss: 34563, Validation Loss: 50800, 198665.43885882874\n",
      "Epoch 90701, Training Loss: 32796, Validation Loss: 63667, 209329.0042162645\n",
      "Epoch 90801, Training Loss: 36074, Validation Loss: 82725, 204805.41643282596\n",
      "Epoch 90901, Training Loss: 35786, Validation Loss: 63169, 216794.08001192796\n",
      "Epoch 91001, Training Loss: 33658, Validation Loss: 53031, 175343.2816167481\n",
      "Epoch 91101, Training Loss: 34178, Validation Loss: 59436, 202924.37717352403\n",
      "Epoch 91201, Training Loss: 34985, Validation Loss: 51005, 187689.86991981653\n",
      "Epoch 91301, Training Loss: 35149, Validation Loss: 57475, 221126.51872458958\n",
      "Epoch 91401, Training Loss: 34632, Validation Loss: 51459, 200033.1129237863\n",
      "Epoch 91501, Training Loss: 34683, Validation Loss: 51700, 194216.53837472622\n",
      "Epoch 91601, Training Loss: 34535, Validation Loss: 50286, 220006.67390513964\n",
      "Epoch 91701, Training Loss: 37406, Validation Loss: 53605, 226141.70930493364\n",
      "Epoch 91801, Training Loss: 34650, Validation Loss: 56148, 173864.9903219629\n",
      "Epoch 91901, Training Loss: 35793, Validation Loss: 74986, 240634.24773587243\n",
      "Epoch 92001, Training Loss: 33093, Validation Loss: 52128, 203306.69324679323\n",
      "Epoch 92101, Training Loss: 34609, Validation Loss: 54257, 182096.87710302908\n",
      "Epoch 92201, Training Loss: 33972, Validation Loss: 58431, 173361.5497153666\n",
      "Epoch 92301, Training Loss: 33922, Validation Loss: 53728, 176325.45885101732\n",
      "Epoch 92401, Training Loss: 34944, Validation Loss: 60674, 222147.1161329417\n",
      "Epoch 92501, Training Loss: 33159, Validation Loss: 49422, 188753.7902884843\n",
      "Epoch 92601, Training Loss: 35181, Validation Loss: 54822, 189197.2202631489\n",
      "Epoch 92701, Training Loss: 35885, Validation Loss: 55765, 203980.7958698258\n",
      "Epoch 92801, Training Loss: 35330, Validation Loss: 51824, 212892.8290693546\n",
      "Epoch 92901, Training Loss: 36006, Validation Loss: 50609, 180109.68242199926\n",
      "Epoch 93001, Training Loss: 34689, Validation Loss: 87758, 227920.20240157095\n",
      "Epoch 93101, Training Loss: 33741, Validation Loss: 52523, 179826.07874367907\n",
      "Epoch 93201, Training Loss: 38248, Validation Loss: 49915, 228410.37905225772\n",
      "Epoch 93301, Training Loss: 34082, Validation Loss: 57999, 184605.54874720122\n",
      "Epoch 93401, Training Loss: 32677, Validation Loss: 72624, 190108.41890738186\n",
      "Epoch 93501, Training Loss: 36263, Validation Loss: 53900, 208023.91306531706\n",
      "Epoch 93601, Training Loss: 34116, Validation Loss: 55553, 199222.9994453478\n",
      "Epoch 93701, Training Loss: 34286, Validation Loss: 55747, 203221.49542328413\n",
      "Epoch 93801, Training Loss: 33850, Validation Loss: 54362, 199583.3780598645\n",
      "Epoch 93901, Training Loss: 33828, Validation Loss: 52545, 185156.72950124726\n",
      "Epoch 94001, Training Loss: 35154, Validation Loss: 51494, 240051.88751163366\n",
      "Epoch 94101, Training Loss: 34410, Validation Loss: 51901, 169101.55924584315\n",
      "Epoch 94201, Training Loss: 35009, Validation Loss: 50361, 193972.82304516784\n",
      "Epoch 94301, Training Loss: 35135, Validation Loss: 51396, 173790.93945060959\n",
      "Epoch 94401, Training Loss: 35568, Validation Loss: 63882, 190395.60884848668\n",
      "Epoch 94501, Training Loss: 34622, Validation Loss: 53798, 241188.3284790217\n",
      "Epoch 94601, Training Loss: 36817, Validation Loss: 51981, 188157.30989177738\n",
      "Epoch 94701, Training Loss: 35504, Validation Loss: 75196, 249727.2196945852\n",
      "Epoch 94801, Training Loss: 38261, Validation Loss: 73856, 226923.83848994793\n",
      "Epoch 94901, Training Loss: 35150, Validation Loss: 54666, 184347.45891492045\n",
      "Epoch 95001, Training Loss: 34064, Validation Loss: 52288, 176010.84544107446\n",
      "Epoch 95101, Training Loss: 33526, Validation Loss: 58899, 184583.87553154028\n",
      "Epoch 95201, Training Loss: 34924, Validation Loss: 53511, 178228.59119166204\n",
      "Epoch 95301, Training Loss: 34455, Validation Loss: 50575, 210295.769893714\n",
      "Epoch 95401, Training Loss: 33851, Validation Loss: 61143, 195810.7856379235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95501, Training Loss: 35223, Validation Loss: 49553, 225920.60206036558\n",
      "Epoch 95601, Training Loss: 36167, Validation Loss: 51639, 169464.88888962648\n",
      "Epoch 95701, Training Loss: 35687, Validation Loss: 61891, 233756.33090064584\n",
      "Epoch 95801, Training Loss: 34085, Validation Loss: 61005, 220893.22390617186\n",
      "Epoch 95901, Training Loss: 33415, Validation Loss: 53016, 177760.61219623467\n",
      "Epoch 96001, Training Loss: 35626, Validation Loss: 55554, 221697.47729722506\n",
      "Epoch 96101, Training Loss: 35321, Validation Loss: 57589, 199127.76514231614\n",
      "Epoch 96201, Training Loss: 34252, Validation Loss: 49340, 190916.41706485368\n",
      "Epoch 96301, Training Loss: 33154, Validation Loss: 50919, 146533.41631487917\n",
      "Epoch 96401, Training Loss: 34545, Validation Loss: 60286, 220010.61627879972\n",
      "Epoch 96501, Training Loss: 34824, Validation Loss: 49918, 210265.69975283733\n",
      "Epoch 96601, Training Loss: 32032, Validation Loss: 55855, 200515.2585871661\n",
      "Epoch 96701, Training Loss: 36206, Validation Loss: 64710, 184265.15998486406\n",
      "Epoch 96801, Training Loss: 34741, Validation Loss: 51782, 147090.86298142248\n",
      "Epoch 96901, Training Loss: 34414, Validation Loss: 76105, 213618.88792486294\n",
      "Epoch 97001, Training Loss: 34277, Validation Loss: 51863, 206723.0074381545\n",
      "Epoch 97101, Training Loss: 34069, Validation Loss: 53376, 175399.246969566\n",
      "Epoch 97201, Training Loss: 34883, Validation Loss: 48896, 220969.96792730235\n",
      "Epoch 97301, Training Loss: 32526, Validation Loss: 53714, 174795.42874176614\n",
      "Epoch 97401, Training Loss: 33025, Validation Loss: 57528, 222187.18858160646\n",
      "Epoch 97501, Training Loss: 35448, Validation Loss: 64167, 238206.4936345052\n",
      "Epoch 97601, Training Loss: 32830, Validation Loss: 53808, 227989.56382140052\n",
      "Epoch 97701, Training Loss: 34517, Validation Loss: 49164, 177435.0670195761\n",
      "Epoch 97801, Training Loss: 33923, Validation Loss: 56119, 218405.11206475671\n",
      "Epoch 97901, Training Loss: 35215, Validation Loss: 64333, 213055.26172076477\n",
      "Epoch 98001, Training Loss: 35201, Validation Loss: 52171, 217506.00933595237\n",
      "Epoch 98101, Training Loss: 34594, Validation Loss: 54850, 214630.76780417268\n",
      "Epoch 98201, Training Loss: 36395, Validation Loss: 56359, 233799.05544421406\n",
      "Epoch 98301, Training Loss: 35066, Validation Loss: 56083, 209401.88224920255\n",
      "Epoch 98401, Training Loss: 37414, Validation Loss: 57067, 223139.985832019\n",
      "Epoch 98501, Training Loss: 35181, Validation Loss: 51220, 207561.99373509953\n",
      "Epoch 98601, Training Loss: 35072, Validation Loss: 73518, 210042.47863192903\n",
      "Epoch 98701, Training Loss: 34399, Validation Loss: 80998, 234708.87492595403\n",
      "Epoch 98801, Training Loss: 32730, Validation Loss: 49591, 206253.01070891353\n",
      "Epoch 98901, Training Loss: 35291, Validation Loss: 60523, 227922.27704772147\n",
      "Epoch 99001, Training Loss: 34710, Validation Loss: 57806, 217562.31264218118\n",
      "Epoch 99101, Training Loss: 34118, Validation Loss: 65898, 219349.0296925486\n",
      "Epoch 99201, Training Loss: 34944, Validation Loss: 50608, 234111.24086000174\n",
      "Epoch 99301, Training Loss: 33695, Validation Loss: 51166, 209351.78520326837\n",
      "Epoch 99401, Training Loss: 33058, Validation Loss: 58905, 177856.19774863427\n",
      "Epoch 99501, Training Loss: 32467, Validation Loss: 54449, 172017.2006075982\n",
      "Epoch 99601, Training Loss: 34058, Validation Loss: 62990, 187562.1473165023\n",
      "Epoch 99701, Training Loss: 34274, Validation Loss: 71115, 182815.03081382858\n",
      "Epoch 99801, Training Loss: 32269, Validation Loss: 52961, 189595.78187732838\n",
      "Epoch 99901, Training Loss: 34117, Validation Loss: 75152, 216417.95504228523\n",
      "Epoch 100001, Training Loss: 37644, Validation Loss: 52399, 207135.7468691679\n",
      "Epoch 100101, Training Loss: 34782, Validation Loss: 57664, 187318.99052169351\n",
      "Epoch 100201, Training Loss: 33196, Validation Loss: 56247, 182621.25091115755\n",
      "Epoch 100301, Training Loss: 33451, Validation Loss: 53135, 181367.5132613981\n",
      "Epoch 100401, Training Loss: 33172, Validation Loss: 71557, 209522.5958902418\n",
      "Epoch 100501, Training Loss: 33420, Validation Loss: 76002, 217758.16888974558\n",
      "Epoch 100601, Training Loss: 32427, Validation Loss: 52071, 158034.37899969172\n",
      "Epoch 100701, Training Loss: 33858, Validation Loss: 50977, 197445.4236393658\n",
      "Epoch 100801, Training Loss: 33260, Validation Loss: 50929, 211582.92197153938\n",
      "Epoch 100901, Training Loss: 35417, Validation Loss: 55611, 181438.06330084338\n",
      "Epoch 101001, Training Loss: 34842, Validation Loss: 49997, 165558.48024081177\n",
      "Epoch 101101, Training Loss: 35038, Validation Loss: 54439, 196211.04908450032\n",
      "Epoch 101201, Training Loss: 33479, Validation Loss: 63149, 192061.5338610768\n",
      "Epoch 101301, Training Loss: 34555, Validation Loss: 49432, 204141.6038121884\n",
      "Epoch 101401, Training Loss: 34419, Validation Loss: 49077, 204223.4577208944\n",
      "Epoch 101501, Training Loss: 36059, Validation Loss: 59275, 245264.5433102659\n",
      "Epoch 101601, Training Loss: 34491, Validation Loss: 67664, 185751.28694246477\n",
      "Epoch 101701, Training Loss: 35598, Validation Loss: 54846, 215037.3985696887\n",
      "Epoch 101801, Training Loss: 35383, Validation Loss: 56561, 199285.47203657753\n",
      "Epoch 101901, Training Loss: 35159, Validation Loss: 48901, 224823.88501232266\n",
      "Epoch 102001, Training Loss: 33529, Validation Loss: 50075, 159659.7284137294\n",
      "Epoch 102101, Training Loss: 35380, Validation Loss: 50530, 185466.8671348367\n",
      "Epoch 102201, Training Loss: 36272, Validation Loss: 70509, 231946.930518168\n",
      "Epoch 102301, Training Loss: 34593, Validation Loss: 52540, 210109.39141966248\n",
      "Epoch 102401, Training Loss: 33478, Validation Loss: 59974, 193480.1980194978\n",
      "Epoch 102501, Training Loss: 34029, Validation Loss: 52265, 175161.39652629197\n",
      "Epoch 102601, Training Loss: 37417, Validation Loss: 52126, 241028.07528928187\n",
      "Epoch 102701, Training Loss: 33180, Validation Loss: 54573, 173153.9570187333\n",
      "Epoch 102801, Training Loss: 34708, Validation Loss: 56174, 193236.96288743595\n",
      "Epoch 102901, Training Loss: 33781, Validation Loss: 50154, 201244.433635292\n",
      "Epoch 103001, Training Loss: 33708, Validation Loss: 62559, 256041.22825059437\n",
      "Epoch 103101, Training Loss: 36301, Validation Loss: 59056, 219858.66079900766\n",
      "Epoch 103201, Training Loss: 33278, Validation Loss: 50197, 209078.25244902822\n",
      "Epoch 103301, Training Loss: 34378, Validation Loss: 51346, 228916.00066590306\n",
      "Epoch 103401, Training Loss: 33904, Validation Loss: 51746, 209367.3677152683\n",
      "Epoch 103501, Training Loss: 35201, Validation Loss: 70635, 241265.10118033408\n",
      "Epoch 103601, Training Loss: 34695, Validation Loss: 48990, 197466.3006271797\n",
      "Epoch 103701, Training Loss: 33690, Validation Loss: 53419, 200271.31990404162\n",
      "Epoch 103801, Training Loss: 34836, Validation Loss: 51913, 197607.49825931515\n",
      "Epoch 103901, Training Loss: 35182, Validation Loss: 52073, 240963.42446021922\n",
      "Epoch 104001, Training Loss: 34092, Validation Loss: 51229, 209725.05337661624\n",
      "Epoch 104101, Training Loss: 36160, Validation Loss: 74952, 274182.4644431204\n",
      "Epoch 104201, Training Loss: 33804, Validation Loss: 50769, 184493.00229027122\n",
      "Epoch 104301, Training Loss: 34084, Validation Loss: 51290, 204750.83152251478\n",
      "Epoch 104401, Training Loss: 34901, Validation Loss: 70147, 169582.12138110894\n",
      "Epoch 104501, Training Loss: 32137, Validation Loss: 54367, 163271.94486155355\n",
      "Epoch 104601, Training Loss: 33187, Validation Loss: 52722, 176740.77080393536\n",
      "Epoch 104701, Training Loss: 35623, Validation Loss: 65798, 210408.96329317315\n",
      "Epoch 104801, Training Loss: 34754, Validation Loss: 53529, 168611.59187169553\n",
      "Epoch 104901, Training Loss: 34054, Validation Loss: 64032, 182881.6081681931\n",
      "Epoch 105001, Training Loss: 35385, Validation Loss: 55676, 182917.7932612578\n",
      "Epoch 105101, Training Loss: 33175, Validation Loss: 53042, 178585.74784166738\n",
      "Epoch 105201, Training Loss: 36041, Validation Loss: 51124, 254548.46474517407\n",
      "Epoch 105301, Training Loss: 34139, Validation Loss: 54896, 210151.7547563969\n",
      "Epoch 105401, Training Loss: 34285, Validation Loss: 52862, 231440.6069472678\n",
      "Epoch 105501, Training Loss: 34060, Validation Loss: 51710, 199442.7382782304\n",
      "Epoch 105601, Training Loss: 33577, Validation Loss: 49464, 166812.52879971726\n",
      "Epoch 105701, Training Loss: 35949, Validation Loss: 50468, 192177.08484522466\n",
      "Epoch 105801, Training Loss: 33721, Validation Loss: 59250, 183047.26084589466\n",
      "Epoch 105901, Training Loss: 34172, Validation Loss: 59891, 163477.543841806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106001, Training Loss: 33268, Validation Loss: 48544, 233430.94895236185\n",
      "Epoch 106101, Training Loss: 34801, Validation Loss: 57742, 206417.30535358048\n",
      "Epoch 106201, Training Loss: 33888, Validation Loss: 52051, 161460.61696544118\n",
      "Epoch 106301, Training Loss: 32057, Validation Loss: 49009, 183039.67671654452\n",
      "Epoch 106401, Training Loss: 35635, Validation Loss: 54729, 177219.63001113702\n",
      "Epoch 106501, Training Loss: 34815, Validation Loss: 49234, 198862.88140236904\n",
      "Epoch 106601, Training Loss: 35583, Validation Loss: 53812, 222814.32126069572\n",
      "Epoch 106701, Training Loss: 35081, Validation Loss: 51890, 210201.85685546172\n",
      "Epoch 106801, Training Loss: 34176, Validation Loss: 57173, 235625.336736485\n",
      "Epoch 106901, Training Loss: 33029, Validation Loss: 51657, 180536.9887629142\n",
      "Epoch 107001, Training Loss: 35675, Validation Loss: 54077, 250139.6300753487\n",
      "Epoch 107101, Training Loss: 35695, Validation Loss: 74594, 268936.1684495662\n",
      "Epoch 107201, Training Loss: 37064, Validation Loss: 69611, 201611.3979070426\n",
      "Epoch 107301, Training Loss: 35883, Validation Loss: 68943, 188469.0757691063\n",
      "Epoch 107401, Training Loss: 32643, Validation Loss: 69140, 255189.90340767754\n",
      "Epoch 107501, Training Loss: 34789, Validation Loss: 51172, 251822.6040995857\n",
      "Epoch 107601, Training Loss: 34194, Validation Loss: 51301, 186217.0001931668\n",
      "Epoch 107701, Training Loss: 33301, Validation Loss: 62020, 177034.33766747275\n",
      "Epoch 107801, Training Loss: 34705, Validation Loss: 50242, 186581.617082176\n",
      "Epoch 107901, Training Loss: 34476, Validation Loss: 54881, 202163.65724700186\n",
      "Epoch 108001, Training Loss: 34507, Validation Loss: 51246, 185162.78115879334\n",
      "Epoch 108101, Training Loss: 34743, Validation Loss: 55123, 179699.9474601976\n",
      "Epoch 108201, Training Loss: 37036, Validation Loss: 65920, 209432.86214468596\n",
      "Epoch 108301, Training Loss: 34969, Validation Loss: 62987, 220485.58640192542\n",
      "Epoch 108401, Training Loss: 34557, Validation Loss: 64807, 238683.19338480465\n",
      "Epoch 108501, Training Loss: 34379, Validation Loss: 56011, 191813.50398117304\n",
      "Epoch 108601, Training Loss: 33419, Validation Loss: 56780, 169989.52053178815\n",
      "Epoch 108701, Training Loss: 32857, Validation Loss: 63483, 164329.48125105465\n",
      "Epoch 108801, Training Loss: 33804, Validation Loss: 52809, 180809.3815597255\n",
      "Epoch 108901, Training Loss: 33403, Validation Loss: 51554, 173916.70974240502\n",
      "Epoch 109001, Training Loss: 35630, Validation Loss: 56044, 222355.06090154822\n",
      "Epoch 109101, Training Loss: 34585, Validation Loss: 52465, 215353.80951629905\n",
      "Epoch 109201, Training Loss: 34645, Validation Loss: 50350, 152579.7987908368\n",
      "Epoch 109301, Training Loss: 34480, Validation Loss: 57140, 190279.67630527334\n",
      "Epoch 109401, Training Loss: 33539, Validation Loss: 56431, 185378.8590674107\n",
      "Epoch 109501, Training Loss: 36316, Validation Loss: 60517, 188109.18448763943\n",
      "Epoch 109601, Training Loss: 35752, Validation Loss: 52533, 197634.01367635923\n",
      "Epoch 109701, Training Loss: 34183, Validation Loss: 49823, 224062.43156682412\n",
      "Epoch 109801, Training Loss: 33732, Validation Loss: 49590, 199633.4158014817\n",
      "Epoch 109901, Training Loss: 33410, Validation Loss: 51293, 201688.41800430906\n",
      "Epoch 110001, Training Loss: 34153, Validation Loss: 50150, 193521.80466195577\n",
      "Epoch 110101, Training Loss: 33523, Validation Loss: 71374, 185359.8523992508\n",
      "Epoch 110201, Training Loss: 34467, Validation Loss: 60655, 222902.9557071762\n",
      "Epoch 110301, Training Loss: 34564, Validation Loss: 54419, 213385.99760917414\n",
      "Epoch 110401, Training Loss: 32893, Validation Loss: 62105, 200109.03806325208\n",
      "Epoch 110501, Training Loss: 34143, Validation Loss: 50831, 166176.50596646438\n",
      "Epoch 110601, Training Loss: 34373, Validation Loss: 94767, 203118.93902548603\n",
      "Epoch 110701, Training Loss: 33783, Validation Loss: 49883, 185971.06572419987\n",
      "Epoch 110801, Training Loss: 33836, Validation Loss: 52746, 259623.9351488694\n",
      "Epoch 110901, Training Loss: 33217, Validation Loss: 51361, 191902.86808546973\n",
      "Epoch 111001, Training Loss: 33707, Validation Loss: 54208, 167868.29081397803\n",
      "Epoch 111101, Training Loss: 36824, Validation Loss: 51860, 188636.82555090575\n",
      "Epoch 111201, Training Loss: 34891, Validation Loss: 55694, 242723.53903093937\n",
      "Epoch 111301, Training Loss: 33052, Validation Loss: 50790, 238091.69482058924\n",
      "Epoch 111401, Training Loss: 37480, Validation Loss: 79347, 217153.1198382899\n",
      "Epoch 111501, Training Loss: 33012, Validation Loss: 66416, 206156.16105383998\n",
      "Epoch 111601, Training Loss: 36552, Validation Loss: 54490, 232952.05763554364\n",
      "Epoch 111701, Training Loss: 33634, Validation Loss: 50711, 168424.40271012465\n",
      "Epoch 111801, Training Loss: 35118, Validation Loss: 54303, 178051.35484365546\n",
      "Epoch 111901, Training Loss: 34126, Validation Loss: 56620, 196173.13152417602\n",
      "Epoch 112001, Training Loss: 35036, Validation Loss: 86656, 211213.90125127547\n",
      "Epoch 112101, Training Loss: 35663, Validation Loss: 61664, 202802.0194036557\n",
      "Epoch 112201, Training Loss: 34735, Validation Loss: 60747, 179719.81137863614\n",
      "Epoch 112301, Training Loss: 35977, Validation Loss: 53233, 206766.11936186856\n",
      "Epoch 112401, Training Loss: 33813, Validation Loss: 53364, 184881.87161020827\n",
      "Epoch 112501, Training Loss: 31992, Validation Loss: 48957, 184944.28844391578\n",
      "Epoch 112601, Training Loss: 33777, Validation Loss: 62630, 206573.6375533368\n",
      "Epoch 112701, Training Loss: 35265, Validation Loss: 51969, 201741.46166625668\n",
      "Epoch 112801, Training Loss: 35720, Validation Loss: 53277, 215641.69500760373\n",
      "Epoch 112901, Training Loss: 33195, Validation Loss: 50122, 178292.92691217686\n",
      "Epoch 113001, Training Loss: 35478, Validation Loss: 53007, 179997.3472502336\n",
      "Epoch 113101, Training Loss: 34944, Validation Loss: 69571, 210287.7559111932\n",
      "Epoch 113201, Training Loss: 33657, Validation Loss: 50613, 167785.87811375217\n",
      "Epoch 113301, Training Loss: 33563, Validation Loss: 62068, 219663.5289223428\n",
      "Epoch 113401, Training Loss: 32877, Validation Loss: 52150, 225028.2774144753\n",
      "Epoch 113501, Training Loss: 33679, Validation Loss: 50244, 197698.20654396794\n",
      "Epoch 113601, Training Loss: 34008, Validation Loss: 67636, 200985.4066810104\n",
      "Epoch 113701, Training Loss: 34900, Validation Loss: 53936, 234993.65792283136\n",
      "Epoch 113801, Training Loss: 34581, Validation Loss: 66571, 230431.98311526573\n",
      "Epoch 113901, Training Loss: 34523, Validation Loss: 58201, 198754.27686413936\n",
      "Epoch 114001, Training Loss: 33623, Validation Loss: 50985, 223612.47292721612\n",
      "Epoch 114101, Training Loss: 34110, Validation Loss: 55975, 195598.4835214693\n",
      "Epoch 114201, Training Loss: 33485, Validation Loss: 89239, 202020.8415497334\n",
      "Epoch 114301, Training Loss: 34627, Validation Loss: 80519, 201215.43611781896\n",
      "Epoch 114401, Training Loss: 33698, Validation Loss: 51353, 188841.00012990265\n",
      "Epoch 114501, Training Loss: 32489, Validation Loss: 61106, 182243.22732527382\n",
      "Epoch 114601, Training Loss: 35158, Validation Loss: 65562, 258355.86062915192\n",
      "Epoch 114701, Training Loss: 35637, Validation Loss: 50415, 179963.75350202378\n",
      "Epoch 114801, Training Loss: 32345, Validation Loss: 57332, 181692.40099811155\n",
      "Epoch 114901, Training Loss: 33760, Validation Loss: 49505, 193953.8025001943\n",
      "Epoch 115001, Training Loss: 33401, Validation Loss: 62107, 177036.03010717873\n",
      "Epoch 115101, Training Loss: 32751, Validation Loss: 49229, 178100.70048953604\n",
      "Epoch 115201, Training Loss: 33667, Validation Loss: 49750, 208760.06989811535\n",
      "Epoch 115301, Training Loss: 34390, Validation Loss: 50615, 170268.60271108217\n",
      "Epoch 115401, Training Loss: 33536, Validation Loss: 50624, 190318.41311269696\n",
      "Epoch 115501, Training Loss: 33486, Validation Loss: 50567, 218952.1176775369\n",
      "Epoch 115601, Training Loss: 33636, Validation Loss: 51168, 203567.59648669898\n",
      "Epoch 115701, Training Loss: 33623, Validation Loss: 51455, 216068.41116787095\n",
      "Epoch 115801, Training Loss: 35468, Validation Loss: 60859, 182145.33384386427\n",
      "Epoch 115901, Training Loss: 34392, Validation Loss: 48178, 172168.97118262836\n",
      "Epoch 116001, Training Loss: 34736, Validation Loss: 62843, 238527.39669011577\n",
      "Epoch 116101, Training Loss: 33496, Validation Loss: 76713, 219342.56665764775\n",
      "Epoch 116201, Training Loss: 34197, Validation Loss: 51623, 198376.5782449752\n",
      "Epoch 116301, Training Loss: 32171, Validation Loss: 50687, 173197.43925606072\n",
      "Epoch 116401, Training Loss: 33833, Validation Loss: 50684, 176573.93072622377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116501, Training Loss: 36849, Validation Loss: 54274, 232115.5989813667\n",
      "Epoch 116601, Training Loss: 33919, Validation Loss: 49612, 235085.77562167632\n",
      "Epoch 116701, Training Loss: 32865, Validation Loss: 62304, 215061.47559849007\n",
      "Epoch 116801, Training Loss: 33747, Validation Loss: 49654, 231600.75356514708\n",
      "Epoch 116901, Training Loss: 34060, Validation Loss: 54502, 222751.34546487895\n",
      "Epoch 117001, Training Loss: 34324, Validation Loss: 52128, 200184.0104416648\n",
      "Epoch 117101, Training Loss: 33528, Validation Loss: 50935, 186518.3904828824\n",
      "Epoch 117201, Training Loss: 34056, Validation Loss: 52537, 210384.83554085458\n",
      "Epoch 117301, Training Loss: 35661, Validation Loss: 53115, 214546.01407588148\n",
      "Epoch 117401, Training Loss: 34904, Validation Loss: 53910, 228183.0455186673\n",
      "Epoch 117501, Training Loss: 35094, Validation Loss: 50809, 190689.05633572862\n",
      "Epoch 117601, Training Loss: 33775, Validation Loss: 55265, 224291.64403821153\n",
      "Epoch 117701, Training Loss: 34523, Validation Loss: 71773, 233279.37626571572\n",
      "Epoch 117801, Training Loss: 34372, Validation Loss: 62550, 205599.8878930857\n",
      "Epoch 117901, Training Loss: 34666, Validation Loss: 52434, 217528.30077189868\n",
      "Epoch 118001, Training Loss: 35443, Validation Loss: 52087, 220888.27119287077\n",
      "Epoch 118101, Training Loss: 34598, Validation Loss: 56410, 197698.35186938362\n",
      "Epoch 118201, Training Loss: 39014, Validation Loss: 91794, 315037.63549678173\n",
      "Epoch 118301, Training Loss: 34569, Validation Loss: 49171, 222582.74495716323\n",
      "Epoch 118401, Training Loss: 35343, Validation Loss: 86827, 199231.58220485327\n",
      "Epoch 118501, Training Loss: 33363, Validation Loss: 56509, 190222.08890959804\n",
      "Epoch 118601, Training Loss: 34267, Validation Loss: 49841, 208044.0039322369\n",
      "Epoch 118701, Training Loss: 35369, Validation Loss: 64251, 176673.83650906416\n",
      "Epoch 118801, Training Loss: 34298, Validation Loss: 50685, 200337.84218659942\n",
      "Epoch 118901, Training Loss: 32477, Validation Loss: 59717, 214338.56946016042\n",
      "Epoch 119001, Training Loss: 32498, Validation Loss: 50044, 209290.05973437123\n",
      "Epoch 119101, Training Loss: 33064, Validation Loss: 59294, 218565.5128901143\n",
      "Epoch 119201, Training Loss: 35447, Validation Loss: 53210, 163816.98800814617\n",
      "Epoch 119301, Training Loss: 34845, Validation Loss: 51384, 212507.4465411722\n",
      "Epoch 119401, Training Loss: 33740, Validation Loss: 50558, 189953.31669430505\n",
      "Epoch 119501, Training Loss: 34365, Validation Loss: 57596, 217790.79197352496\n",
      "Epoch 119601, Training Loss: 34520, Validation Loss: 55731, 184235.48167623454\n",
      "Epoch 119701, Training Loss: 34118, Validation Loss: 49917, 205304.23422543626\n",
      "Epoch 119801, Training Loss: 33515, Validation Loss: 53438, 209825.710609906\n",
      "Epoch 119901, Training Loss: 33426, Validation Loss: 49855, 164052.1372030167\n",
      "Epoch 120001, Training Loss: 33500, Validation Loss: 49417, 214473.76696903145\n",
      "Epoch 120101, Training Loss: 33044, Validation Loss: 60413, 198757.15986071515\n",
      "Epoch 120201, Training Loss: 35700, Validation Loss: 51312, 207451.5239803961\n",
      "Epoch 120301, Training Loss: 34209, Validation Loss: 57400, 235082.70197925795\n",
      "Epoch 120401, Training Loss: 32811, Validation Loss: 54231, 165302.09073666236\n",
      "Epoch 120501, Training Loss: 35035, Validation Loss: 61058, 231408.33256704092\n",
      "Epoch 120601, Training Loss: 33470, Validation Loss: 50586, 186538.0121851707\n",
      "Epoch 120701, Training Loss: 33009, Validation Loss: 58874, 202586.9133797275\n",
      "Epoch 120801, Training Loss: 33883, Validation Loss: 53638, 208496.51614833914\n",
      "Epoch 120901, Training Loss: 36655, Validation Loss: 57396, 248773.84448522073\n",
      "Epoch 121001, Training Loss: 32568, Validation Loss: 51158, 203632.71704965818\n",
      "Epoch 121101, Training Loss: 35157, Validation Loss: 61359, 206039.34833793226\n",
      "Epoch 121201, Training Loss: 34508, Validation Loss: 61260, 230787.1711312882\n",
      "Epoch 121301, Training Loss: 33708, Validation Loss: 50785, 199750.3120445679\n",
      "Epoch 121401, Training Loss: 33791, Validation Loss: 54163, 202175.1794373143\n",
      "Epoch 121501, Training Loss: 33802, Validation Loss: 54908, 217291.1760575263\n",
      "Epoch 121601, Training Loss: 33642, Validation Loss: 54125, 194548.29962018027\n",
      "Epoch 121701, Training Loss: 34296, Validation Loss: 62658, 208964.95170435435\n",
      "Epoch 121801, Training Loss: 33736, Validation Loss: 50156, 196977.62214138857\n",
      "Epoch 121901, Training Loss: 32848, Validation Loss: 51720, 207671.68529243476\n",
      "Epoch 122001, Training Loss: 34207, Validation Loss: 74116, 230471.4548285538\n",
      "Epoch 122101, Training Loss: 35769, Validation Loss: 57100, 180976.67949294657\n",
      "Epoch 122201, Training Loss: 35014, Validation Loss: 50863, 186368.12530398535\n",
      "Epoch 122301, Training Loss: 34138, Validation Loss: 55525, 215192.51061848833\n",
      "Epoch 122401, Training Loss: 34220, Validation Loss: 54770, 253325.91311847465\n",
      "Epoch 122501, Training Loss: 34932, Validation Loss: 64639, 238896.8530983538\n",
      "Epoch 122601, Training Loss: 36420, Validation Loss: 51164, 206084.66511342736\n",
      "Epoch 122701, Training Loss: 35426, Validation Loss: 58663, 232277.24410094286\n",
      "Epoch 122801, Training Loss: 34461, Validation Loss: 54299, 216366.06541631374\n",
      "Epoch 122901, Training Loss: 40393, Validation Loss: 66088, 291849.14202823635\n",
      "Epoch 123001, Training Loss: 32480, Validation Loss: 54048, 192778.99947115124\n",
      "Epoch 123101, Training Loss: 33513, Validation Loss: 87077, 223676.27522261147\n",
      "Epoch 123201, Training Loss: 32682, Validation Loss: 59100, 231408.6446728751\n",
      "Epoch 123301, Training Loss: 32962, Validation Loss: 59302, 183227.6429042366\n",
      "Epoch 123401, Training Loss: 35202, Validation Loss: 50208, 184550.07352938948\n",
      "Epoch 123501, Training Loss: 34974, Validation Loss: 62685, 177161.37613519005\n",
      "Epoch 123601, Training Loss: 35063, Validation Loss: 59382, 216437.10597943782\n",
      "Epoch 123701, Training Loss: 35945, Validation Loss: 90427, 262026.04994593968\n",
      "Epoch 123801, Training Loss: 35075, Validation Loss: 52716, 247522.51267877236\n",
      "Epoch 123901, Training Loss: 33897, Validation Loss: 51448, 180863.90140898898\n",
      "Epoch 124001, Training Loss: 35836, Validation Loss: 56266, 165159.89123003057\n",
      "Epoch 124101, Training Loss: 34782, Validation Loss: 61850, 214226.66401286548\n",
      "Epoch 124201, Training Loss: 34555, Validation Loss: 51774, 224704.1955191556\n",
      "Epoch 124301, Training Loss: 34174, Validation Loss: 67785, 216570.1009773989\n",
      "Epoch 124401, Training Loss: 34678, Validation Loss: 50100, 226636.13893991118\n",
      "Epoch 124501, Training Loss: 33533, Validation Loss: 87289, 209408.58043240503\n",
      "Epoch 124601, Training Loss: 34126, Validation Loss: 49846, 167404.13654839338\n",
      "Epoch 124701, Training Loss: 32913, Validation Loss: 56400, 194347.52456988496\n",
      "Epoch 124801, Training Loss: 32838, Validation Loss: 56664, 171515.3103788827\n",
      "Epoch 124901, Training Loss: 32651, Validation Loss: 53612, 181881.5345773826\n",
      "Epoch 125001, Training Loss: 34986, Validation Loss: 52660, 160356.2024115513\n",
      "Epoch 125101, Training Loss: 33859, Validation Loss: 52147, 186944.94899967522\n",
      "Epoch 125201, Training Loss: 33908, Validation Loss: 61546, 191503.8057674299\n",
      "Epoch 125301, Training Loss: 32718, Validation Loss: 52985, 205506.32135766372\n",
      "Epoch 125401, Training Loss: 35526, Validation Loss: 54643, 173402.04817423457\n",
      "Epoch 125501, Training Loss: 32311, Validation Loss: 53977, 203428.43459200044\n",
      "Epoch 125601, Training Loss: 33539, Validation Loss: 53274, 161020.68468174458\n",
      "Epoch 125701, Training Loss: 33485, Validation Loss: 50968, 171114.86732104473\n",
      "Epoch 125801, Training Loss: 34007, Validation Loss: 55691, 190964.56408237488\n",
      "Epoch 125901, Training Loss: 32947, Validation Loss: 54067, 179228.70401244538\n",
      "Epoch 126001, Training Loss: 32482, Validation Loss: 49492, 191392.91421285295\n",
      "Epoch 126101, Training Loss: 33064, Validation Loss: 52616, 211435.04667095875\n",
      "Epoch 126201, Training Loss: 33309, Validation Loss: 62991, 182251.83481981012\n",
      "Epoch 126301, Training Loss: 33741, Validation Loss: 52372, 218660.0471579527\n",
      "Epoch 126401, Training Loss: 34398, Validation Loss: 57263, 183240.01130670737\n",
      "Epoch 126501, Training Loss: 33513, Validation Loss: 62426, 222484.9259708342\n",
      "Epoch 126601, Training Loss: 33276, Validation Loss: 50785, 206357.84891388981\n",
      "Epoch 126701, Training Loss: 36324, Validation Loss: 53651, 178871.1437275115\n",
      "Epoch 126801, Training Loss: 34435, Validation Loss: 49453, 213272.65601880624\n",
      "Epoch 126901, Training Loss: 32516, Validation Loss: 52665, 217659.76411334824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127001, Training Loss: 34920, Validation Loss: 50584, 227511.4736554393\n",
      "Epoch 127101, Training Loss: 34960, Validation Loss: 50825, 205245.28450301162\n",
      "Epoch 127201, Training Loss: 33256, Validation Loss: 53828, 172070.14504193587\n",
      "Epoch 127301, Training Loss: 33916, Validation Loss: 59817, 237933.09488403748\n",
      "Epoch 127401, Training Loss: 33157, Validation Loss: 49703, 188197.42259637997\n",
      "Epoch 127501, Training Loss: 34617, Validation Loss: 73886, 173579.00866857215\n",
      "Epoch 127601, Training Loss: 34225, Validation Loss: 52927, 186077.16336540773\n",
      "Epoch 127701, Training Loss: 32786, Validation Loss: 51289, 186099.59111409402\n",
      "Epoch 127801, Training Loss: 33244, Validation Loss: 51257, 177259.54086530374\n",
      "Epoch 127901, Training Loss: 34540, Validation Loss: 52102, 154298.09197367492\n",
      "Epoch 128001, Training Loss: 34022, Validation Loss: 53407, 183236.0658131789\n",
      "Epoch 128101, Training Loss: 34546, Validation Loss: 64759, 219966.29517404953\n",
      "Epoch 128201, Training Loss: 33451, Validation Loss: 52790, 200352.0914785572\n",
      "Epoch 128301, Training Loss: 33852, Validation Loss: 58779, 224210.68948637295\n",
      "Epoch 128401, Training Loss: 34434, Validation Loss: 56432, 241589.98909306197\n",
      "Epoch 128501, Training Loss: 32848, Validation Loss: 50073, 182644.94832130425\n",
      "Epoch 128601, Training Loss: 33233, Validation Loss: 52741, 222189.90513048193\n",
      "Epoch 128701, Training Loss: 32602, Validation Loss: 50717, 163298.91532529402\n",
      "Epoch 128801, Training Loss: 32496, Validation Loss: 60277, 165364.32678361642\n",
      "Epoch 128901, Training Loss: 35808, Validation Loss: 60879, 223741.7196882399\n",
      "Epoch 129001, Training Loss: 35065, Validation Loss: 51177, 192618.3781454517\n",
      "Epoch 129101, Training Loss: 35451, Validation Loss: 54339, 198575.361740118\n",
      "Epoch 129201, Training Loss: 32706, Validation Loss: 55605, 222342.82109122735\n",
      "Epoch 129301, Training Loss: 35149, Validation Loss: 67150, 191298.3261804924\n",
      "Epoch 129401, Training Loss: 32800, Validation Loss: 56071, 182527.40967556232\n",
      "Epoch 129501, Training Loss: 35677, Validation Loss: 62169, 171857.44241030907\n",
      "Epoch 129601, Training Loss: 35565, Validation Loss: 54136, 193561.68936302277\n",
      "Epoch 129701, Training Loss: 33835, Validation Loss: 53136, 169175.17539237952\n",
      "Epoch 129801, Training Loss: 33881, Validation Loss: 52590, 206344.0560450103\n",
      "Epoch 129901, Training Loss: 32991, Validation Loss: 53844, 201937.62219799275\n",
      "Epoch 130001, Training Loss: 33135, Validation Loss: 53824, 175427.00104782835\n",
      "Epoch 130101, Training Loss: 32569, Validation Loss: 51582, 215628.12194334713\n",
      "Epoch 130201, Training Loss: 34222, Validation Loss: 53572, 239612.48684323495\n",
      "Epoch 130301, Training Loss: 35164, Validation Loss: 50244, 199580.80920511662\n",
      "Epoch 130401, Training Loss: 34226, Validation Loss: 54590, 206115.803736156\n",
      "Epoch 130501, Training Loss: 33656, Validation Loss: 51500, 186153.82365481262\n",
      "Epoch 130601, Training Loss: 32533, Validation Loss: 49246, 170098.9284331702\n",
      "Epoch 130701, Training Loss: 35079, Validation Loss: 53023, 167416.37807270727\n",
      "Epoch 130801, Training Loss: 32669, Validation Loss: 50649, 187071.49079517092\n",
      "Epoch 130901, Training Loss: 32195, Validation Loss: 49411, 193341.38458789265\n",
      "Epoch 131001, Training Loss: 33566, Validation Loss: 51105, 179847.9043352829\n",
      "Epoch 131101, Training Loss: 33562, Validation Loss: 54889, 205855.24677578625\n",
      "Epoch 131201, Training Loss: 35087, Validation Loss: 53599, 232996.96900219948\n",
      "Epoch 131301, Training Loss: 33352, Validation Loss: 61999, 209812.53330593\n",
      "Epoch 131401, Training Loss: 33241, Validation Loss: 76655, 253359.59449641852\n",
      "Epoch 131501, Training Loss: 32182, Validation Loss: 51553, 167427.64498933312\n",
      "Epoch 131601, Training Loss: 33730, Validation Loss: 57493, 223445.4548225417\n",
      "Epoch 131701, Training Loss: 33978, Validation Loss: 62068, 235047.5750925358\n",
      "Epoch 131801, Training Loss: 33925, Validation Loss: 55082, 169402.42218652822\n",
      "Epoch 131901, Training Loss: 32709, Validation Loss: 51846, 176756.16659397594\n",
      "Epoch 132001, Training Loss: 33796, Validation Loss: 51553, 223966.03861820197\n",
      "Epoch 132101, Training Loss: 35942, Validation Loss: 57344, 207439.95812998526\n",
      "Epoch 132201, Training Loss: 32621, Validation Loss: 52324, 179195.4530128476\n",
      "Epoch 132301, Training Loss: 32662, Validation Loss: 51589, 181860.1981300439\n",
      "Epoch 132401, Training Loss: 33534, Validation Loss: 53259, 212026.9883709297\n",
      "Epoch 132501, Training Loss: 32932, Validation Loss: 55857, 196776.0508856509\n",
      "Epoch 132601, Training Loss: 32795, Validation Loss: 49080, 185633.8275654686\n",
      "Epoch 132701, Training Loss: 33777, Validation Loss: 66906, 244129.83709023465\n",
      "Epoch 132801, Training Loss: 34658, Validation Loss: 52438, 187846.75656830706\n",
      "Epoch 132901, Training Loss: 33930, Validation Loss: 50374, 179495.23217783368\n",
      "Epoch 133001, Training Loss: 35713, Validation Loss: 61184, 234755.1993069771\n",
      "Epoch 133101, Training Loss: 34796, Validation Loss: 53325, 195150.01304730112\n",
      "Epoch 133201, Training Loss: 33415, Validation Loss: 51975, 194950.39672159977\n",
      "Epoch 133301, Training Loss: 33986, Validation Loss: 55109, 220305.4595787226\n",
      "Epoch 133401, Training Loss: 31870, Validation Loss: 54595, 180938.858839154\n",
      "Epoch 133501, Training Loss: 33738, Validation Loss: 57059, 166517.70132364248\n",
      "Epoch 133601, Training Loss: 35586, Validation Loss: 51038, 206033.09257364806\n",
      "Epoch 133701, Training Loss: 32750, Validation Loss: 52291, 161947.85204596713\n",
      "Epoch 133801, Training Loss: 35679, Validation Loss: 54291, 190006.16830324658\n",
      "Epoch 133901, Training Loss: 32354, Validation Loss: 51014, 222161.07719417047\n",
      "Epoch 134001, Training Loss: 34469, Validation Loss: 57292, 191765.25061134354\n",
      "Epoch 134101, Training Loss: 32635, Validation Loss: 53943, 234198.90002826427\n",
      "Epoch 134201, Training Loss: 34299, Validation Loss: 60062, 212743.44134243697\n",
      "Epoch 134301, Training Loss: 32510, Validation Loss: 53434, 208343.06422547172\n",
      "Epoch 134401, Training Loss: 33648, Validation Loss: 69262, 197649.75949681064\n",
      "Epoch 134501, Training Loss: 31793, Validation Loss: 49160, 180182.2131872043\n",
      "Epoch 134601, Training Loss: 33527, Validation Loss: 90362, 235555.35741561154\n",
      "Epoch 134701, Training Loss: 32050, Validation Loss: 51414, 179370.39359934375\n",
      "Epoch 134801, Training Loss: 35173, Validation Loss: 56728, 245152.95573085253\n",
      "Epoch 134901, Training Loss: 36795, Validation Loss: 60127, 186268.1081935675\n",
      "Epoch 135001, Training Loss: 33055, Validation Loss: 56911, 161795.73224196487\n",
      "Epoch 135101, Training Loss: 33975, Validation Loss: 77292, 214659.12669676705\n",
      "Epoch 135201, Training Loss: 33585, Validation Loss: 60987, 181197.12447607412\n",
      "Epoch 135301, Training Loss: 34030, Validation Loss: 54121, 201782.82867000814\n",
      "Epoch 135401, Training Loss: 33225, Validation Loss: 50704, 167687.20977795453\n",
      "Epoch 135501, Training Loss: 35075, Validation Loss: 55509, 219723.0349742144\n",
      "Epoch 135601, Training Loss: 35382, Validation Loss: 52397, 150120.44023431995\n",
      "Epoch 135701, Training Loss: 33550, Validation Loss: 51332, 210422.8827533944\n",
      "Epoch 135801, Training Loss: 33530, Validation Loss: 67745, 192467.56224768565\n",
      "Epoch 135901, Training Loss: 36434, Validation Loss: 71478, 244999.97890135145\n",
      "Epoch 136001, Training Loss: 36039, Validation Loss: 53039, 220788.0382221531\n",
      "Epoch 136101, Training Loss: 32935, Validation Loss: 52011, 201035.6358586081\n",
      "Epoch 136201, Training Loss: 33570, Validation Loss: 56759, 221423.70670547802\n",
      "Epoch 136301, Training Loss: 35758, Validation Loss: 55127, 200827.2140189216\n",
      "Epoch 136401, Training Loss: 32989, Validation Loss: 52966, 210963.90729152827\n",
      "Epoch 136501, Training Loss: 32443, Validation Loss: 63633, 210627.32482903005\n",
      "Epoch 136601, Training Loss: 33338, Validation Loss: 51602, 183910.32433798266\n",
      "Epoch 136701, Training Loss: 32096, Validation Loss: 56686, 162955.79903297158\n",
      "Epoch 136801, Training Loss: 35195, Validation Loss: 56703, 206734.66423919157\n",
      "Epoch 136901, Training Loss: 34951, Validation Loss: 52555, 165411.24755398915\n",
      "Epoch 137001, Training Loss: 33499, Validation Loss: 64947, 213623.0831597382\n",
      "Epoch 137101, Training Loss: 32343, Validation Loss: 48959, 212701.33260351804\n",
      "Epoch 137201, Training Loss: 34803, Validation Loss: 48867, 199586.56635947982\n",
      "Epoch 137301, Training Loss: 34540, Validation Loss: 52771, 184335.15139245908\n",
      "Epoch 137401, Training Loss: 33521, Validation Loss: 62144, 188907.23064759307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137501, Training Loss: 33575, Validation Loss: 50007, 209253.34245677607\n",
      "Epoch 137601, Training Loss: 35758, Validation Loss: 53878, 188532.67871116733\n",
      "Epoch 137701, Training Loss: 34550, Validation Loss: 65035, 173970.69003876683\n",
      "Epoch 137801, Training Loss: 32986, Validation Loss: 51675, 232065.98402240267\n",
      "Epoch 137901, Training Loss: 33873, Validation Loss: 53215, 167842.32360416776\n",
      "Epoch 138001, Training Loss: 33545, Validation Loss: 68774, 228382.4108516442\n",
      "Epoch 138101, Training Loss: 35821, Validation Loss: 55683, 246572.173452917\n",
      "Epoch 138201, Training Loss: 33456, Validation Loss: 52084, 239788.11373220303\n",
      "Epoch 138301, Training Loss: 32971, Validation Loss: 56721, 208039.80387694997\n",
      "Epoch 138401, Training Loss: 34955, Validation Loss: 70228, 221252.19045163944\n",
      "Epoch 138501, Training Loss: 33547, Validation Loss: 53832, 203387.6118073462\n",
      "Epoch 138601, Training Loss: 32427, Validation Loss: 51534, 167722.3359234381\n",
      "Epoch 138701, Training Loss: 32190, Validation Loss: 90138, 207331.57840712796\n",
      "Epoch 138801, Training Loss: 33129, Validation Loss: 58656, 158568.2026155114\n",
      "Epoch 138901, Training Loss: 34211, Validation Loss: 54187, 242475.821603706\n",
      "Epoch 139001, Training Loss: 33350, Validation Loss: 53354, 213483.03418001998\n",
      "Epoch 139101, Training Loss: 34259, Validation Loss: 52489, 220975.48382700144\n",
      "Epoch 139201, Training Loss: 33854, Validation Loss: 50531, 159572.87111657395\n",
      "Epoch 139301, Training Loss: 35320, Validation Loss: 49441, 206649.208754105\n",
      "Epoch 139401, Training Loss: 34439, Validation Loss: 51740, 202693.87102278104\n",
      "Epoch 139501, Training Loss: 34796, Validation Loss: 52490, 189072.35645374746\n",
      "Epoch 139601, Training Loss: 36076, Validation Loss: 51473, 188532.8695363579\n",
      "Epoch 139701, Training Loss: 31841, Validation Loss: 49537, 179370.80963689453\n",
      "Epoch 139801, Training Loss: 35722, Validation Loss: 50611, 220238.94314249684\n",
      "Epoch 139901, Training Loss: 35264, Validation Loss: 50202, 180751.1304371795\n",
      "Epoch 140001, Training Loss: 33759, Validation Loss: 57157, 169720.92879697555\n",
      "Epoch 140101, Training Loss: 33195, Validation Loss: 61086, 206952.49398309994\n",
      "Epoch 140201, Training Loss: 34397, Validation Loss: 53036, 152606.2464322671\n",
      "Epoch 140301, Training Loss: 33341, Validation Loss: 53926, 210692.6131962034\n",
      "Epoch 140401, Training Loss: 33597, Validation Loss: 52026, 144401.6878352396\n",
      "Epoch 140501, Training Loss: 33726, Validation Loss: 59093, 184045.2980124672\n",
      "Epoch 140601, Training Loss: 33928, Validation Loss: 62424, 211385.47213898087\n",
      "Epoch 140701, Training Loss: 32764, Validation Loss: 70176, 216042.80978562738\n",
      "Epoch 140801, Training Loss: 32630, Validation Loss: 50475, 177599.3310888914\n",
      "Epoch 140901, Training Loss: 31957, Validation Loss: 61963, 196702.5264641666\n",
      "Epoch 141001, Training Loss: 33835, Validation Loss: 55864, 197824.26039797635\n",
      "Epoch 141101, Training Loss: 35173, Validation Loss: 65412, 253224.94098811017\n",
      "Epoch 141201, Training Loss: 32619, Validation Loss: 57371, 191199.48963375992\n",
      "Epoch 141301, Training Loss: 33542, Validation Loss: 57929, 216010.34724527635\n",
      "Epoch 141401, Training Loss: 33062, Validation Loss: 57158, 197140.78983451496\n",
      "Epoch 141501, Training Loss: 34502, Validation Loss: 52072, 182113.92635735322\n",
      "Epoch 141601, Training Loss: 35272, Validation Loss: 85569, 222682.57719218067\n",
      "Epoch 141701, Training Loss: 35068, Validation Loss: 67749, 218647.5828246268\n",
      "Epoch 141801, Training Loss: 36602, Validation Loss: 64420, 249898.32160379877\n",
      "Epoch 141901, Training Loss: 31419, Validation Loss: 49396, 150584.07356413474\n",
      "Epoch 142001, Training Loss: 34275, Validation Loss: 51100, 201393.2005544801\n",
      "Epoch 142101, Training Loss: 32194, Validation Loss: 63316, 189866.9032355108\n",
      "Epoch 142201, Training Loss: 33901, Validation Loss: 62926, 217786.59009589037\n",
      "Epoch 142301, Training Loss: 33303, Validation Loss: 68671, 192140.25373298334\n",
      "Epoch 142401, Training Loss: 34054, Validation Loss: 73600, 234755.5441426369\n",
      "Epoch 142501, Training Loss: 34716, Validation Loss: 88286, 214050.4330318095\n",
      "Epoch 142601, Training Loss: 33538, Validation Loss: 59825, 203531.68202944077\n",
      "Epoch 142701, Training Loss: 33672, Validation Loss: 50711, 206843.1321998098\n",
      "Epoch 142801, Training Loss: 35565, Validation Loss: 75422, 220503.0877526852\n",
      "Epoch 142901, Training Loss: 34521, Validation Loss: 49263, 249209.61061649697\n",
      "Epoch 143001, Training Loss: 32481, Validation Loss: 54530, 177838.63787618946\n",
      "Epoch 143101, Training Loss: 34560, Validation Loss: 88797, 222812.50457111083\n",
      "Epoch 143201, Training Loss: 35104, Validation Loss: 54088, 193161.28276137545\n",
      "Epoch 143301, Training Loss: 32520, Validation Loss: 51444, 180046.74489845906\n",
      "Epoch 143401, Training Loss: 34786, Validation Loss: 53458, 262101.87686421376\n",
      "Epoch 143501, Training Loss: 34037, Validation Loss: 50993, 218274.79797751227\n",
      "Epoch 143601, Training Loss: 34627, Validation Loss: 54590, 167223.10425337552\n",
      "Epoch 143701, Training Loss: 34884, Validation Loss: 51665, 159546.5496043976\n",
      "Epoch 143801, Training Loss: 34607, Validation Loss: 48820, 204772.96528442716\n",
      "Epoch 143901, Training Loss: 32357, Validation Loss: 52847, 194131.9679433859\n",
      "Epoch 144001, Training Loss: 35275, Validation Loss: 50924, 213007.4450700413\n",
      "Epoch 144101, Training Loss: 33674, Validation Loss: 62471, 198058.3445303623\n",
      "Epoch 144201, Training Loss: 34362, Validation Loss: 64707, 194459.8868485171\n",
      "Epoch 144301, Training Loss: 34668, Validation Loss: 52607, 186146.4456785882\n",
      "Epoch 144401, Training Loss: 33537, Validation Loss: 61518, 193211.68964650168\n",
      "Epoch 144501, Training Loss: 33332, Validation Loss: 62564, 184901.45229327303\n",
      "Epoch 144601, Training Loss: 34584, Validation Loss: 51396, 218172.37028376153\n",
      "Epoch 144701, Training Loss: 33617, Validation Loss: 53187, 170681.55802639932\n",
      "Epoch 144801, Training Loss: 34663, Validation Loss: 51954, 182631.9012246849\n",
      "Epoch 144901, Training Loss: 33839, Validation Loss: 51805, 189946.98760653866\n",
      "Epoch 145001, Training Loss: 34443, Validation Loss: 93286, 212804.30147808883\n",
      "Epoch 145101, Training Loss: 34980, Validation Loss: 57921, 207939.89002441647\n",
      "Epoch 145201, Training Loss: 33228, Validation Loss: 57715, 164187.77807101258\n",
      "Epoch 145301, Training Loss: 34817, Validation Loss: 51727, 215886.9738066974\n",
      "Epoch 145401, Training Loss: 34463, Validation Loss: 116139, 177261.86715770586\n",
      "Epoch 145501, Training Loss: 34617, Validation Loss: 67359, 194816.9389990743\n",
      "Epoch 145601, Training Loss: 32882, Validation Loss: 51257, 181195.75321648587\n",
      "Epoch 145701, Training Loss: 34332, Validation Loss: 65138, 225979.03059379212\n",
      "Epoch 145801, Training Loss: 34065, Validation Loss: 54612, 223318.4579081732\n",
      "Epoch 145901, Training Loss: 34673, Validation Loss: 50504, 183571.3552695169\n",
      "Epoch 146001, Training Loss: 34374, Validation Loss: 49698, 254864.54727312224\n",
      "Epoch 146101, Training Loss: 34819, Validation Loss: 51486, 263006.3059011922\n",
      "Epoch 146201, Training Loss: 34644, Validation Loss: 66309, 219599.06826693038\n",
      "Epoch 146301, Training Loss: 34080, Validation Loss: 66939, 209345.6587766056\n",
      "Epoch 146401, Training Loss: 33400, Validation Loss: 50409, 196775.82892568622\n",
      "Epoch 146501, Training Loss: 33519, Validation Loss: 55488, 205219.82707172915\n",
      "Epoch 146601, Training Loss: 31813, Validation Loss: 50919, 203290.79438859096\n",
      "Epoch 146701, Training Loss: 35620, Validation Loss: 49834, 171036.9829080095\n",
      "Epoch 146801, Training Loss: 36380, Validation Loss: 77378, 247820.70887396042\n",
      "Epoch 146901, Training Loss: 32198, Validation Loss: 51256, 188073.2265723986\n",
      "Epoch 147001, Training Loss: 34911, Validation Loss: 49736, 200456.45641712644\n",
      "Epoch 147101, Training Loss: 34441, Validation Loss: 53777, 222217.1604186881\n",
      "Epoch 147201, Training Loss: 34695, Validation Loss: 57956, 284952.63834881765\n",
      "Epoch 147301, Training Loss: 32469, Validation Loss: 54194, 194705.81842844852\n",
      "Epoch 147401, Training Loss: 33285, Validation Loss: 55422, 227735.69519287278\n",
      "Epoch 147501, Training Loss: 33888, Validation Loss: 51801, 229151.03776263553\n",
      "Epoch 147601, Training Loss: 34089, Validation Loss: 53791, 197400.10378070132\n",
      "Epoch 147701, Training Loss: 34784, Validation Loss: 67225, 214594.01599809885\n",
      "Epoch 147801, Training Loss: 36455, Validation Loss: 61177, 178029.95114186886\n",
      "Epoch 147901, Training Loss: 34371, Validation Loss: 57942, 235328.70914938473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148001, Training Loss: 35514, Validation Loss: 57265, 190863.87563559576\n",
      "Epoch 148101, Training Loss: 36825, Validation Loss: 52835, 250369.36363864038\n",
      "Epoch 148201, Training Loss: 32447, Validation Loss: 53708, 168345.325380003\n",
      "Epoch 148301, Training Loss: 33601, Validation Loss: 53570, 203336.76194429814\n",
      "Epoch 148401, Training Loss: 37964, Validation Loss: 52046, 188003.2118392746\n",
      "Epoch 148501, Training Loss: 33881, Validation Loss: 51380, 275218.96591611503\n",
      "Epoch 148601, Training Loss: 33421, Validation Loss: 53875, 219444.92470594685\n",
      "Epoch 148701, Training Loss: 33431, Validation Loss: 71812, 192456.43380183153\n",
      "Epoch 148801, Training Loss: 34151, Validation Loss: 57949, 174878.2484135231\n",
      "Epoch 148901, Training Loss: 34185, Validation Loss: 60962, 205473.86978105226\n",
      "Epoch 149001, Training Loss: 33958, Validation Loss: 58428, 198263.89113661845\n",
      "Epoch 149101, Training Loss: 34718, Validation Loss: 52703, 243039.8578310973\n",
      "Epoch 149201, Training Loss: 32964, Validation Loss: 67518, 192325.57440814664\n",
      "Epoch 149301, Training Loss: 33369, Validation Loss: 54002, 204233.9349590009\n",
      "Epoch 149401, Training Loss: 33818, Validation Loss: 53338, 256806.02707664695\n",
      "Epoch 149501, Training Loss: 35042, Validation Loss: 58833, 216260.50295002016\n",
      "Epoch 149601, Training Loss: 36720, Validation Loss: 74514, 207978.27142069544\n",
      "Epoch 149701, Training Loss: 33552, Validation Loss: 49082, 188723.07244101705\n",
      "Epoch 149801, Training Loss: 37099, Validation Loss: 53195, 196901.75957665825\n",
      "Epoch 149901, Training Loss: 33570, Validation Loss: 50519, 168066.98639811893\n",
      "Epoch 150001, Training Loss: 33793, Validation Loss: 52583, 207062.4015897525\n",
      "Epoch 150101, Training Loss: 32573, Validation Loss: 48260, 204134.82442894476\n",
      "Epoch 150201, Training Loss: 35758, Validation Loss: 53726, 195879.92823143266\n",
      "Epoch 150301, Training Loss: 34698, Validation Loss: 50836, 176032.73849929616\n",
      "Epoch 150401, Training Loss: 35321, Validation Loss: 60018, 238492.00941150123\n",
      "Epoch 150501, Training Loss: 33744, Validation Loss: 49587, 203607.2670758334\n",
      "Epoch 150601, Training Loss: 33759, Validation Loss: 50500, 206600.39386253743\n",
      "Epoch 150701, Training Loss: 33766, Validation Loss: 52919, 160382.52533576443\n",
      "Epoch 150801, Training Loss: 32163, Validation Loss: 51861, 234614.44467032526\n",
      "Epoch 150901, Training Loss: 36258, Validation Loss: 60457, 197102.89986212013\n",
      "Epoch 151001, Training Loss: 32547, Validation Loss: 48457, 170080.21119064777\n",
      "Epoch 151101, Training Loss: 33738, Validation Loss: 55483, 188252.47397909567\n",
      "Epoch 151201, Training Loss: 33269, Validation Loss: 50961, 196076.17437597422\n",
      "Epoch 151301, Training Loss: 31990, Validation Loss: 52453, 179705.26700186485\n",
      "Epoch 151401, Training Loss: 34040, Validation Loss: 49785, 181919.48154525374\n",
      "Epoch 151501, Training Loss: 32519, Validation Loss: 71849, 205081.78849923206\n",
      "Epoch 151601, Training Loss: 34437, Validation Loss: 56334, 221176.4324746557\n",
      "Epoch 151701, Training Loss: 32350, Validation Loss: 50416, 205566.48221624977\n",
      "Epoch 151801, Training Loss: 33504, Validation Loss: 52817, 195859.47856878993\n",
      "Epoch 151901, Training Loss: 33319, Validation Loss: 63752, 204779.99236241766\n",
      "Epoch 152001, Training Loss: 32611, Validation Loss: 51330, 182662.51318470473\n",
      "Epoch 152101, Training Loss: 32846, Validation Loss: 54678, 188135.6914029792\n",
      "Epoch 152201, Training Loss: 32854, Validation Loss: 52786, 173180.40105665234\n",
      "Epoch 152301, Training Loss: 33292, Validation Loss: 71496, 201285.868859387\n",
      "Epoch 152401, Training Loss: 34821, Validation Loss: 51699, 204189.84421985855\n",
      "Epoch 152501, Training Loss: 32561, Validation Loss: 55350, 164600.83995444738\n",
      "Epoch 152601, Training Loss: 32906, Validation Loss: 50637, 176624.39977201584\n",
      "Epoch 152701, Training Loss: 32666, Validation Loss: 59585, 235492.04312680304\n",
      "Epoch 152801, Training Loss: 37822, Validation Loss: 55465, 264648.26751199446\n",
      "Epoch 152901, Training Loss: 32944, Validation Loss: 51848, 167001.72869751163\n",
      "Epoch 153001, Training Loss: 31492, Validation Loss: 58168, 175982.43013981063\n",
      "Epoch 153101, Training Loss: 32041, Validation Loss: 49331, 227822.21939362385\n",
      "Epoch 153201, Training Loss: 33218, Validation Loss: 70129, 194429.62467097366\n",
      "Epoch 153301, Training Loss: 34888, Validation Loss: 57847, 185281.0114089707\n",
      "Epoch 153401, Training Loss: 35566, Validation Loss: 76087, 205984.8020018756\n",
      "Epoch 153501, Training Loss: 32858, Validation Loss: 54658, 199382.8584437665\n",
      "Epoch 153601, Training Loss: 35278, Validation Loss: 50711, 317788.5570662434\n",
      "Epoch 153701, Training Loss: 32696, Validation Loss: 71686, 170236.2811593742\n",
      "Epoch 153801, Training Loss: 34729, Validation Loss: 55077, 237307.09651344633\n",
      "Epoch 153901, Training Loss: 34279, Validation Loss: 55264, 228769.0458458003\n",
      "Epoch 154001, Training Loss: 33633, Validation Loss: 58015, 161487.56478305164\n",
      "Epoch 154101, Training Loss: 33290, Validation Loss: 54272, 201271.63890001117\n",
      "Epoch 154201, Training Loss: 32923, Validation Loss: 53498, 188698.5370331742\n",
      "Epoch 154301, Training Loss: 34261, Validation Loss: 49168, 233190.3411067964\n",
      "Epoch 154401, Training Loss: 34290, Validation Loss: 63117, 210388.95141292783\n",
      "Epoch 154501, Training Loss: 33803, Validation Loss: 53016, 217285.09760187197\n",
      "Epoch 154601, Training Loss: 34920, Validation Loss: 67448, 236498.3834085986\n",
      "Epoch 154701, Training Loss: 33161, Validation Loss: 52214, 176048.55686420927\n",
      "Epoch 154801, Training Loss: 32965, Validation Loss: 54504, 195757.82999030044\n",
      "Epoch 154901, Training Loss: 32539, Validation Loss: 50995, 210523.18389716066\n",
      "Epoch 155001, Training Loss: 35033, Validation Loss: 90697, 247576.31683503045\n",
      "Epoch 155101, Training Loss: 33557, Validation Loss: 51207, 194840.54987374798\n",
      "Epoch 155201, Training Loss: 37001, Validation Loss: 50980, 201252.26944242392\n",
      "Epoch 155301, Training Loss: 38332, Validation Loss: 52618, 200685.07713711666\n",
      "Epoch 155401, Training Loss: 33468, Validation Loss: 69331, 182308.69583978984\n",
      "Epoch 155501, Training Loss: 31963, Validation Loss: 59727, 168742.2260431136\n",
      "Epoch 155601, Training Loss: 32815, Validation Loss: 51031, 195485.2209562289\n",
      "Epoch 155701, Training Loss: 34282, Validation Loss: 52271, 177749.933564711\n",
      "Epoch 155801, Training Loss: 32657, Validation Loss: 81959, 201325.82579198413\n",
      "Epoch 155901, Training Loss: 34930, Validation Loss: 54511, 209369.33689317305\n",
      "Epoch 156001, Training Loss: 32968, Validation Loss: 63342, 172996.60996369828\n",
      "Epoch 156101, Training Loss: 33623, Validation Loss: 51736, 185544.231264519\n",
      "Epoch 156201, Training Loss: 33102, Validation Loss: 51303, 165337.59409466587\n",
      "Epoch 156301, Training Loss: 34021, Validation Loss: 53186, 183503.0348855855\n",
      "Epoch 156401, Training Loss: 32544, Validation Loss: 57615, 239840.34425474075\n",
      "Epoch 156501, Training Loss: 32554, Validation Loss: 52217, 173511.61960265928\n",
      "Epoch 156601, Training Loss: 36612, Validation Loss: 67480, 326001.1948263645\n",
      "Epoch 156701, Training Loss: 33819, Validation Loss: 51858, 210463.8261095752\n",
      "Epoch 156801, Training Loss: 33020, Validation Loss: 54595, 199363.73519747553\n",
      "Epoch 156901, Training Loss: 32613, Validation Loss: 50814, 196512.38282296582\n",
      "Epoch 157001, Training Loss: 35080, Validation Loss: 52100, 210733.7753780828\n",
      "Epoch 157101, Training Loss: 33860, Validation Loss: 62589, 247466.52548259989\n",
      "Epoch 157201, Training Loss: 32046, Validation Loss: 50728, 194193.57732125893\n",
      "Epoch 157301, Training Loss: 33002, Validation Loss: 55145, 187491.12581436164\n",
      "Epoch 157401, Training Loss: 33882, Validation Loss: 56281, 242167.92854136447\n",
      "Epoch 157501, Training Loss: 32774, Validation Loss: 51563, 185504.71869596478\n",
      "Epoch 157601, Training Loss: 32868, Validation Loss: 70247, 229530.3793181245\n",
      "Epoch 157701, Training Loss: 33806, Validation Loss: 50479, 204871.81802544347\n",
      "Epoch 157801, Training Loss: 33093, Validation Loss: 57898, 188817.25338451075\n",
      "Epoch 157901, Training Loss: 32249, Validation Loss: 49696, 221388.2317856852\n",
      "Epoch 158001, Training Loss: 32537, Validation Loss: 56088, 187693.2968084618\n",
      "Epoch 158101, Training Loss: 33310, Validation Loss: 49626, 168702.8106359738\n",
      "Epoch 158201, Training Loss: 33036, Validation Loss: 52099, 195927.1520725919\n",
      "Epoch 158301, Training Loss: 34112, Validation Loss: 53022, 195560.20900434768\n",
      "Epoch 158401, Training Loss: 34215, Validation Loss: 63492, 200212.22857380533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158501, Training Loss: 32653, Validation Loss: 52143, 147038.6450810734\n",
      "Epoch 158601, Training Loss: 33835, Validation Loss: 55363, 188780.71978873652\n",
      "Epoch 158701, Training Loss: 33563, Validation Loss: 54594, 211636.83162037792\n",
      "Epoch 158801, Training Loss: 31650, Validation Loss: 68019, 194712.20962391575\n",
      "Epoch 158901, Training Loss: 32842, Validation Loss: 49227, 192157.73627235446\n",
      "Epoch 159001, Training Loss: 34670, Validation Loss: 72933, 218358.4822329407\n",
      "Epoch 159101, Training Loss: 32700, Validation Loss: 49199, 232676.94877664605\n",
      "Epoch 159201, Training Loss: 33803, Validation Loss: 55999, 183187.9594460811\n",
      "Epoch 159301, Training Loss: 32179, Validation Loss: 54377, 208256.9504820206\n",
      "Epoch 159401, Training Loss: 32923, Validation Loss: 59898, 190678.03265025828\n",
      "Epoch 159501, Training Loss: 33771, Validation Loss: 50306, 179583.67709146472\n",
      "Epoch 159601, Training Loss: 34565, Validation Loss: 59718, 277850.54974593845\n",
      "Epoch 159701, Training Loss: 33799, Validation Loss: 52135, 173515.81181498387\n",
      "Epoch 159801, Training Loss: 34151, Validation Loss: 53969, 166901.89947098275\n",
      "Epoch 159901, Training Loss: 32650, Validation Loss: 49452, 199950.93381821841\n",
      "Epoch 160001, Training Loss: 32424, Validation Loss: 48518, 210803.71650027818\n",
      "Epoch 160101, Training Loss: 32548, Validation Loss: 50409, 180787.37253234288\n",
      "Epoch 160201, Training Loss: 32175, Validation Loss: 66079, 185630.22991343852\n",
      "Epoch 160301, Training Loss: 34817, Validation Loss: 52908, 218036.9615190056\n",
      "Epoch 160401, Training Loss: 32783, Validation Loss: 54354, 204558.42837533852\n",
      "Epoch 160501, Training Loss: 33817, Validation Loss: 48331, 207512.32975034835\n",
      "Epoch 160601, Training Loss: 36636, Validation Loss: 56254, 200615.2908692533\n",
      "Epoch 160701, Training Loss: 34840, Validation Loss: 54676, 157542.35070564333\n",
      "Epoch 160801, Training Loss: 34986, Validation Loss: 72459, 217282.0090633908\n",
      "Epoch 160901, Training Loss: 34045, Validation Loss: 51746, 199841.67498890575\n",
      "Epoch 161001, Training Loss: 33959, Validation Loss: 48981, 184628.7202049507\n",
      "Epoch 161101, Training Loss: 35563, Validation Loss: 67224, 275330.6077005466\n",
      "Epoch 161201, Training Loss: 33889, Validation Loss: 51592, 179112.66838740793\n",
      "Epoch 161301, Training Loss: 32879, Validation Loss: 58756, 224827.40797630558\n",
      "Epoch 161401, Training Loss: 31608, Validation Loss: 61572, 174181.9889116915\n",
      "Epoch 161501, Training Loss: 31866, Validation Loss: 61529, 211744.1079187354\n",
      "Epoch 161601, Training Loss: 32979, Validation Loss: 54257, 177174.6956699527\n",
      "Epoch 161701, Training Loss: 33669, Validation Loss: 54224, 252683.35598919666\n",
      "Epoch 161801, Training Loss: 33655, Validation Loss: 51296, 215460.11045537415\n",
      "Epoch 161901, Training Loss: 32286, Validation Loss: 55160, 210988.16294520418\n",
      "Epoch 162001, Training Loss: 34107, Validation Loss: 55019, 163288.93750521514\n",
      "Epoch 162101, Training Loss: 32109, Validation Loss: 51252, 211360.05829018657\n",
      "Epoch 162201, Training Loss: 32802, Validation Loss: 56058, 189060.15629454513\n",
      "Epoch 162301, Training Loss: 32925, Validation Loss: 55543, 219711.70245557945\n",
      "Epoch 162401, Training Loss: 35135, Validation Loss: 52638, 214073.6946819947\n",
      "Epoch 162501, Training Loss: 33526, Validation Loss: 51553, 180770.39842432181\n",
      "Epoch 162601, Training Loss: 34502, Validation Loss: 50872, 218845.72419588882\n",
      "Epoch 162701, Training Loss: 34300, Validation Loss: 68420, 196084.77327393708\n",
      "Epoch 162801, Training Loss: 32585, Validation Loss: 56307, 208804.18945356234\n",
      "Epoch 162901, Training Loss: 34676, Validation Loss: 55442, 235351.266110744\n",
      "Epoch 163001, Training Loss: 33862, Validation Loss: 56022, 221645.31668730732\n",
      "Epoch 163101, Training Loss: 35491, Validation Loss: 54424, 202815.70138347257\n",
      "Epoch 163201, Training Loss: 32538, Validation Loss: 51083, 204212.64421444677\n",
      "Epoch 163301, Training Loss: 34605, Validation Loss: 58266, 189621.49500277496\n",
      "Epoch 163401, Training Loss: 32518, Validation Loss: 54524, 208713.7380819595\n",
      "Epoch 163501, Training Loss: 35281, Validation Loss: 51614, 204035.13004786117\n",
      "Epoch 163601, Training Loss: 34235, Validation Loss: 51982, 201758.73549901554\n",
      "Epoch 163701, Training Loss: 32049, Validation Loss: 52628, 158031.2392833119\n",
      "Epoch 163801, Training Loss: 33798, Validation Loss: 52045, 167815.24115205\n",
      "Epoch 163901, Training Loss: 34287, Validation Loss: 58935, 223007.07117566446\n",
      "Epoch 164001, Training Loss: 31642, Validation Loss: 54501, 178883.47473128795\n",
      "Epoch 164101, Training Loss: 35761, Validation Loss: 59012, 223967.59381950635\n",
      "Epoch 164201, Training Loss: 33689, Validation Loss: 57731, 229837.65532411184\n",
      "Epoch 164301, Training Loss: 36837, Validation Loss: 58894, 189866.24502459113\n",
      "Epoch 164401, Training Loss: 35215, Validation Loss: 51949, 200291.6455095682\n",
      "Epoch 164501, Training Loss: 33310, Validation Loss: 96395, 190299.32833206697\n",
      "Epoch 164601, Training Loss: 33186, Validation Loss: 53605, 189139.97500981996\n",
      "Epoch 164701, Training Loss: 33505, Validation Loss: 52433, 227150.86059072698\n",
      "Epoch 164801, Training Loss: 34213, Validation Loss: 52854, 168632.84613439013\n",
      "Epoch 164901, Training Loss: 34116, Validation Loss: 48902, 204301.36062035974\n",
      "Epoch 165001, Training Loss: 34245, Validation Loss: 68597, 203029.36400440967\n",
      "Epoch 165101, Training Loss: 33831, Validation Loss: 54860, 180667.38922700935\n",
      "Epoch 165201, Training Loss: 31957, Validation Loss: 56539, 176310.51962330568\n",
      "Epoch 165301, Training Loss: 32049, Validation Loss: 57947, 189864.8089858108\n",
      "Epoch 165401, Training Loss: 33849, Validation Loss: 52013, 197035.87835504246\n",
      "Epoch 165501, Training Loss: 33419, Validation Loss: 57491, 216798.3905740097\n",
      "Epoch 165601, Training Loss: 33976, Validation Loss: 52791, 193292.5429643289\n",
      "Epoch 165701, Training Loss: 34202, Validation Loss: 58010, 179364.67792004647\n",
      "Epoch 165801, Training Loss: 31816, Validation Loss: 59321, 191222.8486912955\n",
      "Epoch 165901, Training Loss: 33706, Validation Loss: 69321, 216478.76421742854\n",
      "Epoch 166001, Training Loss: 32773, Validation Loss: 49013, 234060.3656005912\n",
      "Epoch 166101, Training Loss: 32466, Validation Loss: 63435, 207700.2839633226\n",
      "Epoch 166201, Training Loss: 32765, Validation Loss: 51765, 201763.53115248066\n",
      "Epoch 166301, Training Loss: 32656, Validation Loss: 53948, 235081.99834815087\n",
      "Epoch 166401, Training Loss: 33603, Validation Loss: 56568, 178847.53086464552\n",
      "Epoch 166501, Training Loss: 34025, Validation Loss: 70473, 227004.75788058064\n",
      "Epoch 166601, Training Loss: 34603, Validation Loss: 56546, 199335.78980626844\n",
      "Epoch 166701, Training Loss: 33074, Validation Loss: 55513, 207858.95935013168\n",
      "Epoch 166801, Training Loss: 33773, Validation Loss: 54484, 195390.9357155607\n",
      "Epoch 166901, Training Loss: 33354, Validation Loss: 50302, 195186.39478242712\n",
      "Epoch 167001, Training Loss: 34847, Validation Loss: 70723, 209627.2906797895\n",
      "Epoch 167101, Training Loss: 34697, Validation Loss: 50829, 207878.8041473008\n",
      "Epoch 167201, Training Loss: 33717, Validation Loss: 53598, 202077.6646888964\n",
      "Epoch 167301, Training Loss: 32721, Validation Loss: 51383, 224738.92083359658\n",
      "Epoch 167401, Training Loss: 36026, Validation Loss: 54885, 167885.93620875027\n",
      "Epoch 167501, Training Loss: 32072, Validation Loss: 50203, 194390.2930616401\n",
      "Epoch 167601, Training Loss: 32784, Validation Loss: 50062, 193664.2939500108\n",
      "Epoch 167701, Training Loss: 32626, Validation Loss: 52033, 212604.21746573303\n",
      "Epoch 167801, Training Loss: 33019, Validation Loss: 51211, 167280.89548855159\n",
      "Epoch 167901, Training Loss: 34097, Validation Loss: 88691, 233355.39281484726\n",
      "Epoch 168001, Training Loss: 33120, Validation Loss: 58255, 207677.29506965217\n",
      "Epoch 168101, Training Loss: 34394, Validation Loss: 55789, 235350.83176669292\n",
      "Epoch 168201, Training Loss: 33960, Validation Loss: 50028, 195058.56917952327\n",
      "Epoch 168301, Training Loss: 33408, Validation Loss: 59278, 189121.42005661633\n",
      "Epoch 168401, Training Loss: 32655, Validation Loss: 49551, 162684.38467139032\n",
      "Epoch 168501, Training Loss: 33183, Validation Loss: 66222, 220077.49996538425\n",
      "Epoch 168601, Training Loss: 33564, Validation Loss: 78097, 218829.8602735398\n",
      "Epoch 168701, Training Loss: 33328, Validation Loss: 52476, 156046.52081137808\n",
      "Epoch 168801, Training Loss: 50470, Validation Loss: 61637, 43710.206906195344\n",
      "Epoch 168901, Training Loss: 45538, Validation Loss: 59185, 42484.343824141884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169001, Training Loss: 44102, Validation Loss: 62450, 52376.843470201326\n",
      "Epoch 169101, Training Loss: 42383, Validation Loss: 59148, 59236.7442118829\n",
      "Epoch 169201, Training Loss: 42075, Validation Loss: 60270, 63789.703216805334\n",
      "Epoch 169301, Training Loss: 41917, Validation Loss: 55866, 66653.34109548676\n",
      "Epoch 169401, Training Loss: 41943, Validation Loss: 59434, 70410.87929628642\n",
      "Epoch 169501, Training Loss: 41278, Validation Loss: 58101, 75139.20701657278\n",
      "Epoch 169601, Training Loss: 41042, Validation Loss: 57996, 97519.82216370938\n",
      "Epoch 169701, Training Loss: 39984, Validation Loss: 55346, 99590.10239590384\n",
      "Epoch 169801, Training Loss: 38166, Validation Loss: 88655, 160012.7287487977\n",
      "Epoch 169901, Training Loss: 36013, Validation Loss: 56855, 120128.19842855299\n",
      "Epoch 170001, Training Loss: 37411, Validation Loss: 57721, 158531.87641890856\n",
      "Epoch 170101, Training Loss: 37344, Validation Loss: 48954, 139526.52444783415\n",
      "Epoch 170201, Training Loss: 39147, Validation Loss: 62129, 141082.3149513301\n",
      "Epoch 170301, Training Loss: 36508, Validation Loss: 48576, 185972.35384725826\n",
      "Epoch 170401, Training Loss: 36505, Validation Loss: 53555, 144942.44901840563\n",
      "Epoch 170501, Training Loss: 37123, Validation Loss: 50294, 176481.2488883504\n",
      "Epoch 170601, Training Loss: 35152, Validation Loss: 51574, 160008.12066218772\n",
      "Epoch 170701, Training Loss: 37125, Validation Loss: 51093, 154921.65507960305\n",
      "Epoch 170801, Training Loss: 36212, Validation Loss: 49457, 166399.78084559622\n",
      "Epoch 170901, Training Loss: 36093, Validation Loss: 52375, 178149.0208842247\n",
      "Epoch 171001, Training Loss: 38617, Validation Loss: 50968, 253075.87324793683\n",
      "Epoch 171101, Training Loss: 36367, Validation Loss: 49730, 191497.8715957013\n",
      "Epoch 171201, Training Loss: 34998, Validation Loss: 61140, 160695.82536932823\n",
      "Epoch 171301, Training Loss: 36654, Validation Loss: 46892, 187423.20257813483\n",
      "Epoch 171401, Training Loss: 34983, Validation Loss: 55907, 165262.2524076253\n",
      "Epoch 171501, Training Loss: 34877, Validation Loss: 59211, 168818.753810289\n",
      "Epoch 171601, Training Loss: 33759, Validation Loss: 49969, 167426.23978328827\n",
      "Epoch 171701, Training Loss: 35420, Validation Loss: 50119, 213585.8774768239\n",
      "Epoch 171801, Training Loss: 34620, Validation Loss: 47125, 147171.46099664422\n",
      "Epoch 171901, Training Loss: 34714, Validation Loss: 47977, 164776.09649789348\n",
      "Epoch 172001, Training Loss: 36397, Validation Loss: 57411, 153750.52856252543\n",
      "Epoch 172101, Training Loss: 35181, Validation Loss: 54897, 197061.11732274955\n",
      "Epoch 172201, Training Loss: 37887, Validation Loss: 50897, 158198.8029503553\n",
      "Epoch 172301, Training Loss: 34983, Validation Loss: 59214, 185821.97817264145\n",
      "Epoch 172401, Training Loss: 35103, Validation Loss: 72867, 175386.75432540002\n",
      "Epoch 172501, Training Loss: 33490, Validation Loss: 47742, 160263.56892037697\n",
      "Epoch 172601, Training Loss: 35511, Validation Loss: 53246, 166000.77083995697\n",
      "Epoch 172701, Training Loss: 36686, Validation Loss: 53722, 161010.93472549794\n",
      "Epoch 172801, Training Loss: 35919, Validation Loss: 61363, 174529.86963446587\n",
      "Epoch 172901, Training Loss: 34043, Validation Loss: 48683, 173294.66882866598\n",
      "Epoch 173001, Training Loss: 34044, Validation Loss: 49443, 171162.3710806629\n",
      "Epoch 173101, Training Loss: 34592, Validation Loss: 49001, 173802.01310153623\n",
      "Epoch 173201, Training Loss: 34824, Validation Loss: 54205, 161754.6805447731\n",
      "Epoch 173301, Training Loss: 35134, Validation Loss: 51667, 166979.2089462249\n",
      "Epoch 173401, Training Loss: 36307, Validation Loss: 47779, 176634.43626852197\n",
      "Epoch 173501, Training Loss: 33191, Validation Loss: 55263, 159584.02976147717\n",
      "Epoch 173601, Training Loss: 35029, Validation Loss: 56835, 172739.7542645301\n",
      "Epoch 173701, Training Loss: 34682, Validation Loss: 53562, 162607.4898418118\n",
      "Epoch 173801, Training Loss: 34004, Validation Loss: 54241, 151827.89578303645\n",
      "Epoch 173901, Training Loss: 34917, Validation Loss: 52133, 173029.11369537565\n",
      "Epoch 174001, Training Loss: 33602, Validation Loss: 48301, 145887.81662285727\n",
      "Epoch 174101, Training Loss: 34996, Validation Loss: 59644, 176475.91396596457\n",
      "Epoch 174201, Training Loss: 35223, Validation Loss: 55291, 170495.91579558127\n",
      "Epoch 174301, Training Loss: 34959, Validation Loss: 49731, 167682.68531799637\n",
      "Epoch 174401, Training Loss: 37151, Validation Loss: 55857, 179188.4998440789\n",
      "Epoch 174501, Training Loss: 34167, Validation Loss: 51924, 187485.24267132077\n",
      "Epoch 174601, Training Loss: 34643, Validation Loss: 58264, 174114.52758121738\n",
      "Epoch 174701, Training Loss: 35161, Validation Loss: 53847, 171985.22254000028\n",
      "Epoch 174801, Training Loss: 33719, Validation Loss: 51925, 206298.91792824262\n",
      "Epoch 174901, Training Loss: 34215, Validation Loss: 54984, 187915.30170154522\n",
      "Epoch 175001, Training Loss: 33752, Validation Loss: 55577, 180056.85140852453\n",
      "Epoch 175101, Training Loss: 36066, Validation Loss: 49330, 175965.91220620574\n",
      "Epoch 175201, Training Loss: 33598, Validation Loss: 51304, 177003.65124084195\n",
      "Epoch 175301, Training Loss: 34547, Validation Loss: 49374, 166519.08711940967\n",
      "Epoch 175401, Training Loss: 33855, Validation Loss: 48588, 165567.01583859892\n",
      "Epoch 175501, Training Loss: 35386, Validation Loss: 52756, 167444.20307402534\n",
      "Epoch 175601, Training Loss: 34524, Validation Loss: 54362, 168335.42137829206\n",
      "Epoch 175701, Training Loss: 35180, Validation Loss: 61928, 150233.1564584714\n",
      "Epoch 175801, Training Loss: 33212, Validation Loss: 63040, 184455.63499750904\n",
      "Epoch 175901, Training Loss: 33814, Validation Loss: 47843, 150836.873072218\n",
      "Epoch 176001, Training Loss: 35577, Validation Loss: 74662, 188479.61675301843\n",
      "Epoch 176101, Training Loss: 33678, Validation Loss: 52299, 178919.79887143392\n",
      "Epoch 176201, Training Loss: 35056, Validation Loss: 58973, 186217.71885121914\n",
      "Epoch 176301, Training Loss: 33081, Validation Loss: 51087, 145170.7103482233\n",
      "Epoch 176401, Training Loss: 34995, Validation Loss: 75266, 180356.5535274099\n",
      "Epoch 176501, Training Loss: 33637, Validation Loss: 49398, 189067.1493683035\n",
      "Epoch 176601, Training Loss: 33144, Validation Loss: 58870, 191144.15413323208\n",
      "Epoch 176701, Training Loss: 35109, Validation Loss: 60555, 171700.3657423184\n",
      "Epoch 176801, Training Loss: 31977, Validation Loss: 60609, 147598.43600113213\n",
      "Epoch 176901, Training Loss: 35491, Validation Loss: 50866, 160449.42549230703\n",
      "Epoch 177001, Training Loss: 33482, Validation Loss: 51395, 164973.80010585295\n",
      "Epoch 177101, Training Loss: 35832, Validation Loss: 66801, 180609.1065033277\n",
      "Epoch 177201, Training Loss: 33128, Validation Loss: 87319, 179913.35072175777\n",
      "Epoch 177301, Training Loss: 32825, Validation Loss: 47547, 164157.06119375819\n",
      "Epoch 177401, Training Loss: 33523, Validation Loss: 51636, 169400.2930980609\n",
      "Epoch 177501, Training Loss: 34344, Validation Loss: 47362, 178537.35929123\n",
      "Epoch 177601, Training Loss: 35095, Validation Loss: 56533, 192619.20333287932\n",
      "Epoch 177701, Training Loss: 34729, Validation Loss: 51598, 181485.29869994175\n",
      "Epoch 177801, Training Loss: 32304, Validation Loss: 50492, 177111.0432675439\n",
      "Epoch 177901, Training Loss: 33222, Validation Loss: 50640, 174118.74155017806\n",
      "Epoch 178001, Training Loss: 32302, Validation Loss: 51949, 170667.62701468458\n",
      "Epoch 178101, Training Loss: 32555, Validation Loss: 47564, 197415.82450228953\n",
      "Epoch 178201, Training Loss: 34937, Validation Loss: 48183, 179103.92362699367\n",
      "Epoch 178301, Training Loss: 32474, Validation Loss: 49491, 171814.60552931912\n",
      "Epoch 178401, Training Loss: 31754, Validation Loss: 61123, 181090.93060772587\n",
      "Epoch 178501, Training Loss: 33145, Validation Loss: 58966, 184645.7894613184\n",
      "Epoch 178601, Training Loss: 34282, Validation Loss: 67365, 170038.5674858549\n",
      "Epoch 178701, Training Loss: 33455, Validation Loss: 48584, 193303.97230454825\n",
      "Epoch 178801, Training Loss: 32931, Validation Loss: 50572, 151485.8656331922\n",
      "Epoch 178901, Training Loss: 34995, Validation Loss: 57934, 180429.03416989816\n",
      "Epoch 179001, Training Loss: 34449, Validation Loss: 67114, 173166.80068289154\n",
      "Epoch 179101, Training Loss: 35496, Validation Loss: 53735, 182377.75047267773\n",
      "Epoch 179201, Training Loss: 34321, Validation Loss: 50901, 175679.33723160453\n",
      "Epoch 179301, Training Loss: 33634, Validation Loss: 72193, 242400.03307606955\n",
      "Epoch 179401, Training Loss: 33788, Validation Loss: 50762, 176936.04842010132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179501, Training Loss: 34901, Validation Loss: 48794, 154879.88329531605\n",
      "Epoch 179601, Training Loss: 33392, Validation Loss: 56535, 179447.87926718997\n",
      "Epoch 179701, Training Loss: 33694, Validation Loss: 55780, 197476.1894371527\n",
      "Epoch 179801, Training Loss: 34689, Validation Loss: 56152, 233739.55658490106\n",
      "Epoch 179901, Training Loss: 32424, Validation Loss: 52723, 167070.24594891287\n",
      "Epoch 180001, Training Loss: 31211, Validation Loss: 50411, 156740.06501374487\n",
      "Epoch 180101, Training Loss: 35852, Validation Loss: 52225, 172357.79820523196\n",
      "Epoch 180201, Training Loss: 32737, Validation Loss: 52620, 194945.49014862478\n",
      "Epoch 180301, Training Loss: 33299, Validation Loss: 48088, 199905.64560959744\n",
      "Epoch 180401, Training Loss: 32783, Validation Loss: 51584, 184217.96661485356\n",
      "Epoch 180501, Training Loss: 34392, Validation Loss: 61509, 218287.7529447143\n",
      "Epoch 180601, Training Loss: 34066, Validation Loss: 53445, 171059.97002020842\n",
      "Epoch 180701, Training Loss: 34060, Validation Loss: 63279, 198995.24942298874\n",
      "Epoch 180801, Training Loss: 33023, Validation Loss: 57436, 185840.88868569338\n",
      "Epoch 180901, Training Loss: 31965, Validation Loss: 48635, 169604.2137358467\n",
      "Epoch 181001, Training Loss: 33262, Validation Loss: 54016, 181919.0519849443\n",
      "Epoch 181101, Training Loss: 37472, Validation Loss: 111044, 217764.97747551443\n",
      "Epoch 181201, Training Loss: 35146, Validation Loss: 50342, 185820.1643952094\n",
      "Epoch 181301, Training Loss: 36024, Validation Loss: 54475, 193289.5686801543\n",
      "Epoch 181401, Training Loss: 31875, Validation Loss: 52993, 189924.75779337095\n",
      "Epoch 181501, Training Loss: 33200, Validation Loss: 53221, 176324.70217180392\n",
      "Epoch 181601, Training Loss: 34874, Validation Loss: 52385, 221884.46672775949\n",
      "Epoch 181701, Training Loss: 34115, Validation Loss: 52994, 202981.1822780777\n",
      "Epoch 181801, Training Loss: 32026, Validation Loss: 53914, 188896.15669913593\n",
      "Epoch 181901, Training Loss: 32519, Validation Loss: 54635, 167423.09648182156\n",
      "Epoch 182001, Training Loss: 34060, Validation Loss: 51028, 209988.15671643082\n",
      "Epoch 182101, Training Loss: 32575, Validation Loss: 57597, 190233.76513668185\n",
      "Epoch 182201, Training Loss: 34356, Validation Loss: 52512, 233166.07866639193\n",
      "Epoch 182301, Training Loss: 35098, Validation Loss: 77362, 187395.64159199139\n",
      "Epoch 182401, Training Loss: 35055, Validation Loss: 63416, 239701.93916504024\n",
      "Epoch 182501, Training Loss: 32537, Validation Loss: 54060, 141617.8214301797\n",
      "Epoch 182601, Training Loss: 32210, Validation Loss: 51458, 198403.65307793525\n",
      "Epoch 182701, Training Loss: 33394, Validation Loss: 55276, 169868.4291843537\n",
      "Epoch 182801, Training Loss: 31714, Validation Loss: 53962, 136528.29958531188\n",
      "Epoch 182901, Training Loss: 33704, Validation Loss: 62010, 183587.51208907083\n",
      "Epoch 183001, Training Loss: 36921, Validation Loss: 51347, 252250.95227410705\n",
      "Epoch 183101, Training Loss: 32364, Validation Loss: 53382, 180395.1098472368\n",
      "Epoch 183201, Training Loss: 32839, Validation Loss: 60825, 167044.92166375162\n",
      "Epoch 183301, Training Loss: 35090, Validation Loss: 90718, 237849.3379260063\n",
      "Epoch 183401, Training Loss: 32886, Validation Loss: 51983, 187177.26778999637\n",
      "Epoch 183501, Training Loss: 33652, Validation Loss: 68179, 189003.92896240446\n",
      "Epoch 183601, Training Loss: 32741, Validation Loss: 52289, 156035.35901619747\n",
      "Epoch 183701, Training Loss: 34132, Validation Loss: 51532, 183613.2791883134\n",
      "Epoch 183801, Training Loss: 34561, Validation Loss: 51355, 194110.56634719367\n",
      "Epoch 183901, Training Loss: 33709, Validation Loss: 52109, 199066.24312790297\n",
      "Epoch 184001, Training Loss: 33477, Validation Loss: 63267, 196861.43085148744\n",
      "Epoch 184101, Training Loss: 31499, Validation Loss: 55504, 161833.0826239659\n",
      "Epoch 184201, Training Loss: 32868, Validation Loss: 77472, 207579.68669591506\n",
      "Epoch 184301, Training Loss: 31666, Validation Loss: 54519, 174786.71204906038\n",
      "Epoch 184401, Training Loss: 31988, Validation Loss: 50204, 197679.42319613535\n",
      "Epoch 184501, Training Loss: 32970, Validation Loss: 55442, 179103.8019075083\n",
      "Epoch 184601, Training Loss: 36945, Validation Loss: 54599, 209965.63221508838\n",
      "Epoch 184701, Training Loss: 33394, Validation Loss: 50127, 164471.44742074716\n",
      "Epoch 184801, Training Loss: 31995, Validation Loss: 52633, 161335.1314562621\n",
      "Epoch 184901, Training Loss: 32317, Validation Loss: 52160, 190175.9357702655\n",
      "Epoch 185001, Training Loss: 33171, Validation Loss: 50093, 156167.4092444683\n",
      "Epoch 185101, Training Loss: 34518, Validation Loss: 57262, 172920.00911601575\n",
      "Epoch 185201, Training Loss: 32736, Validation Loss: 55570, 139403.8910990988\n",
      "Epoch 185301, Training Loss: 34229, Validation Loss: 58074, 196908.7812634987\n",
      "Epoch 185401, Training Loss: 33078, Validation Loss: 50462, 206945.62529615077\n",
      "Epoch 185501, Training Loss: 32852, Validation Loss: 52498, 177473.21356861966\n",
      "Epoch 185601, Training Loss: 32872, Validation Loss: 53785, 181756.52111494276\n",
      "Epoch 185701, Training Loss: 32799, Validation Loss: 53753, 183351.0238858133\n",
      "Epoch 185801, Training Loss: 32729, Validation Loss: 52221, 167319.4059966093\n",
      "Epoch 185901, Training Loss: 33435, Validation Loss: 48387, 179606.84849435245\n",
      "Epoch 186001, Training Loss: 33038, Validation Loss: 84204, 201701.60083844126\n",
      "Epoch 186101, Training Loss: 34035, Validation Loss: 49375, 191067.3686073587\n",
      "Epoch 186201, Training Loss: 33413, Validation Loss: 51862, 179780.29809639123\n",
      "Epoch 186301, Training Loss: 34856, Validation Loss: 48815, 203338.76937616305\n",
      "Epoch 186401, Training Loss: 31824, Validation Loss: 66836, 167582.90349129416\n",
      "Epoch 186501, Training Loss: 32483, Validation Loss: 49700, 197598.13989316835\n",
      "Epoch 186601, Training Loss: 31998, Validation Loss: 49645, 175595.37964127728\n",
      "Epoch 186701, Training Loss: 32631, Validation Loss: 67675, 192004.1912602605\n",
      "Epoch 186801, Training Loss: 33459, Validation Loss: 50330, 201780.8393211956\n",
      "Epoch 186901, Training Loss: 33989, Validation Loss: 57655, 173492.30913527994\n",
      "Epoch 187001, Training Loss: 33828, Validation Loss: 55881, 215456.3449837011\n",
      "Epoch 187101, Training Loss: 32307, Validation Loss: 54728, 172653.00487102373\n",
      "Epoch 187201, Training Loss: 34048, Validation Loss: 54393, 210289.96915117826\n",
      "Epoch 187301, Training Loss: 33682, Validation Loss: 67257, 207258.23670324904\n",
      "Epoch 187401, Training Loss: 33284, Validation Loss: 50722, 170751.71640987837\n",
      "Epoch 187501, Training Loss: 33642, Validation Loss: 56184, 225125.01140030546\n",
      "Epoch 187601, Training Loss: 33679, Validation Loss: 68592, 197520.66181687333\n",
      "Epoch 187701, Training Loss: 35081, Validation Loss: 56678, 177201.00679316328\n",
      "Epoch 187801, Training Loss: 35739, Validation Loss: 51703, 186279.4478759278\n",
      "Epoch 187901, Training Loss: 34843, Validation Loss: 50696, 161788.75303833393\n",
      "Epoch 188001, Training Loss: 34480, Validation Loss: 50487, 195909.66699647406\n",
      "Epoch 188101, Training Loss: 32890, Validation Loss: 50041, 181452.28493144378\n",
      "Epoch 188201, Training Loss: 32375, Validation Loss: 74714, 181656.85476646875\n",
      "Epoch 188301, Training Loss: 36052, Validation Loss: 105552, 229309.53851322326\n",
      "Epoch 188401, Training Loss: 33242, Validation Loss: 49615, 218992.9043799123\n",
      "Epoch 188501, Training Loss: 34549, Validation Loss: 50377, 176269.4801862554\n",
      "Epoch 188601, Training Loss: 33992, Validation Loss: 49767, 178390.5773255568\n",
      "Epoch 188701, Training Loss: 33909, Validation Loss: 57032, 180433.01036443855\n",
      "Epoch 188801, Training Loss: 33497, Validation Loss: 51836, 193500.27660477828\n",
      "Epoch 188901, Training Loss: 33711, Validation Loss: 52882, 204591.28848014097\n",
      "Epoch 189001, Training Loss: 32551, Validation Loss: 52929, 167905.6224440609\n",
      "Epoch 189101, Training Loss: 33033, Validation Loss: 58903, 188351.73647650643\n",
      "Epoch 189201, Training Loss: 32668, Validation Loss: 49855, 165933.85931543788\n",
      "Epoch 189301, Training Loss: 33860, Validation Loss: 49851, 171743.8513027887\n",
      "Epoch 189401, Training Loss: 32736, Validation Loss: 53690, 218732.2543688043\n",
      "Epoch 189501, Training Loss: 34232, Validation Loss: 56123, 192192.57218074866\n",
      "Epoch 189601, Training Loss: 34059, Validation Loss: 104563, 233758.31623521307\n",
      "Epoch 189701, Training Loss: 34476, Validation Loss: 66387, 211884.0768554134\n",
      "Epoch 189801, Training Loss: 32548, Validation Loss: 52988, 154986.24792172384\n",
      "Epoch 189901, Training Loss: 33654, Validation Loss: 51557, 185232.38158686008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190001, Training Loss: 32543, Validation Loss: 56792, 176250.7817827513\n",
      "Epoch 190101, Training Loss: 34646, Validation Loss: 54231, 160854.00460294378\n",
      "Epoch 190201, Training Loss: 31774, Validation Loss: 53876, 175197.93339162195\n",
      "Epoch 190301, Training Loss: 33150, Validation Loss: 56851, 206577.1401224857\n",
      "Epoch 190401, Training Loss: 33046, Validation Loss: 62362, 156279.41322015572\n",
      "Epoch 190501, Training Loss: 32058, Validation Loss: 49927, 234169.9648817062\n",
      "Epoch 190601, Training Loss: 35205, Validation Loss: 51813, 178873.42739505615\n",
      "Epoch 190701, Training Loss: 36454, Validation Loss: 75074, 227376.6230634288\n",
      "Epoch 190801, Training Loss: 31988, Validation Loss: 54334, 180500.5199737586\n",
      "Epoch 190901, Training Loss: 33140, Validation Loss: 50513, 182092.05231007337\n",
      "Epoch 191001, Training Loss: 34377, Validation Loss: 54785, 183537.76919264148\n",
      "Epoch 191101, Training Loss: 33113, Validation Loss: 66013, 169492.39997985202\n",
      "Epoch 191201, Training Loss: 36385, Validation Loss: 51490, 230532.16118723465\n",
      "Epoch 191301, Training Loss: 32335, Validation Loss: 55100, 159853.46146584148\n",
      "Epoch 191401, Training Loss: 32811, Validation Loss: 58816, 191668.72938324584\n",
      "Epoch 191501, Training Loss: 31199, Validation Loss: 65417, 168440.33902412705\n",
      "Epoch 191601, Training Loss: 34017, Validation Loss: 59566, 199937.6467242249\n",
      "Epoch 191701, Training Loss: 30892, Validation Loss: 48567, 196307.18793174252\n",
      "Epoch 191801, Training Loss: 31452, Validation Loss: 55528, 200134.13970040053\n",
      "Epoch 191901, Training Loss: 32543, Validation Loss: 50973, 175143.4048014929\n",
      "Epoch 192001, Training Loss: 31894, Validation Loss: 50719, 180365.3557075709\n",
      "Epoch 192101, Training Loss: 36412, Validation Loss: 51111, 193202.37140541375\n",
      "Epoch 192201, Training Loss: 33387, Validation Loss: 55099, 170141.60947612047\n",
      "Epoch 192301, Training Loss: 33227, Validation Loss: 51115, 168349.89970636627\n",
      "Epoch 192401, Training Loss: 32001, Validation Loss: 53535, 182733.38934236404\n",
      "Epoch 192501, Training Loss: 34872, Validation Loss: 51003, 241163.94543678124\n",
      "Epoch 192601, Training Loss: 33401, Validation Loss: 52931, 165826.3426890668\n",
      "Epoch 192701, Training Loss: 34508, Validation Loss: 51507, 189105.5257227749\n",
      "Epoch 192801, Training Loss: 35024, Validation Loss: 62306, 237745.23467490097\n",
      "Epoch 192901, Training Loss: 33358, Validation Loss: 50788, 180302.40960324556\n",
      "Epoch 193001, Training Loss: 31496, Validation Loss: 56698, 175124.95208553583\n",
      "Epoch 193101, Training Loss: 32528, Validation Loss: 51564, 161710.2600400069\n",
      "Epoch 193201, Training Loss: 33337, Validation Loss: 50347, 196111.17185172907\n",
      "Epoch 193301, Training Loss: 32859, Validation Loss: 52211, 195428.5660337558\n",
      "Epoch 193401, Training Loss: 31689, Validation Loss: 52306, 264010.6682122778\n",
      "Epoch 193501, Training Loss: 32940, Validation Loss: 50617, 187173.46555289498\n",
      "Epoch 193601, Training Loss: 33313, Validation Loss: 58055, 171956.89787907884\n",
      "Epoch 193701, Training Loss: 33412, Validation Loss: 52839, 173220.60169664185\n",
      "Epoch 193801, Training Loss: 31626, Validation Loss: 48835, 187638.03364081404\n",
      "Epoch 193901, Training Loss: 33878, Validation Loss: 61037, 204660.41797654275\n",
      "Epoch 194001, Training Loss: 33685, Validation Loss: 52882, 158218.01960260872\n",
      "Epoch 194101, Training Loss: 33800, Validation Loss: 52593, 191416.8116529448\n",
      "Epoch 194201, Training Loss: 32519, Validation Loss: 49351, 152713.5084199713\n",
      "Epoch 194301, Training Loss: 32453, Validation Loss: 61113, 199011.8991490756\n",
      "Epoch 194401, Training Loss: 47138, Validation Loss: 67445, 42804.906116399674\n",
      "Epoch 194501, Training Loss: 44548, Validation Loss: 59019, 49714.78645624742\n",
      "Epoch 194601, Training Loss: 44714, Validation Loss: 68393, 58106.39629608034\n",
      "Epoch 194701, Training Loss: 42014, Validation Loss: 59779, 58056.830937079656\n",
      "Epoch 194801, Training Loss: 41005, Validation Loss: 59078, 58374.70057943723\n",
      "Epoch 194901, Training Loss: 41335, Validation Loss: 57691, 53301.377833993756\n",
      "Epoch 195001, Training Loss: 40425, Validation Loss: 58361, 63471.937459010755\n",
      "Epoch 195101, Training Loss: 41795, Validation Loss: 62018, 57451.390954172675\n",
      "Epoch 195201, Training Loss: 39923, Validation Loss: 58729, 59153.50593475435\n",
      "Epoch 195301, Training Loss: 41996, Validation Loss: 57675, 69633.18609635922\n",
      "Epoch 195401, Training Loss: 38621, Validation Loss: 56762, 56074.075566732245\n",
      "Epoch 195501, Training Loss: 37405, Validation Loss: 54695, 57415.73265077348\n",
      "Epoch 195601, Training Loss: 38130, Validation Loss: 58618, 63770.59427682837\n",
      "Epoch 195701, Training Loss: 38061, Validation Loss: 56744, 66197.1756457538\n",
      "Epoch 195801, Training Loss: 39080, Validation Loss: 55073, 78029.1130900067\n",
      "Epoch 195901, Training Loss: 37454, Validation Loss: 54445, 94585.9288222298\n",
      "Epoch 196001, Training Loss: 36351, Validation Loss: 56077, 88614.76040346781\n",
      "Epoch 196101, Training Loss: 35961, Validation Loss: 58235, 84558.43565915305\n",
      "Epoch 196201, Training Loss: 36614, Validation Loss: 61614, 107780.48690241783\n",
      "Epoch 196301, Training Loss: 36314, Validation Loss: 59260, 107886.31940327545\n",
      "Epoch 196401, Training Loss: 35209, Validation Loss: 56969, 111554.90010763194\n",
      "Epoch 196501, Training Loss: 35568, Validation Loss: 56111, 107634.07239252089\n",
      "Epoch 196601, Training Loss: 34752, Validation Loss: 50600, 140823.9252083747\n",
      "Epoch 196701, Training Loss: 32888, Validation Loss: 52778, 130273.71035387518\n",
      "Epoch 196801, Training Loss: 34985, Validation Loss: 53421, 152440.62188565513\n",
      "Epoch 196901, Training Loss: 35972, Validation Loss: 58713, 152786.6321569742\n",
      "Epoch 197001, Training Loss: 33382, Validation Loss: 49675, 140705.58267973986\n",
      "Epoch 197101, Training Loss: 34865, Validation Loss: 57798, 151316.27974355078\n",
      "Epoch 197201, Training Loss: 32731, Validation Loss: 51156, 152576.28889386915\n",
      "Epoch 197301, Training Loss: 33509, Validation Loss: 51966, 209472.20595253355\n",
      "Epoch 197401, Training Loss: 32941, Validation Loss: 53903, 133265.25910086502\n",
      "Epoch 197501, Training Loss: 34744, Validation Loss: 52160, 205214.21125364164\n",
      "Epoch 197601, Training Loss: 35206, Validation Loss: 59116, 201370.3655701241\n",
      "Epoch 197701, Training Loss: 33786, Validation Loss: 53224, 174558.51819304904\n",
      "Epoch 197801, Training Loss: 34551, Validation Loss: 52595, 197717.99548020438\n",
      "Epoch 197901, Training Loss: 32788, Validation Loss: 60650, 196461.71698252493\n",
      "Epoch 198001, Training Loss: 33594, Validation Loss: 50913, 160941.26675189153\n",
      "Epoch 198101, Training Loss: 35558, Validation Loss: 50808, 170197.65982035466\n",
      "Epoch 198201, Training Loss: 32698, Validation Loss: 52724, 191821.66674177255\n",
      "Epoch 198301, Training Loss: 32196, Validation Loss: 57853, 169952.14865206403\n",
      "Epoch 198401, Training Loss: 32283, Validation Loss: 52792, 174906.65231058397\n",
      "Epoch 198501, Training Loss: 35254, Validation Loss: 50072, 200868.4542588927\n",
      "Epoch 198601, Training Loss: 33354, Validation Loss: 52330, 149423.49770901052\n",
      "Epoch 198701, Training Loss: 32481, Validation Loss: 55823, 176521.42191038813\n",
      "Epoch 198801, Training Loss: 33487, Validation Loss: 54291, 161956.3691179925\n",
      "Epoch 198901, Training Loss: 33190, Validation Loss: 57096, 202827.02490511924\n",
      "Epoch 199001, Training Loss: 31959, Validation Loss: 50302, 174216.2678262389\n",
      "Epoch 199101, Training Loss: 33960, Validation Loss: 54256, 211618.5782072238\n",
      "Epoch 199201, Training Loss: 32930, Validation Loss: 52854, 169701.51377020433\n",
      "Epoch 199301, Training Loss: 32346, Validation Loss: 50346, 175270.4774370325\n",
      "Epoch 199401, Training Loss: 33544, Validation Loss: 54702, 200060.67352633015\n",
      "Epoch 199501, Training Loss: 33398, Validation Loss: 64081, 181782.4683075721\n",
      "Epoch 199601, Training Loss: 35301, Validation Loss: 57652, 230016.2048949413\n",
      "Epoch 199701, Training Loss: 34173, Validation Loss: 52357, 165017.50516869425\n",
      "Epoch 199801, Training Loss: 32172, Validation Loss: 50094, 183234.75437822496\n",
      "Epoch 199901, Training Loss: 32946, Validation Loss: 52151, 168785.64037557587\n",
      "Epoch 200001, Training Loss: 32615, Validation Loss: 53098, 183277.12346732523\n",
      "Epoch 200101, Training Loss: 32809, Validation Loss: 58571, 223321.62747470223\n",
      "Epoch 200201, Training Loss: 33592, Validation Loss: 60221, 171488.54093481647\n",
      "Epoch 200301, Training Loss: 35278, Validation Loss: 52598, 230219.95000289907\n",
      "Epoch 200401, Training Loss: 31623, Validation Loss: 49869, 204842.3386588567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200501, Training Loss: 33476, Validation Loss: 53100, 163680.2125626074\n",
      "Epoch 200601, Training Loss: 33966, Validation Loss: 51667, 198997.2902939724\n",
      "Epoch 200701, Training Loss: 32100, Validation Loss: 50332, 202539.86302876985\n",
      "Epoch 200801, Training Loss: 32731, Validation Loss: 67540, 227665.16905004092\n",
      "Epoch 200901, Training Loss: 32640, Validation Loss: 51682, 221030.95935145972\n",
      "Epoch 201001, Training Loss: 34851, Validation Loss: 68247, 200864.46964381696\n",
      "Epoch 201101, Training Loss: 33959, Validation Loss: 50751, 193061.41975422323\n",
      "Epoch 201201, Training Loss: 32605, Validation Loss: 55202, 183081.25777470984\n",
      "Epoch 201301, Training Loss: 33444, Validation Loss: 52309, 167728.57183122408\n",
      "Epoch 201401, Training Loss: 34127, Validation Loss: 51950, 185850.88924502756\n",
      "Epoch 201501, Training Loss: 30965, Validation Loss: 53728, 186371.46420755793\n",
      "Epoch 201601, Training Loss: 33390, Validation Loss: 54949, 212168.56577920704\n",
      "Epoch 201701, Training Loss: 33437, Validation Loss: 57769, 166101.44566161363\n",
      "Epoch 201801, Training Loss: 31981, Validation Loss: 75398, 216201.42239167006\n",
      "Epoch 201901, Training Loss: 33411, Validation Loss: 53243, 196428.59195572275\n",
      "Epoch 202001, Training Loss: 34498, Validation Loss: 54256, 227878.97941462716\n",
      "Epoch 202101, Training Loss: 32904, Validation Loss: 91008, 195091.16321056426\n",
      "Epoch 202201, Training Loss: 31050, Validation Loss: 63341, 202612.02801566245\n",
      "Epoch 202301, Training Loss: 31952, Validation Loss: 63424, 188445.5178766801\n",
      "Epoch 202401, Training Loss: 34641, Validation Loss: 53606, 172821.73294201985\n",
      "Epoch 202501, Training Loss: 32709, Validation Loss: 49234, 176742.4966759693\n",
      "Epoch 202601, Training Loss: 34088, Validation Loss: 74570, 180909.1642617835\n",
      "Epoch 202701, Training Loss: 33664, Validation Loss: 53578, 173400.01902648134\n",
      "Epoch 202801, Training Loss: 34159, Validation Loss: 51915, 204950.86190116635\n",
      "Epoch 202901, Training Loss: 32432, Validation Loss: 56309, 191030.13382382572\n",
      "Epoch 203001, Training Loss: 33971, Validation Loss: 53226, 249608.8127574971\n",
      "Epoch 203101, Training Loss: 39204, Validation Loss: 61243, 176520.92876905028\n",
      "Epoch 203201, Training Loss: 32313, Validation Loss: 50989, 210864.71065198697\n",
      "Epoch 203301, Training Loss: 32343, Validation Loss: 52244, 193749.7137685922\n",
      "Epoch 203401, Training Loss: 30901, Validation Loss: 51854, 207575.94493131476\n",
      "Epoch 203501, Training Loss: 31786, Validation Loss: 54572, 182119.4609711132\n",
      "Epoch 203601, Training Loss: 34130, Validation Loss: 49310, 220908.5618582036\n",
      "Epoch 203701, Training Loss: 33841, Validation Loss: 50549, 184386.09713037664\n",
      "Epoch 203801, Training Loss: 32303, Validation Loss: 52103, 190802.4229048408\n",
      "Epoch 203901, Training Loss: 32558, Validation Loss: 52925, 182365.40486245404\n",
      "Epoch 204001, Training Loss: 32264, Validation Loss: 51554, 199889.300767877\n",
      "Epoch 204101, Training Loss: 33911, Validation Loss: 63222, 202816.61884111838\n",
      "Epoch 204201, Training Loss: 33479, Validation Loss: 55846, 213512.47193364528\n",
      "Epoch 204301, Training Loss: 31812, Validation Loss: 70630, 177021.77943939544\n",
      "Epoch 204401, Training Loss: 34561, Validation Loss: 99262, 286860.020821474\n",
      "Epoch 204501, Training Loss: 32620, Validation Loss: 52815, 168658.4880233649\n",
      "Epoch 204601, Training Loss: 32919, Validation Loss: 54794, 194342.71380469122\n",
      "Epoch 204701, Training Loss: 32661, Validation Loss: 60193, 187152.74012106075\n",
      "Epoch 204801, Training Loss: 33053, Validation Loss: 51386, 186739.8729298395\n",
      "Epoch 204901, Training Loss: 32672, Validation Loss: 77098, 165345.61411651733\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=9e-3,\n",
    "    weight_decay=5e-2\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999999, \n",
    "    patience=10, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
