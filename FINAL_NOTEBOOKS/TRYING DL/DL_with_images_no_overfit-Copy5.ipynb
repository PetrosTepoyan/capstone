{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 64\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.18),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 238861, Validation Loss: 244200, 4.214949734277446\n",
      "Epoch 101, Training Loss: 90427, Validation Loss: 85212, 46591.10569567181\n",
      "Epoch 201, Training Loss: 60667, Validation Loss: 59709, 46996.231576686136\n",
      "Epoch 301, Training Loss: 56532, Validation Loss: 57938, 105827.72296845086\n",
      "Epoch 401, Training Loss: 58469, Validation Loss: 109079, 81073.29031014674\n",
      "Epoch 501, Training Loss: 56858, Validation Loss: 87463, 144752.91718681835\n",
      "Epoch 601, Training Loss: 57392, Validation Loss: 63296, 119062.88803647253\n",
      "Epoch 701, Training Loss: 53282, Validation Loss: 54177, 91077.69759589313\n",
      "Epoch 801, Training Loss: 53872, Validation Loss: 55014, 137481.90727323657\n",
      "Epoch 901, Training Loss: 54227, Validation Loss: 59996, 89935.77397105248\n",
      "Epoch 1001, Training Loss: 57951, Validation Loss: 64338, 145172.02888849788\n",
      "Epoch 1101, Training Loss: 55216, Validation Loss: 61281, 112962.9381357003\n",
      "Epoch 1201, Training Loss: 55150, Validation Loss: 60853, 131534.3590832935\n",
      "Epoch 1301, Training Loss: 54523, Validation Loss: 58896, 93800.138762452\n",
      "Epoch 1401, Training Loss: 55540, Validation Loss: 73685, 116998.74305238602\n",
      "Epoch 1501, Training Loss: 53156, Validation Loss: 68057, 95523.2731177089\n",
      "Epoch 1601, Training Loss: 54818, Validation Loss: 55577, 98129.02495740469\n",
      "Epoch 1701, Training Loss: 56570, Validation Loss: 57929, 70542.79316611537\n",
      "Epoch 1801, Training Loss: 57201, Validation Loss: 54390, 96314.18926376844\n",
      "Epoch 1901, Training Loss: 56106, Validation Loss: 53859, 127912.03149057704\n",
      "Epoch 2001, Training Loss: 52982, Validation Loss: 64799, 123509.77857927831\n",
      "Epoch 2101, Training Loss: 53176, Validation Loss: 63346, 116858.69781766133\n",
      "Epoch 2201, Training Loss: 55530, Validation Loss: 54849, 94117.03732930013\n",
      "Epoch 2301, Training Loss: 54343, Validation Loss: 53281, 72474.02896416423\n",
      "Epoch 2401, Training Loss: 50809, Validation Loss: 55754, 97699.02429627006\n",
      "Epoch 2501, Training Loss: 50974, Validation Loss: 63012, 64665.78344485524\n",
      "Epoch 2601, Training Loss: 50468, Validation Loss: 56666, 87282.56419658713\n",
      "Epoch 2701, Training Loss: 52365, Validation Loss: 68864, 72681.11854759244\n",
      "Epoch 2801, Training Loss: 55544, Validation Loss: 52377, 118734.5290484691\n",
      "Epoch 2901, Training Loss: 51552, Validation Loss: 56107, 96913.09153203729\n",
      "Epoch 3001, Training Loss: 52606, Validation Loss: 53951, 123066.58928477613\n",
      "Epoch 3101, Training Loss: 52260, Validation Loss: 66741, 108897.96181246881\n",
      "Epoch 3201, Training Loss: 50148, Validation Loss: 52169, 108021.06562047779\n",
      "Epoch 3301, Training Loss: 49520, Validation Loss: 55186, 110959.54133164538\n",
      "Epoch 3401, Training Loss: 52976, Validation Loss: 53292, 118821.61996955871\n",
      "Epoch 3501, Training Loss: 51722, Validation Loss: 67714, 113565.50713595732\n",
      "Epoch 3601, Training Loss: 52583, Validation Loss: 56782, 57499.87421507949\n",
      "Epoch 3701, Training Loss: 50041, Validation Loss: 54558, 90898.71588991392\n",
      "Epoch 3801, Training Loss: 53204, Validation Loss: 60066, 109979.0211944052\n",
      "Epoch 3901, Training Loss: 51683, Validation Loss: 53410, 71792.8340932494\n",
      "Epoch 4001, Training Loss: 52063, Validation Loss: 58719, 60910.45679153239\n",
      "Epoch 4101, Training Loss: 48715, Validation Loss: 52994, 107409.08933138657\n",
      "Epoch 4201, Training Loss: 50881, Validation Loss: 53599, 109115.57889321107\n",
      "Epoch 4301, Training Loss: 52614, Validation Loss: 90698, 122799.47087719722\n",
      "Epoch 4401, Training Loss: 50803, Validation Loss: 62667, 108197.1809409825\n",
      "Epoch 4501, Training Loss: 53255, Validation Loss: 54129, 103350.26914675026\n",
      "Epoch 4601, Training Loss: 50865, Validation Loss: 55082, 94494.33921434976\n",
      "Epoch 4701, Training Loss: 50554, Validation Loss: 51104, 61827.01169945488\n",
      "Epoch 4801, Training Loss: 51927, Validation Loss: 73574, 74021.53468567906\n",
      "Epoch 4901, Training Loss: 53335, Validation Loss: 56832, 71807.99330807228\n",
      "Epoch 5001, Training Loss: 50281, Validation Loss: 59327, 79141.61802619472\n",
      "Epoch 5101, Training Loss: 51382, Validation Loss: 55670, 63433.662456837286\n",
      "Epoch 5201, Training Loss: 51048, Validation Loss: 60319, 82096.78550412145\n",
      "Epoch 5301, Training Loss: 52333, Validation Loss: 58280, 92083.07906646487\n",
      "Epoch 5401, Training Loss: 52318, Validation Loss: 54201, 164178.75609409748\n",
      "Epoch 5501, Training Loss: 51369, Validation Loss: 57433, 88734.55694497295\n",
      "Epoch 5601, Training Loss: 51291, Validation Loss: 52763, 78130.55596646467\n",
      "Epoch 5701, Training Loss: 52049, Validation Loss: 54208, 103027.43539133982\n",
      "Epoch 5801, Training Loss: 52953, Validation Loss: 53043, 73220.48964794794\n",
      "Epoch 5901, Training Loss: 51912, Validation Loss: 54240, 113976.48686996933\n",
      "Epoch 6001, Training Loss: 51661, Validation Loss: 55015, 76515.85223025562\n",
      "Epoch 6101, Training Loss: 51288, Validation Loss: 54048, 93901.32382470032\n",
      "Epoch 6201, Training Loss: 49779, Validation Loss: 55140, 72366.01652939346\n",
      "Epoch 6301, Training Loss: 51015, Validation Loss: 53082, 59655.3532144544\n",
      "Epoch 6401, Training Loss: 51403, Validation Loss: 63005, 71040.26209970012\n",
      "Epoch 6501, Training Loss: 53382, Validation Loss: 53828, 81001.26158401388\n",
      "Epoch 6601, Training Loss: 50409, Validation Loss: 53243, 72057.05365519573\n",
      "Epoch 6701, Training Loss: 51216, Validation Loss: 54573, 55172.36019074843\n",
      "Epoch 6801, Training Loss: 53424, Validation Loss: 52853, 91372.86881915951\n",
      "Epoch 6901, Training Loss: 52154, Validation Loss: 53164, 83216.49657598768\n",
      "Epoch 7001, Training Loss: 49073, Validation Loss: 53505, 76044.69897941465\n",
      "Epoch 7101, Training Loss: 51169, Validation Loss: 56970, 103934.46925991372\n",
      "Epoch 7201, Training Loss: 52622, Validation Loss: 58791, 115523.97327706858\n",
      "Epoch 7301, Training Loss: 51117, Validation Loss: 61248, 78729.31536438032\n",
      "Epoch 7401, Training Loss: 48926, Validation Loss: 53359, 50578.63514349986\n",
      "Epoch 7501, Training Loss: 52078, Validation Loss: 52764, 90392.82660522762\n",
      "Epoch 7601, Training Loss: 47751, Validation Loss: 54106, 112330.02207995819\n",
      "Epoch 7701, Training Loss: 45766, Validation Loss: 53431, 91159.96220728732\n",
      "Epoch 7801, Training Loss: 50267, Validation Loss: 51354, 75450.14931597454\n",
      "Epoch 7901, Training Loss: 51311, Validation Loss: 54169, 105700.54895112936\n",
      "Epoch 8001, Training Loss: 51715, Validation Loss: 57132, 70292.3659616026\n",
      "Epoch 8101, Training Loss: 51547, Validation Loss: 54497, 99971.3862931546\n",
      "Epoch 8201, Training Loss: 47337, Validation Loss: 63089, 77069.63432911296\n",
      "Epoch 8301, Training Loss: 50990, Validation Loss: 59385, 74816.48303343961\n",
      "Epoch 8401, Training Loss: 50715, Validation Loss: 51638, 64350.04901095661\n",
      "Epoch 8501, Training Loss: 49798, Validation Loss: 53180, 76005.68560547604\n",
      "Epoch 8601, Training Loss: 48869, Validation Loss: 55247, 90582.03566407296\n",
      "Epoch 8701, Training Loss: 47601, Validation Loss: 55642, 68105.67683856831\n",
      "Epoch 8801, Training Loss: 51347, Validation Loss: 57785, 84434.52200265157\n",
      "Epoch 8901, Training Loss: 47132, Validation Loss: 53076, 92426.7813993742\n",
      "Epoch 9001, Training Loss: 49911, Validation Loss: 59644, 69846.72405229254\n",
      "Epoch 9101, Training Loss: 51967, Validation Loss: 52633, 57237.93872785396\n",
      "Epoch 9201, Training Loss: 50130, Validation Loss: 53797, 72735.59111538157\n",
      "Epoch 9301, Training Loss: 48112, Validation Loss: 52016, 92779.10949106852\n",
      "Epoch 9401, Training Loss: 50042, Validation Loss: 64866, 67712.32523861116\n",
      "Epoch 9501, Training Loss: 48594, Validation Loss: 51605, 53607.932206008765\n",
      "Epoch 9601, Training Loss: 51287, Validation Loss: 71654, 61699.49063047513\n",
      "Epoch 9701, Training Loss: 50205, Validation Loss: 53889, 73383.11738022536\n",
      "Epoch 9801, Training Loss: 49916, Validation Loss: 51840, 65303.883602753434\n",
      "Epoch 9901, Training Loss: 48693, Validation Loss: 51932, 72395.45988377125\n",
      "Epoch 10001, Training Loss: 45681, Validation Loss: 55661, 64598.95960334479\n",
      "Epoch 10101, Training Loss: 49818, Validation Loss: 56892, 86094.6032073912\n",
      "Epoch 10201, Training Loss: 52542, Validation Loss: 55006, 84283.89350665249\n",
      "Epoch 10301, Training Loss: 47836, Validation Loss: 59878, 80665.36212803805\n",
      "Epoch 10401, Training Loss: 51110, Validation Loss: 54036, 88654.9014033454\n",
      "Epoch 10501, Training Loss: 46336, Validation Loss: 55028, 64249.51430589962\n",
      "Epoch 10601, Training Loss: 47335, Validation Loss: 53594, 89396.77663152762\n",
      "Epoch 10701, Training Loss: 46916, Validation Loss: 52204, 67117.07546672558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10801, Training Loss: 53950, Validation Loss: 54552, 73535.5846425256\n",
      "Epoch 10901, Training Loss: 48175, Validation Loss: 60518, 82441.39427669447\n",
      "Epoch 11001, Training Loss: 46779, Validation Loss: 53371, 84815.93888810977\n",
      "Epoch 11101, Training Loss: 50508, Validation Loss: 52406, 75027.76473942808\n",
      "Epoch 11201, Training Loss: 47601, Validation Loss: 56850, 64498.02762344821\n",
      "Epoch 11301, Training Loss: 48320, Validation Loss: 61954, 85143.28298273365\n",
      "Epoch 11401, Training Loss: 48949, Validation Loss: 52691, 67554.34392519917\n",
      "Epoch 11501, Training Loss: 52233, Validation Loss: 61394, 63089.2203073933\n",
      "Epoch 11601, Training Loss: 50126, Validation Loss: 53804, 100870.64150300702\n",
      "Epoch 11701, Training Loss: 50776, Validation Loss: 50905, 86583.78371960456\n",
      "Epoch 11801, Training Loss: 47050, Validation Loss: 52312, 93723.03717988478\n",
      "Epoch 11901, Training Loss: 52211, Validation Loss: 51667, 72024.91314335592\n",
      "Epoch 12001, Training Loss: 48372, Validation Loss: 52611, 77963.11979596289\n",
      "Epoch 12101, Training Loss: 49163, Validation Loss: 57586, 95661.9952861253\n",
      "Epoch 12201, Training Loss: 51006, Validation Loss: 57168, 84729.18610446961\n",
      "Epoch 12301, Training Loss: 48597, Validation Loss: 58998, 127531.25200076537\n",
      "Epoch 12401, Training Loss: 49244, Validation Loss: 53576, 103168.17384831475\n",
      "Epoch 12501, Training Loss: 47292, Validation Loss: 53821, 56016.91292376977\n",
      "Epoch 12601, Training Loss: 50769, Validation Loss: 62191, 95101.56964671507\n",
      "Epoch 12701, Training Loss: 51889, Validation Loss: 54787, 69859.90738100263\n",
      "Epoch 12801, Training Loss: 48879, Validation Loss: 53054, 67670.51301357882\n",
      "Epoch 12901, Training Loss: 47999, Validation Loss: 60544, 66527.73456508444\n",
      "Epoch 13001, Training Loss: 47960, Validation Loss: 54450, 52783.08980706415\n",
      "Epoch 13101, Training Loss: 48707, Validation Loss: 54224, 91787.87677665763\n",
      "Epoch 13201, Training Loss: 49018, Validation Loss: 62188, 70288.68517057943\n",
      "Epoch 13301, Training Loss: 48517, Validation Loss: 55709, 84266.80519526957\n",
      "Epoch 13401, Training Loss: 48913, Validation Loss: 52781, 93981.86542541254\n",
      "Epoch 13501, Training Loss: 49914, Validation Loss: 62177, 99480.49147852958\n",
      "Epoch 13601, Training Loss: 47847, Validation Loss: 55979, 67010.95989417045\n",
      "Epoch 13701, Training Loss: 46888, Validation Loss: 53478, 64210.2765666901\n",
      "Epoch 13801, Training Loss: 44888, Validation Loss: 54292, 58141.03861194547\n",
      "Epoch 13901, Training Loss: 49705, Validation Loss: 57248, 111204.33405413142\n",
      "Epoch 14001, Training Loss: 49178, Validation Loss: 51695, 93011.00862648692\n",
      "Epoch 14101, Training Loss: 49147, Validation Loss: 53161, 54964.1386818386\n",
      "Epoch 14201, Training Loss: 48314, Validation Loss: 60799, 63209.164714019884\n",
      "Epoch 14301, Training Loss: 46010, Validation Loss: 52843, 66797.4385356359\n",
      "Epoch 14401, Training Loss: 48593, Validation Loss: 53747, 72438.17099150286\n",
      "Epoch 14501, Training Loss: 50016, Validation Loss: 64595, 82440.15432155893\n",
      "Epoch 14601, Training Loss: 47688, Validation Loss: 53671, 93381.52439459121\n",
      "Epoch 14701, Training Loss: 48369, Validation Loss: 59385, 67784.84322639805\n",
      "Epoch 14801, Training Loss: 51375, Validation Loss: 54049, 68003.02860759746\n",
      "Epoch 14901, Training Loss: 49393, Validation Loss: 54036, 65707.45927013831\n",
      "Epoch 15001, Training Loss: 48737, Validation Loss: 56142, 56986.56894918712\n",
      "Epoch 15101, Training Loss: 49830, Validation Loss: 50291, 68099.71815712674\n",
      "Epoch 15201, Training Loss: 46492, Validation Loss: 59514, 109995.6617200729\n",
      "Epoch 15301, Training Loss: 46536, Validation Loss: 51104, 83131.90683421702\n",
      "Epoch 15401, Training Loss: 49631, Validation Loss: 53348, 64124.67439674996\n",
      "Epoch 15501, Training Loss: 47743, Validation Loss: 52885, 60328.281505555235\n",
      "Epoch 15601, Training Loss: 46139, Validation Loss: 50837, 85076.01028132107\n",
      "Epoch 15701, Training Loss: 46874, Validation Loss: 58122, 73831.28702764139\n",
      "Epoch 15801, Training Loss: 47817, Validation Loss: 51143, 90930.7226368446\n",
      "Epoch 15901, Training Loss: 45784, Validation Loss: 58946, 66118.88477744098\n",
      "Epoch 16001, Training Loss: 47456, Validation Loss: 53001, 60309.45302176883\n",
      "Epoch 16101, Training Loss: 46289, Validation Loss: 57363, 76824.45477657825\n",
      "Epoch 16201, Training Loss: 45695, Validation Loss: 53650, 70788.43975974059\n",
      "Epoch 16301, Training Loss: 43955, Validation Loss: 63198, 84861.30814910673\n",
      "Epoch 16401, Training Loss: 49142, Validation Loss: 62639, 95931.42889725935\n",
      "Epoch 16501, Training Loss: 47440, Validation Loss: 52743, 79956.30924049589\n",
      "Epoch 16601, Training Loss: 49291, Validation Loss: 67485, 66904.3492784433\n",
      "Epoch 16701, Training Loss: 45571, Validation Loss: 52175, 54799.419024932075\n",
      "Epoch 16801, Training Loss: 50464, Validation Loss: 54595, 66509.55994951197\n",
      "Epoch 16901, Training Loss: 47189, Validation Loss: 52813, 72156.35922899311\n",
      "Epoch 17001, Training Loss: 47513, Validation Loss: 64138, 71314.72952760004\n",
      "Epoch 17101, Training Loss: 49366, Validation Loss: 52374, 60775.45568526458\n",
      "Epoch 17201, Training Loss: 49608, Validation Loss: 53464, 76159.15446474911\n",
      "Epoch 17301, Training Loss: 47560, Validation Loss: 49303, 54778.96448606786\n",
      "Epoch 17401, Training Loss: 46486, Validation Loss: 54177, 83762.29419467523\n",
      "Epoch 17501, Training Loss: 47262, Validation Loss: 62438, 60360.83433083699\n",
      "Epoch 17601, Training Loss: 46581, Validation Loss: 54869, 66337.32073488034\n",
      "Epoch 17701, Training Loss: 46671, Validation Loss: 55803, 78072.45987051682\n",
      "Epoch 17801, Training Loss: 49490, Validation Loss: 52782, 82142.61754774331\n",
      "Epoch 17901, Training Loss: 48276, Validation Loss: 54392, 74355.03298942366\n",
      "Epoch 18001, Training Loss: 46974, Validation Loss: 51685, 70857.20683998047\n",
      "Epoch 18101, Training Loss: 50117, Validation Loss: 61299, 115056.34512373754\n",
      "Epoch 18201, Training Loss: 44221, Validation Loss: 50361, 66637.34703081466\n",
      "Epoch 18301, Training Loss: 44270, Validation Loss: 56159, 48154.81905800151\n",
      "Epoch 18401, Training Loss: 44723, Validation Loss: 53880, 78225.06814567932\n",
      "Epoch 18501, Training Loss: 47046, Validation Loss: 53684, 99199.71862977569\n",
      "Epoch 18601, Training Loss: 49742, Validation Loss: 54779, 56606.0234125267\n",
      "Epoch 18701, Training Loss: 50089, Validation Loss: 53968, 74176.9746175582\n",
      "Epoch 18801, Training Loss: 45557, Validation Loss: 51319, 84631.76630323399\n",
      "Epoch 18901, Training Loss: 50263, Validation Loss: 53704, 73842.11271040373\n",
      "Epoch 19001, Training Loss: 44639, Validation Loss: 55862, 52786.78583812519\n",
      "Epoch 19101, Training Loss: 47343, Validation Loss: 59839, 77409.50443901028\n",
      "Epoch 19201, Training Loss: 46477, Validation Loss: 54888, 70855.83124936301\n",
      "Epoch 19301, Training Loss: 47469, Validation Loss: 52875, 85611.7301792032\n",
      "Epoch 19401, Training Loss: 45789, Validation Loss: 55851, 64033.12079842994\n",
      "Epoch 19501, Training Loss: 47368, Validation Loss: 56166, 94570.35236777297\n",
      "Epoch 19601, Training Loss: 50765, Validation Loss: 60921, 82991.49101589216\n",
      "Epoch 19701, Training Loss: 49888, Validation Loss: 55064, 78152.76316876714\n",
      "Epoch 19801, Training Loss: 47041, Validation Loss: 55116, 66921.51522964744\n",
      "Epoch 19901, Training Loss: 44017, Validation Loss: 54172, 53470.16475267295\n",
      "Epoch 20001, Training Loss: 44755, Validation Loss: 52076, 75335.92032328183\n",
      "Epoch 20101, Training Loss: 43563, Validation Loss: 49365, 71773.51216319065\n",
      "Epoch 20201, Training Loss: 46717, Validation Loss: 59748, 72087.96959791787\n",
      "Epoch 20301, Training Loss: 46725, Validation Loss: 53237, 53980.37439828435\n",
      "Epoch 20401, Training Loss: 48297, Validation Loss: 62974, 68161.56527631533\n",
      "Epoch 20501, Training Loss: 48652, Validation Loss: 53330, 116025.01161634568\n",
      "Epoch 20601, Training Loss: 45683, Validation Loss: 56163, 61220.43647813729\n",
      "Epoch 20701, Training Loss: 45284, Validation Loss: 52372, 77412.3339733685\n",
      "Epoch 20801, Training Loss: 46955, Validation Loss: 52575, 70580.45988802043\n",
      "Epoch 20901, Training Loss: 45843, Validation Loss: 55410, 59624.942396229715\n",
      "Epoch 21001, Training Loss: 48541, Validation Loss: 55039, 87930.41115901108\n",
      "Epoch 21101, Training Loss: 47139, Validation Loss: 51686, 62878.27004067561\n",
      "Epoch 21201, Training Loss: 45766, Validation Loss: 52388, 63354.94317544607\n",
      "Epoch 21301, Training Loss: 48936, Validation Loss: 54330, 60215.84359322642\n",
      "Epoch 21401, Training Loss: 47780, Validation Loss: 54347, 55578.02272130854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21501, Training Loss: 47545, Validation Loss: 56397, 77519.89410798183\n",
      "Epoch 21601, Training Loss: 47079, Validation Loss: 54467, 59418.46822398193\n",
      "Epoch 21701, Training Loss: 46006, Validation Loss: 51961, 59526.32420385396\n",
      "Epoch 21801, Training Loss: 49740, Validation Loss: 50325, 93111.53577954204\n",
      "Epoch 21901, Training Loss: 46980, Validation Loss: 54481, 92065.45365493304\n",
      "Epoch 22001, Training Loss: 47486, Validation Loss: 52304, 56152.45149734978\n",
      "Epoch 22101, Training Loss: 48692, Validation Loss: 54498, 74236.20759611648\n",
      "Epoch 22201, Training Loss: 47513, Validation Loss: 54029, 93820.37526371462\n",
      "Epoch 22301, Training Loss: 47761, Validation Loss: 52159, 67517.65985808753\n",
      "Epoch 22401, Training Loss: 48467, Validation Loss: 54419, 56869.8398675553\n",
      "Epoch 22501, Training Loss: 45424, Validation Loss: 60403, 54406.92254835556\n",
      "Epoch 22601, Training Loss: 47762, Validation Loss: 53233, 63435.264890397106\n",
      "Epoch 22701, Training Loss: 46663, Validation Loss: 50679, 58960.670036822674\n",
      "Epoch 22801, Training Loss: 48620, Validation Loss: 52306, 59731.60956889022\n",
      "Epoch 22901, Training Loss: 46519, Validation Loss: 51484, 62257.888048244895\n",
      "Epoch 23001, Training Loss: 46682, Validation Loss: 55567, 48234.32038271585\n",
      "Epoch 23101, Training Loss: 45692, Validation Loss: 51114, 64796.02148421776\n",
      "Epoch 23201, Training Loss: 47510, Validation Loss: 50682, 50954.895630083156\n",
      "Epoch 23301, Training Loss: 47816, Validation Loss: 56381, 53763.693350934474\n",
      "Epoch 23401, Training Loss: 43017, Validation Loss: 53342, 52603.91375778097\n",
      "Epoch 23501, Training Loss: 44631, Validation Loss: 54272, 56623.17207914171\n",
      "Epoch 23601, Training Loss: 48226, Validation Loss: 58028, 72195.62681690142\n",
      "Epoch 23701, Training Loss: 46904, Validation Loss: 62975, 93565.74430299753\n",
      "Epoch 23801, Training Loss: 45007, Validation Loss: 56295, 65890.77523975336\n",
      "Epoch 23901, Training Loss: 45186, Validation Loss: 52598, 63982.04042314039\n",
      "Epoch 24001, Training Loss: 46399, Validation Loss: 57539, 81679.11692873458\n",
      "Epoch 24101, Training Loss: 47268, Validation Loss: 53728, 87374.60170761419\n",
      "Epoch 24201, Training Loss: 48981, Validation Loss: 56009, 64945.6104511795\n",
      "Epoch 24301, Training Loss: 44356, Validation Loss: 53849, 62662.67017787693\n",
      "Epoch 24401, Training Loss: 45772, Validation Loss: 60510, 59310.38227773194\n",
      "Epoch 24501, Training Loss: 47159, Validation Loss: 54001, 79131.25963301482\n",
      "Epoch 24601, Training Loss: 46016, Validation Loss: 51725, 83028.45035621095\n",
      "Epoch 24701, Training Loss: 44537, Validation Loss: 54731, 72348.61767169547\n",
      "Epoch 24801, Training Loss: 44766, Validation Loss: 53305, 54350.44672664979\n",
      "Epoch 24901, Training Loss: 50673, Validation Loss: 57434, 68668.96048813626\n",
      "Epoch 25001, Training Loss: 45369, Validation Loss: 57249, 64505.171445498076\n",
      "Epoch 25101, Training Loss: 42560, Validation Loss: 52077, 73002.55414664089\n",
      "Epoch 25201, Training Loss: 43874, Validation Loss: 57858, 71691.8772456749\n",
      "Epoch 25301, Training Loss: 47007, Validation Loss: 57392, 58640.690753708885\n",
      "Epoch 25401, Training Loss: 48541, Validation Loss: 55812, 74523.0522068923\n",
      "Epoch 25501, Training Loss: 50695, Validation Loss: 51878, 57696.316053205774\n",
      "Epoch 25601, Training Loss: 46710, Validation Loss: 58564, 71205.37276272713\n",
      "Epoch 25701, Training Loss: 46892, Validation Loss: 56279, 72594.4014668637\n",
      "Epoch 25801, Training Loss: 45158, Validation Loss: 59521, 56131.81344446449\n",
      "Epoch 25901, Training Loss: 45822, Validation Loss: 53236, 55675.66737465512\n",
      "Epoch 26001, Training Loss: 47891, Validation Loss: 58401, 60745.57539655305\n",
      "Epoch 26101, Training Loss: 49135, Validation Loss: 52273, 59921.47328126815\n",
      "Epoch 26201, Training Loss: 46014, Validation Loss: 51371, 57705.74023986803\n",
      "Epoch 26301, Training Loss: 46374, Validation Loss: 53426, 71874.17173438809\n",
      "Epoch 26401, Training Loss: 49108, Validation Loss: 55902, 58158.72524764282\n",
      "Epoch 26501, Training Loss: 47599, Validation Loss: 54554, 60663.14260723326\n",
      "Epoch 26601, Training Loss: 47079, Validation Loss: 57511, 71494.51560731344\n",
      "Epoch 26701, Training Loss: 47181, Validation Loss: 53180, 63699.644414503964\n",
      "Epoch 26801, Training Loss: 47962, Validation Loss: 54654, 55065.04551632048\n",
      "Epoch 26901, Training Loss: 48012, Validation Loss: 53923, 48392.95301283264\n",
      "Epoch 27001, Training Loss: 47833, Validation Loss: 53130, 110176.06352532901\n",
      "Epoch 27101, Training Loss: 46647, Validation Loss: 53040, 62988.510336005245\n",
      "Epoch 27201, Training Loss: 45982, Validation Loss: 54929, 79866.41595540909\n",
      "Epoch 27301, Training Loss: 48045, Validation Loss: 51964, 95618.63637466128\n",
      "Epoch 27401, Training Loss: 49290, Validation Loss: 54210, 55443.76340502829\n",
      "Epoch 27501, Training Loss: 45395, Validation Loss: 55583, 68530.64047321335\n",
      "Epoch 27601, Training Loss: 47595, Validation Loss: 53568, 66935.43069198467\n",
      "Epoch 27701, Training Loss: 46160, Validation Loss: 53428, 48962.000284637434\n",
      "Epoch 27801, Training Loss: 47041, Validation Loss: 53382, 58329.10399860208\n",
      "Epoch 27901, Training Loss: 49311, Validation Loss: 51313, 81075.05809933641\n",
      "Epoch 28001, Training Loss: 47232, Validation Loss: 52346, 83343.60390416141\n",
      "Epoch 28101, Training Loss: 45269, Validation Loss: 53241, 65408.34425516965\n",
      "Epoch 28201, Training Loss: 49258, Validation Loss: 51605, 67323.65888016997\n",
      "Epoch 28301, Training Loss: 45885, Validation Loss: 52267, 100338.0435440073\n",
      "Epoch 28401, Training Loss: 51721, Validation Loss: 55216, 51699.66340146866\n",
      "Epoch 28501, Training Loss: 51391, Validation Loss: 51540, 77709.15018356896\n",
      "Epoch 28601, Training Loss: 46268, Validation Loss: 56067, 58337.52857880068\n",
      "Epoch 28701, Training Loss: 46988, Validation Loss: 55602, 50741.89184257518\n",
      "Epoch 28801, Training Loss: 45666, Validation Loss: 53274, 45993.52244211427\n",
      "Epoch 28901, Training Loss: 47335, Validation Loss: 59715, 60552.53331206235\n",
      "Epoch 29001, Training Loss: 44339, Validation Loss: 53110, 77461.48498718909\n",
      "Epoch 29101, Training Loss: 46446, Validation Loss: 53302, 56675.116879686124\n",
      "Epoch 29201, Training Loss: 44591, Validation Loss: 53582, 64049.9534568844\n",
      "Epoch 29301, Training Loss: 47120, Validation Loss: 54214, 77243.04770410224\n",
      "Epoch 29401, Training Loss: 48023, Validation Loss: 52688, 67447.0372537627\n",
      "Epoch 29501, Training Loss: 46465, Validation Loss: 61160, 34789.954614056864\n",
      "Epoch 29601, Training Loss: 47307, Validation Loss: 54523, 67563.63843367912\n",
      "Epoch 29701, Training Loss: 45959, Validation Loss: 61318, 72093.83503842726\n",
      "Epoch 29801, Training Loss: 47727, Validation Loss: 52187, 71181.12665085796\n",
      "Epoch 29901, Training Loss: 46364, Validation Loss: 56629, 56569.20418435128\n",
      "Epoch 30001, Training Loss: 45340, Validation Loss: 54784, 63591.06625473349\n",
      "Epoch 30101, Training Loss: 47703, Validation Loss: 52263, 58597.90612404203\n",
      "Epoch 30201, Training Loss: 46193, Validation Loss: 53778, 76326.49295839855\n",
      "Epoch 30301, Training Loss: 46033, Validation Loss: 56596, 62977.536250266676\n",
      "Epoch 30401, Training Loss: 49532, Validation Loss: 58777, 60397.496306472145\n",
      "Epoch 30501, Training Loss: 45792, Validation Loss: 61073, 47234.073086327924\n",
      "Epoch 30601, Training Loss: 49013, Validation Loss: 53844, 50867.92161438593\n",
      "Epoch 30701, Training Loss: 46267, Validation Loss: 63241, 83821.85974166529\n",
      "Epoch 30801, Training Loss: 46950, Validation Loss: 53238, 59356.57076486656\n",
      "Epoch 30901, Training Loss: 46288, Validation Loss: 56395, 61602.64984079518\n",
      "Epoch 31001, Training Loss: 46147, Validation Loss: 50847, 87379.76609978947\n",
      "Epoch 31101, Training Loss: 47879, Validation Loss: 62908, 62401.98224463725\n",
      "Epoch 31201, Training Loss: 47336, Validation Loss: 52775, 68613.80715609902\n",
      "Epoch 31301, Training Loss: 47898, Validation Loss: 53311, 49247.240421870265\n",
      "Epoch 31401, Training Loss: 45494, Validation Loss: 52044, 55749.48612084065\n",
      "Epoch 31501, Training Loss: 47297, Validation Loss: 51681, 84218.96437958733\n",
      "Epoch 31601, Training Loss: 45640, Validation Loss: 53664, 94550.97554796257\n",
      "Epoch 31701, Training Loss: 46493, Validation Loss: 62889, 57435.22203180697\n",
      "Epoch 31801, Training Loss: 43787, Validation Loss: 53214, 73718.2548857562\n",
      "Epoch 31901, Training Loss: 46480, Validation Loss: 53431, 56671.9985189816\n",
      "Epoch 32001, Training Loss: 43626, Validation Loss: 54550, 61600.34590490669\n",
      "Epoch 32101, Training Loss: 45310, Validation Loss: 55830, 96506.01180807014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32201, Training Loss: 47059, Validation Loss: 55772, 62176.49963507443\n",
      "Epoch 32301, Training Loss: 47504, Validation Loss: 53291, 55893.4272145222\n",
      "Epoch 32401, Training Loss: 48428, Validation Loss: 52518, 40498.11438488892\n",
      "Epoch 32501, Training Loss: 46342, Validation Loss: 54907, 87504.95021862234\n",
      "Epoch 32601, Training Loss: 45408, Validation Loss: 54585, 71652.45792224759\n",
      "Epoch 32701, Training Loss: 45696, Validation Loss: 62242, 53158.46189491507\n",
      "Epoch 32801, Training Loss: 46574, Validation Loss: 52939, 58676.24763973346\n",
      "Epoch 32901, Training Loss: 48576, Validation Loss: 52886, 57766.413240128524\n",
      "Epoch 33001, Training Loss: 46436, Validation Loss: 62546, 54002.462141756325\n",
      "Epoch 33101, Training Loss: 46710, Validation Loss: 54157, 73184.19536276972\n",
      "Epoch 33201, Training Loss: 43990, Validation Loss: 52847, 52222.91568651723\n",
      "Epoch 33301, Training Loss: 48840, Validation Loss: 64906, 62491.771787636506\n",
      "Epoch 33401, Training Loss: 46707, Validation Loss: 54462, 47971.39951247666\n",
      "Epoch 33501, Training Loss: 45681, Validation Loss: 54585, 64146.30547662013\n",
      "Epoch 33601, Training Loss: 44186, Validation Loss: 50776, 51075.63784940933\n",
      "Epoch 33701, Training Loss: 49512, Validation Loss: 54481, 64279.94864592645\n",
      "Epoch 33801, Training Loss: 43996, Validation Loss: 54420, 61416.65451544735\n",
      "Epoch 33901, Training Loss: 46387, Validation Loss: 51870, 105980.89137916394\n",
      "Epoch 34001, Training Loss: 43736, Validation Loss: 51890, 54043.27129985598\n",
      "Epoch 34101, Training Loss: 45996, Validation Loss: 60987, 74764.71985511782\n",
      "Epoch 34201, Training Loss: 46399, Validation Loss: 55622, 102400.63628696965\n",
      "Epoch 34301, Training Loss: 47865, Validation Loss: 49841, 44749.47144272204\n",
      "Epoch 34401, Training Loss: 44024, Validation Loss: 53675, 56173.76812368366\n",
      "Epoch 34501, Training Loss: 46394, Validation Loss: 53319, 75055.80376497675\n",
      "Epoch 34601, Training Loss: 48504, Validation Loss: 62515, 74583.73987359606\n",
      "Epoch 34701, Training Loss: 45634, Validation Loss: 63009, 63776.763428453436\n",
      "Epoch 34801, Training Loss: 46550, Validation Loss: 54917, 56006.28990150226\n",
      "Epoch 34901, Training Loss: 45065, Validation Loss: 53630, 60935.03936892989\n",
      "Epoch 35001, Training Loss: 46500, Validation Loss: 55659, 45063.28454223887\n",
      "Epoch 35101, Training Loss: 46175, Validation Loss: 52516, 70331.23970739837\n",
      "Epoch 35201, Training Loss: 45601, Validation Loss: 51611, 65635.58716188317\n",
      "Epoch 35301, Training Loss: 45191, Validation Loss: 55421, 60243.758956926606\n",
      "Epoch 35401, Training Loss: 48097, Validation Loss: 54834, 67882.14909583543\n",
      "Epoch 35501, Training Loss: 47679, Validation Loss: 56429, 59023.8169596821\n",
      "Epoch 35601, Training Loss: 46218, Validation Loss: 54795, 58921.9126218149\n",
      "Epoch 35701, Training Loss: 46651, Validation Loss: 56382, 60570.74323391606\n",
      "Epoch 35801, Training Loss: 46082, Validation Loss: 52576, 75921.91392108322\n",
      "Epoch 35901, Training Loss: 45753, Validation Loss: 56807, 41045.61686246037\n",
      "Epoch 36001, Training Loss: 45415, Validation Loss: 52965, 49738.88507661272\n",
      "Epoch 36101, Training Loss: 42578, Validation Loss: 54679, 62594.90002854019\n",
      "Epoch 36201, Training Loss: 45857, Validation Loss: 55774, 64538.97179603361\n",
      "Epoch 36301, Training Loss: 47271, Validation Loss: 55359, 53069.52795795822\n",
      "Epoch 36401, Training Loss: 45914, Validation Loss: 51938, 65863.82507007223\n",
      "Epoch 36501, Training Loss: 47657, Validation Loss: 56240, 54726.82004243609\n",
      "Epoch 36601, Training Loss: 45959, Validation Loss: 57405, 64427.81131874636\n",
      "Epoch 36701, Training Loss: 47760, Validation Loss: 59423, 51648.551214697676\n",
      "Epoch 36801, Training Loss: 47250, Validation Loss: 58963, 63042.42882692653\n",
      "Epoch 36901, Training Loss: 47967, Validation Loss: 56456, 61721.36969680456\n",
      "Epoch 37001, Training Loss: 43690, Validation Loss: 52602, 58545.86078685508\n",
      "Epoch 37101, Training Loss: 46255, Validation Loss: 54271, 70102.38542411748\n",
      "Epoch 37201, Training Loss: 48027, Validation Loss: 55465, 49212.409185522796\n",
      "Epoch 37301, Training Loss: 46937, Validation Loss: 60623, 52844.12837563009\n",
      "Epoch 37401, Training Loss: 45433, Validation Loss: 54600, 72648.034632953\n",
      "Epoch 37501, Training Loss: 47443, Validation Loss: 60107, 46662.06683329946\n",
      "Epoch 37601, Training Loss: 46083, Validation Loss: 52878, 56276.38023834893\n",
      "Epoch 37701, Training Loss: 47197, Validation Loss: 52657, 46951.03468537886\n",
      "Epoch 37801, Training Loss: 48193, Validation Loss: 56426, 54701.23686676533\n",
      "Epoch 37901, Training Loss: 47036, Validation Loss: 51952, 75626.59378437321\n",
      "Epoch 38001, Training Loss: 48272, Validation Loss: 57030, 53547.871417393595\n",
      "Epoch 38101, Training Loss: 46972, Validation Loss: 59551, 103738.87083609986\n",
      "Epoch 38201, Training Loss: 45461, Validation Loss: 53793, 57680.39649907438\n",
      "Epoch 38301, Training Loss: 46720, Validation Loss: 54221, 118104.68823854571\n",
      "Epoch 38401, Training Loss: 44230, Validation Loss: 54413, 71102.5941047042\n",
      "Epoch 38501, Training Loss: 46695, Validation Loss: 55005, 52145.68823178075\n",
      "Epoch 38601, Training Loss: 48002, Validation Loss: 55636, 46855.26884374834\n",
      "Epoch 38701, Training Loss: 45755, Validation Loss: 52983, 55937.9811403706\n",
      "Epoch 38801, Training Loss: 45520, Validation Loss: 55339, 66760.60158657479\n",
      "Epoch 38901, Training Loss: 44581, Validation Loss: 52872, 71174.12014159428\n",
      "Epoch 39001, Training Loss: 45710, Validation Loss: 59485, 56155.73622893112\n",
      "Epoch 39101, Training Loss: 48330, Validation Loss: 57007, 45910.54734783663\n",
      "Epoch 39201, Training Loss: 46335, Validation Loss: 57089, 60712.97232735826\n",
      "Epoch 39301, Training Loss: 46377, Validation Loss: 55845, 64712.15067582331\n",
      "Epoch 39401, Training Loss: 46321, Validation Loss: 55009, 55978.627304756235\n",
      "Epoch 39501, Training Loss: 46158, Validation Loss: 54214, 64338.56205436581\n",
      "Epoch 39601, Training Loss: 45788, Validation Loss: 52390, 63414.591068537644\n",
      "Epoch 39701, Training Loss: 46928, Validation Loss: 55947, 55139.785237350494\n",
      "Epoch 39801, Training Loss: 46070, Validation Loss: 51016, 78320.59507883205\n",
      "Epoch 39901, Training Loss: 45227, Validation Loss: 52293, 45376.751626262616\n",
      "Epoch 40001, Training Loss: 45780, Validation Loss: 55049, 57240.425700236265\n",
      "Epoch 40101, Training Loss: 47324, Validation Loss: 50480, 65246.47824554506\n",
      "Epoch 40201, Training Loss: 47322, Validation Loss: 58069, 54619.94773898663\n",
      "Epoch 40301, Training Loss: 47522, Validation Loss: 57883, 57624.27826720901\n",
      "Epoch 40401, Training Loss: 45548, Validation Loss: 58170, 57528.42768811501\n",
      "Epoch 40501, Training Loss: 50905, Validation Loss: 57941, 68064.17570070237\n",
      "Epoch 40601, Training Loss: 48833, Validation Loss: 56014, 48716.96144129811\n",
      "Epoch 40701, Training Loss: 43705, Validation Loss: 51675, 61709.29053808723\n",
      "Epoch 40801, Training Loss: 42859, Validation Loss: 52509, 50982.99102722475\n",
      "Epoch 40901, Training Loss: 47300, Validation Loss: 63870, 46710.75523756538\n",
      "Epoch 41001, Training Loss: 48324, Validation Loss: 55318, 52226.89375442278\n",
      "Epoch 41101, Training Loss: 48590, Validation Loss: 52768, 40901.03793108765\n",
      "Epoch 41201, Training Loss: 45754, Validation Loss: 55875, 58628.99220356761\n",
      "Epoch 41301, Training Loss: 47156, Validation Loss: 61369, 52831.47549019052\n",
      "Epoch 41401, Training Loss: 47668, Validation Loss: 55823, 42570.58730638943\n",
      "Epoch 41501, Training Loss: 44599, Validation Loss: 56625, 38374.032120871554\n",
      "Epoch 41601, Training Loss: 46572, Validation Loss: 54278, 51409.29179228525\n",
      "Epoch 41701, Training Loss: 46036, Validation Loss: 52604, 61091.996503716604\n",
      "Epoch 41801, Training Loss: 44775, Validation Loss: 61532, 54300.26940816146\n",
      "Epoch 41901, Training Loss: 43548, Validation Loss: 62484, 53909.6927219562\n",
      "Epoch 42001, Training Loss: 45814, Validation Loss: 54524, 42718.8706452835\n",
      "Epoch 42101, Training Loss: 45054, Validation Loss: 56110, 60302.57198546891\n",
      "Epoch 42201, Training Loss: 44852, Validation Loss: 55706, 61786.05890088931\n",
      "Epoch 42301, Training Loss: 46990, Validation Loss: 53726, 54576.27748503288\n",
      "Epoch 42401, Training Loss: 47256, Validation Loss: 55006, 47267.64037065542\n",
      "Epoch 42501, Training Loss: 46221, Validation Loss: 55569, 98614.4883975415\n",
      "Epoch 42601, Training Loss: 45579, Validation Loss: 58324, 55094.19266058262\n",
      "Epoch 42701, Training Loss: 45939, Validation Loss: 54712, 48070.305941476625\n",
      "Epoch 42801, Training Loss: 47906, Validation Loss: 62384, 53135.06684722055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42901, Training Loss: 42810, Validation Loss: 54395, 43905.831042026424\n",
      "Epoch 43001, Training Loss: 44964, Validation Loss: 54609, 38948.17465065432\n",
      "Epoch 43101, Training Loss: 44277, Validation Loss: 54416, 44764.64815862203\n",
      "Epoch 43201, Training Loss: 48486, Validation Loss: 60925, 55002.20301259755\n",
      "Epoch 43301, Training Loss: 47632, Validation Loss: 52278, 48988.98963199052\n",
      "Epoch 43401, Training Loss: 46718, Validation Loss: 51149, 52431.1464741811\n",
      "Epoch 43501, Training Loss: 43699, Validation Loss: 53959, 49066.60540942687\n",
      "Epoch 43601, Training Loss: 45216, Validation Loss: 56192, 55053.401717279405\n",
      "Epoch 43701, Training Loss: 43001, Validation Loss: 57556, 46319.62167806844\n",
      "Epoch 43801, Training Loss: 45137, Validation Loss: 51297, 47428.41185127065\n",
      "Epoch 43901, Training Loss: 45460, Validation Loss: 61944, 56695.01644801701\n",
      "Epoch 44001, Training Loss: 44594, Validation Loss: 51832, 60154.51255115474\n",
      "Epoch 44101, Training Loss: 45383, Validation Loss: 57854, 60179.173161472856\n",
      "Epoch 44201, Training Loss: 46511, Validation Loss: 60661, 57453.06309904047\n",
      "Epoch 44301, Training Loss: 45963, Validation Loss: 54591, 54791.56138545536\n",
      "Epoch 44401, Training Loss: 46099, Validation Loss: 57048, 51955.97050741946\n",
      "Epoch 44501, Training Loss: 44058, Validation Loss: 53597, 58819.21437363271\n",
      "Epoch 44601, Training Loss: 46314, Validation Loss: 54365, 45173.23993937531\n",
      "Epoch 44701, Training Loss: 43047, Validation Loss: 55451, 42201.854449860235\n",
      "Epoch 44801, Training Loss: 46154, Validation Loss: 53548, 64697.21266350794\n",
      "Epoch 44901, Training Loss: 46888, Validation Loss: 55618, 57128.69396903928\n",
      "Epoch 45001, Training Loss: 44948, Validation Loss: 60473, 46957.51436059866\n",
      "Epoch 45101, Training Loss: 47203, Validation Loss: 54486, 83831.25519748288\n",
      "Epoch 45201, Training Loss: 43565, Validation Loss: 54430, 73172.1554206971\n",
      "Epoch 45301, Training Loss: 53562, Validation Loss: 56976, 46431.12746352256\n",
      "Epoch 45401, Training Loss: 47200, Validation Loss: 53380, 55077.18616565334\n",
      "Epoch 45501, Training Loss: 43841, Validation Loss: 52640, 66226.97558589018\n",
      "Epoch 45601, Training Loss: 44356, Validation Loss: 55488, 58994.07202610901\n",
      "Epoch 45701, Training Loss: 46488, Validation Loss: 51724, 53587.68530244391\n",
      "Epoch 45801, Training Loss: 44470, Validation Loss: 57470, 37589.44440620999\n",
      "Epoch 45901, Training Loss: 51495, Validation Loss: 53769, 64243.51204489282\n",
      "Epoch 46001, Training Loss: 47687, Validation Loss: 52105, 55909.96990583595\n",
      "Epoch 46101, Training Loss: 44416, Validation Loss: 56593, 47065.179539616925\n",
      "Epoch 46201, Training Loss: 47128, Validation Loss: 52777, 53177.7609895179\n",
      "Epoch 46301, Training Loss: 46555, Validation Loss: 61687, 38962.4834386688\n",
      "Epoch 46401, Training Loss: 45852, Validation Loss: 52654, 74262.49779499548\n",
      "Epoch 46501, Training Loss: 43737, Validation Loss: 52770, 42102.688840145514\n",
      "Epoch 46601, Training Loss: 44826, Validation Loss: 53887, 45739.396130211324\n",
      "Epoch 46701, Training Loss: 48439, Validation Loss: 54275, 44235.80302542142\n",
      "Epoch 46801, Training Loss: 47518, Validation Loss: 53458, 59524.60868769488\n",
      "Epoch 46901, Training Loss: 45905, Validation Loss: 54300, 44200.77793858791\n",
      "Epoch 47001, Training Loss: 44984, Validation Loss: 53190, 46670.67394812898\n",
      "Epoch 47101, Training Loss: 45055, Validation Loss: 52832, 43475.23810280844\n",
      "Epoch 47201, Training Loss: 44448, Validation Loss: 54708, 45030.59843280409\n",
      "Epoch 47301, Training Loss: 45432, Validation Loss: 54331, 40557.076383472435\n",
      "Epoch 47401, Training Loss: 44109, Validation Loss: 54440, 67047.83653583676\n",
      "Epoch 47501, Training Loss: 43641, Validation Loss: 52089, 53948.73253852257\n",
      "Epoch 47601, Training Loss: 42846, Validation Loss: 54164, 95318.96932671515\n",
      "Epoch 47701, Training Loss: 46437, Validation Loss: 54073, 44069.56202029788\n",
      "Epoch 47801, Training Loss: 46735, Validation Loss: 55788, 52011.09132132621\n",
      "Epoch 47901, Training Loss: 44111, Validation Loss: 59024, 46056.10657274436\n",
      "Epoch 48001, Training Loss: 45426, Validation Loss: 56008, 46878.03195372629\n",
      "Epoch 48101, Training Loss: 43006, Validation Loss: 57052, 56249.558118664565\n",
      "Epoch 48201, Training Loss: 44535, Validation Loss: 51151, 51714.46528262582\n",
      "Epoch 48301, Training Loss: 45533, Validation Loss: 60555, 65730.09848528245\n",
      "Epoch 48401, Training Loss: 46249, Validation Loss: 56727, 41103.43295983117\n",
      "Epoch 48501, Training Loss: 48061, Validation Loss: 52285, 61038.906859045\n",
      "Epoch 48601, Training Loss: 46999, Validation Loss: 58385, 50901.95336005511\n",
      "Epoch 48701, Training Loss: 44558, Validation Loss: 56774, 58627.69961701346\n",
      "Epoch 48801, Training Loss: 49149, Validation Loss: 55208, 55928.73134056397\n",
      "Epoch 48901, Training Loss: 45047, Validation Loss: 54585, 43631.01082738309\n",
      "Epoch 49001, Training Loss: 44206, Validation Loss: 53297, 59932.410483473876\n",
      "Epoch 49101, Training Loss: 46035, Validation Loss: 55632, 67775.64482861967\n",
      "Epoch 49201, Training Loss: 44241, Validation Loss: 54215, 40023.68660277909\n",
      "Epoch 49301, Training Loss: 45152, Validation Loss: 51707, 75817.27626228218\n",
      "Epoch 49401, Training Loss: 44617, Validation Loss: 53940, 44869.06789914603\n",
      "Epoch 49501, Training Loss: 46713, Validation Loss: 54547, 55836.79015937409\n",
      "Epoch 49601, Training Loss: 44059, Validation Loss: 56198, 61103.656753894196\n",
      "Epoch 49701, Training Loss: 45629, Validation Loss: 57051, 58402.39577913218\n",
      "Epoch 49801, Training Loss: 43550, Validation Loss: 52674, 59776.15494139093\n",
      "Epoch 49901, Training Loss: 42591, Validation Loss: 53442, 66132.2318072376\n",
      "Epoch 50001, Training Loss: 45848, Validation Loss: 54987, 71652.51391291442\n",
      "Epoch 50101, Training Loss: 44739, Validation Loss: 52104, 54261.266061169714\n",
      "Epoch 50201, Training Loss: 45083, Validation Loss: 54128, 57372.95573790094\n",
      "Epoch 50301, Training Loss: 49165, Validation Loss: 52353, 45218.16390740461\n",
      "Epoch 50401, Training Loss: 45278, Validation Loss: 51530, 40213.40498516439\n",
      "Epoch 50501, Training Loss: 45702, Validation Loss: 58255, 54571.356330501054\n",
      "Epoch 50601, Training Loss: 47064, Validation Loss: 51314, 40192.86196446089\n",
      "Epoch 50701, Training Loss: 47795, Validation Loss: 53956, 55706.87793427052\n",
      "Epoch 50801, Training Loss: 47165, Validation Loss: 52940, 41578.56315637357\n",
      "Epoch 50901, Training Loss: 48229, Validation Loss: 53543, 47219.646035497484\n",
      "Epoch 51001, Training Loss: 45218, Validation Loss: 52128, 66648.3075946213\n",
      "Epoch 51101, Training Loss: 42625, Validation Loss: 54178, 52750.1685726505\n",
      "Epoch 51201, Training Loss: 44831, Validation Loss: 53217, 35025.44623869553\n",
      "Epoch 51301, Training Loss: 48574, Validation Loss: 53612, 57060.32956540453\n",
      "Epoch 51401, Training Loss: 43901, Validation Loss: 55010, 33861.0290632125\n",
      "Epoch 51501, Training Loss: 45273, Validation Loss: 54417, 43063.23369869273\n",
      "Epoch 51601, Training Loss: 46587, Validation Loss: 55634, 71622.95698953632\n",
      "Epoch 51701, Training Loss: 46016, Validation Loss: 54127, 47573.91807815139\n",
      "Epoch 51801, Training Loss: 46784, Validation Loss: 55888, 90970.72402622474\n",
      "Epoch 51901, Training Loss: 45312, Validation Loss: 55098, 65892.8920877184\n",
      "Epoch 52001, Training Loss: 45119, Validation Loss: 55062, 59352.64740150567\n",
      "Epoch 52101, Training Loss: 42928, Validation Loss: 53494, 41772.41297138637\n",
      "Epoch 52201, Training Loss: 45514, Validation Loss: 56103, 45076.527709560345\n",
      "Epoch 52301, Training Loss: 43724, Validation Loss: 60147, 42968.89550732263\n",
      "Epoch 52401, Training Loss: 44428, Validation Loss: 55030, 52958.52230610782\n",
      "Epoch 52501, Training Loss: 45651, Validation Loss: 54623, 36673.32224309849\n",
      "Epoch 52601, Training Loss: 45104, Validation Loss: 61539, 59118.210761255024\n",
      "Epoch 52701, Training Loss: 43822, Validation Loss: 51769, 56159.12292881708\n",
      "Epoch 52801, Training Loss: 45239, Validation Loss: 55596, 49714.824839316854\n",
      "Epoch 52901, Training Loss: 42738, Validation Loss: 60789, 45875.081365122496\n",
      "Epoch 53001, Training Loss: 45879, Validation Loss: 53286, 53861.91180459633\n",
      "Epoch 53101, Training Loss: 44287, Validation Loss: 51794, 51759.409839871514\n",
      "Epoch 53201, Training Loss: 45208, Validation Loss: 57254, 56654.632684413984\n",
      "Epoch 53301, Training Loss: 45609, Validation Loss: 54627, 50693.27293587884\n",
      "Epoch 53401, Training Loss: 43635, Validation Loss: 55114, 68762.11611482613\n",
      "Epoch 53501, Training Loss: 43481, Validation Loss: 58724, 46928.66576483299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53601, Training Loss: 45104, Validation Loss: 52848, 41208.028966276615\n",
      "Epoch 53701, Training Loss: 43501, Validation Loss: 55302, 62210.77072526971\n",
      "Epoch 53801, Training Loss: 45838, Validation Loss: 50770, 41228.02760806491\n",
      "Epoch 53901, Training Loss: 44878, Validation Loss: 51899, 44340.157228782344\n",
      "Epoch 54001, Training Loss: 48014, Validation Loss: 60699, 52478.587412011526\n",
      "Epoch 54101, Training Loss: 44266, Validation Loss: 58139, 52497.45898750434\n",
      "Epoch 54201, Training Loss: 43151, Validation Loss: 51815, 37974.604966519306\n",
      "Epoch 54301, Training Loss: 42559, Validation Loss: 53616, 43697.099173331495\n",
      "Epoch 54401, Training Loss: 42363, Validation Loss: 51182, 41957.69935254937\n",
      "Epoch 54501, Training Loss: 43762, Validation Loss: 54418, 49721.77332885383\n",
      "Epoch 54601, Training Loss: 45461, Validation Loss: 54535, 51071.30481347661\n",
      "Epoch 54701, Training Loss: 43552, Validation Loss: 54367, 55064.80731935022\n",
      "Epoch 54801, Training Loss: 45444, Validation Loss: 52626, 53285.15262236739\n",
      "Epoch 54901, Training Loss: 44186, Validation Loss: 53129, 27872.220418176108\n",
      "Epoch 55001, Training Loss: 46698, Validation Loss: 52498, 40060.049368782384\n",
      "Epoch 55101, Training Loss: 46328, Validation Loss: 51839, 43183.01797605698\n",
      "Epoch 55201, Training Loss: 45713, Validation Loss: 52838, 62960.978801753256\n",
      "Epoch 55301, Training Loss: 43650, Validation Loss: 57816, 48638.71401103287\n",
      "Epoch 55401, Training Loss: 43712, Validation Loss: 58405, 38458.25124995887\n",
      "Epoch 55501, Training Loss: 43399, Validation Loss: 58966, 57898.592068039696\n",
      "Epoch 55601, Training Loss: 46948, Validation Loss: 53574, 51869.67460552999\n",
      "Epoch 55701, Training Loss: 44262, Validation Loss: 56056, 41962.73046018664\n",
      "Epoch 55801, Training Loss: 45836, Validation Loss: 53008, 47784.07601153694\n",
      "Epoch 55901, Training Loss: 42898, Validation Loss: 53204, 50920.99684534211\n",
      "Epoch 56001, Training Loss: 46337, Validation Loss: 53321, 62181.97804576334\n",
      "Epoch 56101, Training Loss: 45456, Validation Loss: 59098, 48943.00453704561\n",
      "Epoch 56201, Training Loss: 45119, Validation Loss: 53755, 53663.88694948544\n",
      "Epoch 56301, Training Loss: 45781, Validation Loss: 56036, 49602.11859097258\n",
      "Epoch 56401, Training Loss: 45903, Validation Loss: 54872, 57395.36425709966\n",
      "Epoch 56501, Training Loss: 47770, Validation Loss: 54596, 53150.94366679229\n",
      "Epoch 56601, Training Loss: 46712, Validation Loss: 52743, 42159.896741015895\n",
      "Epoch 56701, Training Loss: 43594, Validation Loss: 54672, 57446.149424890165\n",
      "Epoch 56801, Training Loss: 43548, Validation Loss: 53988, 34465.27699566866\n",
      "Epoch 56901, Training Loss: 47681, Validation Loss: 52699, 50175.58289613973\n",
      "Epoch 57001, Training Loss: 43313, Validation Loss: 52540, 56434.5915394875\n",
      "Epoch 57101, Training Loss: 43199, Validation Loss: 56489, 35395.59640278534\n",
      "Epoch 57201, Training Loss: 44706, Validation Loss: 57924, 46128.228203310806\n",
      "Epoch 57301, Training Loss: 44472, Validation Loss: 55655, 45953.53721601294\n",
      "Epoch 57401, Training Loss: 43543, Validation Loss: 53915, 45834.273691963615\n",
      "Epoch 57501, Training Loss: 45046, Validation Loss: 56309, 51024.20039695694\n",
      "Epoch 57601, Training Loss: 43886, Validation Loss: 54741, 70173.72061891935\n",
      "Epoch 57701, Training Loss: 46327, Validation Loss: 53227, 44386.06516096816\n",
      "Epoch 57801, Training Loss: 43545, Validation Loss: 55988, 46726.42297597625\n",
      "Epoch 57901, Training Loss: 45240, Validation Loss: 54932, 40451.440887354984\n",
      "Epoch 58001, Training Loss: 44177, Validation Loss: 55214, 51767.59196800998\n",
      "Epoch 58101, Training Loss: 44171, Validation Loss: 55855, 50609.90572043042\n",
      "Epoch 58201, Training Loss: 45410, Validation Loss: 52443, 59875.35015625879\n",
      "Epoch 58301, Training Loss: 45307, Validation Loss: 55467, 40375.990890071495\n",
      "Epoch 58401, Training Loss: 43576, Validation Loss: 57348, 40362.609983703696\n",
      "Epoch 58501, Training Loss: 44291, Validation Loss: 54925, 64895.07919073617\n",
      "Epoch 58601, Training Loss: 45360, Validation Loss: 55693, 44036.94494900604\n",
      "Epoch 58701, Training Loss: 43840, Validation Loss: 55227, 48145.44996637629\n",
      "Epoch 58801, Training Loss: 43861, Validation Loss: 53942, 57740.10023975445\n",
      "Epoch 58901, Training Loss: 44516, Validation Loss: 53797, 32915.27024545648\n",
      "Epoch 59001, Training Loss: 46390, Validation Loss: 52787, 38509.691929717694\n",
      "Epoch 59101, Training Loss: 46606, Validation Loss: 53147, 52958.79710670395\n",
      "Epoch 59201, Training Loss: 46370, Validation Loss: 52525, 65611.19158614574\n",
      "Epoch 59301, Training Loss: 42489, Validation Loss: 55777, 42177.22629787318\n",
      "Epoch 59401, Training Loss: 42426, Validation Loss: 52080, 64418.88011063872\n",
      "Epoch 59501, Training Loss: 45754, Validation Loss: 54421, 45909.85053165086\n",
      "Epoch 59601, Training Loss: 44979, Validation Loss: 56095, 49056.39623197264\n",
      "Epoch 59701, Training Loss: 45911, Validation Loss: 54241, 48535.91825312883\n",
      "Epoch 59801, Training Loss: 46667, Validation Loss: 52944, 46110.207231840795\n",
      "Epoch 59901, Training Loss: 46765, Validation Loss: 53874, 51457.34695470583\n",
      "Epoch 60001, Training Loss: 46673, Validation Loss: 56757, 45733.53498518575\n",
      "Epoch 60101, Training Loss: 41903, Validation Loss: 52239, 40775.126799351994\n",
      "Epoch 60201, Training Loss: 47175, Validation Loss: 57970, 45803.983255342675\n",
      "Epoch 60301, Training Loss: 44242, Validation Loss: 52605, 36131.52935306024\n",
      "Epoch 60401, Training Loss: 44250, Validation Loss: 54678, 50884.0304609256\n",
      "Epoch 60501, Training Loss: 46213, Validation Loss: 52744, 51918.42197569553\n",
      "Epoch 60601, Training Loss: 44800, Validation Loss: 53138, 35400.61151381798\n",
      "Epoch 60701, Training Loss: 48367, Validation Loss: 56890, 45920.41586611711\n",
      "Epoch 60801, Training Loss: 44681, Validation Loss: 55078, 51710.062082805736\n",
      "Epoch 60901, Training Loss: 46390, Validation Loss: 56482, 47607.513471084356\n",
      "Epoch 61001, Training Loss: 43392, Validation Loss: 52939, 43297.956283564534\n",
      "Epoch 61101, Training Loss: 44349, Validation Loss: 63045, 45594.403469192424\n",
      "Epoch 61201, Training Loss: 47525, Validation Loss: 52817, 40839.92919487671\n",
      "Epoch 61301, Training Loss: 45523, Validation Loss: 53527, 43306.49152846768\n",
      "Epoch 61401, Training Loss: 42940, Validation Loss: 53442, 51723.37255867196\n",
      "Epoch 61501, Training Loss: 44951, Validation Loss: 54702, 52353.53403289593\n",
      "Epoch 61601, Training Loss: 45484, Validation Loss: 53163, 41046.18448820947\n",
      "Epoch 61701, Training Loss: 45927, Validation Loss: 52034, 57357.212966739025\n",
      "Epoch 61801, Training Loss: 41914, Validation Loss: 53038, 50583.82098922003\n",
      "Epoch 61901, Training Loss: 45993, Validation Loss: 53751, 42710.27745321957\n",
      "Epoch 62001, Training Loss: 43466, Validation Loss: 56946, 50227.227854424134\n",
      "Epoch 62101, Training Loss: 45653, Validation Loss: 52712, 45166.71232131331\n",
      "Epoch 62201, Training Loss: 46147, Validation Loss: 56660, 41874.48818564456\n",
      "Epoch 62301, Training Loss: 45197, Validation Loss: 56322, 52059.11068358369\n",
      "Epoch 62401, Training Loss: 45377, Validation Loss: 51927, 43550.68324311103\n",
      "Epoch 62501, Training Loss: 43133, Validation Loss: 51919, 55468.63320160817\n",
      "Epoch 62601, Training Loss: 45354, Validation Loss: 53145, 39842.339710789216\n",
      "Epoch 62701, Training Loss: 45477, Validation Loss: 54582, 56377.284546807\n",
      "Epoch 62801, Training Loss: 43572, Validation Loss: 53949, 39890.32787584639\n",
      "Epoch 62901, Training Loss: 46311, Validation Loss: 55100, 45411.742213926605\n",
      "Epoch 63001, Training Loss: 44101, Validation Loss: 52827, 44188.586323241645\n",
      "Epoch 63101, Training Loss: 46016, Validation Loss: 51874, 55050.24227164963\n",
      "Epoch 63201, Training Loss: 45748, Validation Loss: 54551, 33825.03989972396\n",
      "Epoch 63301, Training Loss: 41956, Validation Loss: 53477, 54128.27163966293\n",
      "Epoch 63401, Training Loss: 43505, Validation Loss: 53850, 57271.249757204416\n",
      "Epoch 63501, Training Loss: 45445, Validation Loss: 53823, 62444.67287794027\n",
      "Epoch 63601, Training Loss: 44084, Validation Loss: 52426, 37008.6938134188\n",
      "Epoch 63701, Training Loss: 46714, Validation Loss: 58085, 47929.41094148829\n",
      "Epoch 63801, Training Loss: 43202, Validation Loss: 51804, 41662.79866649881\n",
      "Epoch 63901, Training Loss: 44873, Validation Loss: 55360, 55937.47516658207\n",
      "Epoch 64001, Training Loss: 43435, Validation Loss: 54010, 36399.632694879816\n",
      "Epoch 64101, Training Loss: 43950, Validation Loss: 59279, 48604.445733998706\n",
      "Epoch 64201, Training Loss: 43390, Validation Loss: 56242, 57926.80490175036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64301, Training Loss: 45850, Validation Loss: 56906, 47504.577513609336\n",
      "Epoch 64401, Training Loss: 44034, Validation Loss: 54306, 52207.61070521862\n",
      "Epoch 64501, Training Loss: 43741, Validation Loss: 55186, 44709.7191508828\n",
      "Epoch 64601, Training Loss: 44377, Validation Loss: 53550, 44776.07172271997\n",
      "Epoch 64701, Training Loss: 45741, Validation Loss: 53982, 43468.07927906702\n",
      "Epoch 64801, Training Loss: 43566, Validation Loss: 54814, 73082.36206569137\n",
      "Epoch 64901, Training Loss: 45665, Validation Loss: 58954, 43391.8446907526\n",
      "Epoch 65001, Training Loss: 47245, Validation Loss: 76421, 47506.42943095187\n",
      "Epoch 65101, Training Loss: 44176, Validation Loss: 52690, 56474.23353145083\n",
      "Epoch 65201, Training Loss: 42741, Validation Loss: 52993, 53591.23173542932\n",
      "Epoch 65301, Training Loss: 45158, Validation Loss: 52309, 36413.16871019909\n",
      "Epoch 65401, Training Loss: 48022, Validation Loss: 52747, 56831.36477245379\n",
      "Epoch 65501, Training Loss: 45394, Validation Loss: 54050, 46577.592239068916\n",
      "Epoch 65601, Training Loss: 43464, Validation Loss: 51445, 50817.97809398166\n",
      "Epoch 65701, Training Loss: 44713, Validation Loss: 53088, 40920.273958982776\n",
      "Epoch 65801, Training Loss: 42722, Validation Loss: 51778, 43937.12405757867\n",
      "Epoch 65901, Training Loss: 46144, Validation Loss: 59656, 43218.96753648751\n",
      "Epoch 66001, Training Loss: 42774, Validation Loss: 60277, 53040.55935093598\n",
      "Epoch 66101, Training Loss: 46155, Validation Loss: 61995, 51273.271754384834\n",
      "Epoch 66201, Training Loss: 42960, Validation Loss: 56365, 53466.81181775717\n",
      "Epoch 66301, Training Loss: 44226, Validation Loss: 59264, 50187.39911738364\n",
      "Epoch 66401, Training Loss: 43273, Validation Loss: 54638, 58273.301470205806\n",
      "Epoch 66501, Training Loss: 44515, Validation Loss: 53783, 43198.60083384961\n",
      "Epoch 66601, Training Loss: 44181, Validation Loss: 59070, 41593.793619389\n",
      "Epoch 66701, Training Loss: 47625, Validation Loss: 55150, 41582.72111007058\n",
      "Epoch 66801, Training Loss: 48815, Validation Loss: 52744, 52738.542322333895\n",
      "Epoch 66901, Training Loss: 43825, Validation Loss: 55309, 38070.26858032001\n",
      "Epoch 67001, Training Loss: 45132, Validation Loss: 55284, 42465.788355719684\n",
      "Epoch 67101, Training Loss: 43697, Validation Loss: 57644, 62581.38123974768\n",
      "Epoch 67201, Training Loss: 45087, Validation Loss: 69221, 60174.8317359859\n",
      "Epoch 67301, Training Loss: 43105, Validation Loss: 54377, 52998.901475897816\n",
      "Epoch 67401, Training Loss: 45709, Validation Loss: 54488, 38746.09259981499\n",
      "Epoch 67501, Training Loss: 44504, Validation Loss: 58331, 39054.59760146724\n",
      "Epoch 67601, Training Loss: 42447, Validation Loss: 53176, 27977.71784714681\n",
      "Epoch 67701, Training Loss: 44354, Validation Loss: 51781, 52124.98940600939\n",
      "Epoch 67801, Training Loss: 45769, Validation Loss: 57795, 51801.06562982296\n",
      "Epoch 67901, Training Loss: 46410, Validation Loss: 58054, 59890.360689950496\n",
      "Epoch 68001, Training Loss: 45753, Validation Loss: 51719, 51172.83158277024\n",
      "Epoch 68101, Training Loss: 43041, Validation Loss: 55142, 40500.56704426112\n",
      "Epoch 68201, Training Loss: 44778, Validation Loss: 51985, 35510.70714127597\n",
      "Epoch 68301, Training Loss: 42957, Validation Loss: 54894, 40110.85484662816\n",
      "Epoch 68401, Training Loss: 44448, Validation Loss: 52853, 48428.14914330173\n",
      "Epoch 68501, Training Loss: 41395, Validation Loss: 54292, 46567.818651387235\n",
      "Epoch 68601, Training Loss: 41528, Validation Loss: 53136, 45280.934935631194\n",
      "Epoch 68701, Training Loss: 44500, Validation Loss: 54173, 37052.28141104952\n",
      "Epoch 68801, Training Loss: 43518, Validation Loss: 53582, 37518.62952049057\n",
      "Epoch 68901, Training Loss: 45056, Validation Loss: 52515, 37291.0363485693\n",
      "Epoch 69001, Training Loss: 44257, Validation Loss: 53909, 44489.92165788066\n",
      "Epoch 69101, Training Loss: 42715, Validation Loss: 55256, 62240.39728586014\n",
      "Epoch 69201, Training Loss: 48229, Validation Loss: 61140, 37161.05845189682\n",
      "Epoch 69301, Training Loss: 47089, Validation Loss: 53380, 46313.67556194719\n",
      "Epoch 69401, Training Loss: 44855, Validation Loss: 52415, 49292.22264703918\n",
      "Epoch 69501, Training Loss: 42784, Validation Loss: 52571, 41578.53432048282\n",
      "Epoch 69601, Training Loss: 42310, Validation Loss: 57479, 45272.36451073296\n",
      "Epoch 69701, Training Loss: 47138, Validation Loss: 54281, 44781.7901530253\n",
      "Epoch 69801, Training Loss: 44891, Validation Loss: 57335, 57175.70165443564\n",
      "Epoch 69901, Training Loss: 43753, Validation Loss: 52631, 44011.86318752222\n",
      "Epoch 70001, Training Loss: 41638, Validation Loss: 64223, 51150.61384148351\n",
      "Epoch 70101, Training Loss: 44393, Validation Loss: 54606, 51426.836821414494\n",
      "Epoch 70201, Training Loss: 45312, Validation Loss: 53324, 39468.064748415956\n",
      "Epoch 70301, Training Loss: 47291, Validation Loss: 53069, 62480.25449250741\n",
      "Epoch 70401, Training Loss: 43607, Validation Loss: 51963, 58122.83352020612\n",
      "Epoch 70501, Training Loss: 46418, Validation Loss: 52362, 40092.238953380154\n",
      "Epoch 70601, Training Loss: 44999, Validation Loss: 54410, 38572.76409165183\n",
      "Epoch 70701, Training Loss: 45895, Validation Loss: 53367, 40210.483418724696\n",
      "Epoch 70801, Training Loss: 43482, Validation Loss: 59855, 38375.06036191562\n",
      "Epoch 70901, Training Loss: 44353, Validation Loss: 50713, 44178.60916423493\n",
      "Epoch 71001, Training Loss: 40906, Validation Loss: 54688, 45554.4914392657\n",
      "Epoch 71101, Training Loss: 43144, Validation Loss: 56315, 48581.577288418666\n",
      "Epoch 71201, Training Loss: 45577, Validation Loss: 55193, 41244.278215957194\n",
      "Epoch 71301, Training Loss: 44714, Validation Loss: 54389, 47446.57857273034\n",
      "Epoch 71401, Training Loss: 41843, Validation Loss: 52360, 47614.290114344076\n",
      "Epoch 71501, Training Loss: 46455, Validation Loss: 53692, 47919.79264057968\n",
      "Epoch 71601, Training Loss: 45515, Validation Loss: 53092, 44838.56419717454\n",
      "Epoch 71701, Training Loss: 42662, Validation Loss: 55528, 35667.98391009039\n",
      "Epoch 71801, Training Loss: 45107, Validation Loss: 61708, 42616.34412578563\n",
      "Epoch 71901, Training Loss: 49199, Validation Loss: 53568, 44732.31895825177\n",
      "Epoch 72001, Training Loss: 45432, Validation Loss: 53807, 53236.0282377449\n",
      "Epoch 72101, Training Loss: 47573, Validation Loss: 55349, 41395.396454423906\n",
      "Epoch 72201, Training Loss: 42404, Validation Loss: 53457, 47813.37132246528\n",
      "Epoch 72301, Training Loss: 46702, Validation Loss: 57690, 44051.19795861167\n",
      "Epoch 72401, Training Loss: 42549, Validation Loss: 53770, 37485.16675413974\n",
      "Epoch 72501, Training Loss: 44053, Validation Loss: 53849, 42049.36880204833\n",
      "Epoch 72601, Training Loss: 41109, Validation Loss: 55686, 29512.85145722884\n",
      "Epoch 72701, Training Loss: 44114, Validation Loss: 55558, 43790.361734869985\n",
      "Epoch 72801, Training Loss: 44672, Validation Loss: 54834, 78895.38654971581\n",
      "Epoch 72901, Training Loss: 45843, Validation Loss: 53569, 46316.75991181101\n",
      "Epoch 73001, Training Loss: 45935, Validation Loss: 54081, 42103.335667350264\n",
      "Epoch 73101, Training Loss: 45669, Validation Loss: 55221, 45297.70949144982\n",
      "Epoch 73201, Training Loss: 46073, Validation Loss: 55832, 45077.01831938807\n",
      "Epoch 73301, Training Loss: 46914, Validation Loss: 54761, 39829.200436181614\n",
      "Epoch 73401, Training Loss: 43911, Validation Loss: 52069, 42139.48364922099\n",
      "Epoch 73501, Training Loss: 44559, Validation Loss: 54471, 50496.55923081126\n",
      "Epoch 73601, Training Loss: 45487, Validation Loss: 53920, 47411.38774128841\n",
      "Epoch 73701, Training Loss: 44908, Validation Loss: 57370, 42533.90525980753\n",
      "Epoch 73801, Training Loss: 45847, Validation Loss: 57089, 48328.34845239185\n",
      "Epoch 73901, Training Loss: 44428, Validation Loss: 51759, 43965.24949415083\n",
      "Epoch 74001, Training Loss: 41779, Validation Loss: 55507, 38082.70967770166\n",
      "Epoch 74101, Training Loss: 45520, Validation Loss: 62076, 41610.88230192911\n",
      "Epoch 74201, Training Loss: 45443, Validation Loss: 56974, 64357.95997183279\n",
      "Epoch 74301, Training Loss: 45098, Validation Loss: 55723, 43322.90120139857\n",
      "Epoch 74401, Training Loss: 43714, Validation Loss: 55901, 43766.188931870245\n",
      "Epoch 74501, Training Loss: 40455, Validation Loss: 52449, 33385.73488962057\n",
      "Epoch 74601, Training Loss: 47746, Validation Loss: 56515, 45529.14251487889\n",
      "Epoch 74701, Training Loss: 43365, Validation Loss: 52728, 42953.06549233061\n",
      "Epoch 74801, Training Loss: 41473, Validation Loss: 52365, 36547.84666968838\n",
      "Epoch 74901, Training Loss: 44195, Validation Loss: 51976, 61903.478709464755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75001, Training Loss: 42657, Validation Loss: 54330, 50130.8212282035\n",
      "Epoch 75101, Training Loss: 42720, Validation Loss: 54389, 46260.85002071158\n",
      "Epoch 75201, Training Loss: 45871, Validation Loss: 53931, 56582.09181000447\n",
      "Epoch 75301, Training Loss: 46435, Validation Loss: 54078, 47354.58687099629\n",
      "Epoch 75401, Training Loss: 42486, Validation Loss: 55924, 38322.32487355681\n",
      "Epoch 75501, Training Loss: 46114, Validation Loss: 51401, 41445.07661042609\n",
      "Epoch 75601, Training Loss: 46716, Validation Loss: 55030, 57143.62923749592\n",
      "Epoch 75701, Training Loss: 48537, Validation Loss: 52846, 43406.76370552823\n",
      "Epoch 75801, Training Loss: 42875, Validation Loss: 54766, 33694.34307794849\n",
      "Epoch 75901, Training Loss: 44936, Validation Loss: 59049, 47994.941303861444\n",
      "Epoch 76001, Training Loss: 44737, Validation Loss: 57458, 34583.131451721456\n",
      "Epoch 76101, Training Loss: 43939, Validation Loss: 51761, 36547.92735954092\n",
      "Epoch 76201, Training Loss: 43060, Validation Loss: 53942, 45424.06286003571\n",
      "Epoch 76301, Training Loss: 48125, Validation Loss: 58672, 42769.32838994616\n",
      "Epoch 76401, Training Loss: 44267, Validation Loss: 70765, 63749.39940286865\n",
      "Epoch 76501, Training Loss: 46380, Validation Loss: 55482, 42792.194591673986\n",
      "Epoch 76601, Training Loss: 44256, Validation Loss: 57047, 44316.77148545016\n",
      "Epoch 76701, Training Loss: 48807, Validation Loss: 52415, 41099.90540692729\n",
      "Epoch 76801, Training Loss: 45960, Validation Loss: 59079, 57321.22414674788\n",
      "Epoch 76901, Training Loss: 44280, Validation Loss: 51959, 58888.25577458317\n",
      "Epoch 77001, Training Loss: 43794, Validation Loss: 52342, 37732.16809240802\n",
      "Epoch 77101, Training Loss: 45122, Validation Loss: 55604, 57627.291049599844\n",
      "Epoch 77201, Training Loss: 44889, Validation Loss: 54143, 38616.80717027377\n",
      "Epoch 77301, Training Loss: 45679, Validation Loss: 51867, 36268.848106396785\n",
      "Epoch 77401, Training Loss: 43349, Validation Loss: 54754, 53089.95347109938\n",
      "Epoch 77501, Training Loss: 46428, Validation Loss: 58418, 50184.664091425446\n",
      "Epoch 77601, Training Loss: 43624, Validation Loss: 55811, 51268.474648662544\n",
      "Epoch 77701, Training Loss: 47895, Validation Loss: 57309, 54727.25979975737\n",
      "Epoch 77801, Training Loss: 44591, Validation Loss: 51724, 36723.09802587344\n",
      "Epoch 77901, Training Loss: 47195, Validation Loss: 56405, 35286.68379925448\n",
      "Epoch 78001, Training Loss: 45300, Validation Loss: 53419, 45499.29791072316\n",
      "Epoch 78101, Training Loss: 43430, Validation Loss: 55525, 29915.36925886291\n",
      "Epoch 78201, Training Loss: 43459, Validation Loss: 54975, 45026.06504248616\n",
      "Epoch 78301, Training Loss: 44783, Validation Loss: 54395, 40647.077760468346\n",
      "Epoch 78401, Training Loss: 44231, Validation Loss: 53792, 43408.18586895139\n",
      "Epoch 78501, Training Loss: 43684, Validation Loss: 56263, 46061.333820043605\n",
      "Epoch 78601, Training Loss: 43747, Validation Loss: 56043, 46195.04249557811\n",
      "Epoch 78701, Training Loss: 41259, Validation Loss: 54064, 42724.108011874705\n",
      "Epoch 78801, Training Loss: 42533, Validation Loss: 56150, 38921.76065711465\n",
      "Epoch 78901, Training Loss: 44987, Validation Loss: 52524, 53792.33420104909\n",
      "Epoch 79001, Training Loss: 44650, Validation Loss: 54145, 35814.461758190955\n",
      "Epoch 79101, Training Loss: 42758, Validation Loss: 56407, 46294.72769519934\n",
      "Epoch 79201, Training Loss: 42907, Validation Loss: 53181, 52841.07637065395\n",
      "Epoch 79301, Training Loss: 45649, Validation Loss: 52553, 47225.81461428853\n",
      "Epoch 79401, Training Loss: 43179, Validation Loss: 54034, 50767.447173060435\n",
      "Epoch 79501, Training Loss: 47072, Validation Loss: 53530, 39152.741854466476\n",
      "Epoch 79601, Training Loss: 41470, Validation Loss: 53524, 41002.355301984004\n",
      "Epoch 79701, Training Loss: 47846, Validation Loss: 54253, 39871.410214992626\n",
      "Epoch 79801, Training Loss: 43860, Validation Loss: 53606, 39366.49195885278\n",
      "Epoch 79901, Training Loss: 47383, Validation Loss: 55795, 41848.81477948036\n",
      "Epoch 80001, Training Loss: 43167, Validation Loss: 53341, 62432.49130745525\n",
      "Epoch 80101, Training Loss: 46546, Validation Loss: 54424, 42002.2997467967\n",
      "Epoch 80201, Training Loss: 44644, Validation Loss: 55996, 48350.64553990197\n",
      "Epoch 80301, Training Loss: 48718, Validation Loss: 58302, 37539.72920411632\n",
      "Epoch 80401, Training Loss: 43036, Validation Loss: 57244, 39327.50261619297\n",
      "Epoch 80501, Training Loss: 44437, Validation Loss: 58443, 39251.872553162255\n",
      "Epoch 80601, Training Loss: 43598, Validation Loss: 54385, 41874.52811433991\n",
      "Epoch 80701, Training Loss: 44138, Validation Loss: 53335, 48429.36829227143\n",
      "Epoch 80801, Training Loss: 46325, Validation Loss: 56124, 34079.52803398945\n",
      "Epoch 80901, Training Loss: 41632, Validation Loss: 55917, 46836.68888897608\n",
      "Epoch 81001, Training Loss: 46578, Validation Loss: 52653, 44071.12760860316\n",
      "Epoch 81101, Training Loss: 44845, Validation Loss: 52876, 40467.214042556734\n",
      "Epoch 81201, Training Loss: 44691, Validation Loss: 57631, 47093.65567139084\n",
      "Epoch 81301, Training Loss: 45850, Validation Loss: 53711, 46245.989454782124\n",
      "Epoch 81401, Training Loss: 42805, Validation Loss: 54323, 27623.932236081997\n",
      "Epoch 81501, Training Loss: 42719, Validation Loss: 52147, 47859.72653212018\n",
      "Epoch 81601, Training Loss: 42010, Validation Loss: 53130, 43602.797162982344\n",
      "Epoch 81701, Training Loss: 42359, Validation Loss: 52340, 34847.17555520325\n",
      "Epoch 81801, Training Loss: 43321, Validation Loss: 52585, 39927.88683074238\n",
      "Epoch 81901, Training Loss: 44793, Validation Loss: 56823, 44514.578662293265\n",
      "Epoch 82001, Training Loss: 45965, Validation Loss: 56987, 62129.923127032984\n",
      "Epoch 82101, Training Loss: 43810, Validation Loss: 54196, 47110.17076329023\n",
      "Epoch 82201, Training Loss: 47627, Validation Loss: 53732, 38099.35380388426\n",
      "Epoch 82301, Training Loss: 43115, Validation Loss: 53573, 48493.74603333359\n",
      "Epoch 82401, Training Loss: 43701, Validation Loss: 53616, 52142.02351374567\n",
      "Epoch 82501, Training Loss: 48165, Validation Loss: 51601, 38378.9993459509\n",
      "Epoch 82601, Training Loss: 44618, Validation Loss: 54031, 37922.40543169176\n",
      "Epoch 82701, Training Loss: 45100, Validation Loss: 53600, 31410.848442259045\n",
      "Epoch 82801, Training Loss: 43718, Validation Loss: 54027, 50883.06640919164\n",
      "Epoch 82901, Training Loss: 44456, Validation Loss: 56973, 55001.980072545186\n",
      "Epoch 83001, Training Loss: 44081, Validation Loss: 54974, 45233.23405378853\n",
      "Epoch 83101, Training Loss: 45339, Validation Loss: 56082, 39174.46638139296\n",
      "Epoch 83201, Training Loss: 44617, Validation Loss: 54536, 41460.0811698148\n",
      "Epoch 83301, Training Loss: 45793, Validation Loss: 53272, 32910.574634193996\n",
      "Epoch 83401, Training Loss: 45746, Validation Loss: 54487, 42054.7770459196\n",
      "Epoch 83501, Training Loss: 43110, Validation Loss: 53396, 44882.61049149756\n",
      "Epoch 83601, Training Loss: 47789, Validation Loss: 57028, 47676.84503203323\n",
      "Epoch 83701, Training Loss: 44703, Validation Loss: 53299, 40768.39058744558\n",
      "Epoch 83801, Training Loss: 44977, Validation Loss: 53755, 58394.89525836023\n",
      "Epoch 83901, Training Loss: 42626, Validation Loss: 53483, 34921.44736667649\n",
      "Epoch 84001, Training Loss: 43473, Validation Loss: 54996, 45231.713809656525\n",
      "Epoch 84101, Training Loss: 44080, Validation Loss: 57855, 36625.741437734985\n",
      "Epoch 84201, Training Loss: 44606, Validation Loss: 54896, 37887.985588706026\n",
      "Epoch 84301, Training Loss: 44212, Validation Loss: 58775, 48726.47270408131\n",
      "Epoch 84401, Training Loss: 46262, Validation Loss: 54795, 37744.03148458723\n",
      "Epoch 84501, Training Loss: 42527, Validation Loss: 53678, 42946.054211293704\n",
      "Epoch 84601, Training Loss: 46293, Validation Loss: 62200, 43379.67899676893\n",
      "Epoch 84701, Training Loss: 45927, Validation Loss: 54241, 54059.60455676712\n",
      "Epoch 84801, Training Loss: 43934, Validation Loss: 56223, 38783.08291949317\n",
      "Epoch 84901, Training Loss: 43225, Validation Loss: 53070, 46985.38293838818\n",
      "Epoch 85001, Training Loss: 45511, Validation Loss: 56476, 41605.48846121226\n",
      "Epoch 85101, Training Loss: 42394, Validation Loss: 52945, 37532.97270083974\n",
      "Epoch 85201, Training Loss: 44816, Validation Loss: 52981, 49798.59821393071\n",
      "Epoch 85301, Training Loss: 43973, Validation Loss: 53969, 36516.77998788104\n",
      "Epoch 85401, Training Loss: 44892, Validation Loss: 55068, 43364.46544177575\n",
      "Epoch 85501, Training Loss: 48395, Validation Loss: 53383, 34053.915480309\n",
      "Epoch 85601, Training Loss: 42962, Validation Loss: 54534, 37767.71929358525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85701, Training Loss: 46963, Validation Loss: 53631, 65032.18620253566\n",
      "Epoch 85801, Training Loss: 46580, Validation Loss: 53408, 42550.690511716864\n",
      "Epoch 85901, Training Loss: 46049, Validation Loss: 53865, 34268.085770359445\n",
      "Epoch 86001, Training Loss: 45312, Validation Loss: 53672, 46078.55300031189\n",
      "Epoch 86101, Training Loss: 43411, Validation Loss: 54053, 35277.04534779417\n",
      "Epoch 86201, Training Loss: 43946, Validation Loss: 56479, 32370.88120080483\n",
      "Epoch 86301, Training Loss: 47840, Validation Loss: 58567, 54229.2220488566\n",
      "Epoch 86401, Training Loss: 48352, Validation Loss: 56942, 37740.24763676989\n",
      "Epoch 86501, Training Loss: 46378, Validation Loss: 53608, 43272.790645081484\n",
      "Epoch 86601, Training Loss: 45279, Validation Loss: 55816, 42740.23181009401\n",
      "Epoch 86701, Training Loss: 44422, Validation Loss: 53549, 37123.04969937962\n",
      "Epoch 86801, Training Loss: 42811, Validation Loss: 53512, 40271.58937414266\n",
      "Epoch 86901, Training Loss: 44216, Validation Loss: 53510, 50023.76970670635\n",
      "Epoch 87001, Training Loss: 45727, Validation Loss: 66498, 34357.41288628261\n",
      "Epoch 87101, Training Loss: 46055, Validation Loss: 56036, 45747.27597299178\n",
      "Epoch 87201, Training Loss: 42166, Validation Loss: 52956, 40557.27524126154\n",
      "Epoch 87301, Training Loss: 45230, Validation Loss: 54823, 47459.51532904664\n",
      "Epoch 87401, Training Loss: 45065, Validation Loss: 54392, 34640.18064556748\n",
      "Epoch 87501, Training Loss: 42924, Validation Loss: 53414, 39102.97593571827\n",
      "Epoch 87601, Training Loss: 44677, Validation Loss: 52707, 42295.91883960294\n",
      "Epoch 87701, Training Loss: 44275, Validation Loss: 53258, 33979.88497141934\n",
      "Epoch 87801, Training Loss: 43295, Validation Loss: 53911, 55066.52917924922\n",
      "Epoch 87901, Training Loss: 45213, Validation Loss: 53847, 42339.87720246255\n",
      "Epoch 88001, Training Loss: 45329, Validation Loss: 56135, 47498.80929055065\n",
      "Epoch 88101, Training Loss: 42472, Validation Loss: 55538, 55329.17943837371\n",
      "Epoch 88201, Training Loss: 47726, Validation Loss: 57810, 38375.75376434662\n",
      "Epoch 88301, Training Loss: 42778, Validation Loss: 57045, 42204.60191555523\n",
      "Epoch 88401, Training Loss: 43876, Validation Loss: 53332, 41118.873567147326\n",
      "Epoch 88501, Training Loss: 44495, Validation Loss: 56577, 34550.33571651832\n",
      "Epoch 88601, Training Loss: 42709, Validation Loss: 56568, 41075.38523520233\n",
      "Epoch 88701, Training Loss: 46240, Validation Loss: 53358, 43982.974720753184\n",
      "Epoch 88801, Training Loss: 41432, Validation Loss: 58752, 36895.98584161602\n",
      "Epoch 88901, Training Loss: 43286, Validation Loss: 55830, 42429.24455516684\n",
      "Epoch 89001, Training Loss: 47543, Validation Loss: 51732, 38135.45112125098\n",
      "Epoch 89101, Training Loss: 44578, Validation Loss: 53328, 49556.56914284156\n",
      "Epoch 89201, Training Loss: 45890, Validation Loss: 52458, 36319.68092625367\n",
      "Epoch 89301, Training Loss: 44557, Validation Loss: 58195, 35452.007613598624\n",
      "Epoch 89401, Training Loss: 42913, Validation Loss: 57122, 52927.15257381369\n",
      "Epoch 89501, Training Loss: 45862, Validation Loss: 51627, 33064.47772231224\n",
      "Epoch 89601, Training Loss: 43279, Validation Loss: 53412, 43398.976377661405\n",
      "Epoch 89701, Training Loss: 41500, Validation Loss: 55948, 29246.998933543236\n",
      "Epoch 89801, Training Loss: 42977, Validation Loss: 53494, 30379.092160932196\n",
      "Epoch 89901, Training Loss: 43765, Validation Loss: 59685, 42427.02808442147\n",
      "Epoch 90001, Training Loss: 43865, Validation Loss: 54795, 41172.3835753931\n",
      "Epoch 90101, Training Loss: 43541, Validation Loss: 52923, 54096.48277983766\n",
      "Epoch 90201, Training Loss: 41964, Validation Loss: 52444, 38661.54722227387\n",
      "Epoch 90301, Training Loss: 41608, Validation Loss: 57736, 45580.21820305939\n",
      "Epoch 90401, Training Loss: 41974, Validation Loss: 54497, 32182.720120619517\n",
      "Epoch 90501, Training Loss: 44459, Validation Loss: 53378, 42734.07642820325\n",
      "Epoch 90601, Training Loss: 44578, Validation Loss: 54063, 34684.08406724113\n",
      "Epoch 90701, Training Loss: 42837, Validation Loss: 53293, 40262.8063718724\n",
      "Epoch 90801, Training Loss: 46217, Validation Loss: 56206, 52638.80088561896\n",
      "Epoch 90901, Training Loss: 43392, Validation Loss: 57096, 58475.54813206964\n",
      "Epoch 91001, Training Loss: 44358, Validation Loss: 55190, 36517.815176615746\n",
      "Epoch 91101, Training Loss: 44320, Validation Loss: 54543, 36808.01856560638\n",
      "Epoch 91201, Training Loss: 42481, Validation Loss: 57173, 46832.0651448935\n",
      "Epoch 91301, Training Loss: 43520, Validation Loss: 56347, 36086.968804113065\n",
      "Epoch 91401, Training Loss: 42915, Validation Loss: 53396, 47364.49902153604\n",
      "Epoch 91501, Training Loss: 43104, Validation Loss: 55416, 45173.63025334845\n",
      "Epoch 91601, Training Loss: 44905, Validation Loss: 56980, 42836.32693716642\n",
      "Epoch 91701, Training Loss: 43608, Validation Loss: 53068, 47109.61209002711\n",
      "Epoch 91801, Training Loss: 44375, Validation Loss: 57406, 45014.44105988938\n",
      "Epoch 91901, Training Loss: 45142, Validation Loss: 53617, 38573.231011253636\n",
      "Epoch 92001, Training Loss: 45147, Validation Loss: 60137, 70434.06036418519\n",
      "Epoch 92101, Training Loss: 45244, Validation Loss: 54014, 46794.9708769271\n",
      "Epoch 92201, Training Loss: 44538, Validation Loss: 53818, 41521.838838687254\n",
      "Epoch 92301, Training Loss: 38740, Validation Loss: 54202, 37625.419925991955\n",
      "Epoch 92401, Training Loss: 43184, Validation Loss: 54726, 37800.68590617521\n",
      "Epoch 92501, Training Loss: 44745, Validation Loss: 53585, 43788.87641441546\n",
      "Epoch 92601, Training Loss: 46228, Validation Loss: 58046, 45120.71077461171\n",
      "Epoch 92701, Training Loss: 41802, Validation Loss: 52654, 37116.07891634904\n",
      "Epoch 92801, Training Loss: 43597, Validation Loss: 57010, 42194.1301095639\n",
      "Epoch 92901, Training Loss: 41810, Validation Loss: 52699, 36646.088128564785\n",
      "Epoch 93001, Training Loss: 42361, Validation Loss: 59219, 48343.16464239801\n",
      "Epoch 93101, Training Loss: 43191, Validation Loss: 52675, 44267.52721595479\n",
      "Epoch 93201, Training Loss: 44891, Validation Loss: 54006, 34530.500998611846\n",
      "Epoch 93301, Training Loss: 42841, Validation Loss: 56634, 39776.451455067196\n",
      "Epoch 93401, Training Loss: 45159, Validation Loss: 53062, 35051.414438158456\n",
      "Epoch 93501, Training Loss: 44770, Validation Loss: 52324, 52579.05976905572\n",
      "Epoch 93601, Training Loss: 46929, Validation Loss: 54356, 48113.40297411475\n",
      "Epoch 93701, Training Loss: 49377, Validation Loss: 54783, 42621.580406641144\n",
      "Epoch 93801, Training Loss: 43854, Validation Loss: 52824, 43353.97783585753\n",
      "Epoch 93901, Training Loss: 42260, Validation Loss: 52853, 36832.96584072313\n",
      "Epoch 94001, Training Loss: 43186, Validation Loss: 55515, 35055.94349641275\n",
      "Epoch 94101, Training Loss: 43359, Validation Loss: 51841, 38243.6318959132\n",
      "Epoch 94201, Training Loss: 40860, Validation Loss: 56585, 50798.69303838084\n",
      "Epoch 94301, Training Loss: 42234, Validation Loss: 55679, 43853.41977960757\n",
      "Epoch 94401, Training Loss: 42151, Validation Loss: 60845, 43701.956802171066\n",
      "Epoch 94501, Training Loss: 42040, Validation Loss: 56505, 35685.24172831527\n",
      "Epoch 94601, Training Loss: 43716, Validation Loss: 55927, 41142.028585570886\n",
      "Epoch 94701, Training Loss: 50610, Validation Loss: 53886, 43758.23702674025\n",
      "Epoch 94801, Training Loss: 47098, Validation Loss: 57997, 43403.32568719291\n",
      "Epoch 94901, Training Loss: 45021, Validation Loss: 57988, 56632.028522141896\n",
      "Epoch 95001, Training Loss: 41314, Validation Loss: 54927, 42818.621840470514\n",
      "Epoch 95101, Training Loss: 45756, Validation Loss: 54036, 48214.95390912968\n",
      "Epoch 95201, Training Loss: 43706, Validation Loss: 62904, 46674.8590489018\n",
      "Epoch 95301, Training Loss: 40851, Validation Loss: 51302, 46630.17745628338\n",
      "Epoch 95401, Training Loss: 45185, Validation Loss: 54320, 27315.43317664498\n",
      "Epoch 95501, Training Loss: 42168, Validation Loss: 52462, 40063.43276798857\n",
      "Epoch 95601, Training Loss: 43808, Validation Loss: 58157, 33141.019296325794\n",
      "Epoch 95701, Training Loss: 42333, Validation Loss: 55572, 42824.66589277488\n",
      "Epoch 95801, Training Loss: 44846, Validation Loss: 54401, 36200.560558902056\n",
      "Epoch 95901, Training Loss: 46096, Validation Loss: 56196, 45661.447756750764\n",
      "Epoch 96001, Training Loss: 42836, Validation Loss: 53476, 32952.541800562845\n",
      "Epoch 96101, Training Loss: 42806, Validation Loss: 54165, 37475.772745385206\n",
      "Epoch 96201, Training Loss: 44170, Validation Loss: 52319, 44065.7253567778\n",
      "Epoch 96301, Training Loss: 41409, Validation Loss: 57561, 31597.026099222665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96401, Training Loss: 47323, Validation Loss: 52864, 34842.56525882699\n",
      "Epoch 96501, Training Loss: 44560, Validation Loss: 55234, 34485.14219965532\n",
      "Epoch 96601, Training Loss: 45880, Validation Loss: 55954, 38620.226262032505\n",
      "Epoch 96701, Training Loss: 43128, Validation Loss: 56548, 34335.256966822024\n",
      "Epoch 96801, Training Loss: 45657, Validation Loss: 57541, 49295.23741113462\n",
      "Epoch 96901, Training Loss: 45567, Validation Loss: 57142, 45188.14102685976\n",
      "Epoch 97001, Training Loss: 45064, Validation Loss: 54637, 39386.516861600045\n",
      "Epoch 97101, Training Loss: 43742, Validation Loss: 52193, 30632.764874367112\n",
      "Epoch 97201, Training Loss: 43341, Validation Loss: 54203, 49195.180973462724\n",
      "Epoch 97301, Training Loss: 44448, Validation Loss: 53932, 31085.73036999248\n",
      "Epoch 97401, Training Loss: 46538, Validation Loss: 53208, 50224.303530200465\n",
      "Epoch 97501, Training Loss: 44588, Validation Loss: 52582, 45159.74301348402\n",
      "Epoch 97601, Training Loss: 46404, Validation Loss: 55503, 51387.32937769943\n",
      "Epoch 97701, Training Loss: 43390, Validation Loss: 53747, 54001.672998595845\n",
      "Epoch 97801, Training Loss: 44452, Validation Loss: 52621, 35759.80204179332\n",
      "Epoch 97901, Training Loss: 43110, Validation Loss: 53894, 51532.80253096442\n",
      "Epoch 98001, Training Loss: 41388, Validation Loss: 52774, 40630.4627150987\n",
      "Epoch 98101, Training Loss: 43372, Validation Loss: 53400, 37939.82251796169\n",
      "Epoch 98201, Training Loss: 45008, Validation Loss: 63128, 60147.406822940764\n",
      "Epoch 98301, Training Loss: 41882, Validation Loss: 60622, 40308.02154502425\n",
      "Epoch 98401, Training Loss: 43447, Validation Loss: 56191, 54671.82353238119\n",
      "Epoch 98501, Training Loss: 43976, Validation Loss: 53911, 41538.78602148343\n",
      "Epoch 98601, Training Loss: 43706, Validation Loss: 51388, 57388.904425236724\n",
      "Epoch 98701, Training Loss: 44148, Validation Loss: 53188, 40408.06978878711\n",
      "Epoch 98801, Training Loss: 44117, Validation Loss: 53471, 44238.411605609435\n",
      "Epoch 98901, Training Loss: 42346, Validation Loss: 60009, 43420.81166821085\n",
      "Epoch 99001, Training Loss: 44842, Validation Loss: 56233, 47938.86204246405\n",
      "Epoch 99101, Training Loss: 45830, Validation Loss: 54590, 42811.250425949904\n",
      "Epoch 99201, Training Loss: 46561, Validation Loss: 54456, 45328.986628343475\n",
      "Epoch 99301, Training Loss: 43976, Validation Loss: 58591, 33058.44119375714\n",
      "Epoch 99401, Training Loss: 44197, Validation Loss: 54246, 30200.15466186084\n",
      "Epoch 99501, Training Loss: 41502, Validation Loss: 54185, 31396.580968177994\n",
      "Epoch 99601, Training Loss: 47183, Validation Loss: 56845, 50386.64900205685\n",
      "Epoch 99701, Training Loss: 41860, Validation Loss: 53215, 30656.290498925053\n",
      "Epoch 99801, Training Loss: 51738, Validation Loss: 53642, 37239.70983193443\n",
      "Epoch 99901, Training Loss: 43483, Validation Loss: 53444, 54845.89975027064\n",
      "Epoch 100001, Training Loss: 45077, Validation Loss: 54987, 38341.16731480337\n",
      "Epoch 100101, Training Loss: 44980, Validation Loss: 55768, 43474.527821385665\n",
      "Epoch 100201, Training Loss: 43043, Validation Loss: 52619, 40700.402519230214\n",
      "Epoch 100301, Training Loss: 46680, Validation Loss: 52633, 50748.49027744179\n",
      "Epoch 100401, Training Loss: 43028, Validation Loss: 53298, 50463.96122408407\n",
      "Epoch 100501, Training Loss: 41219, Validation Loss: 56473, 30547.969203727826\n",
      "Epoch 100601, Training Loss: 43669, Validation Loss: 55898, 39731.88603877207\n",
      "Epoch 100701, Training Loss: 44886, Validation Loss: 53463, 44895.3692767266\n",
      "Epoch 100801, Training Loss: 46011, Validation Loss: 51672, 34032.777828368875\n",
      "Epoch 100901, Training Loss: 47749, Validation Loss: 52850, 37871.38564197975\n",
      "Epoch 101001, Training Loss: 45805, Validation Loss: 53557, 41438.54661787297\n",
      "Epoch 101101, Training Loss: 43951, Validation Loss: 52853, 36737.34317165296\n",
      "Epoch 101201, Training Loss: 44838, Validation Loss: 54486, 37448.158613847634\n",
      "Epoch 101301, Training Loss: 45837, Validation Loss: 54001, 39890.492625742416\n",
      "Epoch 101401, Training Loss: 44218, Validation Loss: 52772, 55405.19444946058\n",
      "Epoch 101501, Training Loss: 43250, Validation Loss: 56533, 36177.76160719819\n",
      "Epoch 101601, Training Loss: 44010, Validation Loss: 54643, 44247.71018406798\n",
      "Epoch 101701, Training Loss: 42538, Validation Loss: 53342, 53037.70161465299\n",
      "Epoch 101801, Training Loss: 44643, Validation Loss: 57319, 49883.69440619322\n",
      "Epoch 101901, Training Loss: 44786, Validation Loss: 51592, 42300.25116299824\n",
      "Epoch 102001, Training Loss: 45297, Validation Loss: 53908, 47273.535287246785\n",
      "Epoch 102101, Training Loss: 44137, Validation Loss: 53468, 31912.7186523517\n",
      "Epoch 102201, Training Loss: 45438, Validation Loss: 54744, 48111.65423698985\n",
      "Epoch 102301, Training Loss: 45584, Validation Loss: 53480, 36535.374666063486\n",
      "Epoch 102401, Training Loss: 44429, Validation Loss: 52050, 43265.13196821152\n",
      "Epoch 102501, Training Loss: 45085, Validation Loss: 53314, 35769.138650033616\n",
      "Epoch 102601, Training Loss: 44739, Validation Loss: 55747, 45484.11669543386\n",
      "Epoch 102701, Training Loss: 44231, Validation Loss: 53140, 45414.383724432206\n",
      "Epoch 102801, Training Loss: 45477, Validation Loss: 57716, 39412.0795442416\n",
      "Epoch 102901, Training Loss: 43927, Validation Loss: 53893, 40227.019402334125\n",
      "Epoch 103001, Training Loss: 45166, Validation Loss: 54186, 44907.15043067286\n",
      "Epoch 103101, Training Loss: 45057, Validation Loss: 53340, 42431.69982548609\n",
      "Epoch 103201, Training Loss: 44321, Validation Loss: 55382, 37614.91342387799\n",
      "Epoch 103301, Training Loss: 43858, Validation Loss: 55013, 53602.49330477626\n",
      "Epoch 103401, Training Loss: 42680, Validation Loss: 51731, 35674.39765211573\n",
      "Epoch 103501, Training Loss: 43301, Validation Loss: 53559, 51623.757481001754\n",
      "Epoch 103601, Training Loss: 43611, Validation Loss: 52657, 36544.00486171068\n",
      "Epoch 103701, Training Loss: 46326, Validation Loss: 55130, 42477.73211829307\n",
      "Epoch 103801, Training Loss: 42401, Validation Loss: 56493, 47681.49580088904\n",
      "Epoch 103901, Training Loss: 41729, Validation Loss: 54345, 33539.59227529338\n",
      "Epoch 104001, Training Loss: 42170, Validation Loss: 53799, 33771.11580998374\n",
      "Epoch 104101, Training Loss: 43422, Validation Loss: 53154, 39090.21645548034\n",
      "Epoch 104201, Training Loss: 48829, Validation Loss: 52284, 31173.263922411654\n",
      "Epoch 104301, Training Loss: 46066, Validation Loss: 53302, 44919.98534908318\n",
      "Epoch 104401, Training Loss: 45990, Validation Loss: 51733, 39843.022354933266\n",
      "Epoch 104501, Training Loss: 45105, Validation Loss: 54465, 45065.47385761979\n",
      "Epoch 104601, Training Loss: 44417, Validation Loss: 53057, 41844.587092141\n",
      "Epoch 104701, Training Loss: 45709, Validation Loss: 53061, 47644.40996657576\n",
      "Epoch 104801, Training Loss: 42504, Validation Loss: 53253, 38196.10908900847\n",
      "Epoch 104901, Training Loss: 44906, Validation Loss: 54103, 36847.9061133677\n",
      "Epoch 105001, Training Loss: 41997, Validation Loss: 55744, 49277.22913980836\n",
      "Epoch 105101, Training Loss: 40529, Validation Loss: 53344, 39431.86869965111\n",
      "Epoch 105201, Training Loss: 42548, Validation Loss: 57542, 48913.09819046422\n",
      "Epoch 105301, Training Loss: 42183, Validation Loss: 54826, 33291.59082452475\n",
      "Epoch 105401, Training Loss: 44341, Validation Loss: 55613, 33626.70281399045\n",
      "Epoch 105501, Training Loss: 40945, Validation Loss: 53809, 34162.453741299665\n",
      "Epoch 105601, Training Loss: 47864, Validation Loss: 52774, 37736.7525010017\n",
      "Epoch 105701, Training Loss: 43883, Validation Loss: 53067, 41670.46570201055\n",
      "Epoch 105801, Training Loss: 43359, Validation Loss: 53217, 40454.08245092375\n",
      "Epoch 105901, Training Loss: 42854, Validation Loss: 53641, 36503.85169840078\n",
      "Epoch 106001, Training Loss: 45075, Validation Loss: 52888, 46862.90113418844\n",
      "Epoch 106101, Training Loss: 42425, Validation Loss: 53614, 46997.3038934457\n",
      "Epoch 106201, Training Loss: 44178, Validation Loss: 53396, 42757.74630749171\n",
      "Epoch 106301, Training Loss: 41657, Validation Loss: 52666, 37813.78524160443\n",
      "Epoch 106401, Training Loss: 43583, Validation Loss: 53516, 55795.54989839404\n",
      "Epoch 106501, Training Loss: 48495, Validation Loss: 53067, 38504.320582348904\n",
      "Epoch 106601, Training Loss: 41999, Validation Loss: 54439, 29580.978048616515\n",
      "Epoch 106701, Training Loss: 45909, Validation Loss: 56011, 34074.47636843418\n",
      "Epoch 106801, Training Loss: 42849, Validation Loss: 54700, 38542.10353133677\n",
      "Epoch 106901, Training Loss: 44319, Validation Loss: 54659, 43744.1710257991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107001, Training Loss: 43654, Validation Loss: 53506, 42807.04522488196\n",
      "Epoch 107101, Training Loss: 41472, Validation Loss: 56245, 38059.835069681874\n",
      "Epoch 107201, Training Loss: 39771, Validation Loss: 53772, 32097.12640359187\n",
      "Epoch 107301, Training Loss: 45927, Validation Loss: 60501, 39205.363681694515\n",
      "Epoch 107401, Training Loss: 44804, Validation Loss: 57923, 38858.582411653806\n",
      "Epoch 107501, Training Loss: 43259, Validation Loss: 52537, 42747.205204064354\n",
      "Epoch 107601, Training Loss: 43845, Validation Loss: 53223, 33010.96968423016\n",
      "Epoch 107701, Training Loss: 45551, Validation Loss: 54739, 42395.54122551395\n",
      "Epoch 107801, Training Loss: 44044, Validation Loss: 53184, 43737.67979046488\n",
      "Epoch 107901, Training Loss: 43406, Validation Loss: 51886, 71047.45003020715\n",
      "Epoch 108001, Training Loss: 46495, Validation Loss: 58772, 41950.7527870126\n",
      "Epoch 108101, Training Loss: 42174, Validation Loss: 54088, 37584.01520820079\n",
      "Epoch 108201, Training Loss: 44909, Validation Loss: 52253, 34644.059879130735\n",
      "Epoch 108301, Training Loss: 46270, Validation Loss: 53430, 41085.99315621537\n",
      "Epoch 108401, Training Loss: 43386, Validation Loss: 53530, 44791.60472641204\n",
      "Epoch 108501, Training Loss: 42605, Validation Loss: 52850, 48558.32758327234\n",
      "Epoch 108601, Training Loss: 44526, Validation Loss: 53973, 40028.53556481741\n",
      "Epoch 108701, Training Loss: 43873, Validation Loss: 54909, 39242.1172064357\n",
      "Epoch 108801, Training Loss: 44800, Validation Loss: 52954, 27938.046051966918\n",
      "Epoch 108901, Training Loss: 42442, Validation Loss: 56842, 49251.76560190404\n",
      "Epoch 109001, Training Loss: 45174, Validation Loss: 56772, 36534.24825615939\n",
      "Epoch 109101, Training Loss: 48905, Validation Loss: 53572, 40241.29855167818\n",
      "Epoch 109201, Training Loss: 45193, Validation Loss: 53238, 44662.0688342443\n",
      "Epoch 109301, Training Loss: 44057, Validation Loss: 54278, 42980.307902428896\n",
      "Epoch 109401, Training Loss: 42921, Validation Loss: 57246, 39232.63472086193\n",
      "Epoch 109501, Training Loss: 44865, Validation Loss: 55248, 38251.10913750344\n",
      "Epoch 109601, Training Loss: 43007, Validation Loss: 55605, 54216.98193615718\n",
      "Epoch 109701, Training Loss: 42073, Validation Loss: 54205, 40261.28497268365\n",
      "Epoch 109801, Training Loss: 44955, Validation Loss: 52625, 57436.99537531804\n",
      "Epoch 109901, Training Loss: 43498, Validation Loss: 53934, 37454.67243988414\n",
      "Epoch 110001, Training Loss: 44849, Validation Loss: 56095, 46908.2550636125\n",
      "Epoch 110101, Training Loss: 45481, Validation Loss: 51974, 50620.613042934965\n",
      "Epoch 110201, Training Loss: 42513, Validation Loss: 55032, 53732.84539355492\n",
      "Epoch 110301, Training Loss: 41704, Validation Loss: 53131, 61773.91922851008\n",
      "Epoch 110401, Training Loss: 44951, Validation Loss: 55387, 51752.75672929812\n",
      "Epoch 110501, Training Loss: 43411, Validation Loss: 53494, 43936.47319401999\n",
      "Epoch 110601, Training Loss: 42935, Validation Loss: 54357, 47909.41534559531\n",
      "Epoch 110701, Training Loss: 43146, Validation Loss: 55554, 35918.37108767138\n",
      "Epoch 110801, Training Loss: 47216, Validation Loss: 52731, 48688.81491440374\n",
      "Epoch 110901, Training Loss: 41736, Validation Loss: 55961, 37466.914759776\n",
      "Epoch 111001, Training Loss: 47120, Validation Loss: 58128, 47919.148178270865\n",
      "Epoch 111101, Training Loss: 46319, Validation Loss: 56098, 48674.51250021692\n",
      "Epoch 111201, Training Loss: 43083, Validation Loss: 54460, 35395.69943190006\n",
      "Epoch 111301, Training Loss: 44229, Validation Loss: 54338, 49711.0689346322\n",
      "Epoch 111401, Training Loss: 44964, Validation Loss: 53976, 30274.846456560725\n",
      "Epoch 111501, Training Loss: 46011, Validation Loss: 56602, 35311.01420538364\n",
      "Epoch 111601, Training Loss: 44883, Validation Loss: 54310, 34793.392784086034\n",
      "Epoch 111701, Training Loss: 44688, Validation Loss: 53983, 35428.861805176355\n",
      "Epoch 111801, Training Loss: 46391, Validation Loss: 58913, 51296.18278852669\n",
      "Epoch 111901, Training Loss: 40401, Validation Loss: 53420, 28686.952983556886\n",
      "Epoch 112001, Training Loss: 47775, Validation Loss: 51857, 42941.82116988187\n",
      "Epoch 112101, Training Loss: 44796, Validation Loss: 51036, 42856.84656040739\n",
      "Epoch 112201, Training Loss: 43583, Validation Loss: 54129, 39476.41234878323\n",
      "Epoch 112301, Training Loss: 44126, Validation Loss: 53197, 31778.911209195954\n",
      "Epoch 112401, Training Loss: 43662, Validation Loss: 56976, 32981.87295648919\n",
      "Epoch 112501, Training Loss: 45599, Validation Loss: 52617, 44974.59200034046\n",
      "Epoch 112601, Training Loss: 44806, Validation Loss: 52961, 44334.01913814582\n",
      "Epoch 112701, Training Loss: 43760, Validation Loss: 53410, 47664.26512436875\n",
      "Epoch 112801, Training Loss: 40286, Validation Loss: 55565, 45156.364138048346\n",
      "Epoch 112901, Training Loss: 40370, Validation Loss: 51959, 31619.977843344226\n",
      "Epoch 113001, Training Loss: 44403, Validation Loss: 53719, 33198.047498779546\n",
      "Epoch 113101, Training Loss: 44361, Validation Loss: 55676, 36267.208396990136\n",
      "Epoch 113201, Training Loss: 43808, Validation Loss: 53902, 33061.91093913021\n",
      "Epoch 113301, Training Loss: 45393, Validation Loss: 53855, 44673.000227696444\n",
      "Epoch 113401, Training Loss: 49165, Validation Loss: 54762, 37041.82221615519\n",
      "Epoch 113501, Training Loss: 45866, Validation Loss: 51835, 32616.07937581035\n",
      "Epoch 113601, Training Loss: 41014, Validation Loss: 53408, 37714.475298349775\n",
      "Epoch 113701, Training Loss: 44388, Validation Loss: 55997, 34965.718431594556\n",
      "Epoch 113801, Training Loss: 43710, Validation Loss: 58426, 36138.977497116466\n",
      "Epoch 113901, Training Loss: 44321, Validation Loss: 53243, 41428.200504338696\n",
      "Epoch 114001, Training Loss: 43136, Validation Loss: 53011, 38644.23171492773\n",
      "Epoch 114101, Training Loss: 44789, Validation Loss: 55025, 38811.62036387305\n",
      "Epoch 114201, Training Loss: 44025, Validation Loss: 53025, 60395.33039094737\n",
      "Epoch 114301, Training Loss: 41980, Validation Loss: 53752, 52257.2148865429\n",
      "Epoch 114401, Training Loss: 43175, Validation Loss: 57604, 44076.95787984957\n",
      "Epoch 114501, Training Loss: 42564, Validation Loss: 53024, 36164.80501311892\n",
      "Epoch 114601, Training Loss: 48421, Validation Loss: 54150, 36849.34740613975\n",
      "Epoch 114701, Training Loss: 44673, Validation Loss: 53384, 48126.44085877507\n",
      "Epoch 114801, Training Loss: 42158, Validation Loss: 52744, 35482.75318768602\n",
      "Epoch 114901, Training Loss: 46753, Validation Loss: 57734, 44193.36632342192\n",
      "Epoch 115001, Training Loss: 43447, Validation Loss: 58146, 31680.26486806799\n",
      "Epoch 115101, Training Loss: 43855, Validation Loss: 53072, 35920.281215108254\n",
      "Epoch 115201, Training Loss: 42392, Validation Loss: 57059, 38256.763316356904\n",
      "Epoch 115301, Training Loss: 43574, Validation Loss: 56812, 42596.472971582916\n",
      "Epoch 115401, Training Loss: 48380, Validation Loss: 56168, 50576.66408028526\n",
      "Epoch 115501, Training Loss: 42118, Validation Loss: 55681, 49354.66171961149\n",
      "Epoch 115601, Training Loss: 44094, Validation Loss: 54811, 33954.53477001968\n",
      "Epoch 115701, Training Loss: 41527, Validation Loss: 51252, 50080.098954419\n",
      "Epoch 115801, Training Loss: 44780, Validation Loss: 52800, 42227.79432974392\n",
      "Epoch 115901, Training Loss: 44736, Validation Loss: 53872, 58282.23143181868\n",
      "Epoch 116001, Training Loss: 45866, Validation Loss: 52083, 41398.279950920754\n",
      "Epoch 116101, Training Loss: 42306, Validation Loss: 53535, 41690.18808298857\n",
      "Epoch 116201, Training Loss: 41451, Validation Loss: 52883, 40697.52737846778\n",
      "Epoch 116301, Training Loss: 42281, Validation Loss: 52870, 39864.39630817754\n",
      "Epoch 116401, Training Loss: 43342, Validation Loss: 53351, 46047.86515051232\n",
      "Epoch 116501, Training Loss: 46437, Validation Loss: 65178, 54354.151896341005\n",
      "Epoch 116601, Training Loss: 45945, Validation Loss: 55514, 36336.05225961314\n",
      "Epoch 116701, Training Loss: 44045, Validation Loss: 53460, 31456.258569889764\n",
      "Epoch 116801, Training Loss: 44887, Validation Loss: 54596, 57606.69493163561\n",
      "Epoch 116901, Training Loss: 46397, Validation Loss: 56072, 29776.329863282455\n",
      "Epoch 117001, Training Loss: 43481, Validation Loss: 53529, 51995.70449250482\n",
      "Epoch 117101, Training Loss: 44733, Validation Loss: 54750, 37933.457568441525\n",
      "Epoch 117201, Training Loss: 44958, Validation Loss: 51295, 39005.126046141835\n",
      "Epoch 117301, Training Loss: 44550, Validation Loss: 53453, 42292.50713814522\n",
      "Epoch 117401, Training Loss: 43562, Validation Loss: 57081, 49243.43680531129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117501, Training Loss: 43004, Validation Loss: 55181, 42514.86526042877\n",
      "Epoch 117601, Training Loss: 46009, Validation Loss: 52120, 39200.038085142136\n",
      "Epoch 117701, Training Loss: 41066, Validation Loss: 54388, 28400.177388058382\n",
      "Epoch 117801, Training Loss: 42585, Validation Loss: 57416, 41057.97607321233\n",
      "Epoch 117901, Training Loss: 44790, Validation Loss: 54603, 34124.326266273274\n",
      "Epoch 118001, Training Loss: 42297, Validation Loss: 52829, 40856.42867242422\n",
      "Epoch 118101, Training Loss: 45395, Validation Loss: 57350, 50318.52888535891\n",
      "Epoch 118201, Training Loss: 46973, Validation Loss: 53542, 42242.863880786375\n",
      "Epoch 118301, Training Loss: 44940, Validation Loss: 53077, 39188.59472998482\n",
      "Epoch 118401, Training Loss: 46694, Validation Loss: 55556, 47971.59083045466\n",
      "Epoch 118501, Training Loss: 47717, Validation Loss: 52316, 35169.54457535642\n",
      "Epoch 118601, Training Loss: 40643, Validation Loss: 52799, 39081.15380326376\n",
      "Epoch 118701, Training Loss: 43345, Validation Loss: 55721, 40447.52446954592\n",
      "Epoch 118801, Training Loss: 43218, Validation Loss: 53512, 40170.1163214658\n",
      "Epoch 118901, Training Loss: 44249, Validation Loss: 53812, 28742.78129830773\n",
      "Epoch 119001, Training Loss: 42432, Validation Loss: 55998, 35276.79927497823\n",
      "Epoch 119101, Training Loss: 43593, Validation Loss: 54496, 37724.874429454816\n",
      "Epoch 119201, Training Loss: 43743, Validation Loss: 60364, 45786.61037629925\n",
      "Epoch 119301, Training Loss: 44325, Validation Loss: 54003, 33700.28536740264\n",
      "Epoch 119401, Training Loss: 44299, Validation Loss: 56016, 38651.71130797873\n",
      "Epoch 119501, Training Loss: 41520, Validation Loss: 53536, 39923.723779510234\n",
      "Epoch 119601, Training Loss: 46619, Validation Loss: 55905, 30234.649415539578\n",
      "Epoch 119701, Training Loss: 43868, Validation Loss: 53100, 36442.582566427394\n",
      "Epoch 119801, Training Loss: 45395, Validation Loss: 52349, 35763.22021621695\n",
      "Epoch 119901, Training Loss: 43868, Validation Loss: 52342, 39578.65148747783\n",
      "Epoch 120001, Training Loss: 44095, Validation Loss: 52860, 36204.08981745281\n",
      "Epoch 120101, Training Loss: 45896, Validation Loss: 54940, 37473.79562003721\n",
      "Epoch 120201, Training Loss: 45324, Validation Loss: 54501, 45163.45434069424\n",
      "Epoch 120301, Training Loss: 44345, Validation Loss: 55609, 54301.04028546775\n",
      "Epoch 120401, Training Loss: 43916, Validation Loss: 57281, 46359.464505275326\n",
      "Epoch 120501, Training Loss: 44399, Validation Loss: 55167, 61334.436487106745\n",
      "Epoch 120601, Training Loss: 44377, Validation Loss: 56415, 39664.682349231975\n",
      "Epoch 120701, Training Loss: 43578, Validation Loss: 55290, 43902.80600414798\n",
      "Epoch 120801, Training Loss: 42424, Validation Loss: 53322, 51432.628547282315\n",
      "Epoch 120901, Training Loss: 42687, Validation Loss: 55666, 46858.727877767866\n",
      "Epoch 121001, Training Loss: 44633, Validation Loss: 52715, 49206.36209180256\n",
      "Epoch 121101, Training Loss: 42680, Validation Loss: 56120, 33298.22463491354\n",
      "Epoch 121201, Training Loss: 43153, Validation Loss: 54067, 43451.317375747516\n",
      "Epoch 121301, Training Loss: 41590, Validation Loss: 56866, 29958.710360133264\n",
      "Epoch 121401, Training Loss: 43743, Validation Loss: 53392, 40564.581124375014\n",
      "Epoch 121501, Training Loss: 45670, Validation Loss: 52395, 42776.79191297624\n",
      "Epoch 121601, Training Loss: 41470, Validation Loss: 52293, 45726.234108610595\n",
      "Epoch 121701, Training Loss: 45785, Validation Loss: 56873, 46353.87174990726\n",
      "Epoch 121801, Training Loss: 42654, Validation Loss: 55413, 28841.397228245118\n",
      "Epoch 121901, Training Loss: 45081, Validation Loss: 53571, 44972.19543627003\n",
      "Epoch 122001, Training Loss: 44842, Validation Loss: 53683, 40781.92136508357\n",
      "Epoch 122101, Training Loss: 47272, Validation Loss: 54106, 33906.8356444195\n",
      "Epoch 122201, Training Loss: 44307, Validation Loss: 55205, 63976.53743015785\n",
      "Epoch 122301, Training Loss: 42831, Validation Loss: 54320, 46645.6713536715\n",
      "Epoch 122401, Training Loss: 44871, Validation Loss: 51861, 66090.53678457324\n",
      "Epoch 122501, Training Loss: 43350, Validation Loss: 54514, 56170.78978152282\n",
      "Epoch 122601, Training Loss: 41573, Validation Loss: 54194, 31705.306727130093\n",
      "Epoch 122701, Training Loss: 43893, Validation Loss: 58075, 29985.598518807634\n",
      "Epoch 122801, Training Loss: 43811, Validation Loss: 52687, 41010.9305081955\n",
      "Epoch 122901, Training Loss: 44232, Validation Loss: 53370, 42872.83181022395\n",
      "Epoch 123001, Training Loss: 41368, Validation Loss: 55530, 32038.820452358723\n",
      "Epoch 123101, Training Loss: 43257, Validation Loss: 57786, 54860.923051859914\n",
      "Epoch 123201, Training Loss: 44145, Validation Loss: 54625, 37104.928842655005\n",
      "Epoch 123301, Training Loss: 43313, Validation Loss: 57184, 36716.34730082645\n",
      "Epoch 123401, Training Loss: 42493, Validation Loss: 55690, 33269.94325413615\n",
      "Epoch 123501, Training Loss: 43430, Validation Loss: 53202, 52834.781440502615\n",
      "Epoch 123601, Training Loss: 45665, Validation Loss: 53141, 34763.24077647944\n",
      "Epoch 123701, Training Loss: 44390, Validation Loss: 54390, 36278.17668582167\n",
      "Epoch 123801, Training Loss: 41591, Validation Loss: 55565, 44664.3230831179\n",
      "Epoch 123901, Training Loss: 42713, Validation Loss: 53023, 33537.85865678007\n",
      "Epoch 124001, Training Loss: 42328, Validation Loss: 55397, 42128.551393992726\n",
      "Epoch 124101, Training Loss: 49941, Validation Loss: 53686, 38471.165741131896\n",
      "Epoch 124201, Training Loss: 44520, Validation Loss: 56185, 44938.166244250984\n",
      "Epoch 124301, Training Loss: 45475, Validation Loss: 52897, 45757.50242971852\n",
      "Epoch 124401, Training Loss: 43450, Validation Loss: 54952, 36275.963775137694\n",
      "Epoch 124501, Training Loss: 44450, Validation Loss: 56580, 33855.27564043681\n",
      "Epoch 124601, Training Loss: 41918, Validation Loss: 53545, 42255.47370958575\n",
      "Epoch 124701, Training Loss: 44813, Validation Loss: 54901, 38071.67445900989\n",
      "Epoch 124801, Training Loss: 45106, Validation Loss: 52252, 34135.0637407147\n",
      "Epoch 124901, Training Loss: 42885, Validation Loss: 53544, 49536.892418900265\n",
      "Epoch 125001, Training Loss: 44593, Validation Loss: 54997, 35291.36271207026\n",
      "Epoch 125101, Training Loss: 41836, Validation Loss: 52629, 38292.52447314676\n",
      "Epoch 125201, Training Loss: 40240, Validation Loss: 57185, 36320.683198282764\n",
      "Epoch 125301, Training Loss: 44588, Validation Loss: 53324, 39637.09918796394\n",
      "Epoch 125401, Training Loss: 44384, Validation Loss: 53884, 40295.77562978968\n",
      "Epoch 125501, Training Loss: 43092, Validation Loss: 55248, 35230.26590638363\n",
      "Epoch 125601, Training Loss: 47339, Validation Loss: 57155, 39512.959411652984\n",
      "Epoch 125701, Training Loss: 45243, Validation Loss: 52608, 44849.301297785074\n",
      "Epoch 125801, Training Loss: 43350, Validation Loss: 55852, 54575.50686253716\n",
      "Epoch 125901, Training Loss: 44783, Validation Loss: 53207, 45635.38901158224\n",
      "Epoch 126001, Training Loss: 42005, Validation Loss: 53305, 27051.77648896915\n",
      "Epoch 126101, Training Loss: 43413, Validation Loss: 53750, 39778.931964912495\n",
      "Epoch 126201, Training Loss: 42996, Validation Loss: 52220, 38805.69583318551\n",
      "Epoch 126301, Training Loss: 45544, Validation Loss: 56609, 45191.32494876443\n",
      "Epoch 126401, Training Loss: 43603, Validation Loss: 52916, 33097.24980846153\n",
      "Epoch 126501, Training Loss: 44746, Validation Loss: 54785, 30565.834400288557\n",
      "Epoch 126601, Training Loss: 42758, Validation Loss: 53206, 40508.71311729396\n",
      "Epoch 126701, Training Loss: 45912, Validation Loss: 57870, 31196.862866967214\n",
      "Epoch 126801, Training Loss: 47729, Validation Loss: 56369, 38246.33698854071\n",
      "Epoch 126901, Training Loss: 47065, Validation Loss: 52084, 68724.00056049172\n",
      "Epoch 127001, Training Loss: 42440, Validation Loss: 53527, 37516.70223680727\n",
      "Epoch 127101, Training Loss: 43300, Validation Loss: 55553, 53362.34767476156\n",
      "Epoch 127201, Training Loss: 44416, Validation Loss: 56919, 40723.89514788371\n",
      "Epoch 127301, Training Loss: 45535, Validation Loss: 56434, 32529.434320877433\n",
      "Epoch 127401, Training Loss: 45386, Validation Loss: 53936, 45399.2495914712\n",
      "Epoch 127501, Training Loss: 43494, Validation Loss: 52592, 37044.705729499925\n",
      "Epoch 127601, Training Loss: 46379, Validation Loss: 54744, 30477.577901323548\n",
      "Epoch 127701, Training Loss: 41903, Validation Loss: 55347, 34067.35782440114\n",
      "Epoch 127801, Training Loss: 42419, Validation Loss: 52876, 37488.35805555485\n",
      "Epoch 127901, Training Loss: 44234, Validation Loss: 55682, 44403.89944577851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128001, Training Loss: 44348, Validation Loss: 54044, 46972.27189434031\n",
      "Epoch 128101, Training Loss: 43833, Validation Loss: 54475, 39396.848658995434\n",
      "Epoch 128201, Training Loss: 45995, Validation Loss: 56759, 38675.09415939096\n",
      "Epoch 128301, Training Loss: 45890, Validation Loss: 55555, 42487.36900317236\n",
      "Epoch 128401, Training Loss: 43649, Validation Loss: 54818, 56041.23241057547\n",
      "Epoch 128501, Training Loss: 43501, Validation Loss: 54350, 39534.75226192679\n",
      "Epoch 128601, Training Loss: 43050, Validation Loss: 54359, 28071.700795595145\n",
      "Epoch 128701, Training Loss: 42298, Validation Loss: 53613, 59746.57970099775\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-2,\n",
    "    weight_decay=3e-8\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.99, \n",
    "    patience=10, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
