{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 96\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "#             nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.12),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.12),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 233250, Validation Loss: 228811, 17456.150529187587\n",
      "Epoch 101, Training Loss: 73953, Validation Loss: 73836, 28948.373108368298\n",
      "Epoch 201, Training Loss: 61695, Validation Loss: 61551, 140596.33993080122\n",
      "Epoch 301, Training Loss: 59300, Validation Loss: 57201, 87006.52696915202\n",
      "Epoch 401, Training Loss: 56728, Validation Loss: 59054, 129429.34795989368\n",
      "Epoch 501, Training Loss: 57860, Validation Loss: 55343, 234188.03060133697\n",
      "Epoch 601, Training Loss: 53569, Validation Loss: 55342, 50985.01714157101\n",
      "Epoch 701, Training Loss: 57793, Validation Loss: 54091, 116404.46745236679\n",
      "Epoch 801, Training Loss: 57410, Validation Loss: 55600, 332747.1361817327\n",
      "Epoch 901, Training Loss: 52209, Validation Loss: 53687, 114660.22571551609\n",
      "Epoch 1001, Training Loss: 57330, Validation Loss: 58480, 239714.70771178324\n",
      "Epoch 1101, Training Loss: 56130, Validation Loss: 54054, 149946.15908990978\n",
      "Epoch 1201, Training Loss: 49300, Validation Loss: 52739, 121451.42079679314\n",
      "Epoch 1301, Training Loss: 49373, Validation Loss: 51499, 191751.97012801317\n",
      "Epoch 1401, Training Loss: 51157, Validation Loss: 55942, 168669.74101540574\n",
      "Epoch 1501, Training Loss: 49994, Validation Loss: 54318, 147034.15207607605\n",
      "Epoch 1601, Training Loss: 56054, Validation Loss: 53070, 301279.25205020816\n",
      "Epoch 1701, Training Loss: 48477, Validation Loss: 54320, 181330.39551442905\n",
      "Epoch 1801, Training Loss: 54373, Validation Loss: 51488, 385448.971197288\n",
      "Epoch 1901, Training Loss: 50622, Validation Loss: 56063, 244137.10895241043\n",
      "Epoch 2001, Training Loss: 51792, Validation Loss: 55833, 311298.75131619006\n",
      "Epoch 2101, Training Loss: 50560, Validation Loss: 52006, 262352.2993003793\n",
      "Epoch 2201, Training Loss: 52463, Validation Loss: 52282, 349811.5767759701\n",
      "Epoch 2301, Training Loss: 50859, Validation Loss: 55213, 351067.2606310641\n",
      "Epoch 2401, Training Loss: 51748, Validation Loss: 55203, 308934.429826399\n",
      "Epoch 2501, Training Loss: 48169, Validation Loss: 52702, 202847.8547307204\n",
      "Epoch 2601, Training Loss: 47023, Validation Loss: 57158, 311423.0448466773\n",
      "Epoch 2701, Training Loss: 47249, Validation Loss: 52779, 171802.19620560328\n",
      "Epoch 2801, Training Loss: 47500, Validation Loss: 52517, 147639.01456603437\n",
      "Epoch 2901, Training Loss: 47996, Validation Loss: 51122, 270412.5074341914\n",
      "Epoch 3001, Training Loss: 48592, Validation Loss: 50412, 113901.78827210514\n",
      "Epoch 3101, Training Loss: 50914, Validation Loss: 52110, 387670.99202688644\n",
      "Epoch 3201, Training Loss: 47089, Validation Loss: 51857, 261192.02043114568\n",
      "Epoch 3301, Training Loss: 53529, Validation Loss: 55300, 405205.4985426374\n",
      "Epoch 3401, Training Loss: 43705, Validation Loss: 49840, 121652.17915371763\n",
      "Epoch 3501, Training Loss: 48521, Validation Loss: 48966, 231970.87714609958\n",
      "Epoch 3601, Training Loss: 47895, Validation Loss: 49745, 135774.29188168628\n",
      "Epoch 3701, Training Loss: 47324, Validation Loss: 51441, 172586.5222692427\n",
      "Epoch 3801, Training Loss: 48214, Validation Loss: 50084, 238505.5823219189\n",
      "Epoch 3901, Training Loss: 49276, Validation Loss: 48814, 190277.37684903448\n",
      "Epoch 4001, Training Loss: 46517, Validation Loss: 53819, 241767.51386189516\n",
      "Epoch 4101, Training Loss: 47244, Validation Loss: 49620, 179856.48705140906\n",
      "Epoch 4201, Training Loss: 46634, Validation Loss: 49785, 162791.42994723274\n",
      "Epoch 4301, Training Loss: 52564, Validation Loss: 54033, 507749.2959037302\n",
      "Epoch 4401, Training Loss: 45396, Validation Loss: 49964, 117905.39984980218\n",
      "Epoch 4501, Training Loss: 47972, Validation Loss: 48798, 195878.38520350284\n",
      "Epoch 4601, Training Loss: 48769, Validation Loss: 50857, 208902.69218805726\n",
      "Epoch 4701, Training Loss: 46507, Validation Loss: 50171, 256304.8228422841\n",
      "Epoch 4801, Training Loss: 48172, Validation Loss: 51192, 159119.9488155295\n",
      "Epoch 4901, Training Loss: 48638, Validation Loss: 50137, 273370.3306683371\n",
      "Epoch 5001, Training Loss: 48159, Validation Loss: 50736, 186858.66632587463\n",
      "Epoch 5101, Training Loss: 50834, Validation Loss: 50510, 183894.228473643\n",
      "Epoch 5201, Training Loss: 49213, Validation Loss: 50295, 296466.8318196696\n",
      "Epoch 5301, Training Loss: 45305, Validation Loss: 49907, 132616.68979392585\n",
      "Epoch 5401, Training Loss: 46065, Validation Loss: 50813, 124149.50380702713\n",
      "Epoch 5501, Training Loss: 43031, Validation Loss: 49867, 159010.940040663\n",
      "Epoch 5601, Training Loss: 47255, Validation Loss: 51740, 199972.90916663114\n",
      "Epoch 5701, Training Loss: 47928, Validation Loss: 52215, 330678.288580353\n",
      "Epoch 5801, Training Loss: 45079, Validation Loss: 50362, 214135.39201412094\n",
      "Epoch 5901, Training Loss: 44467, Validation Loss: 49879, 227413.0677558422\n",
      "Epoch 6001, Training Loss: 48027, Validation Loss: 50342, 166023.02572075938\n",
      "Epoch 6101, Training Loss: 43928, Validation Loss: 49938, 189348.8159923081\n",
      "Epoch 6201, Training Loss: 45456, Validation Loss: 49825, 170450.1081750577\n",
      "Epoch 6301, Training Loss: 45835, Validation Loss: 49278, 301431.02576296724\n",
      "Epoch 6401, Training Loss: 43297, Validation Loss: 49125, 161604.30891345095\n",
      "Epoch 6501, Training Loss: 43703, Validation Loss: 50617, 164850.47351076792\n",
      "Epoch 6601, Training Loss: 44587, Validation Loss: 48959, 132153.6757747782\n",
      "Epoch 6701, Training Loss: 45477, Validation Loss: 50200, 222873.3363655637\n",
      "Epoch 6801, Training Loss: 49411, Validation Loss: 53377, 269741.6603525917\n",
      "Epoch 6901, Training Loss: 44724, Validation Loss: 49285, 209664.74202593454\n",
      "Epoch 7001, Training Loss: 48902, Validation Loss: 52353, 221503.54477535965\n",
      "Epoch 7101, Training Loss: 43797, Validation Loss: 49372, 132114.42550428107\n",
      "Epoch 7201, Training Loss: 44696, Validation Loss: 48680, 141211.99896526826\n",
      "Epoch 7301, Training Loss: 44898, Validation Loss: 49456, 175059.10949121424\n",
      "Epoch 7401, Training Loss: 43206, Validation Loss: 49019, 147888.35927185733\n",
      "Epoch 7501, Training Loss: 44476, Validation Loss: 49708, 200973.8959403493\n",
      "Epoch 7601, Training Loss: 48690, Validation Loss: 49608, 167809.51296650345\n",
      "Epoch 7701, Training Loss: 44924, Validation Loss: 49237, 247885.21502423126\n",
      "Epoch 7801, Training Loss: 50291, Validation Loss: 49669, 460468.4199995797\n",
      "Epoch 7901, Training Loss: 47525, Validation Loss: 50689, 406996.6203781033\n",
      "Epoch 8001, Training Loss: 47441, Validation Loss: 51017, 271999.13661739306\n",
      "Epoch 8101, Training Loss: 45663, Validation Loss: 48523, 181619.4895168161\n",
      "Epoch 8201, Training Loss: 46102, Validation Loss: 48898, 226107.88484241313\n",
      "Epoch 8301, Training Loss: 43000, Validation Loss: 49391, 184290.74551904039\n",
      "Epoch 8401, Training Loss: 50515, Validation Loss: 51181, 209418.4933217743\n",
      "Epoch 8501, Training Loss: 46270, Validation Loss: 52007, 199945.20480861305\n",
      "Epoch 8601, Training Loss: 45733, Validation Loss: 49361, 174063.9026616627\n",
      "Epoch 8701, Training Loss: 43134, Validation Loss: 49564, 119798.27628499387\n",
      "Epoch 8801, Training Loss: 45483, Validation Loss: 48809, 235648.15150879617\n",
      "Epoch 8901, Training Loss: 45837, Validation Loss: 49481, 188801.49408258518\n",
      "Epoch 9001, Training Loss: 46774, Validation Loss: 48766, 313172.15654535405\n",
      "Epoch 9101, Training Loss: 44083, Validation Loss: 49783, 198058.5829646216\n",
      "Epoch 9201, Training Loss: 46656, Validation Loss: 48276, 267723.09532525094\n",
      "Epoch 9301, Training Loss: 49794, Validation Loss: 48852, 257404.28939405215\n",
      "Epoch 9401, Training Loss: 45345, Validation Loss: 48222, 194730.57603702325\n",
      "Epoch 9501, Training Loss: 43827, Validation Loss: 52111, 165220.03454919206\n",
      "Epoch 9601, Training Loss: 44706, Validation Loss: 48567, 184048.7022807022\n",
      "Epoch 9701, Training Loss: 43198, Validation Loss: 48293, 192726.25410092375\n",
      "Epoch 9801, Training Loss: 40982, Validation Loss: 52776, 112992.63795489154\n",
      "Epoch 9901, Training Loss: 46778, Validation Loss: 48245, 225763.6884996885\n",
      "Epoch 10001, Training Loss: 43464, Validation Loss: 50270, 222549.03258396607\n",
      "Epoch 10101, Training Loss: 45968, Validation Loss: 49474, 249269.25707857058\n",
      "Epoch 10201, Training Loss: 43462, Validation Loss: 48518, 190307.05150397937\n",
      "Epoch 10301, Training Loss: 42601, Validation Loss: 52096, 208019.09031174472\n",
      "Epoch 10401, Training Loss: 44953, Validation Loss: 49375, 270245.70456950535\n",
      "Epoch 10501, Training Loss: 42630, Validation Loss: 49548, 170015.81232216634\n",
      "Epoch 10601, Training Loss: 44421, Validation Loss: 50442, 203233.63467397218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10701, Training Loss: 45845, Validation Loss: 47570, 187336.8710130529\n",
      "Epoch 10801, Training Loss: 44053, Validation Loss: 48358, 172412.28410460637\n",
      "Epoch 10901, Training Loss: 42954, Validation Loss: 47412, 143037.55529044426\n",
      "Epoch 11001, Training Loss: 43126, Validation Loss: 50677, 293863.22662056395\n",
      "Epoch 11101, Training Loss: 44148, Validation Loss: 47767, 158546.89231080224\n",
      "Epoch 11201, Training Loss: 44376, Validation Loss: 49115, 309005.91514381766\n",
      "Epoch 11301, Training Loss: 41274, Validation Loss: 48240, 123652.95084579696\n",
      "Epoch 11401, Training Loss: 41099, Validation Loss: 47592, 147291.3442002366\n",
      "Epoch 11501, Training Loss: 41599, Validation Loss: 48653, 209669.93678778168\n",
      "Epoch 11601, Training Loss: 40040, Validation Loss: 47853, 174068.03357639656\n",
      "Epoch 11701, Training Loss: 43290, Validation Loss: 49380, 188271.38363083033\n",
      "Epoch 11801, Training Loss: 45262, Validation Loss: 46937, 287000.23236623715\n",
      "Epoch 11901, Training Loss: 39835, Validation Loss: 47581, 125722.81070265178\n",
      "Epoch 12001, Training Loss: 47050, Validation Loss: 48149, 198029.91709282485\n",
      "Epoch 12101, Training Loss: 42621, Validation Loss: 51302, 194786.2848834695\n",
      "Epoch 12201, Training Loss: 41803, Validation Loss: 47141, 137708.66570556653\n",
      "Epoch 12301, Training Loss: 44142, Validation Loss: 48213, 178269.2479059051\n",
      "Epoch 12401, Training Loss: 41661, Validation Loss: 46865, 184539.15114897865\n",
      "Epoch 12501, Training Loss: 43964, Validation Loss: 46360, 232514.24439458866\n",
      "Epoch 12601, Training Loss: 44646, Validation Loss: 48253, 168426.19855513272\n",
      "Epoch 12701, Training Loss: 43100, Validation Loss: 48318, 241620.987689702\n",
      "Epoch 12801, Training Loss: 40446, Validation Loss: 47384, 230063.8363734229\n",
      "Epoch 12901, Training Loss: 42232, Validation Loss: 48559, 173996.80999316028\n",
      "Epoch 13001, Training Loss: 46768, Validation Loss: 49095, 200945.4354008649\n",
      "Epoch 13101, Training Loss: 44769, Validation Loss: 46847, 193406.07518351855\n",
      "Epoch 13201, Training Loss: 40253, Validation Loss: 49244, 157090.17798495622\n",
      "Epoch 13301, Training Loss: 43982, Validation Loss: 49365, 181406.0940336521\n",
      "Epoch 13401, Training Loss: 43568, Validation Loss: 47818, 206483.12529077296\n",
      "Epoch 13501, Training Loss: 40390, Validation Loss: 49685, 159264.4340800805\n",
      "Epoch 13601, Training Loss: 42886, Validation Loss: 49474, 185659.58746785534\n",
      "Epoch 13701, Training Loss: 39915, Validation Loss: 49322, 129759.84808462608\n",
      "Epoch 13801, Training Loss: 40314, Validation Loss: 46563, 190287.47851701814\n",
      "Epoch 13901, Training Loss: 44309, Validation Loss: 47467, 152878.78313131834\n",
      "Epoch 14001, Training Loss: 41443, Validation Loss: 48033, 181069.09375408795\n",
      "Epoch 14101, Training Loss: 43139, Validation Loss: 48710, 137132.77819195902\n",
      "Epoch 14201, Training Loss: 40305, Validation Loss: 45213, 154858.58825828624\n",
      "Epoch 14301, Training Loss: 40991, Validation Loss: 50647, 195923.2072464079\n",
      "Epoch 14401, Training Loss: 40273, Validation Loss: 49691, 106656.94113530482\n",
      "Epoch 14501, Training Loss: 41611, Validation Loss: 46916, 130291.7158930578\n",
      "Epoch 14601, Training Loss: 41685, Validation Loss: 48925, 210506.10130530046\n",
      "Epoch 14701, Training Loss: 42533, Validation Loss: 45767, 252800.04238274842\n",
      "Epoch 14801, Training Loss: 39823, Validation Loss: 47815, 180877.21875785684\n",
      "Epoch 14901, Training Loss: 47321, Validation Loss: 48145, 366344.52341778716\n",
      "Epoch 15001, Training Loss: 44672, Validation Loss: 46206, 277404.68725326465\n",
      "Epoch 15101, Training Loss: 45260, Validation Loss: 46533, 260801.24650720975\n",
      "Epoch 15201, Training Loss: 38677, Validation Loss: 46159, 186476.10591327175\n",
      "Epoch 15301, Training Loss: 43266, Validation Loss: 47150, 193534.61665069137\n",
      "Epoch 15401, Training Loss: 39484, Validation Loss: 47978, 279077.32555659494\n",
      "Epoch 15501, Training Loss: 42000, Validation Loss: 45768, 251063.76298902705\n",
      "Epoch 15601, Training Loss: 41756, Validation Loss: 52294, 186976.24651984833\n",
      "Epoch 15701, Training Loss: 40534, Validation Loss: 46277, 157060.8807323142\n",
      "Epoch 15801, Training Loss: 43677, Validation Loss: 46372, 186223.405724487\n",
      "Epoch 15901, Training Loss: 44725, Validation Loss: 48145, 116011.97985467942\n",
      "Epoch 16001, Training Loss: 40811, Validation Loss: 45922, 179309.11122794927\n",
      "Epoch 16101, Training Loss: 39699, Validation Loss: 47171, 176194.01985549627\n",
      "Epoch 16201, Training Loss: 42755, Validation Loss: 45943, 116915.75133751861\n",
      "Epoch 16301, Training Loss: 46232, Validation Loss: 49943, 324153.9922991363\n",
      "Epoch 16401, Training Loss: 42555, Validation Loss: 46755, 200315.83618406742\n",
      "Epoch 16501, Training Loss: 38559, Validation Loss: 50897, 200649.0730240523\n",
      "Epoch 16601, Training Loss: 37369, Validation Loss: 49241, 148655.98327971893\n",
      "Epoch 16701, Training Loss: 41462, Validation Loss: 51970, 235335.99322720713\n",
      "Epoch 16801, Training Loss: 43116, Validation Loss: 54185, 314638.93825374905\n",
      "Epoch 16901, Training Loss: 44178, Validation Loss: 48471, 258300.18250482107\n",
      "Epoch 17001, Training Loss: 45163, Validation Loss: 48101, 299256.7465447919\n",
      "Epoch 17101, Training Loss: 43797, Validation Loss: 48921, 258269.8245561817\n",
      "Epoch 17201, Training Loss: 40352, Validation Loss: 50930, 205357.76860590282\n",
      "Epoch 17301, Training Loss: 41064, Validation Loss: 47867, 164071.72592071712\n",
      "Epoch 17401, Training Loss: 42303, Validation Loss: 46506, 167922.81940791395\n",
      "Epoch 17501, Training Loss: 42449, Validation Loss: 48331, 146821.85434825506\n",
      "Epoch 17601, Training Loss: 38523, Validation Loss: 48461, 208148.60901572683\n",
      "Epoch 17701, Training Loss: 39937, Validation Loss: 47290, 191194.50015658818\n",
      "Epoch 17801, Training Loss: 39495, Validation Loss: 48365, 149937.39517248134\n",
      "Epoch 17901, Training Loss: 41935, Validation Loss: 48363, 193218.1476713824\n",
      "Epoch 18001, Training Loss: 39494, Validation Loss: 45927, 160700.30499103674\n",
      "Epoch 18101, Training Loss: 42619, Validation Loss: 49084, 206601.40755700684\n",
      "Epoch 18201, Training Loss: 37308, Validation Loss: 50197, 163589.25518006817\n",
      "Epoch 18301, Training Loss: 44227, Validation Loss: 45811, 208985.58782677076\n",
      "Epoch 18401, Training Loss: 41524, Validation Loss: 47715, 190526.28431296488\n",
      "Epoch 18501, Training Loss: 42214, Validation Loss: 45531, 193102.3514006962\n",
      "Epoch 18601, Training Loss: 42729, Validation Loss: 48665, 148480.92300807984\n",
      "Epoch 18701, Training Loss: 40217, Validation Loss: 48794, 167951.95945380055\n",
      "Epoch 18801, Training Loss: 41519, Validation Loss: 47250, 224920.8369550514\n",
      "Epoch 18901, Training Loss: 39025, Validation Loss: 48709, 204174.37012656522\n",
      "Epoch 19001, Training Loss: 39007, Validation Loss: 46531, 226907.62718626784\n",
      "Epoch 19101, Training Loss: 42436, Validation Loss: 46440, 264147.6454487491\n",
      "Epoch 19201, Training Loss: 40963, Validation Loss: 46782, 221989.35600021857\n",
      "Epoch 19301, Training Loss: 37301, Validation Loss: 46815, 173791.9670307548\n",
      "Epoch 19401, Training Loss: 41704, Validation Loss: 47287, 240324.21294711437\n",
      "Epoch 19501, Training Loss: 38857, Validation Loss: 51862, 209082.987612408\n",
      "Epoch 19601, Training Loss: 37701, Validation Loss: 47528, 123166.20591464976\n",
      "Epoch 19701, Training Loss: 40115, Validation Loss: 46481, 164618.03701198174\n",
      "Epoch 19801, Training Loss: 40457, Validation Loss: 45842, 124921.2659572338\n",
      "Epoch 19901, Training Loss: 38855, Validation Loss: 48172, 154236.14084947828\n",
      "Epoch 20001, Training Loss: 39780, Validation Loss: 47892, 163590.14537518218\n",
      "Epoch 20101, Training Loss: 40530, Validation Loss: 51956, 222432.6648497492\n",
      "Epoch 20201, Training Loss: 39389, Validation Loss: 47999, 148494.03436718544\n",
      "Epoch 20301, Training Loss: 41830, Validation Loss: 45918, 178848.55148642356\n",
      "Epoch 20401, Training Loss: 42033, Validation Loss: 46252, 266488.96086746984\n",
      "Epoch 20501, Training Loss: 40234, Validation Loss: 48879, 198043.33161227222\n",
      "Epoch 20601, Training Loss: 39029, Validation Loss: 46520, 195052.37079551982\n",
      "Epoch 20701, Training Loss: 37112, Validation Loss: 51869, 116188.77607733384\n",
      "Epoch 20801, Training Loss: 42354, Validation Loss: 49214, 330726.9483168898\n",
      "Epoch 20901, Training Loss: 42288, Validation Loss: 49661, 181333.25221437958\n",
      "Epoch 21001, Training Loss: 40826, Validation Loss: 47174, 240273.3649673419\n",
      "Epoch 21101, Training Loss: 39188, Validation Loss: 47723, 212946.95774307428\n",
      "Epoch 21201, Training Loss: 41667, Validation Loss: 48922, 187565.58623674457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21301, Training Loss: 38072, Validation Loss: 50765, 236742.52514017964\n",
      "Epoch 21401, Training Loss: 44128, Validation Loss: 45698, 337586.7657118381\n",
      "Epoch 21501, Training Loss: 38913, Validation Loss: 52187, 162826.76538601876\n",
      "Epoch 21601, Training Loss: 41163, Validation Loss: 50173, 131397.53038999834\n",
      "Epoch 21701, Training Loss: 40805, Validation Loss: 45701, 193662.8907253174\n",
      "Epoch 21801, Training Loss: 44256, Validation Loss: 47223, 215102.13083338\n",
      "Epoch 21901, Training Loss: 38012, Validation Loss: 49346, 181887.52012929335\n",
      "Epoch 22001, Training Loss: 41168, Validation Loss: 46303, 274937.65708569484\n",
      "Epoch 22101, Training Loss: 39667, Validation Loss: 47227, 218930.12284168912\n",
      "Epoch 22201, Training Loss: 41459, Validation Loss: 48583, 166341.32641260215\n",
      "Epoch 22301, Training Loss: 41701, Validation Loss: 48138, 197255.75315050455\n",
      "Epoch 22401, Training Loss: 39868, Validation Loss: 46547, 184864.52168508232\n",
      "Epoch 22501, Training Loss: 39416, Validation Loss: 54680, 175705.78305252388\n",
      "Epoch 22601, Training Loss: 41324, Validation Loss: 48269, 190205.16742424844\n",
      "Epoch 22701, Training Loss: 38749, Validation Loss: 49791, 175015.92118885947\n",
      "Epoch 22801, Training Loss: 40649, Validation Loss: 48083, 178219.12720130116\n",
      "Epoch 22901, Training Loss: 42538, Validation Loss: 51565, 214592.3200081036\n",
      "Epoch 23001, Training Loss: 40016, Validation Loss: 48745, 130053.58912688382\n",
      "Epoch 23101, Training Loss: 40723, Validation Loss: 45571, 294218.9252034332\n",
      "Epoch 23201, Training Loss: 44088, Validation Loss: 47535, 233968.11515553846\n",
      "Epoch 23301, Training Loss: 36651, Validation Loss: 47178, 207495.70524324547\n",
      "Epoch 23401, Training Loss: 39858, Validation Loss: 51360, 260237.58765470065\n",
      "Epoch 23501, Training Loss: 38764, Validation Loss: 56272, 167740.69899845077\n",
      "Epoch 23601, Training Loss: 39281, Validation Loss: 46331, 208651.05780273504\n",
      "Epoch 23701, Training Loss: 39044, Validation Loss: 47489, 278121.11202203506\n",
      "Epoch 23801, Training Loss: 39378, Validation Loss: 49171, 198138.4182245525\n",
      "Epoch 23901, Training Loss: 43812, Validation Loss: 50141, 294545.45685254486\n",
      "Epoch 24001, Training Loss: 39490, Validation Loss: 48613, 170080.60676107768\n",
      "Epoch 24101, Training Loss: 48538, Validation Loss: 46239, 355458.7951758122\n",
      "Epoch 24201, Training Loss: 39849, Validation Loss: 55042, 177629.24615052194\n",
      "Epoch 24301, Training Loss: 42849, Validation Loss: 54625, 194313.9110220743\n",
      "Epoch 24401, Training Loss: 39094, Validation Loss: 46171, 217688.76245508483\n",
      "Epoch 24501, Training Loss: 38111, Validation Loss: 47963, 146500.32957915228\n",
      "Epoch 24601, Training Loss: 40885, Validation Loss: 53986, 203348.88388457964\n",
      "Epoch 24701, Training Loss: 40592, Validation Loss: 47323, 255757.3891650137\n",
      "Epoch 24801, Training Loss: 39215, Validation Loss: 47953, 163099.87339437276\n",
      "Epoch 24901, Training Loss: 42021, Validation Loss: 48455, 163124.64157953914\n",
      "Epoch 25001, Training Loss: 38703, Validation Loss: 49845, 167421.24089506833\n",
      "Epoch 25101, Training Loss: 40950, Validation Loss: 50910, 143660.5984164327\n",
      "Epoch 25201, Training Loss: 39266, Validation Loss: 48318, 147232.8452306215\n",
      "Epoch 25301, Training Loss: 40351, Validation Loss: 46687, 173774.1986218462\n",
      "Epoch 25401, Training Loss: 38343, Validation Loss: 48443, 207289.3911378509\n",
      "Epoch 25501, Training Loss: 41999, Validation Loss: 46954, 147658.89596205344\n",
      "Epoch 25601, Training Loss: 38377, Validation Loss: 50108, 136559.1002519659\n",
      "Epoch 25701, Training Loss: 40534, Validation Loss: 51117, 153841.5724065799\n",
      "Epoch 25801, Training Loss: 38299, Validation Loss: 49536, 168844.28479026203\n",
      "Epoch 25901, Training Loss: 41196, Validation Loss: 51441, 229278.84673511542\n",
      "Epoch 26001, Training Loss: 39576, Validation Loss: 47150, 150822.89513806126\n",
      "Epoch 26101, Training Loss: 41251, Validation Loss: 46862, 158729.77024670233\n",
      "Epoch 26201, Training Loss: 38932, Validation Loss: 48319, 168608.4424769167\n",
      "Epoch 26301, Training Loss: 38862, Validation Loss: 49154, 209210.767124788\n",
      "Epoch 26401, Training Loss: 36336, Validation Loss: 49857, 143567.6814153331\n",
      "Epoch 26501, Training Loss: 37839, Validation Loss: 47693, 131712.0627191425\n",
      "Epoch 26601, Training Loss: 40589, Validation Loss: 46584, 240938.8976780607\n",
      "Epoch 26701, Training Loss: 41177, Validation Loss: 51383, 348184.3368955205\n",
      "Epoch 26801, Training Loss: 37671, Validation Loss: 48172, 155022.6320602957\n",
      "Epoch 26901, Training Loss: 39586, Validation Loss: 50781, 205374.43968647145\n",
      "Epoch 27001, Training Loss: 39991, Validation Loss: 47454, 215844.5889191633\n",
      "Epoch 27101, Training Loss: 42176, Validation Loss: 46211, 209991.82078288822\n",
      "Epoch 27201, Training Loss: 35807, Validation Loss: 46022, 147363.58396096728\n",
      "Epoch 27301, Training Loss: 40734, Validation Loss: 48253, 144261.99211853032\n",
      "Epoch 27401, Training Loss: 40655, Validation Loss: 46062, 186134.06024684967\n",
      "Epoch 27501, Training Loss: 39207, Validation Loss: 50362, 152391.54980757902\n",
      "Epoch 27601, Training Loss: 38180, Validation Loss: 49238, 132275.69657538997\n",
      "Epoch 27701, Training Loss: 37011, Validation Loss: 48903, 214788.26422976993\n",
      "Epoch 27801, Training Loss: 38841, Validation Loss: 53841, 234578.81709404723\n",
      "Epoch 27901, Training Loss: 37931, Validation Loss: 46669, 145493.50819676233\n",
      "Epoch 28001, Training Loss: 38878, Validation Loss: 49527, 116013.95490596746\n",
      "Epoch 28101, Training Loss: 40695, Validation Loss: 47181, 282958.4576543011\n",
      "Epoch 28201, Training Loss: 40878, Validation Loss: 54871, 243234.69134535114\n",
      "Epoch 28301, Training Loss: 40758, Validation Loss: 48317, 268306.54467094486\n",
      "Epoch 28401, Training Loss: 37386, Validation Loss: 48808, 129193.30562347989\n",
      "Epoch 28501, Training Loss: 40424, Validation Loss: 49121, 185040.07544387082\n",
      "Epoch 28601, Training Loss: 38204, Validation Loss: 48943, 119064.3450579871\n",
      "Epoch 28701, Training Loss: 41823, Validation Loss: 49685, 287023.29852624517\n",
      "Epoch 28801, Training Loss: 42395, Validation Loss: 47684, 250818.02418481652\n",
      "Epoch 28901, Training Loss: 38635, Validation Loss: 53203, 199675.16985237066\n",
      "Epoch 29001, Training Loss: 36275, Validation Loss: 48101, 162825.29886764285\n",
      "Epoch 29101, Training Loss: 38717, Validation Loss: 48840, 164186.05963664022\n",
      "Epoch 29201, Training Loss: 38951, Validation Loss: 52673, 216413.32930847452\n",
      "Epoch 29301, Training Loss: 35495, Validation Loss: 46990, 153106.85107730282\n",
      "Epoch 29401, Training Loss: 38835, Validation Loss: 52087, 217269.6135919791\n",
      "Epoch 29501, Training Loss: 36882, Validation Loss: 48710, 177684.66702795203\n",
      "Epoch 29601, Training Loss: 36168, Validation Loss: 51119, 260448.76973141465\n",
      "Epoch 29701, Training Loss: 38908, Validation Loss: 55320, 212518.40315669202\n",
      "Epoch 29801, Training Loss: 39416, Validation Loss: 46841, 201414.0828284061\n",
      "Epoch 29901, Training Loss: 39363, Validation Loss: 45942, 221975.90780902433\n",
      "Epoch 30001, Training Loss: 36262, Validation Loss: 50988, 206887.2872004306\n",
      "Epoch 30101, Training Loss: 41086, Validation Loss: 47732, 220300.350352351\n",
      "Epoch 30201, Training Loss: 37595, Validation Loss: 48065, 248434.27307529267\n",
      "Epoch 30301, Training Loss: 37198, Validation Loss: 52539, 215886.52432870027\n",
      "Epoch 30401, Training Loss: 38840, Validation Loss: 52350, 216431.33368346703\n",
      "Epoch 30501, Training Loss: 40297, Validation Loss: 47404, 197362.0013662109\n",
      "Epoch 30601, Training Loss: 39989, Validation Loss: 53155, 126893.05485421896\n",
      "Epoch 30701, Training Loss: 37752, Validation Loss: 49274, 202865.02654435637\n",
      "Epoch 30801, Training Loss: 33681, Validation Loss: 49270, 152335.96356294537\n",
      "Epoch 30901, Training Loss: 38987, Validation Loss: 48586, 168069.95416492436\n",
      "Epoch 31001, Training Loss: 39673, Validation Loss: 48751, 183377.80380953918\n",
      "Epoch 31101, Training Loss: 38125, Validation Loss: 52319, 207917.75352448082\n",
      "Epoch 31201, Training Loss: 40490, Validation Loss: 50914, 126547.83499369402\n",
      "Epoch 31301, Training Loss: 37267, Validation Loss: 48568, 165973.46602393812\n",
      "Epoch 31401, Training Loss: 35449, Validation Loss: 48191, 172539.80624751502\n",
      "Epoch 31501, Training Loss: 36642, Validation Loss: 47869, 185407.17317079226\n",
      "Epoch 31601, Training Loss: 39201, Validation Loss: 48529, 176635.0223899798\n",
      "Epoch 31701, Training Loss: 39727, Validation Loss: 49575, 178035.7106030876\n",
      "Epoch 31801, Training Loss: 38344, Validation Loss: 50443, 215888.6935795475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31901, Training Loss: 37996, Validation Loss: 50842, 136001.30707066096\n",
      "Epoch 32001, Training Loss: 38267, Validation Loss: 46028, 175571.7908906118\n",
      "Epoch 32101, Training Loss: 35331, Validation Loss: 52030, 191723.54687017435\n",
      "Epoch 32201, Training Loss: 41002, Validation Loss: 48387, 140838.3423424314\n",
      "Epoch 32301, Training Loss: 37431, Validation Loss: 54339, 180620.58309950915\n",
      "Epoch 32401, Training Loss: 37030, Validation Loss: 52989, 141925.94087740182\n",
      "Epoch 32501, Training Loss: 35496, Validation Loss: 49371, 156917.11798932895\n",
      "Epoch 32601, Training Loss: 37825, Validation Loss: 47117, 172161.94333311106\n",
      "Epoch 32701, Training Loss: 35752, Validation Loss: 48795, 167158.02133061553\n",
      "Epoch 32801, Training Loss: 40354, Validation Loss: 53930, 269934.3437529043\n",
      "Epoch 32901, Training Loss: 39135, Validation Loss: 49082, 172969.86931125444\n",
      "Epoch 33001, Training Loss: 37315, Validation Loss: 46689, 209306.78169425912\n",
      "Epoch 33101, Training Loss: 36135, Validation Loss: 48552, 157111.5613875022\n",
      "Epoch 33201, Training Loss: 38514, Validation Loss: 46861, 189028.78060656865\n",
      "Epoch 33301, Training Loss: 37529, Validation Loss: 50947, 262002.66569796143\n",
      "Epoch 33401, Training Loss: 38854, Validation Loss: 49358, 192122.19190758094\n",
      "Epoch 33501, Training Loss: 38009, Validation Loss: 48808, 150453.34545737586\n",
      "Epoch 33601, Training Loss: 36522, Validation Loss: 51316, 171176.19788761853\n",
      "Epoch 33701, Training Loss: 37754, Validation Loss: 51125, 161262.89667848742\n",
      "Epoch 33801, Training Loss: 38083, Validation Loss: 50603, 198628.03145399675\n",
      "Epoch 33901, Training Loss: 41119, Validation Loss: 49173, 299734.1403089244\n",
      "Epoch 34001, Training Loss: 34511, Validation Loss: 47586, 175147.40976798898\n",
      "Epoch 34101, Training Loss: 36123, Validation Loss: 49263, 147415.64601954885\n",
      "Epoch 34201, Training Loss: 34766, Validation Loss: 49609, 161297.57852635797\n",
      "Epoch 34301, Training Loss: 38220, Validation Loss: 52073, 136045.9859803905\n",
      "Epoch 34401, Training Loss: 39896, Validation Loss: 52636, 186656.50346067423\n",
      "Epoch 34501, Training Loss: 37857, Validation Loss: 51622, 156096.9415227273\n",
      "Epoch 34601, Training Loss: 38298, Validation Loss: 49350, 181003.61509914743\n",
      "Epoch 34701, Training Loss: 39105, Validation Loss: 50915, 171691.83290535436\n",
      "Epoch 34801, Training Loss: 38006, Validation Loss: 50818, 217125.69976965242\n",
      "Epoch 34901, Training Loss: 36958, Validation Loss: 49046, 159439.08609141072\n",
      "Epoch 35001, Training Loss: 36894, Validation Loss: 49981, 181982.49194093337\n",
      "Epoch 35101, Training Loss: 39312, Validation Loss: 50511, 233938.18268048126\n",
      "Epoch 35201, Training Loss: 35964, Validation Loss: 48392, 184479.16013664752\n",
      "Epoch 35301, Training Loss: 40810, Validation Loss: 48893, 246935.8126495201\n",
      "Epoch 35401, Training Loss: 39544, Validation Loss: 49884, 202813.6701235194\n",
      "Epoch 35501, Training Loss: 38632, Validation Loss: 47096, 209833.93973831646\n",
      "Epoch 35601, Training Loss: 36010, Validation Loss: 50886, 159224.04794404667\n",
      "Epoch 35701, Training Loss: 36566, Validation Loss: 49327, 172876.37842136587\n",
      "Epoch 35801, Training Loss: 35211, Validation Loss: 48853, 151794.84164528965\n",
      "Epoch 35901, Training Loss: 36941, Validation Loss: 48342, 135601.22729606528\n",
      "Epoch 36001, Training Loss: 39794, Validation Loss: 47267, 252836.17824945704\n",
      "Epoch 36101, Training Loss: 35995, Validation Loss: 48859, 153996.4849527313\n",
      "Epoch 36201, Training Loss: 38092, Validation Loss: 50239, 187080.00726317975\n",
      "Epoch 36301, Training Loss: 36822, Validation Loss: 52206, 138235.79637973034\n",
      "Epoch 36401, Training Loss: 36781, Validation Loss: 50173, 209656.4273714068\n",
      "Epoch 36501, Training Loss: 37129, Validation Loss: 46605, 225552.71485521132\n",
      "Epoch 36601, Training Loss: 37039, Validation Loss: 48168, 183057.88321846537\n",
      "Epoch 36701, Training Loss: 37965, Validation Loss: 49748, 161216.47046673615\n",
      "Epoch 36801, Training Loss: 38303, Validation Loss: 47598, 136009.3146348852\n",
      "Epoch 36901, Training Loss: 39453, Validation Loss: 54123, 148154.8447918658\n",
      "Epoch 37001, Training Loss: 37522, Validation Loss: 50063, 251676.58804190773\n",
      "Epoch 37101, Training Loss: 39976, Validation Loss: 52252, 216970.22265853322\n",
      "Epoch 37201, Training Loss: 40305, Validation Loss: 50341, 229331.41852618274\n",
      "Epoch 37301, Training Loss: 39885, Validation Loss: 51148, 236273.3916462503\n",
      "Epoch 37401, Training Loss: 40599, Validation Loss: 49067, 208868.8289435977\n",
      "Epoch 37501, Training Loss: 36780, Validation Loss: 51306, 205078.15930803915\n",
      "Epoch 37601, Training Loss: 37111, Validation Loss: 52035, 138510.9328529948\n",
      "Epoch 37701, Training Loss: 35471, Validation Loss: 46715, 235170.66338302125\n",
      "Epoch 37801, Training Loss: 39455, Validation Loss: 52711, 189801.4384332593\n",
      "Epoch 37901, Training Loss: 37128, Validation Loss: 49632, 167669.45367296552\n",
      "Epoch 38001, Training Loss: 40026, Validation Loss: 48348, 131881.7685627589\n",
      "Epoch 38101, Training Loss: 36772, Validation Loss: 52598, 171650.5868519936\n",
      "Epoch 38201, Training Loss: 38389, Validation Loss: 50287, 174489.26730926897\n",
      "Epoch 38301, Training Loss: 37161, Validation Loss: 48739, 227221.22389376457\n",
      "Epoch 38401, Training Loss: 37832, Validation Loss: 52596, 233321.0647936154\n",
      "Epoch 38501, Training Loss: 35146, Validation Loss: 52186, 152775.1604681102\n",
      "Epoch 38601, Training Loss: 35557, Validation Loss: 49019, 141143.90363573143\n",
      "Epoch 38701, Training Loss: 36225, Validation Loss: 48045, 171442.6204761925\n",
      "Epoch 38801, Training Loss: 35213, Validation Loss: 48384, 176409.7126189035\n",
      "Epoch 38901, Training Loss: 38769, Validation Loss: 49886, 270676.63822815084\n",
      "Epoch 39001, Training Loss: 34283, Validation Loss: 52196, 127403.44003695133\n",
      "Epoch 39101, Training Loss: 35754, Validation Loss: 49463, 238935.10707591\n",
      "Epoch 39201, Training Loss: 36277, Validation Loss: 50195, 170276.10907310253\n",
      "Epoch 39301, Training Loss: 39961, Validation Loss: 55537, 190475.09653574877\n",
      "Epoch 39401, Training Loss: 37977, Validation Loss: 50360, 146978.6354681339\n",
      "Epoch 39501, Training Loss: 37047, Validation Loss: 49857, 158248.82026807533\n",
      "Epoch 39601, Training Loss: 34190, Validation Loss: 51234, 131163.22929168495\n",
      "Epoch 39701, Training Loss: 35313, Validation Loss: 50553, 234907.5932894003\n",
      "Epoch 39801, Training Loss: 33453, Validation Loss: 53297, 132262.12045886964\n",
      "Epoch 39901, Training Loss: 35988, Validation Loss: 49513, 149194.9696612605\n",
      "Epoch 40001, Training Loss: 35649, Validation Loss: 49403, 135608.17783692057\n",
      "Epoch 40101, Training Loss: 34641, Validation Loss: 48656, 241004.67103477137\n",
      "Epoch 40201, Training Loss: 36237, Validation Loss: 53070, 178032.0752490953\n",
      "Epoch 40301, Training Loss: 35695, Validation Loss: 55811, 206187.28486295498\n",
      "Epoch 40401, Training Loss: 36336, Validation Loss: 49397, 140986.63164709706\n",
      "Epoch 40501, Training Loss: 35801, Validation Loss: 53239, 150389.83879829434\n",
      "Epoch 40601, Training Loss: 36359, Validation Loss: 48634, 286014.37703518156\n",
      "Epoch 40701, Training Loss: 35731, Validation Loss: 51724, 209252.62581561785\n",
      "Epoch 40801, Training Loss: 36748, Validation Loss: 51544, 141475.15654264373\n",
      "Epoch 40901, Training Loss: 35575, Validation Loss: 48428, 150372.24654985656\n",
      "Epoch 41001, Training Loss: 41008, Validation Loss: 50113, 252056.63692004312\n",
      "Epoch 41101, Training Loss: 38528, Validation Loss: 49990, 237751.5902887388\n",
      "Epoch 41201, Training Loss: 34644, Validation Loss: 52640, 292476.56404737505\n",
      "Epoch 41301, Training Loss: 35978, Validation Loss: 51381, 184629.16963405066\n",
      "Epoch 41401, Training Loss: 36436, Validation Loss: 49539, 188371.1542255055\n",
      "Epoch 41501, Training Loss: 35603, Validation Loss: 52959, 171995.53314589983\n",
      "Epoch 41601, Training Loss: 36494, Validation Loss: 51441, 207483.97329245295\n",
      "Epoch 41701, Training Loss: 34725, Validation Loss: 49856, 164888.49425291308\n",
      "Epoch 41801, Training Loss: 35040, Validation Loss: 51296, 139749.70191144236\n",
      "Epoch 41901, Training Loss: 35061, Validation Loss: 49079, 174211.1451377922\n",
      "Epoch 42001, Training Loss: 37022, Validation Loss: 51862, 197905.89410490158\n",
      "Epoch 42101, Training Loss: 34454, Validation Loss: 51431, 174935.11837088535\n",
      "Epoch 42201, Training Loss: 35406, Validation Loss: 49647, 180067.36133482688\n",
      "Epoch 42301, Training Loss: 39529, Validation Loss: 49089, 147525.03614953658\n",
      "Epoch 42401, Training Loss: 39222, Validation Loss: 54382, 303005.3158288052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42501, Training Loss: 35344, Validation Loss: 51845, 152291.18130822756\n",
      "Epoch 42601, Training Loss: 37185, Validation Loss: 48982, 147585.51003443406\n",
      "Epoch 42701, Training Loss: 33492, Validation Loss: 51857, 123745.90335855079\n",
      "Epoch 42801, Training Loss: 36723, Validation Loss: 51901, 299750.8322783569\n",
      "Epoch 42901, Training Loss: 34438, Validation Loss: 52593, 195017.18996144083\n",
      "Epoch 43001, Training Loss: 38405, Validation Loss: 50310, 228142.88384227152\n",
      "Epoch 43101, Training Loss: 35932, Validation Loss: 48442, 185435.34701315462\n",
      "Epoch 43201, Training Loss: 38353, Validation Loss: 50595, 153832.41193809197\n",
      "Epoch 43301, Training Loss: 34674, Validation Loss: 51966, 192461.50610459258\n",
      "Epoch 43401, Training Loss: 37825, Validation Loss: 54137, 273561.52215507394\n",
      "Epoch 43501, Training Loss: 35536, Validation Loss: 49962, 143659.73287037687\n",
      "Epoch 43601, Training Loss: 36006, Validation Loss: 49218, 168048.62993657982\n",
      "Epoch 43701, Training Loss: 37440, Validation Loss: 50281, 133886.66399983983\n",
      "Epoch 43801, Training Loss: 33287, Validation Loss: 52328, 158456.06974840333\n",
      "Epoch 43901, Training Loss: 36239, Validation Loss: 51755, 181798.70299085884\n",
      "Epoch 44001, Training Loss: 36372, Validation Loss: 50709, 130155.71409584706\n",
      "Epoch 44101, Training Loss: 38927, Validation Loss: 50716, 151335.08312091033\n",
      "Epoch 44201, Training Loss: 33569, Validation Loss: 47248, 197490.32980639406\n",
      "Epoch 44301, Training Loss: 38678, Validation Loss: 56001, 230110.28517034606\n",
      "Epoch 44401, Training Loss: 36965, Validation Loss: 52182, 152006.4472851979\n",
      "Epoch 44501, Training Loss: 37316, Validation Loss: 47420, 241199.17973386534\n",
      "Epoch 44601, Training Loss: 37736, Validation Loss: 53739, 180147.68554552912\n",
      "Epoch 44701, Training Loss: 34733, Validation Loss: 49914, 172613.40951322767\n",
      "Epoch 44801, Training Loss: 37117, Validation Loss: 48918, 184421.1557281159\n",
      "Epoch 44901, Training Loss: 36186, Validation Loss: 54069, 168789.8124524804\n",
      "Epoch 45001, Training Loss: 34600, Validation Loss: 51199, 159982.16106399414\n",
      "Epoch 45101, Training Loss: 35727, Validation Loss: 50156, 143414.26349354364\n",
      "Epoch 45201, Training Loss: 35507, Validation Loss: 47634, 271625.797541068\n",
      "Epoch 45301, Training Loss: 36187, Validation Loss: 54293, 169168.14817811147\n",
      "Epoch 45401, Training Loss: 36053, Validation Loss: 54158, 207966.13849853017\n",
      "Epoch 45501, Training Loss: 33963, Validation Loss: 53726, 246643.61281804167\n",
      "Epoch 45601, Training Loss: 34517, Validation Loss: 47784, 203958.8527313126\n",
      "Epoch 45701, Training Loss: 35507, Validation Loss: 50135, 196982.4983189177\n",
      "Epoch 45801, Training Loss: 34385, Validation Loss: 55587, 157449.63567395092\n",
      "Epoch 45901, Training Loss: 32563, Validation Loss: 55194, 138740.21074348417\n",
      "Epoch 46001, Training Loss: 33776, Validation Loss: 49697, 142833.89808248225\n",
      "Epoch 46101, Training Loss: 33011, Validation Loss: 51681, 189379.1680556337\n",
      "Epoch 46201, Training Loss: 34094, Validation Loss: 52373, 135083.66273452752\n",
      "Epoch 46301, Training Loss: 36889, Validation Loss: 49045, 155769.641072518\n",
      "Epoch 46401, Training Loss: 37077, Validation Loss: 52213, 231518.029241296\n",
      "Epoch 46501, Training Loss: 34891, Validation Loss: 46844, 152349.5426117727\n",
      "Epoch 46601, Training Loss: 38559, Validation Loss: 52198, 255432.6415693271\n",
      "Epoch 46701, Training Loss: 36118, Validation Loss: 52757, 144039.45762003658\n",
      "Epoch 46801, Training Loss: 33582, Validation Loss: 50412, 218846.36721569253\n",
      "Epoch 46901, Training Loss: 38339, Validation Loss: 53004, 225722.5936325366\n",
      "Epoch 47001, Training Loss: 35953, Validation Loss: 49318, 107766.51065585362\n",
      "Epoch 47101, Training Loss: 35261, Validation Loss: 50616, 157639.35026295835\n",
      "Epoch 47201, Training Loss: 36379, Validation Loss: 54861, 204034.82519655474\n",
      "Epoch 47301, Training Loss: 35837, Validation Loss: 54703, 174402.16356115518\n",
      "Epoch 47401, Training Loss: 34335, Validation Loss: 49891, 178656.09968109886\n",
      "Epoch 47501, Training Loss: 35331, Validation Loss: 51753, 126836.71875070829\n",
      "Epoch 47601, Training Loss: 34769, Validation Loss: 51590, 146264.3956842356\n",
      "Epoch 47701, Training Loss: 35556, Validation Loss: 53245, 174631.00677354718\n",
      "Epoch 47801, Training Loss: 33038, Validation Loss: 54964, 215016.95580780137\n",
      "Epoch 47901, Training Loss: 33219, Validation Loss: 49774, 153361.77416741833\n",
      "Epoch 48001, Training Loss: 34460, Validation Loss: 52570, 142918.33622095737\n",
      "Epoch 48101, Training Loss: 33934, Validation Loss: 52039, 193684.87266865486\n",
      "Epoch 48201, Training Loss: 33534, Validation Loss: 49245, 167654.41793593144\n",
      "Epoch 48301, Training Loss: 35047, Validation Loss: 52871, 153224.53572404574\n",
      "Epoch 48401, Training Loss: 33745, Validation Loss: 48786, 171733.65333654\n",
      "Epoch 48501, Training Loss: 35385, Validation Loss: 50062, 161421.8086492013\n",
      "Epoch 48601, Training Loss: 32303, Validation Loss: 52017, 199215.4700110217\n",
      "Epoch 48701, Training Loss: 37551, Validation Loss: 50783, 269421.63409523113\n",
      "Epoch 48801, Training Loss: 35383, Validation Loss: 48731, 151516.16402296655\n",
      "Epoch 48901, Training Loss: 35376, Validation Loss: 49419, 259295.64918520232\n",
      "Epoch 49001, Training Loss: 35307, Validation Loss: 51058, 148062.60339904352\n",
      "Epoch 49101, Training Loss: 35357, Validation Loss: 51382, 180901.0738667134\n",
      "Epoch 49201, Training Loss: 37448, Validation Loss: 50916, 236371.95235504094\n",
      "Epoch 49301, Training Loss: 35050, Validation Loss: 51046, 194609.6636022358\n",
      "Epoch 49401, Training Loss: 34534, Validation Loss: 48593, 158720.70992572364\n",
      "Epoch 49501, Training Loss: 35513, Validation Loss: 51845, 215813.6340995217\n",
      "Epoch 49601, Training Loss: 37317, Validation Loss: 54939, 208893.7890345028\n",
      "Epoch 49701, Training Loss: 35276, Validation Loss: 48189, 184186.9176944009\n",
      "Epoch 49801, Training Loss: 35677, Validation Loss: 49775, 199911.72855602822\n",
      "Epoch 49901, Training Loss: 36439, Validation Loss: 50397, 161374.87317614633\n",
      "Epoch 50001, Training Loss: 33963, Validation Loss: 52106, 236825.56219415923\n",
      "Epoch 50101, Training Loss: 33838, Validation Loss: 51374, 196867.03079254748\n",
      "Epoch 50201, Training Loss: 35231, Validation Loss: 48648, 180581.86636575905\n",
      "Epoch 50301, Training Loss: 36049, Validation Loss: 49487, 220411.5629768398\n",
      "Epoch 50401, Training Loss: 31703, Validation Loss: 50194, 112430.57166057969\n",
      "Epoch 50501, Training Loss: 32965, Validation Loss: 51694, 180730.01388061958\n",
      "Epoch 50601, Training Loss: 35550, Validation Loss: 49035, 182659.10231645024\n",
      "Epoch 50701, Training Loss: 34154, Validation Loss: 53341, 127602.60044251336\n",
      "Epoch 50801, Training Loss: 34409, Validation Loss: 52232, 174861.20855375365\n",
      "Epoch 50901, Training Loss: 35803, Validation Loss: 52014, 250087.6977541096\n",
      "Epoch 51001, Training Loss: 34244, Validation Loss: 51512, 191893.12889908362\n",
      "Epoch 51101, Training Loss: 35199, Validation Loss: 49374, 188535.87479612118\n",
      "Epoch 51201, Training Loss: 32552, Validation Loss: 53636, 185173.5200101341\n",
      "Epoch 51301, Training Loss: 35089, Validation Loss: 52587, 146118.35050760108\n",
      "Epoch 51401, Training Loss: 35453, Validation Loss: 52715, 175714.1807613623\n",
      "Epoch 51501, Training Loss: 34828, Validation Loss: 50147, 185888.09520706418\n",
      "Epoch 51601, Training Loss: 34180, Validation Loss: 49148, 178700.14280502312\n",
      "Epoch 51701, Training Loss: 38435, Validation Loss: 48507, 213470.24700698323\n",
      "Epoch 51801, Training Loss: 33745, Validation Loss: 51861, 147835.50758814104\n",
      "Epoch 51901, Training Loss: 33702, Validation Loss: 50898, 181537.34096721568\n",
      "Epoch 52001, Training Loss: 36104, Validation Loss: 51565, 293720.96667825925\n",
      "Epoch 52101, Training Loss: 33264, Validation Loss: 54242, 144779.21814368456\n",
      "Epoch 52201, Training Loss: 37931, Validation Loss: 53630, 210700.59250713428\n",
      "Epoch 52301, Training Loss: 34451, Validation Loss: 50602, 118584.20435077243\n",
      "Epoch 52401, Training Loss: 34493, Validation Loss: 54227, 225949.44271937897\n",
      "Epoch 52501, Training Loss: 33869, Validation Loss: 51471, 153563.3094625308\n",
      "Epoch 52601, Training Loss: 35354, Validation Loss: 51454, 214216.4829483709\n",
      "Epoch 52701, Training Loss: 33417, Validation Loss: 49829, 136383.34930153293\n",
      "Epoch 52801, Training Loss: 34568, Validation Loss: 50862, 185648.8955251781\n",
      "Epoch 52901, Training Loss: 35890, Validation Loss: 53853, 204685.8847646307\n",
      "Epoch 53001, Training Loss: 34582, Validation Loss: 53100, 190810.64110330865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53101, Training Loss: 34693, Validation Loss: 54544, 180291.73761322637\n",
      "Epoch 53201, Training Loss: 33210, Validation Loss: 53129, 151005.55670460852\n",
      "Epoch 53301, Training Loss: 34601, Validation Loss: 56229, 212016.26467567714\n",
      "Epoch 53401, Training Loss: 33314, Validation Loss: 52236, 142218.84453684112\n",
      "Epoch 53501, Training Loss: 34546, Validation Loss: 51135, 147146.84868977786\n",
      "Epoch 53601, Training Loss: 33796, Validation Loss: 49359, 156322.69198835726\n",
      "Epoch 53701, Training Loss: 32229, Validation Loss: 56247, 154724.8982347775\n",
      "Epoch 53801, Training Loss: 33800, Validation Loss: 51159, 181874.76792270213\n",
      "Epoch 53901, Training Loss: 35801, Validation Loss: 52773, 187956.77493680458\n",
      "Epoch 54001, Training Loss: 33859, Validation Loss: 52395, 155920.7846761345\n",
      "Epoch 54101, Training Loss: 35773, Validation Loss: 47821, 221002.83562588706\n",
      "Epoch 54201, Training Loss: 35528, Validation Loss: 49971, 175508.2414412462\n",
      "Epoch 54301, Training Loss: 36264, Validation Loss: 54182, 201023.2721587168\n",
      "Epoch 54401, Training Loss: 36158, Validation Loss: 51066, 201416.31631447864\n",
      "Epoch 54501, Training Loss: 31708, Validation Loss: 57393, 170097.91473709303\n",
      "Epoch 54601, Training Loss: 31929, Validation Loss: 57157, 181631.01490693932\n",
      "Epoch 54701, Training Loss: 33242, Validation Loss: 55449, 281037.0987873045\n",
      "Epoch 54801, Training Loss: 34473, Validation Loss: 54798, 219362.1964528268\n",
      "Epoch 54901, Training Loss: 31547, Validation Loss: 51489, 122450.15020599989\n",
      "Epoch 55001, Training Loss: 32332, Validation Loss: 58133, 145504.28677928678\n",
      "Epoch 55101, Training Loss: 35167, Validation Loss: 50567, 135877.662899409\n",
      "Epoch 55201, Training Loss: 36990, Validation Loss: 49311, 143325.50173672932\n",
      "Epoch 55301, Training Loss: 32043, Validation Loss: 53904, 151177.40539639685\n",
      "Epoch 55401, Training Loss: 35433, Validation Loss: 54619, 203582.60414751564\n",
      "Epoch 55501, Training Loss: 33508, Validation Loss: 57903, 167089.58434108153\n",
      "Epoch 55601, Training Loss: 33879, Validation Loss: 58519, 157808.37441661183\n",
      "Epoch 55701, Training Loss: 36723, Validation Loss: 51569, 223020.19447939727\n",
      "Epoch 55801, Training Loss: 33771, Validation Loss: 48731, 141254.622722727\n",
      "Epoch 55901, Training Loss: 34429, Validation Loss: 54300, 151723.99640970482\n",
      "Epoch 56001, Training Loss: 33809, Validation Loss: 52194, 173031.49210630503\n",
      "Epoch 56101, Training Loss: 34505, Validation Loss: 54112, 255484.6965758139\n",
      "Epoch 56201, Training Loss: 33276, Validation Loss: 50971, 171800.84389312196\n",
      "Epoch 56301, Training Loss: 35055, Validation Loss: 56131, 133680.86723507463\n",
      "Epoch 56401, Training Loss: 36102, Validation Loss: 49612, 227025.251733765\n",
      "Epoch 56501, Training Loss: 34766, Validation Loss: 50656, 200500.03799766154\n",
      "Epoch 56601, Training Loss: 35031, Validation Loss: 55033, 246754.3456950454\n",
      "Epoch 56701, Training Loss: 35317, Validation Loss: 56012, 126914.040310812\n",
      "Epoch 56801, Training Loss: 33468, Validation Loss: 53043, 140513.30807111584\n",
      "Epoch 56901, Training Loss: 34544, Validation Loss: 51913, 149185.48060059515\n",
      "Epoch 57001, Training Loss: 31347, Validation Loss: 53307, 151657.57020913644\n",
      "Epoch 57101, Training Loss: 32605, Validation Loss: 50026, 229156.11755700692\n",
      "Epoch 57201, Training Loss: 34897, Validation Loss: 55732, 214705.18938842075\n",
      "Epoch 57301, Training Loss: 34629, Validation Loss: 54342, 193093.1076061868\n",
      "Epoch 57401, Training Loss: 33765, Validation Loss: 49978, 196858.84700751735\n",
      "Epoch 57501, Training Loss: 34838, Validation Loss: 52424, 127643.68191489165\n",
      "Epoch 57601, Training Loss: 34247, Validation Loss: 56842, 192514.97818776\n",
      "Epoch 57701, Training Loss: 34068, Validation Loss: 52714, 160682.1690696272\n",
      "Epoch 57801, Training Loss: 33434, Validation Loss: 49876, 165856.4912708617\n",
      "Epoch 57901, Training Loss: 33871, Validation Loss: 54702, 200617.1517553587\n",
      "Epoch 58001, Training Loss: 33835, Validation Loss: 51835, 191946.26858499498\n",
      "Epoch 58101, Training Loss: 33983, Validation Loss: 51833, 202590.56215575393\n",
      "Epoch 58201, Training Loss: 30824, Validation Loss: 47971, 177115.1832237901\n",
      "Epoch 58301, Training Loss: 31872, Validation Loss: 56303, 131350.7275828892\n",
      "Epoch 58401, Training Loss: 33933, Validation Loss: 51239, 204938.14275629973\n",
      "Epoch 58501, Training Loss: 35484, Validation Loss: 53104, 206498.4617737762\n",
      "Epoch 58601, Training Loss: 35291, Validation Loss: 50601, 158578.81909622732\n",
      "Epoch 58701, Training Loss: 34032, Validation Loss: 53163, 169901.36034376008\n",
      "Epoch 58801, Training Loss: 33621, Validation Loss: 55350, 127264.17875022232\n",
      "Epoch 58901, Training Loss: 32104, Validation Loss: 51841, 175322.32067960952\n",
      "Epoch 59001, Training Loss: 32243, Validation Loss: 57596, 177835.27381794553\n",
      "Epoch 59101, Training Loss: 32697, Validation Loss: 52161, 146212.27317141727\n",
      "Epoch 59201, Training Loss: 34289, Validation Loss: 50259, 208843.03985785748\n",
      "Epoch 59301, Training Loss: 32960, Validation Loss: 55299, 143369.43619394643\n",
      "Epoch 59401, Training Loss: 34185, Validation Loss: 55030, 168519.5515993585\n",
      "Epoch 59501, Training Loss: 37062, Validation Loss: 53571, 199119.16805787248\n",
      "Epoch 59601, Training Loss: 30162, Validation Loss: 53208, 133031.48688067077\n",
      "Epoch 59701, Training Loss: 33987, Validation Loss: 52474, 163139.2617184741\n",
      "Epoch 59801, Training Loss: 38028, Validation Loss: 53790, 305939.36748893704\n",
      "Epoch 59901, Training Loss: 34604, Validation Loss: 53373, 183341.55340127516\n",
      "Epoch 60001, Training Loss: 33526, Validation Loss: 53370, 140166.1427111129\n",
      "Epoch 60101, Training Loss: 32800, Validation Loss: 52048, 171539.13258325218\n",
      "Epoch 60201, Training Loss: 36399, Validation Loss: 48481, 246736.3707561647\n",
      "Epoch 60301, Training Loss: 33637, Validation Loss: 51978, 146238.07865120968\n",
      "Epoch 60401, Training Loss: 33906, Validation Loss: 52423, 106455.98155898454\n",
      "Epoch 60501, Training Loss: 33992, Validation Loss: 53894, 203087.734295624\n",
      "Epoch 60601, Training Loss: 33835, Validation Loss: 54914, 228614.19274915918\n",
      "Epoch 60701, Training Loss: 32318, Validation Loss: 55146, 128265.12813907467\n",
      "Epoch 60801, Training Loss: 33094, Validation Loss: 55538, 200768.7297192274\n",
      "Epoch 60901, Training Loss: 33202, Validation Loss: 50784, 147850.9751341386\n",
      "Epoch 61001, Training Loss: 34180, Validation Loss: 54840, 162764.7589506501\n",
      "Epoch 61101, Training Loss: 32390, Validation Loss: 53398, 136722.57914336256\n",
      "Epoch 61201, Training Loss: 33813, Validation Loss: 52333, 220925.74046670957\n",
      "Epoch 61301, Training Loss: 33795, Validation Loss: 52287, 167420.8311453634\n",
      "Epoch 61401, Training Loss: 33582, Validation Loss: 52311, 159168.87227693896\n",
      "Epoch 61501, Training Loss: 34216, Validation Loss: 54867, 219142.35700831536\n",
      "Epoch 61601, Training Loss: 31611, Validation Loss: 52138, 135281.02012064122\n",
      "Epoch 61701, Training Loss: 33536, Validation Loss: 54454, 193723.32906677216\n",
      "Epoch 61801, Training Loss: 33360, Validation Loss: 53688, 169507.9195058728\n",
      "Epoch 61901, Training Loss: 33582, Validation Loss: 54580, 161381.41101795607\n",
      "Epoch 62001, Training Loss: 33042, Validation Loss: 54912, 210989.66860105924\n",
      "Epoch 62101, Training Loss: 33704, Validation Loss: 51009, 319828.5101310443\n",
      "Epoch 62201, Training Loss: 34158, Validation Loss: 54170, 198939.99023177856\n",
      "Epoch 62301, Training Loss: 33546, Validation Loss: 53855, 151894.32159491253\n",
      "Epoch 62401, Training Loss: 32567, Validation Loss: 54460, 196625.63018352925\n",
      "Epoch 62501, Training Loss: 34508, Validation Loss: 54105, 188967.25232081962\n",
      "Epoch 62601, Training Loss: 33902, Validation Loss: 51294, 154186.44626412552\n",
      "Epoch 62701, Training Loss: 32883, Validation Loss: 52677, 142805.59355123687\n",
      "Epoch 62801, Training Loss: 35158, Validation Loss: 54303, 189584.4630576494\n",
      "Epoch 62901, Training Loss: 34188, Validation Loss: 51638, 183673.69923364613\n",
      "Epoch 63001, Training Loss: 34777, Validation Loss: 52630, 231267.3378015155\n",
      "Epoch 63101, Training Loss: 34286, Validation Loss: 52763, 150726.57006570874\n",
      "Epoch 63201, Training Loss: 33914, Validation Loss: 54165, 163976.8420902574\n",
      "Epoch 63301, Training Loss: 33754, Validation Loss: 54790, 201669.23618579935\n",
      "Epoch 63401, Training Loss: 33214, Validation Loss: 51877, 147942.75348710545\n",
      "Epoch 63501, Training Loss: 34064, Validation Loss: 56748, 253456.60871916928\n",
      "Epoch 63601, Training Loss: 30137, Validation Loss: 53084, 121729.19063361484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63701, Training Loss: 32543, Validation Loss: 52031, 168133.39015485471\n",
      "Epoch 63801, Training Loss: 33885, Validation Loss: 55705, 179309.42573893777\n",
      "Epoch 63901, Training Loss: 34575, Validation Loss: 51559, 127110.74067591825\n",
      "Epoch 64001, Training Loss: 34754, Validation Loss: 53587, 173148.81783906082\n",
      "Epoch 64101, Training Loss: 31347, Validation Loss: 51362, 167928.97138655427\n",
      "Epoch 64201, Training Loss: 33399, Validation Loss: 49589, 188016.7471435246\n",
      "Epoch 64301, Training Loss: 32321, Validation Loss: 53298, 168316.18917058758\n",
      "Epoch 64401, Training Loss: 34218, Validation Loss: 51165, 162534.42990265178\n",
      "Epoch 64501, Training Loss: 32735, Validation Loss: 50715, 147547.26390520472\n",
      "Epoch 64601, Training Loss: 31951, Validation Loss: 50280, 165441.80061718458\n",
      "Epoch 64701, Training Loss: 35069, Validation Loss: 55520, 255289.10587893947\n",
      "Epoch 64801, Training Loss: 34556, Validation Loss: 53155, 202767.16612736788\n",
      "Epoch 64901, Training Loss: 31306, Validation Loss: 53855, 146416.23262710925\n",
      "Epoch 65001, Training Loss: 35579, Validation Loss: 54105, 251719.69915055195\n",
      "Epoch 65101, Training Loss: 32459, Validation Loss: 51709, 172048.267408137\n",
      "Epoch 65201, Training Loss: 33871, Validation Loss: 52384, 166776.36153704955\n",
      "Epoch 65301, Training Loss: 32938, Validation Loss: 53054, 148851.94120667735\n",
      "Epoch 65401, Training Loss: 31029, Validation Loss: 53133, 128031.83189651898\n",
      "Epoch 65501, Training Loss: 35124, Validation Loss: 51864, 131884.59806315706\n",
      "Epoch 65601, Training Loss: 31988, Validation Loss: 52286, 135460.03385110872\n",
      "Epoch 65701, Training Loss: 32466, Validation Loss: 51431, 147083.6305731781\n",
      "Epoch 65801, Training Loss: 31454, Validation Loss: 54702, 191751.88109316872\n",
      "Epoch 65901, Training Loss: 34778, Validation Loss: 54862, 221054.08600606953\n",
      "Epoch 66001, Training Loss: 31819, Validation Loss: 56080, 139804.45011004736\n",
      "Epoch 66101, Training Loss: 30591, Validation Loss: 55404, 135741.72791026824\n",
      "Epoch 66201, Training Loss: 33797, Validation Loss: 52392, 208769.8954938917\n",
      "Epoch 66301, Training Loss: 32253, Validation Loss: 52554, 193804.9180324047\n",
      "Epoch 66401, Training Loss: 32368, Validation Loss: 52194, 180973.00802538614\n",
      "Epoch 66501, Training Loss: 33946, Validation Loss: 54171, 190934.27866179298\n",
      "Epoch 66601, Training Loss: 32405, Validation Loss: 58119, 129278.49491281876\n",
      "Epoch 66701, Training Loss: 31654, Validation Loss: 56323, 138068.35460322743\n",
      "Epoch 66801, Training Loss: 34694, Validation Loss: 50472, 135041.18742236728\n",
      "Epoch 66901, Training Loss: 33036, Validation Loss: 54021, 155744.03529641143\n",
      "Epoch 67001, Training Loss: 32629, Validation Loss: 53591, 147780.22775222582\n",
      "Epoch 67101, Training Loss: 31402, Validation Loss: 52072, 164890.93225357847\n",
      "Epoch 67201, Training Loss: 32013, Validation Loss: 50323, 158983.73763138408\n",
      "Epoch 67301, Training Loss: 33915, Validation Loss: 51340, 183117.85326300547\n",
      "Epoch 67401, Training Loss: 33221, Validation Loss: 53804, 97872.07327617118\n",
      "Epoch 67501, Training Loss: 33282, Validation Loss: 50796, 196095.54482981225\n",
      "Epoch 67601, Training Loss: 29659, Validation Loss: 50990, 165787.54477272267\n",
      "Epoch 67701, Training Loss: 33355, Validation Loss: 54046, 215315.92561946771\n",
      "Epoch 67801, Training Loss: 35311, Validation Loss: 51996, 153056.77066879123\n",
      "Epoch 67901, Training Loss: 35036, Validation Loss: 54517, 195282.63448965558\n",
      "Epoch 68001, Training Loss: 33592, Validation Loss: 54045, 172692.97341736808\n",
      "Epoch 68101, Training Loss: 33167, Validation Loss: 53474, 142024.3764111163\n",
      "Epoch 68201, Training Loss: 32762, Validation Loss: 56868, 200779.04309322537\n",
      "Epoch 68301, Training Loss: 32171, Validation Loss: 50143, 123030.64157381542\n",
      "Epoch 68401, Training Loss: 35938, Validation Loss: 55664, 201582.30952838433\n",
      "Epoch 68501, Training Loss: 32351, Validation Loss: 50332, 182295.29632789755\n",
      "Epoch 68601, Training Loss: 31035, Validation Loss: 51869, 193308.71727036065\n",
      "Epoch 68701, Training Loss: 31015, Validation Loss: 52955, 145202.73058800548\n",
      "Epoch 68801, Training Loss: 33129, Validation Loss: 53136, 230006.75575496932\n",
      "Epoch 68901, Training Loss: 32288, Validation Loss: 54166, 187427.84216678562\n",
      "Epoch 69001, Training Loss: 32606, Validation Loss: 53372, 130725.28325896629\n",
      "Epoch 69101, Training Loss: 31661, Validation Loss: 56492, 164738.75738945566\n",
      "Epoch 69201, Training Loss: 31483, Validation Loss: 54767, 153761.33467331392\n",
      "Epoch 69301, Training Loss: 33630, Validation Loss: 50633, 147931.97264878257\n",
      "Epoch 69401, Training Loss: 29964, Validation Loss: 53647, 187318.325598223\n",
      "Epoch 69501, Training Loss: 34272, Validation Loss: 54866, 186158.11180277765\n",
      "Epoch 69601, Training Loss: 32369, Validation Loss: 54044, 156656.72529998547\n",
      "Epoch 69701, Training Loss: 32805, Validation Loss: 52359, 198783.94544829638\n",
      "Epoch 69801, Training Loss: 30761, Validation Loss: 53393, 170181.5965110353\n",
      "Epoch 69901, Training Loss: 35015, Validation Loss: 53702, 147087.56113117488\n",
      "Epoch 70001, Training Loss: 33106, Validation Loss: 49327, 184402.15722651532\n",
      "Epoch 70101, Training Loss: 34390, Validation Loss: 51124, 191280.82554182512\n",
      "Epoch 70201, Training Loss: 32237, Validation Loss: 51903, 145903.06130108566\n",
      "Epoch 70301, Training Loss: 29767, Validation Loss: 53459, 116518.78598997048\n",
      "Epoch 70401, Training Loss: 32206, Validation Loss: 52719, 131516.50621550973\n",
      "Epoch 70501, Training Loss: 32305, Validation Loss: 50486, 171492.60258243888\n",
      "Epoch 70601, Training Loss: 33028, Validation Loss: 54178, 167959.96817672634\n",
      "Epoch 70701, Training Loss: 30930, Validation Loss: 50637, 199673.67678219898\n",
      "Epoch 70801, Training Loss: 30861, Validation Loss: 50560, 159209.70301732\n",
      "Epoch 70901, Training Loss: 32167, Validation Loss: 50871, 167437.0767847344\n",
      "Epoch 71001, Training Loss: 30497, Validation Loss: 53522, 157533.46500424878\n",
      "Epoch 71101, Training Loss: 33844, Validation Loss: 53895, 179294.7334799987\n",
      "Epoch 71201, Training Loss: 31841, Validation Loss: 53225, 113629.42267290183\n",
      "Epoch 71301, Training Loss: 32854, Validation Loss: 50561, 163352.49117911237\n",
      "Epoch 71401, Training Loss: 32537, Validation Loss: 53574, 144927.09892483897\n",
      "Epoch 71501, Training Loss: 33615, Validation Loss: 53802, 179523.00961302445\n",
      "Epoch 71601, Training Loss: 34299, Validation Loss: 53254, 276693.8394350642\n",
      "Epoch 71701, Training Loss: 32575, Validation Loss: 53899, 154772.10759234495\n",
      "Epoch 71801, Training Loss: 32153, Validation Loss: 51544, 167921.3865509143\n",
      "Epoch 71901, Training Loss: 32668, Validation Loss: 51804, 134514.33994600372\n",
      "Epoch 72001, Training Loss: 35525, Validation Loss: 57042, 178040.64792333005\n",
      "Epoch 72101, Training Loss: 35013, Validation Loss: 53292, 156543.72464544707\n",
      "Epoch 72201, Training Loss: 31154, Validation Loss: 55289, 151276.45814655733\n",
      "Epoch 72301, Training Loss: 31423, Validation Loss: 54697, 178030.28832087017\n",
      "Epoch 72401, Training Loss: 30726, Validation Loss: 57684, 181054.15592905515\n",
      "Epoch 72501, Training Loss: 34849, Validation Loss: 51961, 277722.25795564754\n",
      "Epoch 72601, Training Loss: 31437, Validation Loss: 55407, 208500.96886974154\n",
      "Epoch 72701, Training Loss: 32524, Validation Loss: 53676, 148183.26625466332\n",
      "Epoch 72801, Training Loss: 32331, Validation Loss: 53707, 138520.81411635494\n",
      "Epoch 72901, Training Loss: 32574, Validation Loss: 54648, 170633.58316753455\n",
      "Epoch 73001, Training Loss: 33758, Validation Loss: 54883, 195460.50133194355\n",
      "Epoch 73101, Training Loss: 32197, Validation Loss: 53789, 182014.5626571848\n",
      "Epoch 73201, Training Loss: 32877, Validation Loss: 53989, 168581.79413387924\n",
      "Epoch 73301, Training Loss: 32444, Validation Loss: 51216, 227664.60488788443\n",
      "Epoch 73401, Training Loss: 31562, Validation Loss: 55941, 149918.93839204547\n",
      "Epoch 73501, Training Loss: 31543, Validation Loss: 51888, 143038.471989764\n",
      "Epoch 73601, Training Loss: 34780, Validation Loss: 50887, 170653.04582563307\n",
      "Epoch 73701, Training Loss: 32190, Validation Loss: 53904, 184072.94636346586\n",
      "Epoch 73801, Training Loss: 32261, Validation Loss: 55396, 123538.66817323638\n",
      "Epoch 73901, Training Loss: 31243, Validation Loss: 54282, 195285.77941172034\n",
      "Epoch 74001, Training Loss: 32444, Validation Loss: 54396, 164009.5681653809\n",
      "Epoch 74101, Training Loss: 30565, Validation Loss: 54097, 159845.9694674867\n",
      "Epoch 74201, Training Loss: 33288, Validation Loss: 54090, 170169.9735880806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74301, Training Loss: 29822, Validation Loss: 55995, 242218.24881535638\n",
      "Epoch 74401, Training Loss: 31917, Validation Loss: 56615, 160740.85548081025\n",
      "Epoch 74501, Training Loss: 32227, Validation Loss: 52270, 150372.88137667548\n",
      "Epoch 74601, Training Loss: 32176, Validation Loss: 51763, 123767.37895235517\n",
      "Epoch 74701, Training Loss: 30079, Validation Loss: 55644, 124461.75858944852\n",
      "Epoch 74801, Training Loss: 32979, Validation Loss: 51641, 193567.2319815505\n",
      "Epoch 74901, Training Loss: 30171, Validation Loss: 52998, 135364.33186624598\n",
      "Epoch 75001, Training Loss: 31241, Validation Loss: 53695, 122741.82086875923\n",
      "Epoch 75101, Training Loss: 31340, Validation Loss: 53612, 128766.49135170633\n",
      "Epoch 75201, Training Loss: 35646, Validation Loss: 55668, 209947.91605213983\n",
      "Epoch 75301, Training Loss: 32377, Validation Loss: 52738, 169303.73859193132\n",
      "Epoch 75401, Training Loss: 30615, Validation Loss: 54941, 150163.53166451945\n",
      "Epoch 75501, Training Loss: 33896, Validation Loss: 51454, 135466.8275354907\n",
      "Epoch 75601, Training Loss: 31101, Validation Loss: 54701, 159858.96012586984\n",
      "Epoch 75701, Training Loss: 31658, Validation Loss: 52559, 156008.41041491626\n",
      "Epoch 75801, Training Loss: 34159, Validation Loss: 52602, 255436.44440469137\n",
      "Epoch 75901, Training Loss: 30113, Validation Loss: 57012, 144383.6506737259\n",
      "Epoch 76001, Training Loss: 31245, Validation Loss: 53756, 178310.36946741026\n",
      "Epoch 76101, Training Loss: 33695, Validation Loss: 52383, 178522.90284881447\n",
      "Epoch 76201, Training Loss: 31976, Validation Loss: 54172, 203787.0812216039\n",
      "Epoch 76301, Training Loss: 31848, Validation Loss: 52754, 129805.30797579621\n",
      "Epoch 76401, Training Loss: 35808, Validation Loss: 50096, 241758.65836075833\n",
      "Epoch 76501, Training Loss: 33854, Validation Loss: 53678, 159133.40259267137\n",
      "Epoch 76601, Training Loss: 33920, Validation Loss: 50817, 158573.96317966675\n",
      "Epoch 76701, Training Loss: 31620, Validation Loss: 54786, 155692.4074383865\n",
      "Epoch 76801, Training Loss: 31211, Validation Loss: 55240, 172476.56433763565\n",
      "Epoch 76901, Training Loss: 33293, Validation Loss: 54010, 154681.7695281429\n",
      "Epoch 77001, Training Loss: 32346, Validation Loss: 51952, 181123.1235902382\n",
      "Epoch 77101, Training Loss: 31797, Validation Loss: 52688, 187714.25638700192\n",
      "Epoch 77201, Training Loss: 30524, Validation Loss: 54935, 189849.42824586702\n",
      "Epoch 77301, Training Loss: 34119, Validation Loss: 52713, 161841.84800379007\n",
      "Epoch 77401, Training Loss: 32124, Validation Loss: 55170, 140777.41947302746\n",
      "Epoch 77501, Training Loss: 31480, Validation Loss: 55172, 199080.00964796846\n",
      "Epoch 77601, Training Loss: 33532, Validation Loss: 56302, 196105.23513057563\n",
      "Epoch 77701, Training Loss: 31664, Validation Loss: 54776, 176359.19499249235\n",
      "Epoch 77801, Training Loss: 34164, Validation Loss: 51104, 148638.186064465\n",
      "Epoch 77901, Training Loss: 33781, Validation Loss: 54200, 168032.1522518736\n",
      "Epoch 78001, Training Loss: 30243, Validation Loss: 52725, 155900.81537474119\n",
      "Epoch 78101, Training Loss: 33218, Validation Loss: 55659, 165309.66640681445\n",
      "Epoch 78201, Training Loss: 29484, Validation Loss: 58537, 147132.49356868424\n",
      "Epoch 78301, Training Loss: 31372, Validation Loss: 56126, 203796.25796444467\n",
      "Epoch 78401, Training Loss: 32044, Validation Loss: 56304, 216677.3541033001\n",
      "Epoch 78501, Training Loss: 33838, Validation Loss: 50140, 201072.11707398962\n",
      "Epoch 78601, Training Loss: 32741, Validation Loss: 51389, 122261.11314955684\n",
      "Epoch 78701, Training Loss: 33698, Validation Loss: 54070, 192042.97616922823\n",
      "Epoch 78801, Training Loss: 29055, Validation Loss: 54022, 103982.70785514753\n",
      "Epoch 78901, Training Loss: 30432, Validation Loss: 56715, 168939.28895980841\n",
      "Epoch 79001, Training Loss: 31229, Validation Loss: 53721, 163368.19515538405\n",
      "Epoch 79101, Training Loss: 31953, Validation Loss: 54699, 187930.77167224363\n",
      "Epoch 79201, Training Loss: 31732, Validation Loss: 55029, 172679.29933006383\n",
      "Epoch 79301, Training Loss: 33997, Validation Loss: 50432, 214851.6404503469\n",
      "Epoch 79401, Training Loss: 30521, Validation Loss: 55549, 170986.52174744965\n",
      "Epoch 79501, Training Loss: 31776, Validation Loss: 55531, 199624.62062694493\n",
      "Epoch 79601, Training Loss: 29376, Validation Loss: 55534, 142987.4995426934\n",
      "Epoch 79701, Training Loss: 31452, Validation Loss: 57423, 197947.21170413692\n",
      "Epoch 79801, Training Loss: 33402, Validation Loss: 51819, 164561.37986143286\n",
      "Epoch 79901, Training Loss: 32573, Validation Loss: 52132, 197613.3922718568\n",
      "Epoch 80001, Training Loss: 30891, Validation Loss: 54021, 187109.92306396426\n",
      "Epoch 80101, Training Loss: 31391, Validation Loss: 53839, 135211.10106377667\n",
      "Epoch 80201, Training Loss: 32871, Validation Loss: 54230, 167365.85026691982\n",
      "Epoch 80301, Training Loss: 30609, Validation Loss: 54114, 201968.29599853858\n",
      "Epoch 80401, Training Loss: 31352, Validation Loss: 54943, 154879.6148459115\n",
      "Epoch 80501, Training Loss: 30667, Validation Loss: 51052, 195118.23165163654\n",
      "Epoch 80601, Training Loss: 30281, Validation Loss: 55114, 132582.53071072578\n",
      "Epoch 80701, Training Loss: 31325, Validation Loss: 53114, 156280.9266551791\n",
      "Epoch 80801, Training Loss: 34733, Validation Loss: 52699, 286658.45023292577\n",
      "Epoch 80901, Training Loss: 29233, Validation Loss: 53981, 157577.48590871334\n",
      "Epoch 81001, Training Loss: 33535, Validation Loss: 55870, 157143.34292708247\n",
      "Epoch 81101, Training Loss: 31997, Validation Loss: 52500, 173722.72013853097\n",
      "Epoch 81201, Training Loss: 31790, Validation Loss: 50115, 178132.01536118635\n",
      "Epoch 81301, Training Loss: 32720, Validation Loss: 51164, 193414.77763254396\n",
      "Epoch 81401, Training Loss: 30659, Validation Loss: 52864, 165382.79420490292\n",
      "Epoch 81501, Training Loss: 30118, Validation Loss: 52632, 182606.37582286075\n",
      "Epoch 81601, Training Loss: 33264, Validation Loss: 52943, 193435.97285186526\n",
      "Epoch 81701, Training Loss: 31266, Validation Loss: 53987, 148399.03654465225\n",
      "Epoch 81801, Training Loss: 33231, Validation Loss: 54158, 175358.17118655713\n",
      "Epoch 81901, Training Loss: 31696, Validation Loss: 53437, 133106.56644609288\n",
      "Epoch 82001, Training Loss: 32081, Validation Loss: 54497, 165966.62338575433\n",
      "Epoch 82101, Training Loss: 30858, Validation Loss: 53879, 209363.28731113797\n",
      "Epoch 82201, Training Loss: 32111, Validation Loss: 52545, 150187.63277030826\n",
      "Epoch 82301, Training Loss: 33297, Validation Loss: 50757, 161822.64689150415\n",
      "Epoch 82401, Training Loss: 33055, Validation Loss: 52645, 187499.78516695424\n",
      "Epoch 82501, Training Loss: 30817, Validation Loss: 55629, 186022.40480316905\n",
      "Epoch 82601, Training Loss: 31264, Validation Loss: 55937, 225779.64651850017\n",
      "Epoch 82701, Training Loss: 30909, Validation Loss: 54536, 176740.4144065557\n",
      "Epoch 82801, Training Loss: 35098, Validation Loss: 55776, 202178.77052683977\n",
      "Epoch 82901, Training Loss: 30888, Validation Loss: 53073, 134283.13300079305\n",
      "Epoch 83001, Training Loss: 32242, Validation Loss: 54064, 162747.98100430888\n",
      "Epoch 83101, Training Loss: 33314, Validation Loss: 54305, 166788.02909532693\n",
      "Epoch 83201, Training Loss: 31532, Validation Loss: 50654, 167521.29392671157\n",
      "Epoch 83301, Training Loss: 33422, Validation Loss: 53050, 147356.92530267962\n",
      "Epoch 83401, Training Loss: 30793, Validation Loss: 52380, 155303.76687619797\n",
      "Epoch 83501, Training Loss: 35668, Validation Loss: 53915, 226303.02987678582\n",
      "Epoch 83601, Training Loss: 28640, Validation Loss: 55632, 111571.7898781112\n",
      "Epoch 83701, Training Loss: 31756, Validation Loss: 52640, 141938.5190708926\n",
      "Epoch 83801, Training Loss: 30947, Validation Loss: 53497, 202836.34584488146\n",
      "Epoch 83901, Training Loss: 31586, Validation Loss: 58539, 132628.0682033979\n",
      "Epoch 84001, Training Loss: 32230, Validation Loss: 49886, 129511.68737232422\n",
      "Epoch 84101, Training Loss: 32254, Validation Loss: 53518, 133222.8584439952\n",
      "Epoch 84201, Training Loss: 31923, Validation Loss: 55181, 163218.8476955805\n",
      "Epoch 84301, Training Loss: 31976, Validation Loss: 55138, 157557.72084189072\n",
      "Epoch 84401, Training Loss: 30130, Validation Loss: 52195, 176870.5747119504\n",
      "Epoch 84501, Training Loss: 35525, Validation Loss: 53913, 122386.55764899403\n",
      "Epoch 84601, Training Loss: 31680, Validation Loss: 52859, 123579.45976091344\n",
      "Epoch 84701, Training Loss: 30962, Validation Loss: 52170, 164932.09990357593\n",
      "Epoch 84801, Training Loss: 33754, Validation Loss: 51957, 184118.2637896886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84901, Training Loss: 31903, Validation Loss: 53062, 210075.51820409615\n",
      "Epoch 85001, Training Loss: 29693, Validation Loss: 54748, 130894.69661750749\n",
      "Epoch 85101, Training Loss: 33241, Validation Loss: 57684, 160151.75511263902\n",
      "Epoch 85201, Training Loss: 31335, Validation Loss: 53168, 134307.50306098102\n",
      "Epoch 85301, Training Loss: 32140, Validation Loss: 53518, 195870.5886389745\n",
      "Epoch 85401, Training Loss: 30813, Validation Loss: 54013, 163069.00586405746\n",
      "Epoch 85501, Training Loss: 31080, Validation Loss: 51522, 103386.73332268454\n",
      "Epoch 85601, Training Loss: 33097, Validation Loss: 54570, 151492.06909740844\n",
      "Epoch 85701, Training Loss: 29387, Validation Loss: 53761, 164245.073663038\n",
      "Epoch 85801, Training Loss: 33641, Validation Loss: 53568, 167158.61699603393\n",
      "Epoch 85901, Training Loss: 33692, Validation Loss: 51632, 225937.0311572854\n",
      "Epoch 86001, Training Loss: 29791, Validation Loss: 53850, 189886.56667690605\n",
      "Epoch 86101, Training Loss: 30829, Validation Loss: 51309, 163801.7530935546\n",
      "Epoch 86201, Training Loss: 31747, Validation Loss: 51059, 133677.83029409335\n",
      "Epoch 86301, Training Loss: 30870, Validation Loss: 52982, 133170.99856292788\n",
      "Epoch 86401, Training Loss: 32617, Validation Loss: 51941, 159501.41929194558\n",
      "Epoch 86501, Training Loss: 33625, Validation Loss: 53531, 150053.1351496382\n",
      "Epoch 86601, Training Loss: 30641, Validation Loss: 51548, 117488.04831092576\n",
      "Epoch 86701, Training Loss: 28702, Validation Loss: 52194, 167076.16593288953\n",
      "Epoch 86801, Training Loss: 34328, Validation Loss: 53279, 167730.74376012327\n",
      "Epoch 86901, Training Loss: 35283, Validation Loss: 50412, 231969.67289022318\n",
      "Epoch 87001, Training Loss: 31732, Validation Loss: 53390, 189497.22418950475\n",
      "Epoch 87101, Training Loss: 31303, Validation Loss: 54167, 131299.61610043977\n",
      "Epoch 87201, Training Loss: 32651, Validation Loss: 57529, 202103.73342113136\n",
      "Epoch 87301, Training Loss: 30817, Validation Loss: 52292, 134120.78229010498\n",
      "Epoch 87401, Training Loss: 34303, Validation Loss: 53318, 145205.32015677434\n",
      "Epoch 87501, Training Loss: 30004, Validation Loss: 51785, 129754.39444463991\n",
      "Epoch 87601, Training Loss: 33721, Validation Loss: 51546, 142078.52202614295\n",
      "Epoch 87701, Training Loss: 32325, Validation Loss: 53692, 153939.0285521202\n",
      "Epoch 87801, Training Loss: 30829, Validation Loss: 51807, 138618.92027533907\n",
      "Epoch 87901, Training Loss: 33103, Validation Loss: 59348, 169878.93429944594\n",
      "Epoch 88001, Training Loss: 30204, Validation Loss: 54196, 171087.45698621185\n",
      "Epoch 88101, Training Loss: 33350, Validation Loss: 56320, 212513.1736278399\n",
      "Epoch 88201, Training Loss: 32663, Validation Loss: 51221, 173436.94261874352\n",
      "Epoch 88301, Training Loss: 30933, Validation Loss: 52194, 120211.86599927144\n",
      "Epoch 88401, Training Loss: 35334, Validation Loss: 54947, 149291.97493837727\n",
      "Epoch 88501, Training Loss: 33332, Validation Loss: 56180, 156227.33734542725\n",
      "Epoch 88601, Training Loss: 31079, Validation Loss: 51921, 142169.46242423105\n",
      "Epoch 88701, Training Loss: 28928, Validation Loss: 50350, 127380.09282738798\n",
      "Epoch 88801, Training Loss: 30343, Validation Loss: 51940, 134380.5650907912\n",
      "Epoch 88901, Training Loss: 33306, Validation Loss: 54038, 145349.41910907687\n",
      "Epoch 89001, Training Loss: 33449, Validation Loss: 53721, 157335.2367311214\n",
      "Epoch 89101, Training Loss: 32571, Validation Loss: 55484, 121743.18832777643\n",
      "Epoch 89201, Training Loss: 31800, Validation Loss: 51312, 201025.28130150345\n",
      "Epoch 89301, Training Loss: 30801, Validation Loss: 53049, 130483.97243525386\n",
      "Epoch 89401, Training Loss: 30553, Validation Loss: 51979, 170412.4627647793\n",
      "Epoch 89501, Training Loss: 33538, Validation Loss: 51306, 201183.81556097916\n",
      "Epoch 89601, Training Loss: 29281, Validation Loss: 54110, 153560.97795649245\n",
      "Epoch 89701, Training Loss: 33994, Validation Loss: 53618, 230922.89818100657\n",
      "Epoch 89801, Training Loss: 30597, Validation Loss: 55910, 152631.8923250687\n",
      "Epoch 89901, Training Loss: 31329, Validation Loss: 53545, 158578.5211615278\n",
      "Epoch 90001, Training Loss: 31119, Validation Loss: 51449, 180361.14513413803\n",
      "Epoch 90101, Training Loss: 33992, Validation Loss: 50287, 228358.60109436815\n",
      "Epoch 90201, Training Loss: 31697, Validation Loss: 53241, 136977.49367024956\n",
      "Epoch 90301, Training Loss: 32820, Validation Loss: 50728, 134610.21637402262\n",
      "Epoch 90401, Training Loss: 31379, Validation Loss: 56420, 135601.93065049042\n",
      "Epoch 90501, Training Loss: 30614, Validation Loss: 52236, 142366.46395816212\n",
      "Epoch 90601, Training Loss: 34189, Validation Loss: 53896, 178554.17392964274\n",
      "Epoch 90701, Training Loss: 31870, Validation Loss: 52445, 128986.65346548939\n",
      "Epoch 90801, Training Loss: 30354, Validation Loss: 54052, 168952.01758380063\n",
      "Epoch 90901, Training Loss: 31658, Validation Loss: 49274, 159954.47207767397\n",
      "Epoch 91001, Training Loss: 30182, Validation Loss: 53852, 121142.44991450726\n",
      "Epoch 91101, Training Loss: 30777, Validation Loss: 52470, 141509.4922422937\n",
      "Epoch 91201, Training Loss: 31025, Validation Loss: 55844, 163534.29302854408\n",
      "Epoch 91301, Training Loss: 31898, Validation Loss: 53650, 127756.69291844756\n",
      "Epoch 91401, Training Loss: 31732, Validation Loss: 51771, 157098.52381630696\n",
      "Epoch 91501, Training Loss: 33202, Validation Loss: 51908, 185918.77735528428\n",
      "Epoch 91601, Training Loss: 31857, Validation Loss: 52403, 169977.9078901776\n",
      "Epoch 91701, Training Loss: 31144, Validation Loss: 54777, 152972.93977003763\n",
      "Epoch 91801, Training Loss: 31281, Validation Loss: 54339, 152784.97068317875\n",
      "Epoch 91901, Training Loss: 31287, Validation Loss: 51335, 134582.0751428205\n",
      "Epoch 92001, Training Loss: 30135, Validation Loss: 52157, 161626.86475373644\n",
      "Epoch 92101, Training Loss: 30124, Validation Loss: 52462, 179235.23865216074\n",
      "Epoch 92201, Training Loss: 31798, Validation Loss: 52260, 141079.84941951578\n",
      "Epoch 92301, Training Loss: 33342, Validation Loss: 55514, 157516.3131079217\n",
      "Epoch 92401, Training Loss: 31166, Validation Loss: 54507, 134948.93452083625\n",
      "Epoch 92501, Training Loss: 32866, Validation Loss: 55655, 161318.75766955796\n",
      "Epoch 92601, Training Loss: 29550, Validation Loss: 53744, 136046.91144813894\n",
      "Epoch 92701, Training Loss: 31802, Validation Loss: 52931, 149681.62390919225\n",
      "Epoch 92801, Training Loss: 29405, Validation Loss: 52685, 128502.17005292124\n",
      "Epoch 92901, Training Loss: 29650, Validation Loss: 55764, 188747.19206275357\n",
      "Epoch 93001, Training Loss: 33671, Validation Loss: 54359, 241722.17665115953\n",
      "Epoch 93101, Training Loss: 35528, Validation Loss: 52722, 153272.92127488993\n",
      "Epoch 93201, Training Loss: 32669, Validation Loss: 52350, 137650.41366955717\n",
      "Epoch 93301, Training Loss: 32499, Validation Loss: 54147, 168091.7559325257\n",
      "Epoch 93401, Training Loss: 30236, Validation Loss: 56513, 154875.2371944363\n",
      "Epoch 93501, Training Loss: 28223, Validation Loss: 55905, 150948.05805862122\n",
      "Epoch 93601, Training Loss: 33707, Validation Loss: 51107, 175975.07994882474\n",
      "Epoch 93701, Training Loss: 33199, Validation Loss: 55571, 149533.31798738518\n",
      "Epoch 93801, Training Loss: 32399, Validation Loss: 53449, 203163.94758835013\n",
      "Epoch 93901, Training Loss: 32697, Validation Loss: 49558, 149126.84870398903\n",
      "Epoch 94001, Training Loss: 30626, Validation Loss: 55220, 147478.99533100534\n",
      "Epoch 94101, Training Loss: 30859, Validation Loss: 57052, 126443.41437142238\n",
      "Epoch 94201, Training Loss: 32263, Validation Loss: 53745, 129936.96110923542\n",
      "Epoch 94301, Training Loss: 29399, Validation Loss: 54061, 147516.1279609437\n",
      "Epoch 94401, Training Loss: 31703, Validation Loss: 51882, 152151.661424242\n",
      "Epoch 94501, Training Loss: 32531, Validation Loss: 54226, 214087.4613393869\n",
      "Epoch 94601, Training Loss: 30387, Validation Loss: 58246, 150329.69426481088\n",
      "Epoch 94701, Training Loss: 29623, Validation Loss: 54889, 118795.41953734767\n",
      "Epoch 94801, Training Loss: 29353, Validation Loss: 56226, 128635.93655931484\n",
      "Epoch 94901, Training Loss: 28644, Validation Loss: 55667, 131451.3755528502\n",
      "Epoch 95001, Training Loss: 33617, Validation Loss: 51617, 214516.84616431207\n",
      "Epoch 95101, Training Loss: 29483, Validation Loss: 51500, 153389.10217403856\n",
      "Epoch 95201, Training Loss: 31756, Validation Loss: 54318, 133581.332248833\n",
      "Epoch 95301, Training Loss: 31220, Validation Loss: 50755, 170036.07207208607\n",
      "Epoch 95401, Training Loss: 29941, Validation Loss: 51761, 117751.09515609728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95501, Training Loss: 32350, Validation Loss: 53654, 117781.53213657986\n",
      "Epoch 95601, Training Loss: 32131, Validation Loss: 52246, 161602.67840943238\n",
      "Epoch 95701, Training Loss: 33275, Validation Loss: 52474, 215603.12570602182\n",
      "Epoch 95801, Training Loss: 31504, Validation Loss: 53437, 182077.48155055815\n",
      "Epoch 95901, Training Loss: 30661, Validation Loss: 54536, 162065.49165751672\n",
      "Epoch 96001, Training Loss: 30243, Validation Loss: 56265, 156405.6582478936\n",
      "Epoch 96101, Training Loss: 31820, Validation Loss: 55118, 167601.44543924314\n",
      "Epoch 96201, Training Loss: 30806, Validation Loss: 52215, 151653.206237699\n",
      "Epoch 96301, Training Loss: 32468, Validation Loss: 52528, 160726.3010554154\n",
      "Epoch 96401, Training Loss: 30669, Validation Loss: 55887, 140951.00679701814\n",
      "Epoch 96501, Training Loss: 29286, Validation Loss: 57552, 155635.56919250544\n",
      "Epoch 96601, Training Loss: 28488, Validation Loss: 52744, 138888.4112147947\n",
      "Epoch 96701, Training Loss: 31063, Validation Loss: 51434, 158148.90758209603\n",
      "Epoch 96801, Training Loss: 30166, Validation Loss: 56046, 150748.46219947576\n",
      "Epoch 96901, Training Loss: 31376, Validation Loss: 53860, 163001.65451643828\n",
      "Epoch 97001, Training Loss: 29300, Validation Loss: 53340, 175036.1217509614\n",
      "Epoch 97101, Training Loss: 32612, Validation Loss: 53354, 161998.34815104882\n",
      "Epoch 97201, Training Loss: 31468, Validation Loss: 54247, 186331.5915182191\n",
      "Epoch 97301, Training Loss: 31845, Validation Loss: 53886, 155406.9659860784\n",
      "Epoch 97401, Training Loss: 33926, Validation Loss: 51591, 140977.01951765094\n",
      "Epoch 97501, Training Loss: 33891, Validation Loss: 53086, 182261.2880293239\n",
      "Epoch 97601, Training Loss: 32126, Validation Loss: 53144, 240316.35965175318\n",
      "Epoch 97701, Training Loss: 29020, Validation Loss: 55877, 151412.5939298236\n",
      "Epoch 97801, Training Loss: 31480, Validation Loss: 52825, 185386.28129642815\n",
      "Epoch 97901, Training Loss: 29965, Validation Loss: 51704, 138078.47275130052\n",
      "Epoch 98001, Training Loss: 31053, Validation Loss: 51193, 130240.29502833453\n",
      "Epoch 98101, Training Loss: 32338, Validation Loss: 52736, 129212.25721359882\n",
      "Epoch 98201, Training Loss: 30202, Validation Loss: 50750, 136202.28961137592\n",
      "Epoch 98301, Training Loss: 32149, Validation Loss: 55226, 170456.43889579512\n",
      "Epoch 98401, Training Loss: 31865, Validation Loss: 53371, 141843.5816075738\n",
      "Epoch 98501, Training Loss: 34984, Validation Loss: 55144, 144305.31308622655\n",
      "Epoch 98601, Training Loss: 30656, Validation Loss: 52333, 152416.47263978422\n",
      "Epoch 98701, Training Loss: 32519, Validation Loss: 54586, 161435.86855117235\n",
      "Epoch 98801, Training Loss: 29367, Validation Loss: 54275, 176412.3958107072\n",
      "Epoch 98901, Training Loss: 31710, Validation Loss: 52700, 168350.25662838598\n",
      "Epoch 99001, Training Loss: 31605, Validation Loss: 53849, 139112.65198746574\n",
      "Epoch 99101, Training Loss: 32322, Validation Loss: 53326, 121768.38129497698\n",
      "Epoch 99201, Training Loss: 27826, Validation Loss: 53705, 154198.8341386917\n",
      "Epoch 99301, Training Loss: 28763, Validation Loss: 54406, 149096.8594871147\n",
      "Epoch 99401, Training Loss: 32738, Validation Loss: 52936, 176937.46449183137\n",
      "Epoch 99501, Training Loss: 30963, Validation Loss: 54525, 152008.26991500516\n",
      "Epoch 99601, Training Loss: 31552, Validation Loss: 53570, 130390.08702360567\n",
      "Epoch 99701, Training Loss: 30421, Validation Loss: 52295, 139826.35685538253\n",
      "Epoch 99801, Training Loss: 28598, Validation Loss: 51786, 157004.8299970676\n",
      "Epoch 99901, Training Loss: 32077, Validation Loss: 53392, 181968.42073364998\n",
      "Epoch 100001, Training Loss: 30375, Validation Loss: 51360, 155750.64157924082\n",
      "Epoch 100101, Training Loss: 29828, Validation Loss: 53980, 181764.5016672965\n",
      "Epoch 100201, Training Loss: 29099, Validation Loss: 52074, 152698.52668470782\n",
      "Epoch 100301, Training Loss: 30484, Validation Loss: 54402, 158089.18703691816\n",
      "Epoch 100401, Training Loss: 29556, Validation Loss: 53321, 114032.24125155302\n",
      "Epoch 100501, Training Loss: 30943, Validation Loss: 56610, 156785.8552393781\n",
      "Epoch 100601, Training Loss: 32338, Validation Loss: 55097, 187432.2464798895\n",
      "Epoch 100701, Training Loss: 29688, Validation Loss: 52996, 123982.74932413928\n",
      "Epoch 100801, Training Loss: 31807, Validation Loss: 55788, 141630.6380258473\n",
      "Epoch 100901, Training Loss: 28115, Validation Loss: 54597, 137898.5431599914\n",
      "Epoch 101001, Training Loss: 29931, Validation Loss: 54462, 137870.2734954696\n",
      "Epoch 101101, Training Loss: 31620, Validation Loss: 52194, 126059.04501909646\n",
      "Epoch 101201, Training Loss: 30491, Validation Loss: 53132, 145594.67509275468\n",
      "Epoch 101301, Training Loss: 32843, Validation Loss: 56562, 133273.44734370653\n",
      "Epoch 101401, Training Loss: 31776, Validation Loss: 53299, 130413.87101683793\n",
      "Epoch 101501, Training Loss: 32020, Validation Loss: 53053, 167007.59741107558\n",
      "Epoch 101601, Training Loss: 31446, Validation Loss: 52149, 160125.40475113256\n",
      "Epoch 101701, Training Loss: 29927, Validation Loss: 52388, 123188.25239201788\n",
      "Epoch 101801, Training Loss: 29392, Validation Loss: 56338, 174615.38158057057\n",
      "Epoch 101901, Training Loss: 32785, Validation Loss: 52755, 170943.5321459802\n",
      "Epoch 102001, Training Loss: 32293, Validation Loss: 53139, 193810.9795589454\n",
      "Epoch 102101, Training Loss: 29977, Validation Loss: 53799, 132021.58824926548\n",
      "Epoch 102201, Training Loss: 29492, Validation Loss: 55724, 142843.12829674105\n",
      "Epoch 102301, Training Loss: 31230, Validation Loss: 54558, 150144.4878903006\n",
      "Epoch 102401, Training Loss: 32171, Validation Loss: 55594, 175937.8947793711\n",
      "Epoch 102501, Training Loss: 30930, Validation Loss: 55411, 203289.68893549047\n",
      "Epoch 102601, Training Loss: 31277, Validation Loss: 54282, 228397.36548670265\n",
      "Epoch 102701, Training Loss: 29681, Validation Loss: 54457, 170818.36844694553\n",
      "Epoch 102801, Training Loss: 31483, Validation Loss: 51827, 199971.9492867225\n",
      "Epoch 102901, Training Loss: 31962, Validation Loss: 53726, 149955.13722441238\n",
      "Epoch 103001, Training Loss: 32112, Validation Loss: 52863, 231692.61309375343\n",
      "Epoch 103101, Training Loss: 33328, Validation Loss: 51780, 157630.37483924456\n",
      "Epoch 103201, Training Loss: 30978, Validation Loss: 51711, 138473.77126838523\n",
      "Epoch 103301, Training Loss: 31208, Validation Loss: 53376, 161213.81157932448\n",
      "Epoch 103401, Training Loss: 30608, Validation Loss: 51113, 123253.42750042242\n",
      "Epoch 103501, Training Loss: 31461, Validation Loss: 53164, 171817.86813415602\n",
      "Epoch 103601, Training Loss: 31050, Validation Loss: 54545, 188092.98544546845\n",
      "Epoch 103701, Training Loss: 32614, Validation Loss: 56239, 167702.63353107782\n",
      "Epoch 103801, Training Loss: 31859, Validation Loss: 51050, 160569.25865161276\n",
      "Epoch 103901, Training Loss: 30573, Validation Loss: 52442, 168970.29442710124\n",
      "Epoch 104001, Training Loss: 31321, Validation Loss: 54365, 136605.7696003618\n",
      "Epoch 104101, Training Loss: 29109, Validation Loss: 54001, 150758.2680481258\n",
      "Epoch 104201, Training Loss: 31107, Validation Loss: 51456, 196780.14456727216\n",
      "Epoch 104301, Training Loss: 34596, Validation Loss: 54312, 225740.21196760726\n",
      "Epoch 104401, Training Loss: 29708, Validation Loss: 52229, 192444.5615788898\n",
      "Epoch 104501, Training Loss: 31222, Validation Loss: 54568, 141877.210275806\n",
      "Epoch 104601, Training Loss: 33133, Validation Loss: 53239, 177629.10385577581\n",
      "Epoch 104701, Training Loss: 30573, Validation Loss: 52640, 166214.73519130776\n",
      "Epoch 104801, Training Loss: 30071, Validation Loss: 52798, 134209.60674232032\n",
      "Epoch 104901, Training Loss: 31412, Validation Loss: 53886, 135858.07118984393\n",
      "Epoch 105001, Training Loss: 29652, Validation Loss: 52448, 142541.34789242034\n",
      "Epoch 105101, Training Loss: 32702, Validation Loss: 56936, 183464.62527364295\n",
      "Epoch 105201, Training Loss: 31507, Validation Loss: 54137, 174141.427761022\n",
      "Epoch 105301, Training Loss: 32073, Validation Loss: 52489, 137943.9994941251\n",
      "Epoch 105401, Training Loss: 30663, Validation Loss: 58046, 109443.16549415071\n",
      "Epoch 105501, Training Loss: 32841, Validation Loss: 52448, 178566.5816143062\n",
      "Epoch 105601, Training Loss: 30706, Validation Loss: 53118, 137788.90559585896\n",
      "Epoch 105701, Training Loss: 30305, Validation Loss: 52949, 192104.94419430345\n",
      "Epoch 105801, Training Loss: 29564, Validation Loss: 54610, 169883.11511992037\n",
      "Epoch 105901, Training Loss: 29273, Validation Loss: 55008, 218097.10025823046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106001, Training Loss: 28559, Validation Loss: 51612, 139906.87735261905\n",
      "Epoch 106101, Training Loss: 31171, Validation Loss: 53042, 196810.64901557832\n",
      "Epoch 106201, Training Loss: 30701, Validation Loss: 52254, 149089.7897214862\n",
      "Epoch 106301, Training Loss: 30095, Validation Loss: 52131, 144092.460064541\n",
      "Epoch 106401, Training Loss: 31199, Validation Loss: 55242, 136015.70211341218\n",
      "Epoch 106501, Training Loss: 31141, Validation Loss: 51964, 133911.74199611883\n",
      "Epoch 106601, Training Loss: 30984, Validation Loss: 52181, 204363.98778296448\n",
      "Epoch 106701, Training Loss: 30022, Validation Loss: 52954, 153918.0227136555\n",
      "Epoch 106801, Training Loss: 28642, Validation Loss: 54407, 139173.06025359267\n",
      "Epoch 106901, Training Loss: 32315, Validation Loss: 52095, 149237.61187192946\n",
      "Epoch 107001, Training Loss: 30210, Validation Loss: 51329, 122147.86776811664\n",
      "Epoch 107101, Training Loss: 30938, Validation Loss: 57447, 138147.87992097728\n",
      "Epoch 107201, Training Loss: 31836, Validation Loss: 54191, 143214.76317222204\n",
      "Epoch 107301, Training Loss: 31182, Validation Loss: 53985, 146851.24451837337\n",
      "Epoch 107401, Training Loss: 30751, Validation Loss: 52060, 161987.55465964487\n",
      "Epoch 107501, Training Loss: 31335, Validation Loss: 53275, 160998.78530489886\n",
      "Epoch 107601, Training Loss: 29916, Validation Loss: 52156, 154345.38257831775\n",
      "Epoch 107701, Training Loss: 30302, Validation Loss: 55885, 129641.81745225226\n",
      "Epoch 107801, Training Loss: 30795, Validation Loss: 51947, 177240.79873213606\n",
      "Epoch 107901, Training Loss: 31549, Validation Loss: 55785, 189644.14501097545\n",
      "Epoch 108001, Training Loss: 30985, Validation Loss: 57885, 131937.86966273913\n",
      "Epoch 108101, Training Loss: 28139, Validation Loss: 49864, 183427.36978336787\n",
      "Epoch 108201, Training Loss: 31526, Validation Loss: 53032, 139853.36895503002\n",
      "Epoch 108301, Training Loss: 31218, Validation Loss: 54377, 142326.93067878208\n",
      "Epoch 108401, Training Loss: 33567, Validation Loss: 52930, 168550.22608572763\n",
      "Epoch 108501, Training Loss: 29452, Validation Loss: 53341, 123822.0469908741\n",
      "Epoch 108601, Training Loss: 32326, Validation Loss: 55950, 130902.58575080254\n",
      "Epoch 108701, Training Loss: 29665, Validation Loss: 56835, 225087.2222450561\n",
      "Epoch 108801, Training Loss: 31498, Validation Loss: 54038, 117225.17279105009\n",
      "Epoch 108901, Training Loss: 30661, Validation Loss: 56008, 220010.01692475952\n",
      "Epoch 109001, Training Loss: 29447, Validation Loss: 53357, 120412.97573743989\n",
      "Epoch 109101, Training Loss: 32429, Validation Loss: 57663, 177979.13596843765\n",
      "Epoch 109201, Training Loss: 31122, Validation Loss: 55453, 168724.7171627815\n",
      "Epoch 109301, Training Loss: 30982, Validation Loss: 54564, 141390.6768808221\n",
      "Epoch 109401, Training Loss: 28188, Validation Loss: 53640, 119948.4700991722\n",
      "Epoch 109501, Training Loss: 31432, Validation Loss: 55221, 149866.87383602644\n",
      "Epoch 109601, Training Loss: 30047, Validation Loss: 56686, 157625.8482770106\n",
      "Epoch 109701, Training Loss: 31639, Validation Loss: 55679, 151959.29478010454\n",
      "Epoch 109801, Training Loss: 31122, Validation Loss: 51317, 167939.98829487598\n",
      "Epoch 109901, Training Loss: 30094, Validation Loss: 54802, 189464.38024956558\n",
      "Epoch 110001, Training Loss: 29850, Validation Loss: 52288, 124985.64533652346\n",
      "Epoch 110101, Training Loss: 31399, Validation Loss: 50877, 184005.6477627967\n",
      "Epoch 110201, Training Loss: 29244, Validation Loss: 53412, 139990.54540011796\n",
      "Epoch 110301, Training Loss: 29420, Validation Loss: 52886, 197477.77669235933\n",
      "Epoch 110401, Training Loss: 29912, Validation Loss: 53645, 173298.79983237988\n",
      "Epoch 110501, Training Loss: 32109, Validation Loss: 51311, 180486.8047499423\n",
      "Epoch 110601, Training Loss: 28435, Validation Loss: 53509, 120164.65523174043\n",
      "Epoch 110701, Training Loss: 30370, Validation Loss: 55110, 149240.86622154948\n",
      "Epoch 110801, Training Loss: 31482, Validation Loss: 52789, 150606.18100622427\n",
      "Epoch 110901, Training Loss: 29820, Validation Loss: 53259, 130891.34108901415\n",
      "Epoch 111001, Training Loss: 30988, Validation Loss: 54838, 125823.24628639665\n",
      "Epoch 111101, Training Loss: 32058, Validation Loss: 54252, 244819.82443604022\n",
      "Epoch 111201, Training Loss: 31088, Validation Loss: 52365, 154879.73446283478\n",
      "Epoch 111301, Training Loss: 30856, Validation Loss: 56961, 179488.91841126676\n",
      "Epoch 111401, Training Loss: 31119, Validation Loss: 49639, 142885.39522633728\n",
      "Epoch 111501, Training Loss: 32725, Validation Loss: 53200, 183205.153033115\n",
      "Epoch 111601, Training Loss: 31425, Validation Loss: 55464, 160756.93388020623\n",
      "Epoch 111701, Training Loss: 28238, Validation Loss: 53569, 158515.19042454066\n",
      "Epoch 111801, Training Loss: 29853, Validation Loss: 54010, 143423.86571848634\n",
      "Epoch 111901, Training Loss: 30144, Validation Loss: 54755, 168989.4732188802\n",
      "Epoch 112001, Training Loss: 31585, Validation Loss: 56499, 197414.30452084492\n",
      "Epoch 112101, Training Loss: 29587, Validation Loss: 56336, 142362.55791159856\n",
      "Epoch 112201, Training Loss: 33259, Validation Loss: 53477, 113228.61470536469\n",
      "Epoch 112301, Training Loss: 32378, Validation Loss: 58046, 155872.77639598082\n",
      "Epoch 112401, Training Loss: 30041, Validation Loss: 53224, 114315.0477507478\n",
      "Epoch 112501, Training Loss: 32886, Validation Loss: 53350, 173111.3835501931\n",
      "Epoch 112601, Training Loss: 28621, Validation Loss: 53735, 149357.4752468116\n",
      "Epoch 112701, Training Loss: 32012, Validation Loss: 54019, 178496.73156710155\n",
      "Epoch 112801, Training Loss: 30793, Validation Loss: 52689, 153252.59826875528\n",
      "Epoch 112901, Training Loss: 27431, Validation Loss: 54185, 145297.54757748917\n",
      "Epoch 113001, Training Loss: 30673, Validation Loss: 53689, 137311.41474789454\n",
      "Epoch 113101, Training Loss: 30848, Validation Loss: 56204, 143606.55199543372\n",
      "Epoch 113201, Training Loss: 30452, Validation Loss: 55527, 244742.31842243497\n",
      "Epoch 113301, Training Loss: 30706, Validation Loss: 52000, 162505.7303507419\n",
      "Epoch 113401, Training Loss: 30133, Validation Loss: 51844, 175942.14966489864\n",
      "Epoch 113501, Training Loss: 31799, Validation Loss: 52643, 173170.44567882086\n",
      "Epoch 113601, Training Loss: 30322, Validation Loss: 54616, 119796.15537417453\n",
      "Epoch 113701, Training Loss: 28633, Validation Loss: 54983, 199468.64880386856\n",
      "Epoch 113801, Training Loss: 29534, Validation Loss: 53888, 185046.73596691413\n",
      "Epoch 113901, Training Loss: 30567, Validation Loss: 53180, 185546.2969368707\n",
      "Epoch 114001, Training Loss: 27334, Validation Loss: 54191, 124771.13311788654\n",
      "Epoch 114101, Training Loss: 31227, Validation Loss: 54768, 169401.55491025437\n",
      "Epoch 114201, Training Loss: 30481, Validation Loss: 53503, 126145.37121769735\n",
      "Epoch 114301, Training Loss: 30233, Validation Loss: 49843, 177410.85948707585\n",
      "Epoch 114401, Training Loss: 29244, Validation Loss: 54868, 153960.3151735168\n",
      "Epoch 114501, Training Loss: 29664, Validation Loss: 52307, 125407.63194577943\n",
      "Epoch 114601, Training Loss: 30660, Validation Loss: 54945, 168568.14226934718\n",
      "Epoch 114701, Training Loss: 28978, Validation Loss: 54061, 145669.5990603842\n",
      "Epoch 114801, Training Loss: 30964, Validation Loss: 51725, 217483.80277875872\n",
      "Epoch 114901, Training Loss: 30318, Validation Loss: 57121, 134564.03202641927\n",
      "Epoch 115001, Training Loss: 30944, Validation Loss: 54343, 123629.94518427919\n",
      "Epoch 115101, Training Loss: 32694, Validation Loss: 53002, 200522.13357947502\n",
      "Epoch 115201, Training Loss: 31318, Validation Loss: 54952, 186682.21113504295\n",
      "Epoch 115301, Training Loss: 28700, Validation Loss: 55183, 168862.29956478157\n",
      "Epoch 115401, Training Loss: 31230, Validation Loss: 54394, 159899.408221778\n",
      "Epoch 115501, Training Loss: 28843, Validation Loss: 58310, 142915.55414910847\n",
      "Epoch 115601, Training Loss: 27821, Validation Loss: 53135, 125700.66596458177\n",
      "Epoch 115701, Training Loss: 30894, Validation Loss: 53302, 162722.4092242551\n",
      "Epoch 115801, Training Loss: 32235, Validation Loss: 53548, 191327.4907652121\n",
      "Epoch 115901, Training Loss: 28385, Validation Loss: 52434, 171808.55036057284\n",
      "Epoch 116001, Training Loss: 30450, Validation Loss: 50950, 137833.53615347962\n",
      "Epoch 116101, Training Loss: 32804, Validation Loss: 52033, 166341.0472217327\n",
      "Epoch 116201, Training Loss: 31672, Validation Loss: 53178, 133233.6466276618\n",
      "Epoch 116301, Training Loss: 30493, Validation Loss: 58081, 203501.4953882801\n",
      "Epoch 116401, Training Loss: 28929, Validation Loss: 51309, 134262.8125855331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116501, Training Loss: 31729, Validation Loss: 55282, 170260.67569353766\n",
      "Epoch 116601, Training Loss: 30182, Validation Loss: 53261, 214683.92795259404\n",
      "Epoch 116701, Training Loss: 29723, Validation Loss: 55854, 117635.03809427684\n",
      "Epoch 116801, Training Loss: 28502, Validation Loss: 54203, 125832.5119888517\n",
      "Epoch 116901, Training Loss: 29559, Validation Loss: 53167, 144236.42692221873\n",
      "Epoch 117001, Training Loss: 29410, Validation Loss: 53372, 188057.52913354596\n",
      "Epoch 117101, Training Loss: 29859, Validation Loss: 57009, 191796.68837639104\n",
      "Epoch 117201, Training Loss: 29881, Validation Loss: 52054, 179898.9960859093\n",
      "Epoch 117301, Training Loss: 28425, Validation Loss: 52782, 127488.8965217123\n",
      "Epoch 117401, Training Loss: 28746, Validation Loss: 54928, 175148.5997919235\n",
      "Epoch 117501, Training Loss: 30381, Validation Loss: 55309, 173166.32695024597\n",
      "Epoch 117601, Training Loss: 29099, Validation Loss: 55840, 146818.10312889682\n",
      "Epoch 117701, Training Loss: 27955, Validation Loss: 55698, 144455.23841107773\n",
      "Epoch 117801, Training Loss: 31668, Validation Loss: 53302, 139063.9656335651\n",
      "Epoch 117901, Training Loss: 31121, Validation Loss: 50448, 172557.34163499717\n",
      "Epoch 118001, Training Loss: 32185, Validation Loss: 54475, 261692.85095486624\n",
      "Epoch 118101, Training Loss: 31578, Validation Loss: 56057, 174003.70029789387\n",
      "Epoch 118201, Training Loss: 32019, Validation Loss: 50863, 166634.81847682514\n",
      "Epoch 118301, Training Loss: 30555, Validation Loss: 52977, 148821.64421262286\n",
      "Epoch 118401, Training Loss: 29776, Validation Loss: 54032, 179511.73585047867\n",
      "Epoch 118501, Training Loss: 28949, Validation Loss: 56814, 125523.18936194263\n",
      "Epoch 118601, Training Loss: 30168, Validation Loss: 52679, 151815.98849405954\n",
      "Epoch 118701, Training Loss: 31552, Validation Loss: 52578, 166355.54465115632\n",
      "Epoch 118801, Training Loss: 29440, Validation Loss: 51686, 150597.7374811145\n",
      "Epoch 118901, Training Loss: 30672, Validation Loss: 55566, 181090.05943929017\n",
      "Epoch 119001, Training Loss: 32726, Validation Loss: 53099, 157057.88448720746\n",
      "Epoch 119101, Training Loss: 29773, Validation Loss: 54196, 165264.04565180055\n",
      "Epoch 119201, Training Loss: 30949, Validation Loss: 55171, 116159.2020530306\n",
      "Epoch 119301, Training Loss: 27585, Validation Loss: 53139, 113622.37676578933\n",
      "Epoch 119401, Training Loss: 31297, Validation Loss: 55920, 141053.7182240778\n",
      "Epoch 119501, Training Loss: 30651, Validation Loss: 56577, 140591.7478086128\n",
      "Epoch 119601, Training Loss: 30563, Validation Loss: 53077, 171498.84307975342\n",
      "Epoch 119701, Training Loss: 29762, Validation Loss: 54417, 139136.81515918163\n",
      "Epoch 119801, Training Loss: 28487, Validation Loss: 54784, 155415.12646987176\n",
      "Epoch 119901, Training Loss: 30937, Validation Loss: 54989, 147253.80376302206\n",
      "Epoch 120001, Training Loss: 32537, Validation Loss: 52869, 136176.5147617159\n",
      "Epoch 120101, Training Loss: 30080, Validation Loss: 52210, 145913.03845967518\n",
      "Epoch 120201, Training Loss: 31387, Validation Loss: 55586, 137837.39324518872\n",
      "Epoch 120301, Training Loss: 29747, Validation Loss: 54483, 166751.78837874037\n",
      "Epoch 120401, Training Loss: 30672, Validation Loss: 55799, 157655.7266706147\n",
      "Epoch 120501, Training Loss: 29184, Validation Loss: 52675, 132758.51169608446\n",
      "Epoch 120601, Training Loss: 29437, Validation Loss: 55827, 143439.4960267469\n",
      "Epoch 120701, Training Loss: 33005, Validation Loss: 52189, 192160.620141372\n",
      "Epoch 120801, Training Loss: 29285, Validation Loss: 53398, 145192.11525108537\n",
      "Epoch 120901, Training Loss: 31997, Validation Loss: 52176, 143071.93485480183\n",
      "Epoch 121001, Training Loss: 29944, Validation Loss: 54618, 140009.80745778576\n",
      "Epoch 121101, Training Loss: 31076, Validation Loss: 58553, 166850.0681108342\n",
      "Epoch 121201, Training Loss: 30816, Validation Loss: 52003, 240202.11065752633\n",
      "Epoch 121301, Training Loss: 31427, Validation Loss: 53022, 183444.68136963403\n",
      "Epoch 121401, Training Loss: 30181, Validation Loss: 55182, 187614.4571250156\n",
      "Epoch 121501, Training Loss: 29696, Validation Loss: 53677, 167458.99644072287\n",
      "Epoch 121601, Training Loss: 32670, Validation Loss: 55302, 195427.7574697285\n",
      "Epoch 121701, Training Loss: 30992, Validation Loss: 54359, 140931.9120584754\n",
      "Epoch 121801, Training Loss: 31320, Validation Loss: 51951, 193085.67870705854\n",
      "Epoch 121901, Training Loss: 30989, Validation Loss: 51953, 125811.91157413433\n",
      "Epoch 122001, Training Loss: 28988, Validation Loss: 54865, 135765.69995663097\n",
      "Epoch 122101, Training Loss: 31978, Validation Loss: 50808, 147952.7810026137\n",
      "Epoch 122201, Training Loss: 30604, Validation Loss: 53897, 134786.95472627974\n",
      "Epoch 122301, Training Loss: 29378, Validation Loss: 53738, 134845.5578660903\n",
      "Epoch 122401, Training Loss: 31163, Validation Loss: 55459, 133280.61866015513\n",
      "Epoch 122501, Training Loss: 28442, Validation Loss: 54548, 157661.47638080388\n",
      "Epoch 122601, Training Loss: 28758, Validation Loss: 51873, 165735.7299913596\n",
      "Epoch 122701, Training Loss: 27583, Validation Loss: 53919, 121902.15544496918\n",
      "Epoch 122801, Training Loss: 29669, Validation Loss: 55409, 213860.8268608517\n",
      "Epoch 122901, Training Loss: 31238, Validation Loss: 55483, 169220.68305675752\n",
      "Epoch 123001, Training Loss: 30989, Validation Loss: 58985, 169073.25122551725\n",
      "Epoch 123101, Training Loss: 28430, Validation Loss: 54802, 157195.49917047372\n",
      "Epoch 123201, Training Loss: 30405, Validation Loss: 54625, 144665.95015366646\n",
      "Epoch 123301, Training Loss: 28634, Validation Loss: 52188, 132975.16538286346\n",
      "Epoch 123401, Training Loss: 31261, Validation Loss: 56843, 141269.67791039296\n",
      "Epoch 123501, Training Loss: 30540, Validation Loss: 54843, 165106.92900968646\n",
      "Epoch 123601, Training Loss: 31133, Validation Loss: 51751, 160790.57449765643\n",
      "Epoch 123701, Training Loss: 29986, Validation Loss: 53333, 130305.38213664868\n",
      "Epoch 123801, Training Loss: 30863, Validation Loss: 54700, 148068.24088350363\n",
      "Epoch 123901, Training Loss: 29456, Validation Loss: 53522, 152734.8901559817\n",
      "Epoch 124001, Training Loss: 28555, Validation Loss: 55290, 184987.24250514305\n",
      "Epoch 124101, Training Loss: 29362, Validation Loss: 53303, 171116.4041576634\n",
      "Epoch 124201, Training Loss: 29250, Validation Loss: 51858, 165659.4194280063\n",
      "Epoch 124301, Training Loss: 29134, Validation Loss: 53630, 143422.99944734652\n",
      "Epoch 124401, Training Loss: 29637, Validation Loss: 52542, 154370.08224756282\n",
      "Epoch 124501, Training Loss: 28581, Validation Loss: 51341, 116997.28702668281\n",
      "Epoch 124601, Training Loss: 29877, Validation Loss: 53542, 197254.3566396398\n",
      "Epoch 124701, Training Loss: 28734, Validation Loss: 54677, 148109.27067401225\n",
      "Epoch 124801, Training Loss: 29761, Validation Loss: 55077, 156035.6086621976\n",
      "Epoch 124901, Training Loss: 29462, Validation Loss: 50415, 138872.47047137257\n",
      "Epoch 125001, Training Loss: 30364, Validation Loss: 57115, 210367.85349504554\n",
      "Epoch 125101, Training Loss: 29806, Validation Loss: 56922, 186744.40568056467\n",
      "Epoch 125201, Training Loss: 27927, Validation Loss: 53361, 122060.50508113556\n",
      "Epoch 125301, Training Loss: 30329, Validation Loss: 52997, 133475.9996804622\n",
      "Epoch 125401, Training Loss: 31676, Validation Loss: 55167, 143130.7756836311\n",
      "Epoch 125501, Training Loss: 31399, Validation Loss: 55892, 146527.53857478875\n",
      "Epoch 125601, Training Loss: 30385, Validation Loss: 55778, 223840.03912713393\n",
      "Epoch 125701, Training Loss: 31585, Validation Loss: 53128, 162620.3771140936\n",
      "Epoch 125801, Training Loss: 30196, Validation Loss: 52616, 121124.58815162697\n",
      "Epoch 125901, Training Loss: 30794, Validation Loss: 54044, 136552.8786317861\n",
      "Epoch 126001, Training Loss: 30134, Validation Loss: 53457, 129042.81052159809\n",
      "Epoch 126101, Training Loss: 26545, Validation Loss: 51007, 146724.69612474376\n",
      "Epoch 126201, Training Loss: 31186, Validation Loss: 54976, 139662.45945608144\n",
      "Epoch 126301, Training Loss: 29123, Validation Loss: 54606, 133738.41118487902\n",
      "Epoch 126401, Training Loss: 29021, Validation Loss: 54215, 144724.01040378062\n",
      "Epoch 126501, Training Loss: 30920, Validation Loss: 50686, 165895.7338709935\n",
      "Epoch 126601, Training Loss: 30596, Validation Loss: 54070, 119898.14462825462\n",
      "Epoch 126701, Training Loss: 29035, Validation Loss: 54564, 139519.16353667574\n",
      "Epoch 126801, Training Loss: 27376, Validation Loss: 56898, 149224.02971622004\n",
      "Epoch 126901, Training Loss: 28415, Validation Loss: 54163, 194425.65377521305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127001, Training Loss: 31713, Validation Loss: 54141, 126021.00269063213\n",
      "Epoch 127101, Training Loss: 30430, Validation Loss: 56705, 183681.81034868126\n",
      "Epoch 127201, Training Loss: 28324, Validation Loss: 54880, 126223.23353112547\n",
      "Epoch 127301, Training Loss: 32972, Validation Loss: 50325, 154882.09074066192\n",
      "Epoch 127401, Training Loss: 29676, Validation Loss: 54346, 154585.13316911925\n",
      "Epoch 127501, Training Loss: 27493, Validation Loss: 51091, 110494.91933415529\n",
      "Epoch 127601, Training Loss: 29468, Validation Loss: 52919, 247489.30355575992\n",
      "Epoch 127701, Training Loss: 30597, Validation Loss: 54621, 161965.96131365316\n",
      "Epoch 127801, Training Loss: 30996, Validation Loss: 51172, 150473.57792006715\n",
      "Epoch 127901, Training Loss: 32111, Validation Loss: 52135, 160208.11058939478\n",
      "Epoch 128001, Training Loss: 29820, Validation Loss: 55139, 150138.99824832228\n",
      "Epoch 128101, Training Loss: 29251, Validation Loss: 52860, 172864.55426016677\n",
      "Epoch 128201, Training Loss: 28347, Validation Loss: 50690, 171008.69074251698\n",
      "Epoch 128301, Training Loss: 28390, Validation Loss: 55260, 144685.15052400288\n",
      "Epoch 128401, Training Loss: 32394, Validation Loss: 56375, 172258.70134713655\n",
      "Epoch 128501, Training Loss: 29035, Validation Loss: 55009, 135976.61763326125\n",
      "Epoch 128601, Training Loss: 30851, Validation Loss: 56072, 207834.23001987612\n",
      "Epoch 128701, Training Loss: 31711, Validation Loss: 54455, 154046.20861697677\n",
      "Epoch 128801, Training Loss: 30075, Validation Loss: 55605, 154539.49152357876\n",
      "Epoch 128901, Training Loss: 31730, Validation Loss: 54537, 142750.63030846557\n",
      "Epoch 129001, Training Loss: 30289, Validation Loss: 52190, 135316.40241447627\n",
      "Epoch 129101, Training Loss: 35093, Validation Loss: 57154, 201934.30829359716\n",
      "Epoch 129201, Training Loss: 31942, Validation Loss: 54332, 161579.320792018\n",
      "Epoch 129301, Training Loss: 30369, Validation Loss: 52715, 244989.40314481617\n",
      "Epoch 129401, Training Loss: 33101, Validation Loss: 52863, 146849.87532754333\n",
      "Epoch 129501, Training Loss: 30705, Validation Loss: 50618, 147616.04026403642\n",
      "Epoch 129601, Training Loss: 30482, Validation Loss: 56704, 145735.22347803894\n",
      "Epoch 129701, Training Loss: 28395, Validation Loss: 55421, 146775.05995624384\n",
      "Epoch 129801, Training Loss: 28536, Validation Loss: 55595, 174575.09220084964\n",
      "Epoch 129901, Training Loss: 31050, Validation Loss: 53639, 171279.90041903674\n",
      "Epoch 130001, Training Loss: 29429, Validation Loss: 51798, 140253.93793140873\n",
      "Epoch 130101, Training Loss: 29884, Validation Loss: 53301, 150654.2040671541\n",
      "Epoch 130201, Training Loss: 28553, Validation Loss: 53615, 137938.4614274267\n",
      "Epoch 130301, Training Loss: 30330, Validation Loss: 53391, 162682.27470248568\n",
      "Epoch 130401, Training Loss: 30442, Validation Loss: 54240, 232360.96855088338\n",
      "Epoch 130501, Training Loss: 30904, Validation Loss: 55991, 232104.6487898711\n",
      "Epoch 130601, Training Loss: 30359, Validation Loss: 55819, 140379.4034993771\n",
      "Epoch 130701, Training Loss: 32110, Validation Loss: 56736, 208165.26331801704\n",
      "Epoch 130801, Training Loss: 29855, Validation Loss: 53487, 125131.85671972447\n",
      "Epoch 130901, Training Loss: 29261, Validation Loss: 55713, 148029.6664662217\n",
      "Epoch 131001, Training Loss: 30456, Validation Loss: 53351, 176902.77107013031\n",
      "Epoch 131101, Training Loss: 29135, Validation Loss: 53386, 130409.22418292488\n",
      "Epoch 131201, Training Loss: 31781, Validation Loss: 54643, 190269.70533511645\n",
      "Epoch 131301, Training Loss: 31493, Validation Loss: 55269, 130107.56154698727\n",
      "Epoch 131401, Training Loss: 29705, Validation Loss: 54096, 92775.91894727743\n",
      "Epoch 131501, Training Loss: 28591, Validation Loss: 56352, 144446.12801220748\n",
      "Epoch 131601, Training Loss: 32056, Validation Loss: 55608, 134045.15248287236\n",
      "Epoch 131701, Training Loss: 31032, Validation Loss: 54434, 133986.3879954835\n",
      "Epoch 131801, Training Loss: 27815, Validation Loss: 52851, 108319.05475984514\n",
      "Epoch 131901, Training Loss: 31567, Validation Loss: 57103, 186400.15066659765\n",
      "Epoch 132001, Training Loss: 31782, Validation Loss: 55827, 151463.89469934884\n",
      "Epoch 132101, Training Loss: 28873, Validation Loss: 54302, 115956.02245950699\n",
      "Epoch 132201, Training Loss: 30319, Validation Loss: 54386, 151173.40301719672\n",
      "Epoch 132301, Training Loss: 29788, Validation Loss: 58341, 183161.65764444985\n",
      "Epoch 132401, Training Loss: 29352, Validation Loss: 55124, 130029.11157217126\n",
      "Epoch 132501, Training Loss: 29535, Validation Loss: 53286, 143710.49501230763\n",
      "Epoch 132601, Training Loss: 31767, Validation Loss: 51280, 177894.4168939926\n",
      "Epoch 132701, Training Loss: 28220, Validation Loss: 58013, 187494.05651000887\n",
      "Epoch 132801, Training Loss: 27564, Validation Loss: 53836, 196578.35374843024\n",
      "Epoch 132901, Training Loss: 30171, Validation Loss: 50902, 141429.53306698994\n",
      "Epoch 133001, Training Loss: 31048, Validation Loss: 58920, 150572.243469297\n",
      "Epoch 133101, Training Loss: 30738, Validation Loss: 51929, 153423.8375087094\n",
      "Epoch 133201, Training Loss: 30303, Validation Loss: 52641, 129811.8289190397\n",
      "Epoch 133301, Training Loss: 26989, Validation Loss: 53436, 125605.25366290745\n",
      "Epoch 133401, Training Loss: 30972, Validation Loss: 54491, 165840.01782000888\n",
      "Epoch 133501, Training Loss: 27878, Validation Loss: 52436, 177579.18336425532\n",
      "Epoch 133601, Training Loss: 30677, Validation Loss: 51529, 111334.54382180388\n",
      "Epoch 133701, Training Loss: 31336, Validation Loss: 56736, 236156.16491128667\n",
      "Epoch 133801, Training Loss: 29028, Validation Loss: 53596, 155672.98334673353\n",
      "Epoch 133901, Training Loss: 28615, Validation Loss: 53569, 151391.25190915525\n",
      "Epoch 134001, Training Loss: 29367, Validation Loss: 53737, 147378.98660750707\n",
      "Epoch 134101, Training Loss: 29297, Validation Loss: 51330, 142795.43250274382\n",
      "Epoch 134201, Training Loss: 31210, Validation Loss: 51712, 131538.24108523267\n",
      "Epoch 134301, Training Loss: 28033, Validation Loss: 54161, 126386.1041781971\n",
      "Epoch 134401, Training Loss: 31863, Validation Loss: 52531, 156613.57792547546\n",
      "Epoch 134501, Training Loss: 33354, Validation Loss: 52940, 143821.05811560477\n",
      "Epoch 134601, Training Loss: 31644, Validation Loss: 52638, 151764.50015564763\n",
      "Epoch 134701, Training Loss: 31390, Validation Loss: 52498, 171786.59455370813\n",
      "Epoch 134801, Training Loss: 31255, Validation Loss: 53491, 151130.86950241294\n",
      "Epoch 134901, Training Loss: 28914, Validation Loss: 54595, 111055.29646585474\n",
      "Epoch 135001, Training Loss: 31025, Validation Loss: 52837, 170303.73231181642\n",
      "Epoch 135101, Training Loss: 29051, Validation Loss: 53580, 137353.62950288257\n",
      "Epoch 135201, Training Loss: 31333, Validation Loss: 54917, 207562.17554735663\n",
      "Epoch 135301, Training Loss: 29021, Validation Loss: 53625, 124908.48754020203\n",
      "Epoch 135401, Training Loss: 29755, Validation Loss: 52525, 191872.99215192845\n",
      "Epoch 135501, Training Loss: 29618, Validation Loss: 52681, 199174.82829929472\n",
      "Epoch 135601, Training Loss: 28770, Validation Loss: 53028, 130087.08223312713\n",
      "Epoch 135701, Training Loss: 28507, Validation Loss: 53263, 141951.1148655966\n",
      "Epoch 135801, Training Loss: 30677, Validation Loss: 55110, 171873.75012770036\n",
      "Epoch 135901, Training Loss: 28675, Validation Loss: 55591, 153177.77369156395\n",
      "Epoch 136001, Training Loss: 29259, Validation Loss: 51252, 128179.96190412797\n",
      "Epoch 136101, Training Loss: 29995, Validation Loss: 55109, 148595.5311774962\n",
      "Epoch 136201, Training Loss: 29043, Validation Loss: 55186, 144676.6454848031\n",
      "Epoch 136301, Training Loss: 27471, Validation Loss: 55873, 146332.8893256772\n",
      "Epoch 136401, Training Loss: 29944, Validation Loss: 52134, 138643.47061629285\n",
      "Epoch 136501, Training Loss: 28355, Validation Loss: 53669, 133039.25476650853\n",
      "Epoch 136601, Training Loss: 31590, Validation Loss: 51695, 157282.0935039049\n",
      "Epoch 136701, Training Loss: 30922, Validation Loss: 55708, 160033.92071776494\n",
      "Epoch 136801, Training Loss: 29826, Validation Loss: 56188, 149216.6756026091\n",
      "Epoch 136901, Training Loss: 32292, Validation Loss: 56497, 158674.24867055714\n",
      "Epoch 137001, Training Loss: 29967, Validation Loss: 51601, 156038.12903980812\n",
      "Epoch 137101, Training Loss: 28923, Validation Loss: 53976, 187678.30461908187\n",
      "Epoch 137201, Training Loss: 32702, Validation Loss: 54629, 168717.04096001654\n",
      "Epoch 137301, Training Loss: 29750, Validation Loss: 54140, 150057.40850875966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137401, Training Loss: 29760, Validation Loss: 53982, 159510.16919462127\n",
      "Epoch 137501, Training Loss: 29086, Validation Loss: 55584, 138715.85285746807\n",
      "Epoch 137601, Training Loss: 29091, Validation Loss: 55421, 139567.410347921\n",
      "Epoch 137701, Training Loss: 27293, Validation Loss: 54609, 147623.08893144052\n",
      "Epoch 137801, Training Loss: 31480, Validation Loss: 55325, 200894.83531995388\n",
      "Epoch 137901, Training Loss: 30884, Validation Loss: 50579, 122720.46126822848\n",
      "Epoch 138001, Training Loss: 30096, Validation Loss: 52767, 127715.84081642081\n",
      "Epoch 138101, Training Loss: 30773, Validation Loss: 53423, 176367.53755237488\n",
      "Epoch 138201, Training Loss: 29741, Validation Loss: 56487, 169688.7749607557\n",
      "Epoch 138301, Training Loss: 28975, Validation Loss: 51354, 152158.32419379358\n",
      "Epoch 138401, Training Loss: 29303, Validation Loss: 54833, 166447.3601229379\n",
      "Epoch 138501, Training Loss: 29513, Validation Loss: 50995, 173976.2490140208\n",
      "Epoch 138601, Training Loss: 30408, Validation Loss: 53006, 144190.42095781726\n",
      "Epoch 138701, Training Loss: 29843, Validation Loss: 53655, 152067.16114901754\n",
      "Epoch 138801, Training Loss: 28622, Validation Loss: 54821, 141627.71446136947\n",
      "Epoch 138901, Training Loss: 27539, Validation Loss: 54258, 149691.82506093735\n",
      "Epoch 139001, Training Loss: 29944, Validation Loss: 56012, 176838.26893857773\n",
      "Epoch 139101, Training Loss: 29721, Validation Loss: 52730, 206264.93809555462\n",
      "Epoch 139201, Training Loss: 29979, Validation Loss: 54572, 190835.9254986453\n",
      "Epoch 139301, Training Loss: 28928, Validation Loss: 52423, 112025.96957381407\n",
      "Epoch 139401, Training Loss: 29381, Validation Loss: 54257, 184390.24875797261\n",
      "Epoch 139501, Training Loss: 28410, Validation Loss: 53291, 147306.8359400174\n",
      "Epoch 139601, Training Loss: 26534, Validation Loss: 53515, 163813.62074447243\n",
      "Epoch 139701, Training Loss: 30730, Validation Loss: 53544, 157043.65238586356\n",
      "Epoch 139801, Training Loss: 28266, Validation Loss: 52799, 177253.47926552757\n",
      "Epoch 139901, Training Loss: 29820, Validation Loss: 54515, 133866.2981540635\n",
      "Epoch 140001, Training Loss: 30716, Validation Loss: 54001, 147990.17294289454\n",
      "Epoch 140101, Training Loss: 29142, Validation Loss: 54385, 161277.7338232711\n",
      "Epoch 140201, Training Loss: 27573, Validation Loss: 53136, 172478.64007800518\n",
      "Epoch 140301, Training Loss: 30403, Validation Loss: 52230, 210438.31592343128\n",
      "Epoch 140401, Training Loss: 28290, Validation Loss: 51501, 131497.05768549818\n",
      "Epoch 140501, Training Loss: 28746, Validation Loss: 56951, 189964.76250075936\n",
      "Epoch 140601, Training Loss: 31701, Validation Loss: 53086, 164838.541284156\n",
      "Epoch 140701, Training Loss: 31252, Validation Loss: 56068, 175473.3445216041\n",
      "Epoch 140801, Training Loss: 29649, Validation Loss: 55398, 163350.2531983706\n",
      "Epoch 140901, Training Loss: 30229, Validation Loss: 54690, 122497.0545220255\n",
      "Epoch 141001, Training Loss: 28776, Validation Loss: 54082, 131731.6948163002\n",
      "Epoch 141101, Training Loss: 31437, Validation Loss: 52242, 202527.13207437648\n",
      "Epoch 141201, Training Loss: 29857, Validation Loss: 53265, 156532.57731081345\n",
      "Epoch 141301, Training Loss: 28979, Validation Loss: 55418, 146331.43621236007\n",
      "Epoch 141401, Training Loss: 29699, Validation Loss: 52376, 135789.74827487822\n",
      "Epoch 141501, Training Loss: 29311, Validation Loss: 53533, 154911.53896144917\n",
      "Epoch 141601, Training Loss: 29763, Validation Loss: 53723, 167135.81192706505\n",
      "Epoch 141701, Training Loss: 28212, Validation Loss: 57068, 211615.92347303312\n",
      "Epoch 141801, Training Loss: 30478, Validation Loss: 52029, 145803.52620003655\n",
      "Epoch 141901, Training Loss: 28728, Validation Loss: 55406, 157610.0791305152\n",
      "Epoch 142001, Training Loss: 31292, Validation Loss: 51434, 137867.2001193714\n",
      "Epoch 142101, Training Loss: 28427, Validation Loss: 52292, 114230.4901940541\n",
      "Epoch 142201, Training Loss: 30036, Validation Loss: 54573, 149798.8819158687\n",
      "Epoch 142301, Training Loss: 28672, Validation Loss: 53847, 140597.25410264687\n",
      "Epoch 142401, Training Loss: 28978, Validation Loss: 57781, 133886.435043339\n",
      "Epoch 142501, Training Loss: 30786, Validation Loss: 51069, 161849.30736003874\n",
      "Epoch 142601, Training Loss: 30172, Validation Loss: 53389, 133104.16256041857\n",
      "Epoch 142701, Training Loss: 28304, Validation Loss: 52506, 140290.3867054138\n",
      "Epoch 142801, Training Loss: 29803, Validation Loss: 49827, 123251.31437810599\n",
      "Epoch 142901, Training Loss: 28431, Validation Loss: 53997, 174041.09991677364\n",
      "Epoch 143001, Training Loss: 28389, Validation Loss: 56171, 154596.88940071617\n",
      "Epoch 143101, Training Loss: 29128, Validation Loss: 53310, 112560.79221357805\n",
      "Epoch 143201, Training Loss: 29942, Validation Loss: 55258, 110332.8153641793\n",
      "Epoch 143301, Training Loss: 26394, Validation Loss: 55174, 128949.51210612897\n",
      "Epoch 143401, Training Loss: 27499, Validation Loss: 58670, 148220.2782987522\n",
      "Epoch 143501, Training Loss: 30054, Validation Loss: 51890, 104306.82006370882\n",
      "Epoch 143601, Training Loss: 30327, Validation Loss: 51328, 136201.67721508894\n",
      "Epoch 143701, Training Loss: 30936, Validation Loss: 53215, 159115.67798779404\n",
      "Epoch 143801, Training Loss: 29858, Validation Loss: 54832, 156218.8848363205\n",
      "Epoch 143901, Training Loss: 29648, Validation Loss: 53815, 205174.83640202388\n",
      "Epoch 144001, Training Loss: 27580, Validation Loss: 54436, 150136.40183206127\n",
      "Epoch 144101, Training Loss: 29397, Validation Loss: 56561, 117737.23966621871\n",
      "Epoch 144201, Training Loss: 29651, Validation Loss: 51836, 154212.3666813178\n",
      "Epoch 144301, Training Loss: 30429, Validation Loss: 54059, 175666.19525992408\n",
      "Epoch 144401, Training Loss: 28920, Validation Loss: 53006, 134660.80095235872\n",
      "Epoch 144501, Training Loss: 30527, Validation Loss: 53405, 187373.39971765978\n",
      "Epoch 144601, Training Loss: 30686, Validation Loss: 52871, 162714.05046273713\n",
      "Epoch 144701, Training Loss: 29774, Validation Loss: 52947, 156011.2705853846\n",
      "Epoch 144801, Training Loss: 30334, Validation Loss: 51843, 117944.08473720007\n",
      "Epoch 144901, Training Loss: 28926, Validation Loss: 55632, 128739.02812701771\n",
      "Epoch 145001, Training Loss: 27452, Validation Loss: 51517, 157654.14313171653\n",
      "Epoch 145101, Training Loss: 29270, Validation Loss: 52940, 162190.13536758424\n",
      "Epoch 145201, Training Loss: 31372, Validation Loss: 55246, 195719.88781436786\n",
      "Epoch 145301, Training Loss: 30018, Validation Loss: 50513, 134188.84059663236\n",
      "Epoch 145401, Training Loss: 29010, Validation Loss: 52821, 139446.20787895896\n",
      "Epoch 145501, Training Loss: 30418, Validation Loss: 54996, 170337.21750036094\n",
      "Epoch 145601, Training Loss: 31353, Validation Loss: 52073, 120073.53621995654\n",
      "Epoch 145701, Training Loss: 30927, Validation Loss: 53925, 159687.26562480684\n",
      "Epoch 145801, Training Loss: 28686, Validation Loss: 53444, 139044.4028523736\n",
      "Epoch 145901, Training Loss: 28499, Validation Loss: 52218, 151877.70326525756\n",
      "Epoch 146001, Training Loss: 29832, Validation Loss: 55717, 183855.79173954958\n",
      "Epoch 146101, Training Loss: 27908, Validation Loss: 51920, 121412.4274658279\n",
      "Epoch 146201, Training Loss: 29365, Validation Loss: 51842, 147424.43488599666\n",
      "Epoch 146301, Training Loss: 29475, Validation Loss: 51497, 129987.97145116597\n",
      "Epoch 146401, Training Loss: 30530, Validation Loss: 54595, 209279.09406834678\n",
      "Epoch 146501, Training Loss: 31058, Validation Loss: 55963, 147877.45072812587\n",
      "Epoch 146601, Training Loss: 28409, Validation Loss: 54981, 140984.59471327442\n",
      "Epoch 146701, Training Loss: 28593, Validation Loss: 53773, 147660.40267785164\n",
      "Epoch 146801, Training Loss: 29568, Validation Loss: 53542, 177731.5584138125\n",
      "Epoch 146901, Training Loss: 29235, Validation Loss: 53888, 132670.94196218497\n",
      "Epoch 147001, Training Loss: 29764, Validation Loss: 54418, 168740.39796844297\n",
      "Epoch 147101, Training Loss: 31737, Validation Loss: 52306, 142010.01181677988\n",
      "Epoch 147201, Training Loss: 28113, Validation Loss: 51208, 147410.74407998324\n",
      "Epoch 147301, Training Loss: 30562, Validation Loss: 50560, 179203.60575073966\n",
      "Epoch 147401, Training Loss: 29820, Validation Loss: 53288, 158278.32634143243\n",
      "Epoch 147501, Training Loss: 28935, Validation Loss: 54610, 128237.64044006744\n",
      "Epoch 147601, Training Loss: 27242, Validation Loss: 53078, 117914.10820438985\n",
      "Epoch 147701, Training Loss: 27861, Validation Loss: 54067, 138144.84276430088\n",
      "Epoch 147801, Training Loss: 28777, Validation Loss: 56874, 157893.18692537278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147901, Training Loss: 28368, Validation Loss: 54446, 136961.08810596648\n",
      "Epoch 148001, Training Loss: 27725, Validation Loss: 52256, 143907.58313993912\n",
      "Epoch 148101, Training Loss: 32023, Validation Loss: 51014, 162433.1575421498\n",
      "Epoch 148201, Training Loss: 28916, Validation Loss: 53975, 123161.65167185622\n",
      "Epoch 148301, Training Loss: 30112, Validation Loss: 54924, 198480.47976249724\n",
      "Epoch 148401, Training Loss: 29573, Validation Loss: 53908, 181797.0783857743\n",
      "Epoch 148501, Training Loss: 32081, Validation Loss: 52240, 225485.79284240934\n",
      "Epoch 148601, Training Loss: 27352, Validation Loss: 54021, 126638.28746232996\n",
      "Epoch 148701, Training Loss: 27475, Validation Loss: 53986, 116131.45552831875\n",
      "Epoch 148801, Training Loss: 27233, Validation Loss: 54088, 145943.77683837523\n",
      "Epoch 148901, Training Loss: 31571, Validation Loss: 54032, 186577.0771565749\n",
      "Epoch 149001, Training Loss: 30468, Validation Loss: 53548, 151747.14888485704\n",
      "Epoch 149101, Training Loss: 29349, Validation Loss: 55044, 142594.38793162518\n",
      "Epoch 149201, Training Loss: 30658, Validation Loss: 54524, 135516.5397433909\n",
      "Epoch 149301, Training Loss: 27632, Validation Loss: 53649, 123011.2712683337\n",
      "Epoch 149401, Training Loss: 28194, Validation Loss: 56212, 156471.98249933205\n",
      "Epoch 149501, Training Loss: 30414, Validation Loss: 55481, 179998.99313656366\n",
      "Epoch 149601, Training Loss: 29383, Validation Loss: 57445, 157815.3380170807\n",
      "Epoch 149701, Training Loss: 30771, Validation Loss: 52008, 187217.29715209757\n",
      "Epoch 149801, Training Loss: 28590, Validation Loss: 54166, 170118.99853078232\n",
      "Epoch 149901, Training Loss: 27950, Validation Loss: 53159, 139100.64379359773\n",
      "Epoch 150001, Training Loss: 28461, Validation Loss: 52531, 175893.20777464588\n",
      "Epoch 150101, Training Loss: 28872, Validation Loss: 52944, 147291.18602483088\n",
      "Epoch 150201, Training Loss: 30331, Validation Loss: 53515, 175289.92436064695\n",
      "Epoch 150301, Training Loss: 30927, Validation Loss: 52653, 148337.17250461495\n",
      "Epoch 150401, Training Loss: 30124, Validation Loss: 58401, 155954.27198515204\n",
      "Epoch 150501, Training Loss: 29726, Validation Loss: 53079, 130341.53105075378\n",
      "Epoch 150601, Training Loss: 30019, Validation Loss: 56370, 118161.5602428391\n",
      "Epoch 150701, Training Loss: 29208, Validation Loss: 53239, 201470.8496501634\n",
      "Epoch 150801, Training Loss: 28889, Validation Loss: 54004, 171757.25721433223\n",
      "Epoch 150901, Training Loss: 27308, Validation Loss: 52717, 127388.99363059142\n",
      "Epoch 151001, Training Loss: 28482, Validation Loss: 53901, 118781.14856755106\n",
      "Epoch 151101, Training Loss: 29701, Validation Loss: 54560, 145290.49673307754\n",
      "Epoch 151201, Training Loss: 30447, Validation Loss: 51628, 187334.98943201863\n",
      "Epoch 151301, Training Loss: 29309, Validation Loss: 50918, 134424.08147238594\n",
      "Epoch 151401, Training Loss: 29394, Validation Loss: 53655, 135024.15063811213\n",
      "Epoch 151501, Training Loss: 31290, Validation Loss: 56662, 185284.86106454735\n",
      "Epoch 151601, Training Loss: 29374, Validation Loss: 52859, 159152.93164963837\n",
      "Epoch 151701, Training Loss: 30039, Validation Loss: 51916, 151237.44216291356\n",
      "Epoch 151801, Training Loss: 30691, Validation Loss: 52710, 200380.29575098216\n",
      "Epoch 151901, Training Loss: 27231, Validation Loss: 56016, 120157.30588323435\n",
      "Epoch 152001, Training Loss: 30139, Validation Loss: 52703, 166858.91249891673\n",
      "Epoch 152101, Training Loss: 31298, Validation Loss: 55859, 170721.73874362884\n",
      "Epoch 152201, Training Loss: 28705, Validation Loss: 53162, 132355.27485459627\n",
      "Epoch 152301, Training Loss: 27736, Validation Loss: 53831, 140199.71650318138\n",
      "Epoch 152401, Training Loss: 29561, Validation Loss: 54282, 138430.15379204022\n",
      "Epoch 152501, Training Loss: 32360, Validation Loss: 55899, 209562.7264622208\n",
      "Epoch 152601, Training Loss: 28990, Validation Loss: 55031, 115894.94452130616\n",
      "Epoch 152701, Training Loss: 31368, Validation Loss: 51925, 198142.076591039\n",
      "Epoch 152801, Training Loss: 29248, Validation Loss: 55610, 202270.17770375856\n",
      "Epoch 152901, Training Loss: 27368, Validation Loss: 53905, 154743.20850810548\n",
      "Epoch 153001, Training Loss: 31357, Validation Loss: 48937, 213093.94517849348\n",
      "Epoch 153101, Training Loss: 30010, Validation Loss: 52765, 112282.38636955159\n",
      "Epoch 153201, Training Loss: 28964, Validation Loss: 53030, 172918.09226465496\n",
      "Epoch 153301, Training Loss: 28905, Validation Loss: 53020, 161559.02794385512\n",
      "Epoch 153401, Training Loss: 30913, Validation Loss: 53766, 144831.4008517109\n",
      "Epoch 153501, Training Loss: 32640, Validation Loss: 50845, 205428.21265930196\n",
      "Epoch 153601, Training Loss: 28189, Validation Loss: 52860, 129120.91815876291\n",
      "Epoch 153701, Training Loss: 30935, Validation Loss: 52376, 175716.0031510148\n",
      "Epoch 153801, Training Loss: 28379, Validation Loss: 58255, 133800.22664121274\n",
      "Epoch 153901, Training Loss: 30786, Validation Loss: 54861, 151377.80308678746\n",
      "Epoch 154001, Training Loss: 27290, Validation Loss: 51792, 131228.5671106215\n",
      "Epoch 154101, Training Loss: 26727, Validation Loss: 54098, 161855.50278822528\n",
      "Epoch 154201, Training Loss: 30182, Validation Loss: 55397, 167389.75332605452\n",
      "Epoch 154301, Training Loss: 29270, Validation Loss: 53540, 168285.81720443218\n",
      "Epoch 154401, Training Loss: 27807, Validation Loss: 52114, 147501.1045256829\n",
      "Epoch 154501, Training Loss: 30939, Validation Loss: 53604, 121846.3207168685\n",
      "Epoch 154601, Training Loss: 28289, Validation Loss: 57676, 145560.07021074928\n",
      "Epoch 154701, Training Loss: 28245, Validation Loss: 50790, 156684.10042607205\n",
      "Epoch 154801, Training Loss: 30501, Validation Loss: 52547, 136222.29223973505\n",
      "Epoch 154901, Training Loss: 29896, Validation Loss: 53432, 125782.63540604927\n",
      "Epoch 155001, Training Loss: 28932, Validation Loss: 56887, 149881.60330050343\n",
      "Epoch 155101, Training Loss: 28395, Validation Loss: 57489, 151723.50812038797\n",
      "Epoch 155201, Training Loss: 30636, Validation Loss: 55053, 185339.07815440084\n",
      "Epoch 155301, Training Loss: 29369, Validation Loss: 54825, 151271.54421957102\n",
      "Epoch 155401, Training Loss: 29015, Validation Loss: 55365, 126795.64196072922\n",
      "Epoch 155501, Training Loss: 29501, Validation Loss: 52794, 121981.94939405475\n",
      "Epoch 155601, Training Loss: 29834, Validation Loss: 55194, 198862.23195112954\n",
      "Epoch 155701, Training Loss: 29877, Validation Loss: 55216, 159444.36910463255\n",
      "Epoch 155801, Training Loss: 29150, Validation Loss: 54130, 144363.21716804686\n",
      "Epoch 155901, Training Loss: 30201, Validation Loss: 54275, 170137.65444677006\n",
      "Epoch 156001, Training Loss: 28637, Validation Loss: 52522, 153362.233155712\n",
      "Epoch 156101, Training Loss: 28795, Validation Loss: 52847, 173779.3366520781\n",
      "Epoch 156201, Training Loss: 27952, Validation Loss: 54105, 143764.92058032064\n",
      "Epoch 156301, Training Loss: 29511, Validation Loss: 52978, 179484.54289225244\n",
      "Epoch 156401, Training Loss: 28019, Validation Loss: 53681, 161420.22504518434\n",
      "Epoch 156501, Training Loss: 31005, Validation Loss: 53074, 186286.26787500843\n",
      "Epoch 156601, Training Loss: 29452, Validation Loss: 55989, 142722.1502649407\n",
      "Epoch 156701, Training Loss: 29727, Validation Loss: 50617, 157510.15146554288\n",
      "Epoch 156801, Training Loss: 29054, Validation Loss: 53398, 128823.18536277121\n",
      "Epoch 156901, Training Loss: 30076, Validation Loss: 54400, 189535.8052941117\n",
      "Epoch 157001, Training Loss: 28429, Validation Loss: 54944, 125697.6429800964\n",
      "Epoch 157101, Training Loss: 27817, Validation Loss: 53574, 149157.23957386255\n",
      "Epoch 157201, Training Loss: 28068, Validation Loss: 53705, 127700.44714522614\n",
      "Epoch 157301, Training Loss: 27610, Validation Loss: 54123, 129211.08964878495\n",
      "Epoch 157401, Training Loss: 29233, Validation Loss: 55060, 247797.8107475283\n",
      "Epoch 157501, Training Loss: 30603, Validation Loss: 56058, 185429.36245217177\n",
      "Epoch 157601, Training Loss: 28994, Validation Loss: 54218, 198505.86561090872\n",
      "Epoch 157701, Training Loss: 30628, Validation Loss: 51243, 172373.91839991414\n",
      "Epoch 157801, Training Loss: 28412, Validation Loss: 56508, 162308.5588132325\n",
      "Epoch 157901, Training Loss: 30368, Validation Loss: 55265, 179390.9720799375\n",
      "Epoch 158001, Training Loss: 31791, Validation Loss: 51345, 136895.38710191986\n",
      "Epoch 158101, Training Loss: 27921, Validation Loss: 52881, 117319.404637434\n",
      "Epoch 158201, Training Loss: 27618, Validation Loss: 53616, 133867.63841477764\n",
      "Epoch 158301, Training Loss: 29456, Validation Loss: 53167, 142802.73861054544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158401, Training Loss: 28284, Validation Loss: 53002, 114901.24211986503\n",
      "Epoch 158501, Training Loss: 29153, Validation Loss: 54502, 171780.07614342935\n",
      "Epoch 158601, Training Loss: 31235, Validation Loss: 52791, 138004.76894016707\n",
      "Epoch 158701, Training Loss: 29370, Validation Loss: 53413, 190820.08192162646\n",
      "Epoch 158801, Training Loss: 28761, Validation Loss: 54813, 186073.54255245754\n",
      "Epoch 158901, Training Loss: 28518, Validation Loss: 53320, 123393.17413775489\n",
      "Epoch 159001, Training Loss: 29527, Validation Loss: 52837, 137536.84108051754\n",
      "Epoch 159101, Training Loss: 26978, Validation Loss: 57324, 132934.62835840904\n",
      "Epoch 159201, Training Loss: 28662, Validation Loss: 55303, 160729.41734236397\n",
      "Epoch 159301, Training Loss: 31459, Validation Loss: 53850, 227871.2936412726\n",
      "Epoch 159401, Training Loss: 27963, Validation Loss: 54656, 94561.72003218836\n",
      "Epoch 159501, Training Loss: 31243, Validation Loss: 52366, 228752.71672378512\n",
      "Epoch 159601, Training Loss: 27495, Validation Loss: 52789, 106827.65623228275\n",
      "Epoch 159701, Training Loss: 29128, Validation Loss: 56154, 152995.21546935567\n",
      "Epoch 159801, Training Loss: 29491, Validation Loss: 54644, 143005.70269104556\n",
      "Epoch 159901, Training Loss: 28445, Validation Loss: 53184, 138928.62902857605\n",
      "Epoch 160001, Training Loss: 28920, Validation Loss: 53345, 222162.3162397771\n",
      "Epoch 160101, Training Loss: 29963, Validation Loss: 53700, 128735.18580740977\n",
      "Epoch 160201, Training Loss: 26763, Validation Loss: 52269, 130394.02874171776\n",
      "Epoch 160301, Training Loss: 28242, Validation Loss: 51073, 129795.42667454068\n",
      "Epoch 160401, Training Loss: 32016, Validation Loss: 53038, 202509.3652244121\n",
      "Epoch 160501, Training Loss: 30496, Validation Loss: 53251, 140333.19461837932\n",
      "Epoch 160601, Training Loss: 27741, Validation Loss: 51028, 143063.29914722478\n",
      "Epoch 160701, Training Loss: 27958, Validation Loss: 52004, 136405.34373263284\n",
      "Epoch 160801, Training Loss: 29776, Validation Loss: 51804, 134619.12551372833\n",
      "Epoch 160901, Training Loss: 28310, Validation Loss: 50434, 187191.364555698\n",
      "Epoch 161001, Training Loss: 28357, Validation Loss: 54377, 124545.74327748692\n",
      "Epoch 161101, Training Loss: 31084, Validation Loss: 51103, 156895.5630258499\n",
      "Epoch 161201, Training Loss: 31279, Validation Loss: 52931, 180774.8410157273\n",
      "Epoch 161301, Training Loss: 27894, Validation Loss: 54888, 133067.99038209338\n",
      "Epoch 161401, Training Loss: 28221, Validation Loss: 52487, 185407.22206961122\n",
      "Epoch 161501, Training Loss: 28687, Validation Loss: 55861, 144709.4101101314\n",
      "Epoch 161601, Training Loss: 29674, Validation Loss: 52716, 131880.42272247552\n",
      "Epoch 161701, Training Loss: 29575, Validation Loss: 56505, 158883.212510429\n",
      "Epoch 161801, Training Loss: 29530, Validation Loss: 53765, 156401.94858590575\n",
      "Epoch 161901, Training Loss: 29769, Validation Loss: 52708, 158627.1753416674\n",
      "Epoch 162001, Training Loss: 26946, Validation Loss: 53880, 111583.29600893869\n",
      "Epoch 162101, Training Loss: 26975, Validation Loss: 56743, 113522.72560410714\n",
      "Epoch 162201, Training Loss: 30808, Validation Loss: 52971, 127179.90180173342\n",
      "Epoch 162301, Training Loss: 28963, Validation Loss: 54127, 116819.78714089376\n",
      "Epoch 162401, Training Loss: 25377, Validation Loss: 53988, 127857.20429098506\n",
      "Epoch 162501, Training Loss: 30648, Validation Loss: 52951, 131953.8413793747\n",
      "Epoch 162601, Training Loss: 29143, Validation Loss: 55266, 166410.80738291403\n",
      "Epoch 162701, Training Loss: 28815, Validation Loss: 51157, 142019.9659080641\n",
      "Epoch 162801, Training Loss: 28979, Validation Loss: 51586, 130040.05714814346\n",
      "Epoch 162901, Training Loss: 29695, Validation Loss: 52768, 160502.1840146529\n",
      "Epoch 163001, Training Loss: 29872, Validation Loss: 52816, 206680.92066195942\n",
      "Epoch 163101, Training Loss: 30380, Validation Loss: 56838, 145214.41174331313\n",
      "Epoch 163201, Training Loss: 31644, Validation Loss: 55593, 222261.88176834342\n",
      "Epoch 163301, Training Loss: 30693, Validation Loss: 54651, 159612.47103376169\n",
      "Epoch 163401, Training Loss: 29329, Validation Loss: 52317, 155949.85926449054\n",
      "Epoch 163501, Training Loss: 30476, Validation Loss: 52679, 153727.19158873157\n",
      "Epoch 163601, Training Loss: 29867, Validation Loss: 55148, 147949.26555125447\n",
      "Epoch 163701, Training Loss: 27312, Validation Loss: 53670, 193099.61493662887\n",
      "Epoch 163801, Training Loss: 27269, Validation Loss: 52872, 129134.31732116321\n",
      "Epoch 163901, Training Loss: 30248, Validation Loss: 54622, 126998.7785213032\n",
      "Epoch 164001, Training Loss: 28992, Validation Loss: 51756, 148885.5442713195\n",
      "Epoch 164101, Training Loss: 29487, Validation Loss: 52871, 166804.5986761691\n",
      "Epoch 164201, Training Loss: 27447, Validation Loss: 54358, 132541.85384564483\n",
      "Epoch 164301, Training Loss: 28416, Validation Loss: 55141, 135415.2423724802\n",
      "Epoch 164401, Training Loss: 29103, Validation Loss: 50272, 166390.56930357966\n",
      "Epoch 164501, Training Loss: 30813, Validation Loss: 52755, 146650.26755041818\n",
      "Epoch 164601, Training Loss: 29702, Validation Loss: 53320, 137571.72965551206\n",
      "Epoch 164701, Training Loss: 28215, Validation Loss: 51999, 181884.49791550598\n",
      "Epoch 164801, Training Loss: 29711, Validation Loss: 55932, 165170.8418218892\n",
      "Epoch 164901, Training Loss: 32098, Validation Loss: 55381, 110098.08416636371\n",
      "Epoch 165001, Training Loss: 28826, Validation Loss: 52406, 157560.43406365256\n",
      "Epoch 165101, Training Loss: 28419, Validation Loss: 57934, 222607.6392277931\n",
      "Epoch 165201, Training Loss: 29607, Validation Loss: 55975, 165859.25958357743\n",
      "Epoch 165301, Training Loss: 30346, Validation Loss: 51837, 187405.20607923422\n",
      "Epoch 165401, Training Loss: 26686, Validation Loss: 53991, 162395.91752050482\n",
      "Epoch 165501, Training Loss: 30162, Validation Loss: 54150, 137821.4869274802\n",
      "Epoch 165601, Training Loss: 27861, Validation Loss: 53876, 141405.3240156757\n",
      "Epoch 165701, Training Loss: 28781, Validation Loss: 55700, 128981.76335721923\n",
      "Epoch 165801, Training Loss: 28009, Validation Loss: 54771, 146631.6354035975\n",
      "Epoch 165901, Training Loss: 28033, Validation Loss: 51559, 153428.2633761003\n",
      "Epoch 166001, Training Loss: 27821, Validation Loss: 52146, 141279.10084803103\n",
      "Epoch 166101, Training Loss: 28633, Validation Loss: 55547, 159254.89863484667\n",
      "Epoch 166201, Training Loss: 29937, Validation Loss: 57519, 180531.14332195502\n",
      "Epoch 166301, Training Loss: 29441, Validation Loss: 51936, 158680.2015704844\n",
      "Epoch 166401, Training Loss: 29965, Validation Loss: 50261, 125603.94157955784\n",
      "Epoch 166501, Training Loss: 30039, Validation Loss: 51617, 169557.793363556\n",
      "Epoch 166601, Training Loss: 28258, Validation Loss: 52755, 119980.15148677312\n",
      "Epoch 166701, Training Loss: 28424, Validation Loss: 52769, 148328.22156083642\n",
      "Epoch 166801, Training Loss: 28487, Validation Loss: 54794, 125246.52282378772\n",
      "Epoch 166901, Training Loss: 29326, Validation Loss: 53487, 165129.61378416812\n",
      "Epoch 167001, Training Loss: 28822, Validation Loss: 55399, 135804.54533143897\n",
      "Epoch 167101, Training Loss: 31596, Validation Loss: 53179, 183262.22877257378\n",
      "Epoch 167201, Training Loss: 28359, Validation Loss: 53623, 131110.4375748243\n",
      "Epoch 167301, Training Loss: 27445, Validation Loss: 53046, 162158.13218896347\n",
      "Epoch 167401, Training Loss: 29701, Validation Loss: 51173, 105481.55089860731\n",
      "Epoch 167501, Training Loss: 27414, Validation Loss: 50616, 167673.34167168068\n",
      "Epoch 167601, Training Loss: 29466, Validation Loss: 56160, 171666.91778542675\n",
      "Epoch 167701, Training Loss: 28745, Validation Loss: 53114, 178515.58730116347\n",
      "Epoch 167801, Training Loss: 29507, Validation Loss: 53785, 179678.9632781011\n",
      "Epoch 167901, Training Loss: 31964, Validation Loss: 56288, 219030.98483970927\n",
      "Epoch 168001, Training Loss: 28317, Validation Loss: 53991, 162592.39415205002\n",
      "Epoch 168101, Training Loss: 29543, Validation Loss: 53137, 208780.62092851056\n",
      "Epoch 168201, Training Loss: 28942, Validation Loss: 52878, 177347.0999517416\n",
      "Epoch 168301, Training Loss: 27856, Validation Loss: 54164, 133515.08115815662\n",
      "Epoch 168401, Training Loss: 28250, Validation Loss: 54066, 137252.60302362032\n",
      "Epoch 168501, Training Loss: 31302, Validation Loss: 55528, 208199.4969018063\n",
      "Epoch 168601, Training Loss: 29025, Validation Loss: 53543, 197228.14677850474\n",
      "Epoch 168701, Training Loss: 30069, Validation Loss: 52808, 159284.78023028397\n",
      "Epoch 168801, Training Loss: 28173, Validation Loss: 52721, 125400.43439483672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168901, Training Loss: 27347, Validation Loss: 49958, 133637.9732865301\n",
      "Epoch 169001, Training Loss: 28724, Validation Loss: 51893, 150755.69344135534\n",
      "Epoch 169101, Training Loss: 27384, Validation Loss: 54519, 148400.99995436767\n",
      "Epoch 169201, Training Loss: 28154, Validation Loss: 51751, 166214.35581082208\n",
      "Epoch 169301, Training Loss: 27876, Validation Loss: 53633, 118978.25516246837\n",
      "Epoch 169401, Training Loss: 28227, Validation Loss: 56086, 137965.16269962146\n",
      "Epoch 169501, Training Loss: 29905, Validation Loss: 50906, 132261.09955344655\n",
      "Epoch 169601, Training Loss: 28797, Validation Loss: 50959, 119585.9415152227\n",
      "Epoch 169701, Training Loss: 28029, Validation Loss: 50453, 147015.29574213602\n",
      "Epoch 169801, Training Loss: 28056, Validation Loss: 53487, 129528.10163274048\n",
      "Epoch 169901, Training Loss: 31017, Validation Loss: 54073, 154072.88705681864\n",
      "Epoch 170001, Training Loss: 29008, Validation Loss: 53812, 172224.1661383521\n",
      "Epoch 170101, Training Loss: 30393, Validation Loss: 53889, 134276.834769279\n",
      "Epoch 170201, Training Loss: 26815, Validation Loss: 53263, 111225.51767270772\n",
      "Epoch 170301, Training Loss: 28429, Validation Loss: 50932, 131273.45038776414\n",
      "Epoch 170401, Training Loss: 29397, Validation Loss: 52836, 136653.84279363748\n",
      "Epoch 170501, Training Loss: 31608, Validation Loss: 52032, 159978.0581931884\n",
      "Epoch 170601, Training Loss: 31296, Validation Loss: 55610, 249177.8750376108\n",
      "Epoch 170701, Training Loss: 27900, Validation Loss: 51519, 154822.66800977415\n",
      "Epoch 170801, Training Loss: 30065, Validation Loss: 54797, 137511.79694977112\n",
      "Epoch 170901, Training Loss: 27936, Validation Loss: 51566, 168420.75650206252\n",
      "Epoch 171001, Training Loss: 30860, Validation Loss: 56366, 158605.21718270704\n",
      "Epoch 171101, Training Loss: 29148, Validation Loss: 55280, 162688.75626906074\n",
      "Epoch 171201, Training Loss: 28688, Validation Loss: 54425, 136146.80777337772\n",
      "Epoch 171301, Training Loss: 28142, Validation Loss: 54729, 130614.63470205483\n",
      "Epoch 171401, Training Loss: 26321, Validation Loss: 53032, 133812.8551973847\n",
      "Epoch 171501, Training Loss: 28931, Validation Loss: 54426, 115887.3149755379\n",
      "Epoch 171601, Training Loss: 31748, Validation Loss: 53301, 131844.290665589\n",
      "Epoch 171701, Training Loss: 29639, Validation Loss: 56015, 156314.4114311796\n",
      "Epoch 171801, Training Loss: 29642, Validation Loss: 53016, 149609.16309481222\n",
      "Epoch 171901, Training Loss: 26695, Validation Loss: 52989, 147758.75044351164\n",
      "Epoch 172001, Training Loss: 28304, Validation Loss: 55845, 132575.749630698\n",
      "Epoch 172101, Training Loss: 29639, Validation Loss: 54771, 147120.23129706053\n",
      "Epoch 172201, Training Loss: 29929, Validation Loss: 54691, 119723.95583910821\n",
      "Epoch 172301, Training Loss: 27385, Validation Loss: 53949, 218646.23636116748\n",
      "Epoch 172401, Training Loss: 30031, Validation Loss: 54014, 132733.78421069297\n",
      "Epoch 172501, Training Loss: 28882, Validation Loss: 52927, 165225.10249578973\n",
      "Epoch 172601, Training Loss: 26700, Validation Loss: 54168, 143545.58410651225\n",
      "Epoch 172701, Training Loss: 26752, Validation Loss: 51679, 126158.88783075701\n",
      "Epoch 172801, Training Loss: 26477, Validation Loss: 52960, 119707.88088093007\n",
      "Epoch 172901, Training Loss: 27648, Validation Loss: 52980, 135773.71829502736\n",
      "Epoch 173001, Training Loss: 28023, Validation Loss: 54444, 151794.03287057683\n",
      "Epoch 173101, Training Loss: 31918, Validation Loss: 54676, 132426.7160275783\n",
      "Epoch 173201, Training Loss: 28935, Validation Loss: 52351, 166185.18287508158\n",
      "Epoch 173301, Training Loss: 27307, Validation Loss: 53965, 120213.96828292178\n",
      "Epoch 173401, Training Loss: 29018, Validation Loss: 54756, 122054.66737147816\n",
      "Epoch 173501, Training Loss: 26206, Validation Loss: 58028, 129533.43083961273\n",
      "Epoch 173601, Training Loss: 29809, Validation Loss: 52009, 192623.29270364592\n",
      "Epoch 173701, Training Loss: 28397, Validation Loss: 50985, 138824.8802725665\n",
      "Epoch 173801, Training Loss: 32542, Validation Loss: 54616, 155445.02466221998\n",
      "Epoch 173901, Training Loss: 30164, Validation Loss: 55576, 144901.58184819564\n",
      "Epoch 174001, Training Loss: 28718, Validation Loss: 54292, 142276.77903138453\n",
      "Epoch 174101, Training Loss: 27216, Validation Loss: 52902, 153726.220155016\n",
      "Epoch 174201, Training Loss: 28471, Validation Loss: 53899, 149166.67277741592\n",
      "Epoch 174301, Training Loss: 26294, Validation Loss: 55839, 125204.15603041202\n",
      "Epoch 174401, Training Loss: 29393, Validation Loss: 51731, 148098.5130188948\n",
      "Epoch 174501, Training Loss: 28154, Validation Loss: 49933, 147538.77684419483\n",
      "Epoch 174601, Training Loss: 28433, Validation Loss: 54767, 126013.75229715307\n",
      "Epoch 174701, Training Loss: 31572, Validation Loss: 54457, 138773.6556600311\n",
      "Epoch 174801, Training Loss: 29676, Validation Loss: 55450, 175323.13806578293\n",
      "Epoch 174901, Training Loss: 28031, Validation Loss: 54920, 154173.15766573246\n",
      "Epoch 175001, Training Loss: 28052, Validation Loss: 53735, 135090.65430392002\n",
      "Epoch 175101, Training Loss: 27685, Validation Loss: 50153, 141927.51166668956\n",
      "Epoch 175201, Training Loss: 28578, Validation Loss: 56721, 119518.69821438199\n",
      "Epoch 175301, Training Loss: 31929, Validation Loss: 54277, 187964.48730320355\n",
      "Epoch 175401, Training Loss: 28190, Validation Loss: 53494, 143992.1713194576\n",
      "Epoch 175501, Training Loss: 29204, Validation Loss: 51687, 145327.82276284322\n",
      "Epoch 175601, Training Loss: 30852, Validation Loss: 54814, 123921.29630082706\n",
      "Epoch 175701, Training Loss: 27868, Validation Loss: 52421, 148300.35967798333\n",
      "Epoch 175801, Training Loss: 29925, Validation Loss: 51836, 158196.636091459\n",
      "Epoch 175901, Training Loss: 30864, Validation Loss: 49969, 165705.23739388873\n",
      "Epoch 176001, Training Loss: 28191, Validation Loss: 53161, 136393.4492342199\n",
      "Epoch 176101, Training Loss: 26782, Validation Loss: 53816, 119128.28252779832\n",
      "Epoch 176201, Training Loss: 28087, Validation Loss: 54168, 140824.89826574727\n",
      "Epoch 176301, Training Loss: 26881, Validation Loss: 54522, 143116.0542959339\n",
      "Epoch 176401, Training Loss: 29291, Validation Loss: 53597, 128873.81630416507\n",
      "Epoch 176501, Training Loss: 29322, Validation Loss: 54957, 154195.55011344128\n",
      "Epoch 176601, Training Loss: 29161, Validation Loss: 51900, 151444.47530056824\n",
      "Epoch 176701, Training Loss: 29738, Validation Loss: 51198, 169440.63973586587\n",
      "Epoch 176801, Training Loss: 31221, Validation Loss: 53391, 161884.01752469552\n",
      "Epoch 176901, Training Loss: 29332, Validation Loss: 52349, 162892.55302708608\n",
      "Epoch 177001, Training Loss: 28504, Validation Loss: 54952, 125501.99164778058\n",
      "Epoch 177101, Training Loss: 29141, Validation Loss: 53087, 150467.37820022667\n",
      "Epoch 177201, Training Loss: 28495, Validation Loss: 53381, 147088.79041282213\n",
      "Epoch 177301, Training Loss: 30153, Validation Loss: 52383, 131316.8203587064\n",
      "Epoch 177401, Training Loss: 28222, Validation Loss: 54507, 128599.22213336996\n",
      "Epoch 177501, Training Loss: 31054, Validation Loss: 54464, 174230.1529018488\n",
      "Epoch 177601, Training Loss: 28264, Validation Loss: 54853, 171359.1478284072\n",
      "Epoch 177701, Training Loss: 27700, Validation Loss: 53384, 188565.97725680727\n",
      "Epoch 177801, Training Loss: 26337, Validation Loss: 54337, 134594.4204593953\n",
      "Epoch 177901, Training Loss: 28452, Validation Loss: 53434, 219795.02838873837\n",
      "Epoch 178001, Training Loss: 33164, Validation Loss: 54793, 169692.0394280304\n",
      "Epoch 178101, Training Loss: 26914, Validation Loss: 53677, 139522.62504220722\n",
      "Epoch 178201, Training Loss: 30875, Validation Loss: 54645, 159621.13334119195\n",
      "Epoch 178301, Training Loss: 29165, Validation Loss: 52792, 187266.08762560834\n",
      "Epoch 178401, Training Loss: 29113, Validation Loss: 53401, 107161.26746646354\n",
      "Epoch 178501, Training Loss: 28056, Validation Loss: 54842, 137213.7243850937\n",
      "Epoch 178601, Training Loss: 30130, Validation Loss: 55064, 157467.59670707586\n",
      "Epoch 178701, Training Loss: 30737, Validation Loss: 59212, 233208.6182148001\n",
      "Epoch 178801, Training Loss: 29508, Validation Loss: 54201, 116323.85820832259\n",
      "Epoch 178901, Training Loss: 30117, Validation Loss: 56070, 194543.7071762023\n",
      "Epoch 179001, Training Loss: 27723, Validation Loss: 55976, 126125.33266530321\n",
      "Epoch 179101, Training Loss: 30663, Validation Loss: 51761, 142166.9715039991\n",
      "Epoch 179201, Training Loss: 30070, Validation Loss: 55468, 138555.57713675871\n",
      "Epoch 179301, Training Loss: 29796, Validation Loss: 54435, 174140.04572464936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179401, Training Loss: 29212, Validation Loss: 54009, 138013.6275287757\n",
      "Epoch 179501, Training Loss: 27812, Validation Loss: 52222, 144235.57268055037\n",
      "Epoch 179601, Training Loss: 28374, Validation Loss: 54716, 145596.99847933775\n",
      "Epoch 179701, Training Loss: 26762, Validation Loss: 55173, 132161.08083667766\n",
      "Epoch 179801, Training Loss: 28455, Validation Loss: 53953, 163341.06138524134\n",
      "Epoch 179901, Training Loss: 28801, Validation Loss: 52636, 121017.50222677847\n",
      "Epoch 180001, Training Loss: 26923, Validation Loss: 51298, 161044.18758280206\n",
      "Epoch 180101, Training Loss: 27401, Validation Loss: 54697, 149796.29571798182\n",
      "Epoch 180201, Training Loss: 32170, Validation Loss: 52833, 165210.85846996953\n",
      "Epoch 180301, Training Loss: 28139, Validation Loss: 51739, 156832.3416942556\n",
      "Epoch 180401, Training Loss: 29840, Validation Loss: 54169, 113171.78678983815\n",
      "Epoch 180501, Training Loss: 29472, Validation Loss: 50149, 180052.54229523847\n",
      "Epoch 180601, Training Loss: 30676, Validation Loss: 54532, 158690.71378957524\n",
      "Epoch 180701, Training Loss: 29550, Validation Loss: 53648, 177055.7443226345\n",
      "Epoch 180801, Training Loss: 27672, Validation Loss: 50097, 159215.13530541482\n",
      "Epoch 180901, Training Loss: 28405, Validation Loss: 53833, 152374.25825439827\n",
      "Epoch 181001, Training Loss: 30928, Validation Loss: 54997, 115679.47370071911\n",
      "Epoch 181101, Training Loss: 29935, Validation Loss: 53300, 187076.9853420929\n",
      "Epoch 181201, Training Loss: 26783, Validation Loss: 50342, 134040.88703416634\n",
      "Epoch 181301, Training Loss: 28811, Validation Loss: 52485, 184586.64534788625\n",
      "Epoch 181401, Training Loss: 30043, Validation Loss: 51730, 158781.87997918428\n",
      "Epoch 181501, Training Loss: 28611, Validation Loss: 56298, 169328.06248235566\n",
      "Epoch 181601, Training Loss: 27929, Validation Loss: 55479, 168042.777837264\n",
      "Epoch 181701, Training Loss: 28103, Validation Loss: 52986, 147517.3871576039\n",
      "Epoch 181801, Training Loss: 28916, Validation Loss: 53455, 147865.3094055938\n",
      "Epoch 181901, Training Loss: 29115, Validation Loss: 52482, 149850.7179085496\n",
      "Epoch 182001, Training Loss: 29447, Validation Loss: 51169, 163959.41451255124\n",
      "Epoch 182101, Training Loss: 28657, Validation Loss: 54182, 110213.29228480991\n",
      "Epoch 182201, Training Loss: 28367, Validation Loss: 52839, 124778.40175163798\n",
      "Epoch 182301, Training Loss: 25831, Validation Loss: 54027, 151056.6003517009\n",
      "Epoch 182401, Training Loss: 30662, Validation Loss: 53589, 176100.58166839214\n",
      "Epoch 182501, Training Loss: 28370, Validation Loss: 53034, 104120.80426978727\n",
      "Epoch 182601, Training Loss: 28547, Validation Loss: 52714, 143016.50336246143\n",
      "Epoch 182701, Training Loss: 28239, Validation Loss: 53520, 162544.25302443822\n",
      "Epoch 182801, Training Loss: 28867, Validation Loss: 56671, 129045.60113405122\n",
      "Epoch 182901, Training Loss: 30483, Validation Loss: 51633, 134884.92112678758\n",
      "Epoch 183001, Training Loss: 27500, Validation Loss: 54262, 153131.7913457005\n",
      "Epoch 183101, Training Loss: 31004, Validation Loss: 51652, 207326.82152400506\n",
      "Epoch 183201, Training Loss: 28086, Validation Loss: 55522, 118446.04769807779\n",
      "Epoch 183301, Training Loss: 28297, Validation Loss: 52416, 187930.9351446144\n",
      "Epoch 183401, Training Loss: 28044, Validation Loss: 53350, 119315.67578348401\n",
      "Epoch 183501, Training Loss: 27590, Validation Loss: 53919, 127205.36622262416\n",
      "Epoch 183601, Training Loss: 29710, Validation Loss: 52739, 155978.30933821737\n",
      "Epoch 183701, Training Loss: 29532, Validation Loss: 49964, 145282.7921272526\n",
      "Epoch 183801, Training Loss: 25894, Validation Loss: 55050, 146862.93317964938\n",
      "Epoch 183901, Training Loss: 27927, Validation Loss: 52770, 127245.51322599595\n",
      "Epoch 184001, Training Loss: 26846, Validation Loss: 54166, 137158.64815066566\n",
      "Epoch 184101, Training Loss: 28247, Validation Loss: 53151, 129217.75470122858\n",
      "Epoch 184201, Training Loss: 29393, Validation Loss: 53429, 141563.35027773422\n",
      "Epoch 184301, Training Loss: 27211, Validation Loss: 57720, 127046.84535206757\n",
      "Epoch 184401, Training Loss: 28185, Validation Loss: 55257, 114469.0198890351\n",
      "Epoch 184501, Training Loss: 27282, Validation Loss: 53793, 123508.73921710777\n",
      "Epoch 184601, Training Loss: 25736, Validation Loss: 53119, 119981.1468769795\n",
      "Epoch 184701, Training Loss: 29626, Validation Loss: 50716, 176486.51291448888\n",
      "Epoch 184801, Training Loss: 30126, Validation Loss: 57457, 193716.05812281498\n",
      "Epoch 184901, Training Loss: 30926, Validation Loss: 53164, 152532.77590543823\n",
      "Epoch 185001, Training Loss: 29823, Validation Loss: 54589, 184473.94787957752\n",
      "Epoch 185101, Training Loss: 30575, Validation Loss: 55181, 155984.91537530092\n",
      "Epoch 185201, Training Loss: 27222, Validation Loss: 53103, 174219.1398814203\n",
      "Epoch 185301, Training Loss: 29095, Validation Loss: 56418, 149850.08337857912\n",
      "Epoch 185401, Training Loss: 28914, Validation Loss: 49709, 151838.6229277118\n",
      "Epoch 185501, Training Loss: 29234, Validation Loss: 54190, 141695.2602731846\n",
      "Epoch 185601, Training Loss: 27389, Validation Loss: 52218, 142850.03193245592\n",
      "Epoch 185701, Training Loss: 27341, Validation Loss: 54730, 180325.9951829234\n",
      "Epoch 185801, Training Loss: 30426, Validation Loss: 56055, 161030.95810295158\n",
      "Epoch 185901, Training Loss: 27055, Validation Loss: 52243, 133497.13624194128\n",
      "Epoch 186001, Training Loss: 26730, Validation Loss: 54133, 141164.49897167398\n",
      "Epoch 186101, Training Loss: 29975, Validation Loss: 52774, 167972.343312686\n",
      "Epoch 186201, Training Loss: 30443, Validation Loss: 55492, 118319.72431316029\n",
      "Epoch 186301, Training Loss: 27776, Validation Loss: 53987, 116761.16040035454\n",
      "Epoch 186401, Training Loss: 27319, Validation Loss: 53215, 128769.15517869464\n",
      "Epoch 186501, Training Loss: 28937, Validation Loss: 55852, 148846.42557302036\n",
      "Epoch 186601, Training Loss: 27140, Validation Loss: 55045, 158243.80353372823\n",
      "Epoch 186701, Training Loss: 26186, Validation Loss: 53823, 152531.53972612115\n",
      "Epoch 186801, Training Loss: 26453, Validation Loss: 51533, 147115.43063778835\n",
      "Epoch 186901, Training Loss: 27675, Validation Loss: 54639, 170467.78553739528\n",
      "Epoch 187001, Training Loss: 28032, Validation Loss: 54855, 157239.98239914843\n",
      "Epoch 187101, Training Loss: 27452, Validation Loss: 55003, 133187.45555346142\n",
      "Epoch 187201, Training Loss: 28754, Validation Loss: 52731, 194352.78697089167\n",
      "Epoch 187301, Training Loss: 30390, Validation Loss: 53375, 164554.78930014055\n",
      "Epoch 187401, Training Loss: 27839, Validation Loss: 53391, 133593.98168365905\n",
      "Epoch 187501, Training Loss: 28343, Validation Loss: 55595, 121849.56495461955\n",
      "Epoch 187601, Training Loss: 27517, Validation Loss: 56004, 160226.96057717953\n",
      "Epoch 187701, Training Loss: 27848, Validation Loss: 53051, 146430.80433558932\n",
      "Epoch 187801, Training Loss: 28485, Validation Loss: 54608, 138238.28354170077\n",
      "Epoch 187901, Training Loss: 26232, Validation Loss: 54789, 127620.24357287209\n",
      "Epoch 188001, Training Loss: 30190, Validation Loss: 53724, 150848.6116329159\n",
      "Epoch 188101, Training Loss: 29210, Validation Loss: 55192, 222612.78904919303\n",
      "Epoch 188201, Training Loss: 26790, Validation Loss: 58686, 143274.697821049\n",
      "Epoch 188301, Training Loss: 28094, Validation Loss: 54976, 129060.23218949111\n",
      "Epoch 188401, Training Loss: 28796, Validation Loss: 54982, 120062.02584828933\n",
      "Epoch 188501, Training Loss: 29792, Validation Loss: 56448, 162094.6246527345\n",
      "Epoch 188601, Training Loss: 28641, Validation Loss: 52389, 149297.46355043535\n",
      "Epoch 188701, Training Loss: 29909, Validation Loss: 53901, 169387.91802896556\n",
      "Epoch 188801, Training Loss: 28204, Validation Loss: 54857, 153921.3633780361\n",
      "Epoch 188901, Training Loss: 27523, Validation Loss: 55894, 146242.6781897715\n",
      "Epoch 189001, Training Loss: 27169, Validation Loss: 53978, 117530.72046584524\n",
      "Epoch 189101, Training Loss: 26777, Validation Loss: 55267, 128859.26956744202\n",
      "Epoch 189201, Training Loss: 30186, Validation Loss: 52593, 130599.1831317799\n",
      "Epoch 189301, Training Loss: 27862, Validation Loss: 53725, 123546.54509408528\n",
      "Epoch 189401, Training Loss: 28617, Validation Loss: 55482, 192462.24522809157\n",
      "Epoch 189501, Training Loss: 29912, Validation Loss: 52879, 156591.96227399193\n",
      "Epoch 189601, Training Loss: 29983, Validation Loss: 54637, 198422.49784603118\n",
      "Epoch 189701, Training Loss: 28297, Validation Loss: 51571, 178940.08496161885\n",
      "Epoch 189801, Training Loss: 27080, Validation Loss: 55783, 182478.94485800786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189901, Training Loss: 31190, Validation Loss: 51868, 171221.9654597651\n",
      "Epoch 190001, Training Loss: 29000, Validation Loss: 51270, 171995.24860240682\n",
      "Epoch 190101, Training Loss: 27372, Validation Loss: 54761, 137202.0983364009\n",
      "Epoch 190201, Training Loss: 29219, Validation Loss: 53728, 162407.23558702035\n",
      "Epoch 190301, Training Loss: 28212, Validation Loss: 53195, 161404.529010482\n",
      "Epoch 190401, Training Loss: 29927, Validation Loss: 55532, 272009.43155824445\n",
      "Epoch 190501, Training Loss: 30499, Validation Loss: 55289, 128769.76500115874\n",
      "Epoch 190601, Training Loss: 27549, Validation Loss: 56388, 137987.56053929596\n",
      "Epoch 190701, Training Loss: 26864, Validation Loss: 55595, 127097.74928895973\n",
      "Epoch 190801, Training Loss: 27926, Validation Loss: 53081, 114101.30956790618\n",
      "Epoch 190901, Training Loss: 29217, Validation Loss: 50825, 183729.81295057666\n",
      "Epoch 191001, Training Loss: 26879, Validation Loss: 53467, 137199.3576285741\n",
      "Epoch 191101, Training Loss: 29225, Validation Loss: 52762, 191010.19282022145\n",
      "Epoch 191201, Training Loss: 27691, Validation Loss: 54985, 174008.8029340597\n",
      "Epoch 191301, Training Loss: 28878, Validation Loss: 53783, 190009.07209078592\n",
      "Epoch 191401, Training Loss: 29100, Validation Loss: 54595, 169656.97113472346\n",
      "Epoch 191501, Training Loss: 27224, Validation Loss: 57890, 149997.8977452912\n",
      "Epoch 191601, Training Loss: 26186, Validation Loss: 53385, 112476.7265304111\n",
      "Epoch 191701, Training Loss: 27623, Validation Loss: 56480, 143952.5517527718\n",
      "Epoch 191801, Training Loss: 28982, Validation Loss: 52334, 144147.8972462316\n",
      "Epoch 191901, Training Loss: 27849, Validation Loss: 52259, 176013.31408096664\n",
      "Epoch 192001, Training Loss: 26234, Validation Loss: 55359, 142246.9920596308\n",
      "Epoch 192101, Training Loss: 27488, Validation Loss: 53079, 131370.2891253423\n",
      "Epoch 192201, Training Loss: 29675, Validation Loss: 53749, 152087.70705314726\n",
      "Epoch 192301, Training Loss: 29546, Validation Loss: 52148, 157740.51708764813\n",
      "Epoch 192401, Training Loss: 26554, Validation Loss: 55346, 128364.10441555183\n",
      "Epoch 192501, Training Loss: 27962, Validation Loss: 51509, 145362.87631881854\n",
      "Epoch 192601, Training Loss: 27184, Validation Loss: 53770, 129277.10854434156\n",
      "Epoch 192701, Training Loss: 29791, Validation Loss: 51972, 123624.69797695086\n",
      "Epoch 192801, Training Loss: 29977, Validation Loss: 52655, 168917.7448656242\n",
      "Epoch 192901, Training Loss: 27198, Validation Loss: 56889, 210725.03000127885\n",
      "Epoch 193001, Training Loss: 29388, Validation Loss: 53821, 133471.20992140277\n",
      "Epoch 193101, Training Loss: 27066, Validation Loss: 54754, 133495.60107364564\n",
      "Epoch 193201, Training Loss: 27218, Validation Loss: 53289, 125329.69898785041\n",
      "Epoch 193301, Training Loss: 28165, Validation Loss: 54573, 160754.110396617\n",
      "Epoch 193401, Training Loss: 26943, Validation Loss: 55893, 165013.52872865726\n",
      "Epoch 193501, Training Loss: 28467, Validation Loss: 52612, 170888.17189163092\n",
      "Epoch 193601, Training Loss: 27461, Validation Loss: 53389, 134033.23223777508\n",
      "Epoch 193701, Training Loss: 24828, Validation Loss: 55473, 107448.99882452826\n",
      "Epoch 193801, Training Loss: 31134, Validation Loss: 53581, 136225.58761488835\n",
      "Epoch 193901, Training Loss: 29526, Validation Loss: 56430, 123266.52513450374\n",
      "Epoch 194001, Training Loss: 28597, Validation Loss: 52836, 156049.74343329587\n",
      "Epoch 194101, Training Loss: 28794, Validation Loss: 52335, 151659.71300668045\n",
      "Epoch 194201, Training Loss: 27766, Validation Loss: 54021, 130876.33308263573\n",
      "Epoch 194301, Training Loss: 29745, Validation Loss: 51050, 144309.94073072422\n",
      "Epoch 194401, Training Loss: 27460, Validation Loss: 53939, 138670.20270963284\n",
      "Epoch 194501, Training Loss: 28739, Validation Loss: 51351, 132679.90131083952\n",
      "Epoch 194601, Training Loss: 28574, Validation Loss: 53963, 149509.09641925804\n",
      "Epoch 194701, Training Loss: 28390, Validation Loss: 53578, 179000.23669512503\n",
      "Epoch 194801, Training Loss: 27268, Validation Loss: 57172, 150501.21510207208\n",
      "Epoch 194901, Training Loss: 28848, Validation Loss: 56313, 204967.0020569242\n",
      "Epoch 195001, Training Loss: 26990, Validation Loss: 55875, 131560.7532776942\n",
      "Epoch 195101, Training Loss: 28702, Validation Loss: 54352, 138440.2011582154\n",
      "Epoch 195201, Training Loss: 31048, Validation Loss: 55920, 130606.70787453958\n",
      "Epoch 195301, Training Loss: 27752, Validation Loss: 52824, 142786.79217484334\n",
      "Epoch 195401, Training Loss: 30907, Validation Loss: 54078, 214098.87247144434\n",
      "Epoch 195501, Training Loss: 27327, Validation Loss: 52976, 213603.4854796632\n",
      "Epoch 195601, Training Loss: 29668, Validation Loss: 54071, 188939.7884276662\n",
      "Epoch 195701, Training Loss: 29606, Validation Loss: 54991, 164884.701520049\n",
      "Epoch 195801, Training Loss: 27953, Validation Loss: 53664, 156487.2209151851\n",
      "Epoch 195901, Training Loss: 30192, Validation Loss: 53215, 156309.860046531\n",
      "Epoch 196001, Training Loss: 27845, Validation Loss: 52531, 151969.87408144373\n",
      "Epoch 196101, Training Loss: 26266, Validation Loss: 52981, 114469.51992818608\n",
      "Epoch 196201, Training Loss: 30508, Validation Loss: 57716, 149945.80567220954\n",
      "Epoch 196301, Training Loss: 27887, Validation Loss: 56140, 161041.6091706486\n",
      "Epoch 196401, Training Loss: 31253, Validation Loss: 50531, 149845.87598896353\n",
      "Epoch 196501, Training Loss: 30393, Validation Loss: 52230, 173574.02576810375\n",
      "Epoch 196601, Training Loss: 33101, Validation Loss: 53540, 216722.98168242452\n",
      "Epoch 196701, Training Loss: 29023, Validation Loss: 54880, 121807.7814342783\n",
      "Epoch 196801, Training Loss: 31245, Validation Loss: 53996, 176313.47882364204\n",
      "Epoch 196901, Training Loss: 27932, Validation Loss: 52352, 120549.05673299449\n",
      "Epoch 197001, Training Loss: 28725, Validation Loss: 53743, 140442.64888407994\n",
      "Epoch 197101, Training Loss: 30809, Validation Loss: 52510, 122563.56930454478\n",
      "Epoch 197201, Training Loss: 28145, Validation Loss: 55084, 141222.26569902824\n",
      "Epoch 197301, Training Loss: 26874, Validation Loss: 52774, 115710.17807202887\n",
      "Epoch 197401, Training Loss: 28324, Validation Loss: 52913, 152610.0385333007\n",
      "Epoch 197501, Training Loss: 29611, Validation Loss: 53811, 172141.85526509475\n",
      "Epoch 197601, Training Loss: 27421, Validation Loss: 57262, 150112.2225525954\n",
      "Epoch 197701, Training Loss: 26244, Validation Loss: 53695, 110781.98945038253\n",
      "Epoch 197801, Training Loss: 26498, Validation Loss: 54215, 152292.31132553605\n",
      "Epoch 197901, Training Loss: 27042, Validation Loss: 56194, 158824.8087559469\n",
      "Epoch 198001, Training Loss: 28572, Validation Loss: 52242, 138226.35740364096\n",
      "Epoch 198101, Training Loss: 29232, Validation Loss: 52697, 145505.55259147446\n",
      "Epoch 198201, Training Loss: 26612, Validation Loss: 53210, 149665.0932310703\n",
      "Epoch 198301, Training Loss: 26361, Validation Loss: 54357, 132347.63094776566\n",
      "Epoch 198401, Training Loss: 27164, Validation Loss: 54997, 150447.59444682975\n",
      "Epoch 198501, Training Loss: 28594, Validation Loss: 54116, 120819.65596692113\n",
      "Epoch 198601, Training Loss: 30253, Validation Loss: 53532, 165079.54947089148\n",
      "Epoch 198701, Training Loss: 27184, Validation Loss: 50622, 156346.45413148895\n",
      "Epoch 198801, Training Loss: 28179, Validation Loss: 55088, 144401.5594233314\n",
      "Epoch 198901, Training Loss: 27924, Validation Loss: 54724, 124253.90348559845\n",
      "Epoch 199001, Training Loss: 31811, Validation Loss: 55286, 156303.41309793954\n",
      "Epoch 199101, Training Loss: 28300, Validation Loss: 56079, 180786.5434720734\n",
      "Epoch 199201, Training Loss: 26503, Validation Loss: 51681, 132903.61641686998\n",
      "Epoch 199301, Training Loss: 26774, Validation Loss: 54372, 117512.73725751658\n",
      "Epoch 199401, Training Loss: 28620, Validation Loss: 52287, 184827.08877737206\n",
      "Epoch 199501, Training Loss: 28789, Validation Loss: 54587, 156731.8569184716\n",
      "Epoch 199601, Training Loss: 29255, Validation Loss: 55695, 185689.61753235655\n",
      "Epoch 199701, Training Loss: 27904, Validation Loss: 52893, 151919.30675010558\n",
      "Epoch 199801, Training Loss: 27934, Validation Loss: 54003, 155581.0420646052\n",
      "Epoch 199901, Training Loss: 30607, Validation Loss: 54203, 171570.53022069263\n",
      "Epoch 200001, Training Loss: 30096, Validation Loss: 53235, 136468.52908656735\n",
      "Epoch 200101, Training Loss: 29039, Validation Loss: 55308, 160722.0454107171\n",
      "Epoch 200201, Training Loss: 28231, Validation Loss: 55758, 154641.65353341284\n",
      "Epoch 200301, Training Loss: 27816, Validation Loss: 55116, 139533.00259694763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200401, Training Loss: 28310, Validation Loss: 51903, 153920.93376236226\n",
      "Epoch 200501, Training Loss: 30390, Validation Loss: 57683, 194607.37751750267\n",
      "Epoch 200601, Training Loss: 27870, Validation Loss: 53108, 146645.2765614007\n",
      "Epoch 200701, Training Loss: 28516, Validation Loss: 54150, 221881.1901490478\n",
      "Epoch 200801, Training Loss: 27725, Validation Loss: 52367, 159466.1414081483\n",
      "Epoch 200901, Training Loss: 28085, Validation Loss: 56234, 177730.0584667297\n",
      "Epoch 201001, Training Loss: 30443, Validation Loss: 53883, 182619.25262686028\n",
      "Epoch 201101, Training Loss: 27459, Validation Loss: 56162, 151554.02257054372\n",
      "Epoch 201201, Training Loss: 28275, Validation Loss: 53468, 117715.07776935505\n",
      "Epoch 201301, Training Loss: 29720, Validation Loss: 53767, 151177.476785825\n",
      "Epoch 201401, Training Loss: 26972, Validation Loss: 53083, 165167.83411013152\n",
      "Epoch 201501, Training Loss: 28469, Validation Loss: 52467, 158596.64855544866\n",
      "Epoch 201601, Training Loss: 26611, Validation Loss: 53117, 152965.55241242328\n",
      "Epoch 201701, Training Loss: 26263, Validation Loss: 53958, 156148.91267728855\n",
      "Epoch 201801, Training Loss: 28137, Validation Loss: 53023, 151363.59770490206\n",
      "Epoch 201901, Training Loss: 28602, Validation Loss: 52480, 152682.63174417082\n",
      "Epoch 202001, Training Loss: 30156, Validation Loss: 55570, 152527.28319723342\n",
      "Epoch 202101, Training Loss: 28116, Validation Loss: 53332, 161655.95590416895\n",
      "Epoch 202201, Training Loss: 25890, Validation Loss: 54583, 132791.67727583947\n",
      "Epoch 202301, Training Loss: 30754, Validation Loss: 52947, 149055.38220243566\n",
      "Epoch 202401, Training Loss: 27797, Validation Loss: 52232, 168551.88405664565\n",
      "Epoch 202501, Training Loss: 26433, Validation Loss: 54831, 148084.55257130976\n",
      "Epoch 202601, Training Loss: 26355, Validation Loss: 55455, 132028.83135882346\n",
      "Epoch 202701, Training Loss: 28112, Validation Loss: 53203, 172010.116200568\n",
      "Epoch 202801, Training Loss: 29920, Validation Loss: 56019, 166910.60621002028\n",
      "Epoch 202901, Training Loss: 28852, Validation Loss: 55619, 149925.91771948052\n",
      "Epoch 203001, Training Loss: 28008, Validation Loss: 54826, 137927.28703693274\n",
      "Epoch 203101, Training Loss: 28557, Validation Loss: 51678, 168657.3097213977\n",
      "Epoch 203201, Training Loss: 28761, Validation Loss: 51539, 172376.38624937038\n",
      "Epoch 203301, Training Loss: 29643, Validation Loss: 53884, 180547.38429080695\n",
      "Epoch 203401, Training Loss: 28147, Validation Loss: 52156, 123979.0193669345\n",
      "Epoch 203501, Training Loss: 28663, Validation Loss: 54517, 123905.64500755956\n",
      "Epoch 203601, Training Loss: 28710, Validation Loss: 53502, 128309.38089946534\n",
      "Epoch 203701, Training Loss: 28489, Validation Loss: 51611, 131030.64209981349\n",
      "Epoch 203801, Training Loss: 24557, Validation Loss: 55310, 119891.99785516608\n",
      "Epoch 203901, Training Loss: 28994, Validation Loss: 53036, 149490.1030616106\n",
      "Epoch 204001, Training Loss: 29088, Validation Loss: 53094, 131081.62338333146\n",
      "Epoch 204101, Training Loss: 27665, Validation Loss: 54993, 172340.405959673\n",
      "Epoch 204201, Training Loss: 27877, Validation Loss: 54237, 164612.39942369427\n",
      "Epoch 204301, Training Loss: 27174, Validation Loss: 56817, 189404.82551428163\n",
      "Epoch 204401, Training Loss: 28689, Validation Loss: 53465, 192781.22674771585\n",
      "Epoch 204501, Training Loss: 27153, Validation Loss: 52067, 167888.40270882024\n",
      "Epoch 204601, Training Loss: 26363, Validation Loss: 51785, 120512.34320936231\n",
      "Epoch 204701, Training Loss: 28060, Validation Loss: 52869, 133962.58668306924\n",
      "Epoch 204801, Training Loss: 28573, Validation Loss: 51921, 134922.41782920406\n",
      "Epoch 204901, Training Loss: 27266, Validation Loss: 52933, 159946.21822257934\n",
      "Epoch 205001, Training Loss: 26355, Validation Loss: 53041, 118268.37883598212\n",
      "Epoch 205101, Training Loss: 27434, Validation Loss: 53499, 168024.52387002623\n",
      "Epoch 205201, Training Loss: 31022, Validation Loss: 52160, 146003.0012363072\n",
      "Epoch 205301, Training Loss: 31276, Validation Loss: 53505, 143398.62461852052\n",
      "Epoch 205401, Training Loss: 29017, Validation Loss: 52820, 127466.44167595844\n",
      "Epoch 205501, Training Loss: 28508, Validation Loss: 51389, 155335.3156900012\n",
      "Epoch 205601, Training Loss: 26602, Validation Loss: 55909, 132372.17587177316\n",
      "Epoch 205701, Training Loss: 28068, Validation Loss: 56872, 145405.2236848015\n",
      "Epoch 205801, Training Loss: 27738, Validation Loss: 54342, 162985.7670347461\n",
      "Epoch 205901, Training Loss: 29289, Validation Loss: 56950, 146492.24231990436\n",
      "Epoch 206001, Training Loss: 29924, Validation Loss: 50858, 154028.99022696223\n",
      "Epoch 206101, Training Loss: 26593, Validation Loss: 54919, 159528.5232424173\n",
      "Epoch 206201, Training Loss: 29371, Validation Loss: 52671, 130930.19493569275\n",
      "Epoch 206301, Training Loss: 28503, Validation Loss: 51382, 132600.67905362693\n",
      "Epoch 206401, Training Loss: 28143, Validation Loss: 56836, 143844.60236843064\n",
      "Epoch 206501, Training Loss: 28511, Validation Loss: 55491, 177672.96720495867\n",
      "Epoch 206601, Training Loss: 26595, Validation Loss: 52808, 114561.84566973586\n",
      "Epoch 206701, Training Loss: 28183, Validation Loss: 53491, 130621.54510272435\n",
      "Epoch 206801, Training Loss: 28058, Validation Loss: 52442, 151171.6695468294\n",
      "Epoch 206901, Training Loss: 29006, Validation Loss: 51430, 168557.3640684143\n",
      "Epoch 207001, Training Loss: 27129, Validation Loss: 53902, 154847.37741029172\n",
      "Epoch 207101, Training Loss: 27327, Validation Loss: 54450, 99093.29926057038\n",
      "Epoch 207201, Training Loss: 27699, Validation Loss: 53803, 177167.0085743553\n",
      "Epoch 207301, Training Loss: 26612, Validation Loss: 51111, 131838.81667079087\n",
      "Epoch 207401, Training Loss: 27865, Validation Loss: 53757, 142101.8448847327\n",
      "Epoch 207501, Training Loss: 26244, Validation Loss: 52102, 116786.04650871737\n",
      "Epoch 207601, Training Loss: 26425, Validation Loss: 56034, 134710.28294414317\n",
      "Epoch 207701, Training Loss: 31285, Validation Loss: 53241, 187109.65112048804\n",
      "Epoch 207801, Training Loss: 30204, Validation Loss: 54884, 151515.04511120895\n",
      "Epoch 207901, Training Loss: 28293, Validation Loss: 54317, 154840.84225404102\n",
      "Epoch 208001, Training Loss: 29299, Validation Loss: 56839, 161513.02601203727\n",
      "Epoch 208101, Training Loss: 27812, Validation Loss: 51513, 135404.85194807342\n",
      "Epoch 208201, Training Loss: 26617, Validation Loss: 51719, 169013.57538152943\n",
      "Epoch 208301, Training Loss: 26114, Validation Loss: 54112, 159631.8967079233\n",
      "Epoch 208401, Training Loss: 27871, Validation Loss: 52861, 136293.38763533335\n",
      "Epoch 208501, Training Loss: 28003, Validation Loss: 54623, 176644.37906598262\n",
      "Epoch 208601, Training Loss: 26233, Validation Loss: 53745, 124514.27667456011\n",
      "Epoch 208701, Training Loss: 28168, Validation Loss: 56477, 140640.13931795725\n",
      "Epoch 208801, Training Loss: 28304, Validation Loss: 54516, 138620.29870958944\n",
      "Epoch 208901, Training Loss: 27288, Validation Loss: 54558, 150687.84272922357\n",
      "Epoch 209001, Training Loss: 29258, Validation Loss: 54913, 143424.89688565957\n",
      "Epoch 209101, Training Loss: 27792, Validation Loss: 53074, 122081.5215278084\n",
      "Epoch 209201, Training Loss: 29074, Validation Loss: 51605, 168863.6828239184\n",
      "Epoch 209301, Training Loss: 27031, Validation Loss: 54458, 121553.26086509046\n",
      "Epoch 209401, Training Loss: 27904, Validation Loss: 57608, 179653.0598128667\n",
      "Epoch 209501, Training Loss: 26943, Validation Loss: 52699, 143308.35759528732\n",
      "Epoch 209601, Training Loss: 29578, Validation Loss: 56359, 148515.30435688575\n",
      "Epoch 209701, Training Loss: 28418, Validation Loss: 53365, 165836.36948238642\n",
      "Epoch 209801, Training Loss: 28379, Validation Loss: 55143, 147130.99625426895\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         l1_losses\u001b[38;5;241m.\u001b[39mappend(criterion_abs(outputs, prices_viewed))\n\u001b[1;32m     51\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader))  \u001b[38;5;66;03m# Average loss for this epoch\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m l1_mean_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml1_losses\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(l1_losses)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Print epoch's summary\u001b[39;00m\n\u001b[1;32m     54\u001b[0m epochs_suc\u001b[38;5;241m.\u001b[39mappend(epoch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=9e-3,\n",
    "    weight_decay=3e-4\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999999, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
