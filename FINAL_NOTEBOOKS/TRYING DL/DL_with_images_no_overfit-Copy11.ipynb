{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.18):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 32\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Dropout(0.01),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.1, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 32165, Validation Loss: 42276, 437897.68338317826\n",
      "Epoch 101, Training Loss: 31279, Validation Loss: 41893, 374602.61641755054\n",
      "Epoch 201, Training Loss: 31801, Validation Loss: 41937, 483587.3804459777\n",
      "Epoch 301, Training Loss: 31445, Validation Loss: 41965, 587991.6689905802\n",
      "Epoch 401, Training Loss: 31279, Validation Loss: 42080, 506938.10011778114\n",
      "Epoch 501, Training Loss: 30985, Validation Loss: 41876, 536733.7570460455\n",
      "Epoch 601, Training Loss: 31468, Validation Loss: 41993, 542780.3244692485\n",
      "Epoch 701, Training Loss: 31576, Validation Loss: 42330, 561904.3480680472\n",
      "Epoch 801, Training Loss: 31416, Validation Loss: 42203, 501779.37233989174\n",
      "Epoch 901, Training Loss: 31478, Validation Loss: 41834, 333938.665860009\n",
      "Epoch 1001, Training Loss: 31486, Validation Loss: 42181, 430222.5523783317\n",
      "Epoch 1101, Training Loss: 30600, Validation Loss: 41897, 243963.2182260701\n",
      "Epoch 1201, Training Loss: 30897, Validation Loss: 42032, 526832.4444273484\n",
      "Epoch 1301, Training Loss: 31987, Validation Loss: 41903, 572784.8104782705\n",
      "Epoch 1401, Training Loss: 32600, Validation Loss: 41956, 617726.0504347888\n",
      "Epoch 1501, Training Loss: 30795, Validation Loss: 42339, 590551.5299343627\n",
      "Epoch 1601, Training Loss: 30954, Validation Loss: 41879, 500547.79778331163\n",
      "Epoch 1701, Training Loss: 31502, Validation Loss: 41870, 335862.70075094653\n",
      "Epoch 1801, Training Loss: 31230, Validation Loss: 41884, 478292.37089419644\n",
      "Epoch 1901, Training Loss: 32991, Validation Loss: 41750, 949728.4580971868\n",
      "Epoch 2001, Training Loss: 31719, Validation Loss: 41735, 412256.78735662263\n",
      "Epoch 2101, Training Loss: 31071, Validation Loss: 42405, 743665.2108217358\n",
      "Epoch 2201, Training Loss: 31347, Validation Loss: 42023, 696513.1568834968\n",
      "Epoch 2301, Training Loss: 31571, Validation Loss: 41850, 544768.5465444239\n",
      "Epoch 2401, Training Loss: 31329, Validation Loss: 41710, 677585.4261463097\n",
      "Epoch 2501, Training Loss: 31738, Validation Loss: 41957, 396875.70783980406\n",
      "Epoch 2601, Training Loss: 31438, Validation Loss: 41723, 799346.7161036258\n",
      "Epoch 2701, Training Loss: 31469, Validation Loss: 41580, 678824.0962941019\n",
      "Epoch 2801, Training Loss: 32001, Validation Loss: 42226, 675538.0128560585\n",
      "Epoch 2901, Training Loss: 30708, Validation Loss: 41726, 420944.79753484734\n",
      "Epoch 3001, Training Loss: 31512, Validation Loss: 41963, 681895.9798467968\n",
      "Epoch 3101, Training Loss: 31414, Validation Loss: 41615, 692365.5434450107\n",
      "Epoch 3201, Training Loss: 31280, Validation Loss: 41970, 418943.8026753189\n",
      "Epoch 3301, Training Loss: 31068, Validation Loss: 41515, 722718.7430597347\n",
      "Epoch 3401, Training Loss: 31233, Validation Loss: 41568, 513002.5281436294\n",
      "Epoch 3501, Training Loss: 31791, Validation Loss: 41909, 713804.4095526494\n",
      "Epoch 3601, Training Loss: 31471, Validation Loss: 42125, 593027.1187381026\n",
      "Epoch 3701, Training Loss: 31400, Validation Loss: 42049, 675854.4036228455\n",
      "Epoch 3801, Training Loss: 31431, Validation Loss: 41979, 891337.2392222097\n",
      "Epoch 3901, Training Loss: 31241, Validation Loss: 41983, 726496.7636455887\n",
      "Epoch 4001, Training Loss: 31538, Validation Loss: 42048, 373148.5596037694\n",
      "Epoch 4101, Training Loss: 31634, Validation Loss: 41679, 861131.0308138648\n",
      "Epoch 4201, Training Loss: 31278, Validation Loss: 41511, 445910.45035107184\n",
      "Epoch 4301, Training Loss: 31849, Validation Loss: 41529, 570600.0964003094\n",
      "Epoch 4401, Training Loss: 31430, Validation Loss: 41863, 516861.32230697974\n",
      "Epoch 4501, Training Loss: 30228, Validation Loss: 41771, 609065.3812971445\n",
      "Epoch 4601, Training Loss: 30824, Validation Loss: 42358, 581407.5359331848\n",
      "Epoch 4701, Training Loss: 30762, Validation Loss: 41740, 493528.30726775166\n",
      "Epoch 4801, Training Loss: 31208, Validation Loss: 41793, 636202.6000328505\n",
      "Epoch 4901, Training Loss: 31217, Validation Loss: 41852, 517770.0259368178\n",
      "Epoch 5001, Training Loss: 31860, Validation Loss: 41959, 615511.1048820333\n",
      "Epoch 5101, Training Loss: 31151, Validation Loss: 41961, 862917.178507863\n",
      "Epoch 5201, Training Loss: 31785, Validation Loss: 41861, 394647.9892759491\n",
      "Epoch 5301, Training Loss: 30677, Validation Loss: 41729, 483094.6352233503\n",
      "Epoch 5401, Training Loss: 32125, Validation Loss: 42311, 880238.1391713434\n",
      "Epoch 5501, Training Loss: 30459, Validation Loss: 42130, 470920.15746008157\n",
      "Epoch 5601, Training Loss: 31125, Validation Loss: 42054, 685128.759825565\n",
      "Epoch 5701, Training Loss: 32154, Validation Loss: 41814, 453177.3437951971\n",
      "Epoch 5801, Training Loss: 30748, Validation Loss: 41744, 309532.11551464594\n",
      "Epoch 5901, Training Loss: 31365, Validation Loss: 41945, 477872.4228229644\n",
      "Epoch 6001, Training Loss: 30543, Validation Loss: 42059, 900758.5816021467\n",
      "Epoch 6101, Training Loss: 31450, Validation Loss: 41622, 1020395.3292525494\n",
      "Epoch 6201, Training Loss: 31034, Validation Loss: 41801, 522969.2975254074\n",
      "Epoch 6301, Training Loss: 31310, Validation Loss: 41763, 821609.5413104378\n",
      "Epoch 6401, Training Loss: 32133, Validation Loss: 41817, 907286.4436942366\n",
      "Epoch 6501, Training Loss: 31332, Validation Loss: 42117, 749566.5156987387\n",
      "Epoch 6601, Training Loss: 30921, Validation Loss: 41905, 427035.8842716562\n",
      "Epoch 6701, Training Loss: 30650, Validation Loss: 42011, 427116.2206312887\n",
      "Epoch 6801, Training Loss: 30494, Validation Loss: 41843, 970551.4616777311\n",
      "Epoch 6901, Training Loss: 31003, Validation Loss: 41900, 500500.9747484183\n",
      "Epoch 7001, Training Loss: 31494, Validation Loss: 41734, 672372.331803688\n",
      "Epoch 7101, Training Loss: 30774, Validation Loss: 41955, 529917.3044719873\n",
      "Epoch 7201, Training Loss: 30950, Validation Loss: 41793, 398437.2566754633\n",
      "Epoch 7301, Training Loss: 31580, Validation Loss: 42253, 723255.7270563154\n",
      "Epoch 7401, Training Loss: 31294, Validation Loss: 41891, 270607.5621576883\n",
      "Epoch 7501, Training Loss: 31223, Validation Loss: 41861, 339191.50235813705\n",
      "Epoch 7601, Training Loss: 31231, Validation Loss: 42171, 507831.7192828022\n",
      "Epoch 7701, Training Loss: 31271, Validation Loss: 41842, 695130.2996694547\n",
      "Epoch 7801, Training Loss: 30722, Validation Loss: 41952, 408824.24854700954\n",
      "Epoch 7901, Training Loss: 31742, Validation Loss: 42259, 606677.5714906584\n",
      "Epoch 8001, Training Loss: 31763, Validation Loss: 41988, 704630.3010380648\n",
      "Epoch 8101, Training Loss: 31018, Validation Loss: 41519, 768921.4705792784\n",
      "Epoch 8201, Training Loss: 31519, Validation Loss: 42067, 583178.8523707623\n",
      "Epoch 8301, Training Loss: 30970, Validation Loss: 41972, 522546.61964048946\n",
      "Epoch 8401, Training Loss: 31250, Validation Loss: 41710, 435268.68528435385\n",
      "Epoch 8501, Training Loss: 31969, Validation Loss: 42016, 1365397.377895758\n",
      "Epoch 8601, Training Loss: 31500, Validation Loss: 42033, 531063.8308322734\n",
      "Epoch 8701, Training Loss: 31027, Validation Loss: 42022, 1113644.7059765214\n",
      "Epoch 8801, Training Loss: 30787, Validation Loss: 42247, 474706.1232444475\n",
      "Epoch 8901, Training Loss: 30880, Validation Loss: 42024, 478313.24638928164\n",
      "Epoch 9001, Training Loss: 30647, Validation Loss: 41804, 389536.94004862936\n",
      "Epoch 9101, Training Loss: 31562, Validation Loss: 42160, 762952.6571467515\n",
      "Epoch 9201, Training Loss: 30596, Validation Loss: 41965, 710281.0167704864\n",
      "Epoch 9301, Training Loss: 31443, Validation Loss: 41851, 574177.9672207658\n",
      "Epoch 9401, Training Loss: 30965, Validation Loss: 42109, 681532.4636973912\n",
      "Epoch 9501, Training Loss: 31699, Validation Loss: 42279, 827203.8144342912\n",
      "Epoch 9601, Training Loss: 31008, Validation Loss: 41920, 530745.9018924002\n",
      "Epoch 9701, Training Loss: 30821, Validation Loss: 41893, 474185.188818422\n",
      "Epoch 9801, Training Loss: 32399, Validation Loss: 41789, 977858.4993544841\n",
      "Epoch 9901, Training Loss: 31061, Validation Loss: 42036, 898767.6354260372\n",
      "Epoch 10001, Training Loss: 30750, Validation Loss: 41922, 351696.65031552163\n",
      "Epoch 10101, Training Loss: 31355, Validation Loss: 41750, 666441.2534638479\n",
      "Epoch 10201, Training Loss: 31114, Validation Loss: 41870, 526537.503700102\n",
      "Epoch 10301, Training Loss: 31000, Validation Loss: 41898, 623390.5351081389\n",
      "Epoch 10401, Training Loss: 32232, Validation Loss: 42602, 1063001.5173194022\n",
      "Epoch 10501, Training Loss: 30568, Validation Loss: 41779, 551841.1518599561\n",
      "Epoch 10601, Training Loss: 30939, Validation Loss: 41804, 357512.3600398536\n",
      "Epoch 10701, Training Loss: 30998, Validation Loss: 41938, 319315.41327521816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10801, Training Loss: 31629, Validation Loss: 42119, 1032734.7375985411\n",
      "Epoch 10901, Training Loss: 31605, Validation Loss: 41857, 753019.5123561736\n",
      "Epoch 11001, Training Loss: 31118, Validation Loss: 41860, 449214.02879497834\n",
      "Epoch 11101, Training Loss: 31363, Validation Loss: 41582, 789938.5353313305\n",
      "Epoch 11201, Training Loss: 30741, Validation Loss: 42026, 679258.934755355\n",
      "Epoch 11301, Training Loss: 31422, Validation Loss: 41665, 716804.3539988119\n",
      "Epoch 11401, Training Loss: 31079, Validation Loss: 41995, 470441.674722189\n",
      "Epoch 11501, Training Loss: 31316, Validation Loss: 42294, 545431.4754620875\n",
      "Epoch 11601, Training Loss: 31352, Validation Loss: 42061, 484076.54099178704\n",
      "Epoch 11701, Training Loss: 31338, Validation Loss: 41782, 868765.7204533253\n",
      "Epoch 11801, Training Loss: 30474, Validation Loss: 41887, 943872.5895313093\n",
      "Epoch 11901, Training Loss: 32067, Validation Loss: 42625, 1131767.0287534175\n",
      "Epoch 12001, Training Loss: 30703, Validation Loss: 42569, 696511.5747538905\n",
      "Epoch 12101, Training Loss: 31342, Validation Loss: 42272, 805720.5318967681\n",
      "Epoch 12201, Training Loss: 31109, Validation Loss: 42100, 768237.5776915802\n",
      "Epoch 12301, Training Loss: 31279, Validation Loss: 41889, 519035.37504556583\n",
      "Epoch 12401, Training Loss: 31222, Validation Loss: 41982, 442943.892614272\n",
      "Epoch 12501, Training Loss: 30775, Validation Loss: 41868, 509114.6706119017\n",
      "Epoch 12601, Training Loss: 31200, Validation Loss: 42018, 824592.2560064855\n",
      "Epoch 12701, Training Loss: 30807, Validation Loss: 41943, 551747.5958702916\n",
      "Epoch 12801, Training Loss: 30258, Validation Loss: 42382, 398588.1128549649\n",
      "Epoch 12901, Training Loss: 31478, Validation Loss: 42121, 592624.2928223809\n",
      "Epoch 13001, Training Loss: 30943, Validation Loss: 42011, 431759.40888790746\n",
      "Epoch 13101, Training Loss: 31341, Validation Loss: 41999, 741981.9139273055\n",
      "Epoch 13201, Training Loss: 31927, Validation Loss: 41960, 822347.7662294741\n",
      "Epoch 13301, Training Loss: 31938, Validation Loss: 42389, 1021392.4216829023\n",
      "Epoch 13401, Training Loss: 32198, Validation Loss: 41949, 562012.2994101105\n",
      "Epoch 13501, Training Loss: 31125, Validation Loss: 41625, 870246.52614865\n",
      "Epoch 13601, Training Loss: 30574, Validation Loss: 41956, 662074.9455922908\n",
      "Epoch 13701, Training Loss: 31556, Validation Loss: 41865, 701324.1356234333\n",
      "Epoch 13801, Training Loss: 31541, Validation Loss: 42166, 804536.561100959\n",
      "Epoch 13901, Training Loss: 30411, Validation Loss: 41649, 628149.3827201526\n",
      "Epoch 14001, Training Loss: 31525, Validation Loss: 41830, 580088.0930519145\n",
      "Epoch 14101, Training Loss: 30957, Validation Loss: 42160, 547097.5338786461\n",
      "Epoch 14201, Training Loss: 31014, Validation Loss: 42153, 830136.1308462512\n",
      "Epoch 14301, Training Loss: 30810, Validation Loss: 42149, 617589.0991729299\n",
      "Epoch 14401, Training Loss: 31324, Validation Loss: 42274, 477030.5532292618\n",
      "Epoch 14501, Training Loss: 30415, Validation Loss: 42111, 589801.5676800756\n",
      "Epoch 14601, Training Loss: 31132, Validation Loss: 42145, 628269.805720282\n",
      "Epoch 14701, Training Loss: 31157, Validation Loss: 42011, 619359.7311200566\n",
      "Epoch 14801, Training Loss: 31343, Validation Loss: 41796, 812566.8814152357\n",
      "Epoch 14901, Training Loss: 30589, Validation Loss: 42320, 581515.7871794222\n",
      "Epoch 15001, Training Loss: 31933, Validation Loss: 41841, 901753.96629222\n",
      "Epoch 15101, Training Loss: 30860, Validation Loss: 41920, 1033610.077559624\n",
      "Epoch 15201, Training Loss: 31168, Validation Loss: 42205, 651586.1914690357\n",
      "Epoch 15301, Training Loss: 31670, Validation Loss: 42060, 602367.0402446348\n",
      "Epoch 15401, Training Loss: 31158, Validation Loss: 41647, 487347.9079178685\n",
      "Epoch 15501, Training Loss: 31046, Validation Loss: 41920, 479093.097820978\n",
      "Epoch 15601, Training Loss: 30588, Validation Loss: 42191, 584543.5425549748\n",
      "Epoch 15701, Training Loss: 30775, Validation Loss: 41935, 817370.7159792257\n",
      "Epoch 15801, Training Loss: 31281, Validation Loss: 41946, 640023.661284833\n",
      "Epoch 15901, Training Loss: 31588, Validation Loss: 42176, 660769.4280916981\n",
      "Epoch 16001, Training Loss: 31443, Validation Loss: 42079, 663008.4206508731\n",
      "Epoch 16101, Training Loss: 30959, Validation Loss: 42114, 662239.2169302448\n",
      "Epoch 16201, Training Loss: 30999, Validation Loss: 41934, 868055.9739451931\n",
      "Epoch 16301, Training Loss: 30704, Validation Loss: 41963, 823813.6884565117\n",
      "Epoch 16401, Training Loss: 30848, Validation Loss: 42667, 769636.7400137001\n",
      "Epoch 16501, Training Loss: 30808, Validation Loss: 42117, 280352.46638238354\n",
      "Epoch 16601, Training Loss: 31339, Validation Loss: 42637, 583806.8065085348\n",
      "Epoch 16701, Training Loss: 30689, Validation Loss: 41787, 714793.4468090305\n",
      "Epoch 16801, Training Loss: 30816, Validation Loss: 42056, 716752.433195349\n",
      "Epoch 16901, Training Loss: 31474, Validation Loss: 42222, 995885.6465746714\n",
      "Epoch 17001, Training Loss: 31180, Validation Loss: 42171, 1117119.732758593\n",
      "Epoch 17101, Training Loss: 31002, Validation Loss: 42037, 749954.0771486026\n",
      "Epoch 17201, Training Loss: 31249, Validation Loss: 42112, 553482.377355164\n",
      "Epoch 17301, Training Loss: 31331, Validation Loss: 42027, 601219.9408337025\n",
      "Epoch 17401, Training Loss: 31301, Validation Loss: 42191, 575261.8271531173\n",
      "Epoch 17501, Training Loss: 30499, Validation Loss: 42281, 760145.1908111575\n",
      "Epoch 17601, Training Loss: 30939, Validation Loss: 42253, 948348.4457628641\n",
      "Epoch 17701, Training Loss: 31758, Validation Loss: 41954, 1022341.5752302093\n",
      "Epoch 17801, Training Loss: 30810, Validation Loss: 41845, 754077.0416100174\n",
      "Epoch 17901, Training Loss: 30407, Validation Loss: 42824, 745572.9698762019\n",
      "Epoch 18001, Training Loss: 31915, Validation Loss: 42008, 511451.6972710659\n",
      "Epoch 18101, Training Loss: 30767, Validation Loss: 42004, 482440.90289170604\n",
      "Epoch 18201, Training Loss: 31496, Validation Loss: 42321, 356071.255790723\n",
      "Epoch 18301, Training Loss: 30866, Validation Loss: 42257, 764345.9243438676\n",
      "Epoch 18401, Training Loss: 30726, Validation Loss: 42024, 789691.4055512289\n",
      "Epoch 18501, Training Loss: 31091, Validation Loss: 42101, 728879.8977926297\n",
      "Epoch 18601, Training Loss: 30975, Validation Loss: 42293, 567085.5112438776\n",
      "Epoch 18701, Training Loss: 30923, Validation Loss: 42342, 878273.9495569256\n",
      "Epoch 18801, Training Loss: 31136, Validation Loss: 41842, 1066485.7516842536\n",
      "Epoch 18901, Training Loss: 31168, Validation Loss: 42251, 443303.0202825447\n",
      "Epoch 19001, Training Loss: 30730, Validation Loss: 42146, 705237.8486704149\n",
      "Epoch 19101, Training Loss: 31518, Validation Loss: 42034, 557371.7696536947\n",
      "Epoch 19201, Training Loss: 31312, Validation Loss: 42239, 795037.3193414547\n",
      "Epoch 19301, Training Loss: 31674, Validation Loss: 42472, 1004515.2843989102\n",
      "Epoch 19401, Training Loss: 30996, Validation Loss: 42278, 582608.8106186062\n",
      "Epoch 19501, Training Loss: 31093, Validation Loss: 42112, 412605.81630398904\n",
      "Epoch 19601, Training Loss: 31565, Validation Loss: 42223, 1139090.3764925194\n",
      "Epoch 19701, Training Loss: 30749, Validation Loss: 42066, 497103.9798919539\n",
      "Epoch 19801, Training Loss: 31844, Validation Loss: 42108, 732558.3913750896\n",
      "Epoch 19901, Training Loss: 30641, Validation Loss: 42060, 390339.9673973407\n",
      "Epoch 20001, Training Loss: 31439, Validation Loss: 42076, 863264.922450902\n",
      "Epoch 20101, Training Loss: 31159, Validation Loss: 41983, 829488.6344503362\n",
      "Epoch 20201, Training Loss: 30911, Validation Loss: 42090, 530329.126179362\n",
      "Epoch 20301, Training Loss: 30890, Validation Loss: 41987, 829582.8882007491\n",
      "Epoch 20401, Training Loss: 31575, Validation Loss: 41857, 617537.6820102794\n",
      "Epoch 20501, Training Loss: 31410, Validation Loss: 42359, 461860.8091107112\n",
      "Epoch 20601, Training Loss: 31039, Validation Loss: 42021, 676500.0318136355\n",
      "Epoch 20701, Training Loss: 31167, Validation Loss: 42547, 790712.2864697075\n",
      "Epoch 20801, Training Loss: 30708, Validation Loss: 41992, 801287.1613879108\n",
      "Epoch 20901, Training Loss: 31222, Validation Loss: 42126, 414507.41790084\n",
      "Epoch 21001, Training Loss: 30310, Validation Loss: 42136, 925114.3422818674\n",
      "Epoch 21101, Training Loss: 31281, Validation Loss: 42208, 719678.5203763534\n",
      "Epoch 21201, Training Loss: 30003, Validation Loss: 42077, 440792.927278115\n",
      "Epoch 21301, Training Loss: 30518, Validation Loss: 42205, 494394.62802908354\n",
      "Epoch 21401, Training Loss: 30352, Validation Loss: 41978, 608320.7220802738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21501, Training Loss: 31575, Validation Loss: 42414, 936088.0345840035\n",
      "Epoch 21601, Training Loss: 30778, Validation Loss: 42211, 581719.3969061744\n",
      "Epoch 21701, Training Loss: 31820, Validation Loss: 42121, 1205134.3162629807\n",
      "Epoch 21801, Training Loss: 31065, Validation Loss: 42408, 436599.7481660602\n",
      "Epoch 21901, Training Loss: 30874, Validation Loss: 42288, 626788.901886286\n",
      "Epoch 22001, Training Loss: 30617, Validation Loss: 42176, 643305.7673697585\n",
      "Epoch 22101, Training Loss: 30653, Validation Loss: 42486, 612291.8190690164\n",
      "Epoch 22201, Training Loss: 30903, Validation Loss: 41967, 570269.1611705819\n",
      "Epoch 22301, Training Loss: 30651, Validation Loss: 41839, 795292.577522894\n",
      "Epoch 22401, Training Loss: 30860, Validation Loss: 41709, 964765.1804871847\n",
      "Epoch 22501, Training Loss: 31122, Validation Loss: 42107, 790067.0945843743\n",
      "Epoch 22601, Training Loss: 30604, Validation Loss: 42293, 526161.7644963367\n",
      "Epoch 22701, Training Loss: 30825, Validation Loss: 42290, 601649.5889623697\n",
      "Epoch 22801, Training Loss: 30790, Validation Loss: 42216, 729612.5684518153\n",
      "Epoch 22901, Training Loss: 30963, Validation Loss: 42038, 371424.61280300666\n",
      "Epoch 23001, Training Loss: 31403, Validation Loss: 42048, 898711.5357874112\n",
      "Epoch 23101, Training Loss: 31013, Validation Loss: 41956, 852339.7741637666\n",
      "Epoch 23201, Training Loss: 31512, Validation Loss: 42404, 939580.5344414656\n",
      "Epoch 23301, Training Loss: 31400, Validation Loss: 42124, 592367.6966748389\n",
      "Epoch 23401, Training Loss: 30839, Validation Loss: 42079, 777651.9096116008\n",
      "Epoch 23501, Training Loss: 32133, Validation Loss: 42256, 442238.667121901\n",
      "Epoch 23601, Training Loss: 31130, Validation Loss: 42355, 676983.3022024573\n",
      "Epoch 23701, Training Loss: 30712, Validation Loss: 42270, 968260.9807463255\n",
      "Epoch 23801, Training Loss: 30998, Validation Loss: 42203, 666526.7341802559\n",
      "Epoch 23901, Training Loss: 30399, Validation Loss: 42395, 797065.6335514409\n",
      "Epoch 24001, Training Loss: 31405, Validation Loss: 42208, 683274.6342289338\n",
      "Epoch 24101, Training Loss: 31298, Validation Loss: 42063, 966039.2923348272\n",
      "Epoch 24201, Training Loss: 31236, Validation Loss: 41844, 416716.5481436074\n",
      "Epoch 24301, Training Loss: 30859, Validation Loss: 42550, 512536.00675729004\n",
      "Epoch 24401, Training Loss: 31447, Validation Loss: 42268, 517353.26951608557\n",
      "Epoch 24501, Training Loss: 31354, Validation Loss: 42167, 838607.1486514322\n",
      "Epoch 24601, Training Loss: 30736, Validation Loss: 42073, 635187.6719290431\n",
      "Epoch 24701, Training Loss: 31241, Validation Loss: 42227, 743004.5001921665\n",
      "Epoch 24801, Training Loss: 30502, Validation Loss: 42498, 554577.2834915906\n",
      "Epoch 24901, Training Loss: 31636, Validation Loss: 42165, 1221929.2544591636\n",
      "Epoch 25001, Training Loss: 30665, Validation Loss: 42188, 981805.0633210493\n",
      "Epoch 25101, Training Loss: 31341, Validation Loss: 41975, 767084.1668151717\n",
      "Epoch 25201, Training Loss: 30292, Validation Loss: 42426, 489988.71274247154\n",
      "Epoch 25301, Training Loss: 31158, Validation Loss: 41740, 806662.2266877083\n",
      "Epoch 25401, Training Loss: 30468, Validation Loss: 42480, 838937.4159864403\n",
      "Epoch 25501, Training Loss: 31383, Validation Loss: 42063, 582203.773493731\n",
      "Epoch 25601, Training Loss: 31454, Validation Loss: 42434, 918181.8247284953\n",
      "Epoch 25701, Training Loss: 30572, Validation Loss: 42189, 731799.876400408\n",
      "Epoch 25801, Training Loss: 31260, Validation Loss: 42052, 545140.2122441537\n",
      "Epoch 25901, Training Loss: 31271, Validation Loss: 41924, 1184100.9190701675\n",
      "Epoch 26001, Training Loss: 30726, Validation Loss: 42498, 604458.2618176993\n",
      "Epoch 26101, Training Loss: 30576, Validation Loss: 42466, 425847.53928651\n",
      "Epoch 26201, Training Loss: 31849, Validation Loss: 42205, 1030933.1262345576\n",
      "Epoch 26301, Training Loss: 30357, Validation Loss: 42440, 385576.4652919883\n",
      "Epoch 26401, Training Loss: 31969, Validation Loss: 42410, 373059.03707811894\n",
      "Epoch 26501, Training Loss: 30792, Validation Loss: 42095, 610339.969715628\n",
      "Epoch 26601, Training Loss: 30985, Validation Loss: 42197, 472732.9702182058\n",
      "Epoch 26701, Training Loss: 32165, Validation Loss: 42030, 447437.92226142663\n",
      "Epoch 26801, Training Loss: 30587, Validation Loss: 41786, 861032.9217387446\n",
      "Epoch 26901, Training Loss: 30967, Validation Loss: 42169, 832126.9782827982\n",
      "Epoch 27001, Training Loss: 30789, Validation Loss: 42142, 537813.1170676312\n",
      "Epoch 27101, Training Loss: 31116, Validation Loss: 42535, 845855.6965022032\n",
      "Epoch 27201, Training Loss: 31065, Validation Loss: 42222, 734193.1120457248\n",
      "Epoch 27301, Training Loss: 31330, Validation Loss: 42464, 649430.270434567\n",
      "Epoch 27401, Training Loss: 31417, Validation Loss: 42284, 431700.8978460795\n",
      "Epoch 27501, Training Loss: 30600, Validation Loss: 42178, 861162.2202896607\n",
      "Epoch 27601, Training Loss: 31215, Validation Loss: 42287, 585127.9607598187\n",
      "Epoch 27701, Training Loss: 31410, Validation Loss: 42310, 985437.141091868\n",
      "Epoch 27801, Training Loss: 31723, Validation Loss: 42291, 638742.7573399408\n",
      "Epoch 27901, Training Loss: 31603, Validation Loss: 42215, 1163312.3592450079\n",
      "Epoch 28001, Training Loss: 31108, Validation Loss: 42354, 1098467.3353576441\n",
      "Epoch 28101, Training Loss: 30879, Validation Loss: 42046, 433403.69435985194\n",
      "Epoch 28201, Training Loss: 30400, Validation Loss: 42459, 694023.7963906864\n",
      "Epoch 28301, Training Loss: 32145, Validation Loss: 42617, 950725.5321197349\n",
      "Epoch 28401, Training Loss: 30571, Validation Loss: 42083, 582048.6897420955\n",
      "Epoch 28501, Training Loss: 31201, Validation Loss: 41754, 719745.3092394074\n",
      "Epoch 28601, Training Loss: 30925, Validation Loss: 42682, 761840.9439047979\n",
      "Epoch 28701, Training Loss: 31238, Validation Loss: 42527, 621220.8459810792\n",
      "Epoch 28801, Training Loss: 31037, Validation Loss: 42188, 595581.0495304436\n",
      "Epoch 28901, Training Loss: 31134, Validation Loss: 42418, 821550.8747911415\n",
      "Epoch 29001, Training Loss: 30722, Validation Loss: 42559, 732087.6997698873\n",
      "Epoch 29101, Training Loss: 30886, Validation Loss: 42269, 668046.2235377372\n",
      "Epoch 29201, Training Loss: 31157, Validation Loss: 42662, 680482.3720555956\n",
      "Epoch 29301, Training Loss: 30755, Validation Loss: 42140, 439030.3512570479\n",
      "Epoch 29401, Training Loss: 31167, Validation Loss: 41917, 951131.7998099035\n",
      "Epoch 29501, Training Loss: 30191, Validation Loss: 42524, 564430.1345258738\n",
      "Epoch 29601, Training Loss: 30847, Validation Loss: 42268, 897558.2161616419\n",
      "Epoch 29701, Training Loss: 30886, Validation Loss: 42452, 725991.7464603083\n",
      "Epoch 29801, Training Loss: 31882, Validation Loss: 42762, 818037.7209764837\n",
      "Epoch 29901, Training Loss: 31134, Validation Loss: 42592, 852676.0345867655\n",
      "Epoch 30001, Training Loss: 30980, Validation Loss: 42636, 1058340.3365250544\n",
      "Epoch 30101, Training Loss: 31923, Validation Loss: 42199, 903319.0843564951\n",
      "Epoch 30201, Training Loss: 31125, Validation Loss: 42079, 769516.1369913468\n",
      "Epoch 30301, Training Loss: 31910, Validation Loss: 42366, 1193971.3734671958\n",
      "Epoch 30401, Training Loss: 30948, Validation Loss: 42349, 674396.4580523118\n",
      "Epoch 30501, Training Loss: 30949, Validation Loss: 42419, 1004818.0356479908\n",
      "Epoch 30601, Training Loss: 31181, Validation Loss: 42387, 564263.6098466712\n",
      "Epoch 30701, Training Loss: 31662, Validation Loss: 42136, 797069.8988509706\n",
      "Epoch 30801, Training Loss: 30789, Validation Loss: 42171, 682425.4206906927\n",
      "Epoch 30901, Training Loss: 30061, Validation Loss: 42404, 442213.27904061007\n",
      "Epoch 31001, Training Loss: 31269, Validation Loss: 42619, 630889.4821221371\n",
      "Epoch 31101, Training Loss: 30567, Validation Loss: 42346, 623515.9061128282\n",
      "Epoch 31201, Training Loss: 30387, Validation Loss: 42521, 667904.0356135814\n",
      "Epoch 31301, Training Loss: 31171, Validation Loss: 42667, 379015.5809071517\n",
      "Epoch 31401, Training Loss: 30050, Validation Loss: 42638, 284074.7988412454\n",
      "Epoch 31501, Training Loss: 30979, Validation Loss: 42058, 1091869.6882531852\n",
      "Epoch 31601, Training Loss: 30191, Validation Loss: 42517, 582949.46508412\n",
      "Epoch 31701, Training Loss: 30607, Validation Loss: 42210, 559871.706216501\n",
      "Epoch 31801, Training Loss: 31103, Validation Loss: 42603, 764307.0345941099\n",
      "Epoch 31901, Training Loss: 30350, Validation Loss: 42457, 649294.5089784896\n",
      "Epoch 32001, Training Loss: 31057, Validation Loss: 42304, 665356.8409274814\n",
      "Epoch 32101, Training Loss: 30702, Validation Loss: 42509, 638625.5031096677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32201, Training Loss: 30838, Validation Loss: 42324, 479827.8120811215\n",
      "Epoch 32301, Training Loss: 30771, Validation Loss: 42355, 594080.6293253623\n",
      "Epoch 32401, Training Loss: 30936, Validation Loss: 42216, 774688.9549125468\n",
      "Epoch 32501, Training Loss: 31583, Validation Loss: 42527, 485889.76714849565\n",
      "Epoch 32601, Training Loss: 31829, Validation Loss: 42209, 665046.8611159164\n",
      "Epoch 32701, Training Loss: 31790, Validation Loss: 42137, 947680.2656645523\n",
      "Epoch 32801, Training Loss: 31219, Validation Loss: 42374, 1230582.582260212\n",
      "Epoch 32901, Training Loss: 30341, Validation Loss: 42572, 715353.9163383471\n",
      "Epoch 33001, Training Loss: 31149, Validation Loss: 42399, 506650.30385517416\n",
      "Epoch 33101, Training Loss: 31154, Validation Loss: 42378, 1235226.996771582\n",
      "Epoch 33201, Training Loss: 30696, Validation Loss: 42684, 491587.24726365553\n",
      "Epoch 33301, Training Loss: 30481, Validation Loss: 42562, 563558.0814606233\n",
      "Epoch 33401, Training Loss: 31128, Validation Loss: 42408, 455654.8847529949\n",
      "Epoch 33501, Training Loss: 31001, Validation Loss: 42301, 970018.346166565\n",
      "Epoch 33601, Training Loss: 30854, Validation Loss: 42823, 1187895.892324764\n",
      "Epoch 33701, Training Loss: 30447, Validation Loss: 42364, 441191.23214697686\n",
      "Epoch 33801, Training Loss: 31190, Validation Loss: 42541, 410188.49166809407\n",
      "Epoch 33901, Training Loss: 31821, Validation Loss: 42365, 366080.1334379892\n",
      "Epoch 34001, Training Loss: 31777, Validation Loss: 42280, 925526.9732813603\n",
      "Epoch 34101, Training Loss: 30730, Validation Loss: 42511, 592195.82473589\n",
      "Epoch 34201, Training Loss: 30636, Validation Loss: 42160, 799772.6972652704\n",
      "Epoch 34301, Training Loss: 31137, Validation Loss: 42498, 630452.3254042809\n",
      "Epoch 34401, Training Loss: 31093, Validation Loss: 42494, 773721.1611518841\n",
      "Epoch 34501, Training Loss: 30639, Validation Loss: 42626, 498000.7100356858\n",
      "Epoch 34601, Training Loss: 30786, Validation Loss: 42580, 466485.83340043906\n",
      "Epoch 34701, Training Loss: 30393, Validation Loss: 42240, 1055689.8068173632\n",
      "Epoch 34801, Training Loss: 31216, Validation Loss: 42414, 841598.2222601337\n",
      "Epoch 34901, Training Loss: 31543, Validation Loss: 42195, 681503.4719302969\n",
      "Epoch 35001, Training Loss: 32011, Validation Loss: 42738, 1171366.846037157\n",
      "Epoch 35101, Training Loss: 30488, Validation Loss: 42667, 876736.1316097128\n",
      "Epoch 35201, Training Loss: 31102, Validation Loss: 42096, 648225.1371728784\n",
      "Epoch 35301, Training Loss: 31319, Validation Loss: 42312, 931945.0472266006\n",
      "Epoch 35401, Training Loss: 30523, Validation Loss: 42274, 397857.4099539662\n",
      "Epoch 35501, Training Loss: 30813, Validation Loss: 42128, 1075681.6394788944\n",
      "Epoch 35601, Training Loss: 31448, Validation Loss: 42629, 869973.9986974327\n",
      "Epoch 35701, Training Loss: 30948, Validation Loss: 42369, 657106.0869808024\n",
      "Epoch 35801, Training Loss: 31420, Validation Loss: 42387, 1157820.4223632817\n",
      "Epoch 35901, Training Loss: 30185, Validation Loss: 42203, 797138.7440525478\n",
      "Epoch 36001, Training Loss: 31347, Validation Loss: 42385, 1236778.6218428803\n",
      "Epoch 36101, Training Loss: 30789, Validation Loss: 42166, 965070.8742980075\n",
      "Epoch 36201, Training Loss: 31391, Validation Loss: 42832, 1134661.2323197508\n",
      "Epoch 36301, Training Loss: 31447, Validation Loss: 42064, 861731.8695190751\n",
      "Epoch 36401, Training Loss: 30873, Validation Loss: 42321, 637345.0928570431\n",
      "Epoch 36501, Training Loss: 30930, Validation Loss: 42575, 517373.48024601396\n",
      "Epoch 36601, Training Loss: 31163, Validation Loss: 42408, 756397.3110241053\n",
      "Epoch 36701, Training Loss: 31089, Validation Loss: 42589, 705602.4196071621\n",
      "Epoch 36801, Training Loss: 30432, Validation Loss: 42342, 631234.3019266856\n",
      "Epoch 36901, Training Loss: 30391, Validation Loss: 42336, 1044676.5875499476\n",
      "Epoch 37001, Training Loss: 31280, Validation Loss: 42693, 723545.0418858078\n",
      "Epoch 37101, Training Loss: 31423, Validation Loss: 42362, 1215002.7279736437\n",
      "Epoch 37201, Training Loss: 30374, Validation Loss: 42750, 1174757.5025108913\n",
      "Epoch 37301, Training Loss: 30469, Validation Loss: 42626, 982580.7488826814\n",
      "Epoch 37401, Training Loss: 30151, Validation Loss: 42498, 842620.6063328186\n",
      "Epoch 37501, Training Loss: 31444, Validation Loss: 42283, 866076.7389806426\n",
      "Epoch 37601, Training Loss: 30633, Validation Loss: 42482, 676442.8900417953\n",
      "Epoch 37701, Training Loss: 31274, Validation Loss: 42415, 926843.0325532545\n",
      "Epoch 37801, Training Loss: 30586, Validation Loss: 42124, 585032.6274355109\n",
      "Epoch 37901, Training Loss: 31208, Validation Loss: 42528, 684242.9586897678\n",
      "Epoch 38001, Training Loss: 31171, Validation Loss: 42519, 961742.4621493856\n",
      "Epoch 38101, Training Loss: 31045, Validation Loss: 42686, 578657.0648363072\n",
      "Epoch 38201, Training Loss: 30972, Validation Loss: 42499, 806475.7646847212\n",
      "Epoch 38301, Training Loss: 30643, Validation Loss: 42538, 628010.3542838801\n",
      "Epoch 38401, Training Loss: 30441, Validation Loss: 42995, 1141652.1158529802\n",
      "Epoch 38501, Training Loss: 30531, Validation Loss: 42451, 444995.647030691\n",
      "Epoch 38601, Training Loss: 31690, Validation Loss: 42532, 846736.8040911965\n",
      "Epoch 38701, Training Loss: 31200, Validation Loss: 42267, 716209.3796530764\n",
      "Epoch 38801, Training Loss: 30548, Validation Loss: 42219, 369186.9692078443\n",
      "Epoch 38901, Training Loss: 30324, Validation Loss: 42405, 605781.7736954477\n",
      "Epoch 39001, Training Loss: 30670, Validation Loss: 42382, 424752.10846598545\n",
      "Epoch 39101, Training Loss: 30728, Validation Loss: 42452, 747437.0337646253\n",
      "Epoch 39201, Training Loss: 29932, Validation Loss: 42776, 675355.6003867997\n",
      "Epoch 39301, Training Loss: 30695, Validation Loss: 42170, 842067.0358872829\n",
      "Epoch 39401, Training Loss: 30926, Validation Loss: 42345, 836515.7169291268\n",
      "Epoch 39501, Training Loss: 30521, Validation Loss: 42452, 458955.42112745345\n",
      "Epoch 39601, Training Loss: 31355, Validation Loss: 42564, 812369.0585255191\n",
      "Epoch 39701, Training Loss: 31528, Validation Loss: 42220, 820236.5499144802\n",
      "Epoch 39801, Training Loss: 31218, Validation Loss: 42333, 1007095.8822886981\n",
      "Epoch 39901, Training Loss: 30781, Validation Loss: 42373, 545743.1010800531\n",
      "Epoch 40001, Training Loss: 30416, Validation Loss: 42365, 685803.9442885676\n",
      "Epoch 40101, Training Loss: 31705, Validation Loss: 42162, 1463792.549844406\n",
      "Epoch 40201, Training Loss: 30288, Validation Loss: 42290, 468590.13852366776\n",
      "Epoch 40301, Training Loss: 31153, Validation Loss: 42550, 1049611.574762728\n",
      "Epoch 40401, Training Loss: 30822, Validation Loss: 42580, 397915.49329643365\n",
      "Epoch 40501, Training Loss: 30868, Validation Loss: 42477, 1265580.225643135\n",
      "Epoch 40601, Training Loss: 30617, Validation Loss: 42738, 514938.68318491796\n",
      "Epoch 40701, Training Loss: 30439, Validation Loss: 42595, 710469.2916215734\n",
      "Epoch 40801, Training Loss: 31142, Validation Loss: 42396, 883078.8661620069\n",
      "Epoch 40901, Training Loss: 31169, Validation Loss: 42556, 1032383.1994072637\n",
      "Epoch 41001, Training Loss: 30702, Validation Loss: 42573, 644903.6171876546\n",
      "Epoch 41101, Training Loss: 30639, Validation Loss: 42853, 529477.5322816919\n",
      "Epoch 41201, Training Loss: 30685, Validation Loss: 42529, 516278.0965721699\n",
      "Epoch 41301, Training Loss: 30212, Validation Loss: 42340, 1140576.2199219912\n",
      "Epoch 41401, Training Loss: 30371, Validation Loss: 42669, 411544.3467284232\n",
      "Epoch 41501, Training Loss: 31404, Validation Loss: 42250, 1220852.8014505415\n",
      "Epoch 41601, Training Loss: 30600, Validation Loss: 42395, 670568.8391217245\n",
      "Epoch 41701, Training Loss: 30276, Validation Loss: 42606, 499029.89469177404\n",
      "Epoch 41801, Training Loss: 31025, Validation Loss: 42031, 1014193.0177607882\n",
      "Epoch 41901, Training Loss: 29949, Validation Loss: 42705, 868887.8582660296\n",
      "Epoch 42001, Training Loss: 30745, Validation Loss: 42375, 423471.49296894827\n",
      "Epoch 42101, Training Loss: 30701, Validation Loss: 42295, 829071.1952054232\n",
      "Epoch 42201, Training Loss: 31240, Validation Loss: 42439, 1093535.3336547143\n",
      "Epoch 42301, Training Loss: 31099, Validation Loss: 42334, 588721.3360987756\n",
      "Epoch 42401, Training Loss: 31154, Validation Loss: 42785, 1133168.0108557933\n",
      "Epoch 42501, Training Loss: 30694, Validation Loss: 42573, 594610.9017635667\n",
      "Epoch 42601, Training Loss: 31236, Validation Loss: 42463, 640098.0891932844\n",
      "Epoch 42701, Training Loss: 30234, Validation Loss: 42618, 536896.9320671654\n",
      "Epoch 42801, Training Loss: 30851, Validation Loss: 42476, 1237783.004023103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42901, Training Loss: 31209, Validation Loss: 42383, 1144711.0776301045\n",
      "Epoch 43001, Training Loss: 30891, Validation Loss: 42537, 463873.8777855798\n",
      "Epoch 43101, Training Loss: 30976, Validation Loss: 42421, 607066.268080538\n",
      "Epoch 43201, Training Loss: 31039, Validation Loss: 42222, 647428.0312514767\n",
      "Epoch 43301, Training Loss: 30758, Validation Loss: 42353, 573040.4948218723\n",
      "Epoch 43401, Training Loss: 30184, Validation Loss: 42325, 449668.87467630516\n",
      "Epoch 43501, Training Loss: 30774, Validation Loss: 42747, 730612.4392633733\n",
      "Epoch 43601, Training Loss: 31550, Validation Loss: 42439, 899708.8447182985\n",
      "Epoch 43701, Training Loss: 31085, Validation Loss: 42553, 784840.1410329266\n",
      "Epoch 43801, Training Loss: 30638, Validation Loss: 42494, 533202.9416798225\n",
      "Epoch 43901, Training Loss: 31292, Validation Loss: 42181, 1055689.8320680282\n",
      "Epoch 44001, Training Loss: 31070, Validation Loss: 42562, 1026163.8894970064\n",
      "Epoch 44101, Training Loss: 30392, Validation Loss: 42836, 839246.0826751258\n",
      "Epoch 44201, Training Loss: 31136, Validation Loss: 42652, 927103.9040026887\n",
      "Epoch 44301, Training Loss: 30831, Validation Loss: 42748, 885801.7451997902\n",
      "Epoch 44401, Training Loss: 30268, Validation Loss: 42356, 429719.62336533365\n",
      "Epoch 44501, Training Loss: 31410, Validation Loss: 42503, 1109122.6594737002\n",
      "Epoch 44601, Training Loss: 30711, Validation Loss: 42450, 632567.995248268\n",
      "Epoch 44701, Training Loss: 31582, Validation Loss: 42189, 1104522.355421232\n",
      "Epoch 44801, Training Loss: 31034, Validation Loss: 42695, 445816.70774412656\n",
      "Epoch 44901, Training Loss: 30194, Validation Loss: 42798, 731663.4447685379\n",
      "Epoch 45001, Training Loss: 30106, Validation Loss: 42475, 798335.868179165\n",
      "Epoch 45101, Training Loss: 31453, Validation Loss: 42502, 740184.14323962\n",
      "Epoch 45201, Training Loss: 30348, Validation Loss: 42754, 1131715.464950054\n",
      "Epoch 45301, Training Loss: 30214, Validation Loss: 42747, 523919.526960107\n",
      "Epoch 45401, Training Loss: 31053, Validation Loss: 42696, 1277519.7503763898\n",
      "Epoch 45501, Training Loss: 30855, Validation Loss: 42643, 885543.0025727047\n",
      "Epoch 45601, Training Loss: 30461, Validation Loss: 42491, 817134.9185789631\n",
      "Epoch 45701, Training Loss: 31584, Validation Loss: 42865, 1372420.4020400757\n",
      "Epoch 45801, Training Loss: 31685, Validation Loss: 42503, 1095918.019941354\n",
      "Epoch 45901, Training Loss: 30852, Validation Loss: 42316, 658421.7279781603\n",
      "Epoch 46001, Training Loss: 30993, Validation Loss: 42827, 1030283.7517303191\n",
      "Epoch 46101, Training Loss: 31374, Validation Loss: 42547, 1106305.0572013003\n",
      "Epoch 46201, Training Loss: 31782, Validation Loss: 42342, 1216653.3677873027\n",
      "Epoch 46301, Training Loss: 30656, Validation Loss: 43052, 842883.0975282032\n",
      "Epoch 46401, Training Loss: 31062, Validation Loss: 42651, 405536.2423131975\n",
      "Epoch 46501, Training Loss: 30152, Validation Loss: 42475, 915646.4996320734\n",
      "Epoch 46601, Training Loss: 30929, Validation Loss: 42934, 930233.4120880769\n",
      "Epoch 46701, Training Loss: 30414, Validation Loss: 42619, 757747.4764208824\n",
      "Epoch 46801, Training Loss: 30198, Validation Loss: 42643, 773414.3034418505\n",
      "Epoch 46901, Training Loss: 30674, Validation Loss: 42632, 744161.3539460987\n",
      "Epoch 47001, Training Loss: 30942, Validation Loss: 42513, 1024564.7797838086\n",
      "Epoch 47101, Training Loss: 30488, Validation Loss: 42489, 731028.808686028\n",
      "Epoch 47201, Training Loss: 31069, Validation Loss: 42526, 1151449.900336348\n",
      "Epoch 47301, Training Loss: 31452, Validation Loss: 42236, 1150547.129336233\n",
      "Epoch 47401, Training Loss: 31113, Validation Loss: 42733, 742739.9668337832\n",
      "Epoch 47501, Training Loss: 31085, Validation Loss: 42716, 329661.6631641718\n",
      "Epoch 47601, Training Loss: 31411, Validation Loss: 42478, 782362.9708728844\n",
      "Epoch 47701, Training Loss: 30895, Validation Loss: 42561, 754873.9243853582\n",
      "Epoch 47801, Training Loss: 30577, Validation Loss: 42415, 780490.5449818149\n",
      "Epoch 47901, Training Loss: 31166, Validation Loss: 42473, 795606.223692045\n",
      "Epoch 48001, Training Loss: 30666, Validation Loss: 42527, 1071020.8775520266\n",
      "Epoch 48101, Training Loss: 30779, Validation Loss: 42683, 949756.4445394119\n",
      "Epoch 48201, Training Loss: 31591, Validation Loss: 42967, 882233.2697869543\n",
      "Epoch 48301, Training Loss: 30334, Validation Loss: 42756, 623037.0489890218\n",
      "Epoch 48401, Training Loss: 30956, Validation Loss: 42733, 862083.1017347384\n",
      "Epoch 48501, Training Loss: 31192, Validation Loss: 42386, 1023904.9602929762\n",
      "Epoch 48601, Training Loss: 31573, Validation Loss: 42908, 1110796.1195734532\n",
      "Epoch 48701, Training Loss: 30501, Validation Loss: 43411, 1054078.4714618872\n",
      "Epoch 48801, Training Loss: 30536, Validation Loss: 42713, 981235.4598234637\n",
      "Epoch 48901, Training Loss: 31466, Validation Loss: 42411, 567030.2124949007\n",
      "Epoch 49001, Training Loss: 30253, Validation Loss: 42485, 883002.1380583348\n",
      "Epoch 49101, Training Loss: 30505, Validation Loss: 42170, 908968.5881513049\n",
      "Epoch 49201, Training Loss: 31727, Validation Loss: 42835, 923847.004269828\n",
      "Epoch 49301, Training Loss: 31483, Validation Loss: 42641, 816184.9816688878\n",
      "Epoch 49401, Training Loss: 30853, Validation Loss: 42639, 443748.9437015486\n",
      "Epoch 49501, Training Loss: 30291, Validation Loss: 42635, 874408.1134126305\n",
      "Epoch 49601, Training Loss: 30545, Validation Loss: 42798, 462610.05802976526\n",
      "Epoch 49701, Training Loss: 30056, Validation Loss: 42563, 1149770.4691807535\n",
      "Epoch 49801, Training Loss: 30579, Validation Loss: 42820, 924097.3255161073\n",
      "Epoch 49901, Training Loss: 31059, Validation Loss: 42710, 630669.7388893671\n",
      "Epoch 50001, Training Loss: 30434, Validation Loss: 42290, 707000.8203621175\n",
      "Epoch 50101, Training Loss: 31468, Validation Loss: 42486, 1201707.8684180796\n",
      "Epoch 50201, Training Loss: 29644, Validation Loss: 42794, 825441.0589224218\n",
      "Epoch 50301, Training Loss: 30016, Validation Loss: 42605, 901550.6262654128\n",
      "Epoch 50401, Training Loss: 30729, Validation Loss: 42880, 799994.8804600826\n",
      "Epoch 50501, Training Loss: 31144, Validation Loss: 43051, 787133.9423717257\n",
      "Epoch 50601, Training Loss: 31089, Validation Loss: 42616, 687862.08028148\n",
      "Epoch 50701, Training Loss: 32370, Validation Loss: 42771, 1325933.2811703917\n",
      "Epoch 50801, Training Loss: 30631, Validation Loss: 42860, 700102.3646976812\n",
      "Epoch 50901, Training Loss: 30986, Validation Loss: 42911, 1160415.7297739338\n",
      "Epoch 51001, Training Loss: 30810, Validation Loss: 42345, 680756.7566495562\n",
      "Epoch 51101, Training Loss: 30465, Validation Loss: 42933, 1034728.5997163408\n",
      "Epoch 51201, Training Loss: 30491, Validation Loss: 42382, 514405.7580963063\n",
      "Epoch 51301, Training Loss: 31152, Validation Loss: 42746, 1125894.958033488\n",
      "Epoch 51401, Training Loss: 30777, Validation Loss: 42831, 982572.1071038629\n",
      "Epoch 51501, Training Loss: 30691, Validation Loss: 42633, 487304.0666606268\n",
      "Epoch 51601, Training Loss: 30782, Validation Loss: 42700, 1293604.4133385108\n",
      "Epoch 51701, Training Loss: 30139, Validation Loss: 42692, 620081.6090385794\n",
      "Epoch 51801, Training Loss: 31661, Validation Loss: 43308, 1291542.631410259\n",
      "Epoch 51901, Training Loss: 30917, Validation Loss: 42786, 609194.8996628581\n",
      "Epoch 52001, Training Loss: 30643, Validation Loss: 42680, 617915.0495475798\n",
      "Epoch 52101, Training Loss: 30823, Validation Loss: 42488, 767224.7607664131\n",
      "Epoch 52201, Training Loss: 32698, Validation Loss: 43172, 1548019.8098026689\n",
      "Epoch 52301, Training Loss: 30649, Validation Loss: 42596, 685116.4220627949\n",
      "Epoch 52401, Training Loss: 30588, Validation Loss: 42837, 1087165.5696628264\n",
      "Epoch 52501, Training Loss: 31006, Validation Loss: 42584, 548297.2055652206\n",
      "Epoch 52601, Training Loss: 31053, Validation Loss: 42595, 602091.8191683702\n",
      "Epoch 52701, Training Loss: 30963, Validation Loss: 42925, 936963.7821649809\n",
      "Epoch 52801, Training Loss: 29960, Validation Loss: 42724, 687750.2636539023\n",
      "Epoch 52901, Training Loss: 30549, Validation Loss: 42422, 1048319.20381771\n",
      "Epoch 53001, Training Loss: 30074, Validation Loss: 42805, 658171.6224482012\n",
      "Epoch 53101, Training Loss: 30598, Validation Loss: 42307, 1366685.7074256348\n",
      "Epoch 53201, Training Loss: 30765, Validation Loss: 42469, 686976.5441052465\n",
      "Epoch 53301, Training Loss: 30752, Validation Loss: 42415, 595194.4042687693\n",
      "Epoch 53401, Training Loss: 30898, Validation Loss: 42855, 531536.2012301526\n",
      "Epoch 53501, Training Loss: 31408, Validation Loss: 42779, 1005276.9466017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53601, Training Loss: 30587, Validation Loss: 42594, 794502.2248879522\n",
      "Epoch 53701, Training Loss: 30746, Validation Loss: 42829, 706298.361653214\n",
      "Epoch 53801, Training Loss: 30681, Validation Loss: 42867, 1135092.7324402337\n",
      "Epoch 53901, Training Loss: 30298, Validation Loss: 42719, 927603.8639749986\n",
      "Epoch 54001, Training Loss: 30629, Validation Loss: 42954, 958717.6236856198\n",
      "Epoch 54101, Training Loss: 30908, Validation Loss: 42795, 659943.7802691488\n",
      "Epoch 54201, Training Loss: 29990, Validation Loss: 42595, 743012.8471107722\n",
      "Epoch 54301, Training Loss: 30900, Validation Loss: 42597, 554634.7802218089\n",
      "Epoch 54401, Training Loss: 30670, Validation Loss: 42726, 733791.6888535953\n",
      "Epoch 54501, Training Loss: 30119, Validation Loss: 42855, 871451.3782193941\n",
      "Epoch 54601, Training Loss: 30424, Validation Loss: 42667, 683716.1711564716\n",
      "Epoch 54701, Training Loss: 31098, Validation Loss: 42588, 673871.7611470901\n",
      "Epoch 54801, Training Loss: 31273, Validation Loss: 42579, 1428575.6889409726\n",
      "Epoch 54901, Training Loss: 30864, Validation Loss: 42968, 552735.7512720226\n",
      "Epoch 55001, Training Loss: 30549, Validation Loss: 42973, 763820.9819811533\n",
      "Epoch 55101, Training Loss: 30861, Validation Loss: 42938, 964309.3454565145\n",
      "Epoch 55201, Training Loss: 31093, Validation Loss: 42818, 516784.8298545719\n",
      "Epoch 55301, Training Loss: 30792, Validation Loss: 42744, 1022936.4678398097\n",
      "Epoch 55401, Training Loss: 30152, Validation Loss: 42875, 479823.5203062235\n",
      "Epoch 55501, Training Loss: 30468, Validation Loss: 42498, 878561.4424286635\n",
      "Epoch 55601, Training Loss: 30422, Validation Loss: 43134, 926396.156572529\n",
      "Epoch 55701, Training Loss: 30663, Validation Loss: 42785, 1291901.2892391498\n",
      "Epoch 55801, Training Loss: 31107, Validation Loss: 42758, 1164088.0757983464\n",
      "Epoch 55901, Training Loss: 30762, Validation Loss: 42434, 1295779.5273983113\n",
      "Epoch 56001, Training Loss: 30536, Validation Loss: 42482, 543181.0738119581\n",
      "Epoch 56101, Training Loss: 31064, Validation Loss: 42563, 1368683.1691568438\n",
      "Epoch 56201, Training Loss: 30949, Validation Loss: 43062, 1178623.2628336374\n",
      "Epoch 56301, Training Loss: 31301, Validation Loss: 42704, 700131.8987001351\n",
      "Epoch 56401, Training Loss: 30617, Validation Loss: 42789, 765434.2145693048\n",
      "Epoch 56501, Training Loss: 30374, Validation Loss: 42401, 1436498.1666963904\n",
      "Epoch 56601, Training Loss: 31723, Validation Loss: 42648, 842814.6866347769\n",
      "Epoch 56701, Training Loss: 30535, Validation Loss: 42933, 473090.30800569954\n",
      "Epoch 56801, Training Loss: 30822, Validation Loss: 42679, 695629.4720772763\n",
      "Epoch 56901, Training Loss: 30609, Validation Loss: 42798, 766334.8999099667\n",
      "Epoch 57001, Training Loss: 31008, Validation Loss: 43012, 699621.6853062893\n",
      "Epoch 57101, Training Loss: 30838, Validation Loss: 43116, 1460701.0217998368\n",
      "Epoch 57201, Training Loss: 31398, Validation Loss: 43115, 1301686.626099858\n",
      "Epoch 57301, Training Loss: 31033, Validation Loss: 42592, 832267.0700259964\n",
      "Epoch 57401, Training Loss: 31037, Validation Loss: 42624, 976719.8983094178\n",
      "Epoch 57501, Training Loss: 30171, Validation Loss: 42867, 575509.4254857459\n",
      "Epoch 57601, Training Loss: 31316, Validation Loss: 43069, 603290.6032628489\n",
      "Epoch 57701, Training Loss: 31069, Validation Loss: 42639, 924400.4349127893\n",
      "Epoch 57801, Training Loss: 30770, Validation Loss: 42722, 1042703.8841085642\n",
      "Epoch 57901, Training Loss: 30863, Validation Loss: 42836, 513892.5568274426\n",
      "Epoch 58001, Training Loss: 30607, Validation Loss: 42701, 552514.5477722554\n",
      "Epoch 58101, Training Loss: 31274, Validation Loss: 42867, 1050793.9215811086\n",
      "Epoch 58201, Training Loss: 30907, Validation Loss: 42966, 845280.3198351305\n",
      "Epoch 58301, Training Loss: 30170, Validation Loss: 43065, 805023.8641225613\n",
      "Epoch 58401, Training Loss: 30426, Validation Loss: 42823, 891968.1869456755\n",
      "Epoch 58501, Training Loss: 31135, Validation Loss: 42975, 729537.9309917444\n",
      "Epoch 58601, Training Loss: 31398, Validation Loss: 42942, 615231.2222446294\n",
      "Epoch 58701, Training Loss: 30702, Validation Loss: 42814, 864644.698407855\n",
      "Epoch 58801, Training Loss: 31193, Validation Loss: 42605, 987758.8284072273\n",
      "Epoch 58901, Training Loss: 30114, Validation Loss: 43046, 1169107.0905576558\n",
      "Epoch 59001, Training Loss: 30833, Validation Loss: 42965, 1238571.0457562853\n",
      "Epoch 59101, Training Loss: 30673, Validation Loss: 42991, 1138147.1528686641\n",
      "Epoch 59201, Training Loss: 30076, Validation Loss: 42702, 458550.9218272033\n",
      "Epoch 59301, Training Loss: 30394, Validation Loss: 42738, 599478.5394957593\n",
      "Epoch 59401, Training Loss: 30587, Validation Loss: 42953, 522701.13812807587\n",
      "Epoch 59501, Training Loss: 31477, Validation Loss: 43098, 770868.7086328038\n",
      "Epoch 59601, Training Loss: 30679, Validation Loss: 42273, 1170568.5221940305\n",
      "Epoch 59701, Training Loss: 30499, Validation Loss: 42685, 993987.2891668611\n",
      "Epoch 59801, Training Loss: 31190, Validation Loss: 42205, 935340.7659389215\n",
      "Epoch 59901, Training Loss: 31509, Validation Loss: 42774, 635421.7192271922\n",
      "Epoch 60001, Training Loss: 30603, Validation Loss: 42743, 644808.5668446756\n",
      "Epoch 60101, Training Loss: 30773, Validation Loss: 42515, 842530.761197849\n",
      "Epoch 60201, Training Loss: 31289, Validation Loss: 43386, 1196316.6742837834\n",
      "Epoch 60301, Training Loss: 30388, Validation Loss: 42718, 694521.9450387211\n",
      "Epoch 60401, Training Loss: 30541, Validation Loss: 42664, 424078.95944395196\n",
      "Epoch 60501, Training Loss: 30749, Validation Loss: 42454, 885856.0009462757\n",
      "Epoch 60601, Training Loss: 31262, Validation Loss: 42901, 1675839.03000572\n",
      "Epoch 60701, Training Loss: 30453, Validation Loss: 42877, 719760.1914943616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [102]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m datas, prices \u001b[38;5;241m=\u001b[39m tuple_\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m prices_viewed \u001b[38;5;241m=\u001b[39m prices\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, prices_viewed)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36mTabularFFNNSimple.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(x)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=9e-4,\n",
    "    weight_decay=0.06\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999999, \n",
    "    patience=2, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "# plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"../../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e6f42ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TabularFFNNSimple' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         total_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnumel(layer\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Neurons\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_neurons,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Weights\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_weights,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Biases\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_biases,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Trainable Parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_trainable_params\n\u001b[1;32m     15\u001b[0m     }\n\u001b[0;32m---> 17\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(stats)\n",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36mmodel_statistics\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      5\u001b[0m total_trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[0;32m----> 8\u001b[0m     total_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnumel(\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Neurons\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_neurons,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Weights\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_weights,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Biases\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_biases,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Trainable Parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_trainable_params\n\u001b[1;32m     15\u001b[0m }\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TabularFFNNSimple' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "def model_statistics(model):\n",
    "    total_neurons = 0\n",
    "    total_weights = 0\n",
    "    total_biases = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for layer in model.modules():\n",
    "        total_weights += torch.numel(layer.weight)\n",
    "\n",
    "    return {\n",
    "        \"Total Neurons\": total_neurons,\n",
    "        \"Total Weights\": total_weights,\n",
    "        \"Total Biases\": total_biases,\n",
    "        \"Total Trainable Parameters\": total_trainable_params\n",
    "    }\n",
    "\n",
    "stats = model_statistics(model)\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
