{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 64\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "#             nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.12),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.12),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 237837, Validation Loss: 233567, 16088.987721204141\n",
      "Epoch 101, Training Loss: 79267, Validation Loss: 74937, 56038.374050600476\n",
      "Epoch 201, Training Loss: 63992, Validation Loss: 63848, 148909.89249542562\n",
      "Epoch 301, Training Loss: 61666, Validation Loss: 58373, 160549.10161126804\n",
      "Epoch 401, Training Loss: 61064, Validation Loss: 63088, 206048.01006451095\n",
      "Epoch 501, Training Loss: 57163, Validation Loss: 56238, 121615.26958535962\n",
      "Epoch 601, Training Loss: 53673, Validation Loss: 55349, 115536.48968533461\n",
      "Epoch 701, Training Loss: 56179, Validation Loss: 54369, 236018.80009538322\n",
      "Epoch 801, Training Loss: 54426, Validation Loss: 54327, 70460.21394255779\n",
      "Epoch 901, Training Loss: 52874, Validation Loss: 53626, 112942.93390291267\n",
      "Epoch 1001, Training Loss: 56292, Validation Loss: 54136, 165994.35355415978\n",
      "Epoch 1101, Training Loss: 54874, Validation Loss: 52719, 106310.0986500975\n",
      "Epoch 1201, Training Loss: 52260, Validation Loss: 55126, 182535.5019237129\n",
      "Epoch 1301, Training Loss: 59431, Validation Loss: 53305, 390596.5782054986\n",
      "Epoch 1401, Training Loss: 51341, Validation Loss: 54720, 143471.7489216987\n",
      "Epoch 1501, Training Loss: 55591, Validation Loss: 56726, 338117.2961778084\n",
      "Epoch 1601, Training Loss: 53312, Validation Loss: 52006, 206128.28945296723\n",
      "Epoch 1701, Training Loss: 48570, Validation Loss: 51831, 87180.67803551292\n",
      "Epoch 1801, Training Loss: 49541, Validation Loss: 50973, 114269.11791654122\n",
      "Epoch 1901, Training Loss: 53424, Validation Loss: 50861, 205747.0727802643\n",
      "Epoch 2001, Training Loss: 52676, Validation Loss: 52130, 314450.0764698233\n",
      "Epoch 2101, Training Loss: 53484, Validation Loss: 51297, 214694.04792014524\n",
      "Epoch 2201, Training Loss: 50187, Validation Loss: 50560, 126950.0123457027\n",
      "Epoch 2301, Training Loss: 45681, Validation Loss: 51500, 220628.58043595706\n",
      "Epoch 2401, Training Loss: 51986, Validation Loss: 53339, 289377.523264891\n",
      "Epoch 2501, Training Loss: 49167, Validation Loss: 50250, 173613.144875154\n",
      "Epoch 2601, Training Loss: 49431, Validation Loss: 50571, 117356.4509580193\n",
      "Epoch 2701, Training Loss: 52914, Validation Loss: 52009, 285200.1962098464\n",
      "Epoch 2801, Training Loss: 52109, Validation Loss: 50085, 182488.31301915055\n",
      "Epoch 2901, Training Loss: 54197, Validation Loss: 55290, 500308.22600101796\n",
      "Epoch 3001, Training Loss: 53937, Validation Loss: 50001, 313341.5659034143\n",
      "Epoch 3101, Training Loss: 49684, Validation Loss: 50215, 161951.7768873954\n",
      "Epoch 3201, Training Loss: 46403, Validation Loss: 49736, 136969.52063771765\n",
      "Epoch 3301, Training Loss: 53661, Validation Loss: 50526, 301310.3018571121\n",
      "Epoch 3401, Training Loss: 48819, Validation Loss: 49693, 213355.25022609252\n",
      "Epoch 3501, Training Loss: 49286, Validation Loss: 50662, 189483.69073076211\n",
      "Epoch 3601, Training Loss: 49426, Validation Loss: 50050, 240296.75508195217\n",
      "Epoch 3701, Training Loss: 50691, Validation Loss: 49629, 199711.84228991964\n",
      "Epoch 3801, Training Loss: 50953, Validation Loss: 49667, 170255.20518873775\n",
      "Epoch 3901, Training Loss: 48374, Validation Loss: 49591, 134815.32008056602\n",
      "Epoch 4001, Training Loss: 49464, Validation Loss: 49480, 154564.83266597695\n",
      "Epoch 4101, Training Loss: 49874, Validation Loss: 48979, 203044.14756730595\n",
      "Epoch 4201, Training Loss: 49300, Validation Loss: 49107, 131616.62940705978\n",
      "Epoch 4301, Training Loss: 48462, Validation Loss: 50500, 137236.83644312862\n",
      "Epoch 4401, Training Loss: 52158, Validation Loss: 49863, 224871.32238106857\n",
      "Epoch 4501, Training Loss: 48922, Validation Loss: 49714, 256972.94022014132\n",
      "Epoch 4601, Training Loss: 47443, Validation Loss: 49055, 288967.8205477733\n",
      "Epoch 4701, Training Loss: 49767, Validation Loss: 51149, 219166.03930715358\n",
      "Epoch 4801, Training Loss: 47619, Validation Loss: 50338, 198286.98242767135\n",
      "Epoch 4901, Training Loss: 48487, Validation Loss: 49963, 97893.89651254653\n",
      "Epoch 5001, Training Loss: 47153, Validation Loss: 48221, 166389.4959232727\n",
      "Epoch 5101, Training Loss: 46432, Validation Loss: 49592, 191663.99383153804\n",
      "Epoch 5201, Training Loss: 51369, Validation Loss: 48901, 239582.36950409014\n",
      "Epoch 5301, Training Loss: 52499, Validation Loss: 55661, 403621.5075474414\n",
      "Epoch 5401, Training Loss: 49128, Validation Loss: 48661, 200076.7880452856\n",
      "Epoch 5501, Training Loss: 44634, Validation Loss: 49375, 157600.33478711752\n",
      "Epoch 5601, Training Loss: 47498, Validation Loss: 49663, 157798.28060543706\n",
      "Epoch 5701, Training Loss: 47960, Validation Loss: 49337, 126920.2189054616\n",
      "Epoch 5801, Training Loss: 46964, Validation Loss: 50478, 139037.42152828505\n",
      "Epoch 5901, Training Loss: 47579, Validation Loss: 50116, 210830.0939236988\n",
      "Epoch 6001, Training Loss: 46014, Validation Loss: 53908, 199768.7719732366\n",
      "Epoch 6101, Training Loss: 47518, Validation Loss: 49000, 181500.78834396155\n",
      "Epoch 6201, Training Loss: 49962, Validation Loss: 48828, 171317.01399523346\n",
      "Epoch 6301, Training Loss: 46576, Validation Loss: 49443, 187533.38130030956\n",
      "Epoch 6401, Training Loss: 47301, Validation Loss: 51045, 153524.28907134852\n",
      "Epoch 6501, Training Loss: 48240, Validation Loss: 49657, 251384.64167414652\n",
      "Epoch 6601, Training Loss: 46639, Validation Loss: 50605, 221056.40786448718\n",
      "Epoch 6701, Training Loss: 48825, Validation Loss: 49614, 188231.09503314926\n",
      "Epoch 6801, Training Loss: 47598, Validation Loss: 49203, 258617.11405714648\n",
      "Epoch 6901, Training Loss: 47917, Validation Loss: 50228, 178422.99390071965\n",
      "Epoch 7001, Training Loss: 47593, Validation Loss: 49480, 97922.5414726211\n",
      "Epoch 7101, Training Loss: 48484, Validation Loss: 49357, 274726.9711687665\n",
      "Epoch 7201, Training Loss: 46843, Validation Loss: 52069, 241039.0834572968\n",
      "Epoch 7301, Training Loss: 49115, Validation Loss: 50489, 272022.57687385916\n",
      "Epoch 7401, Training Loss: 44833, Validation Loss: 52873, 204813.18838843712\n",
      "Epoch 7501, Training Loss: 49446, Validation Loss: 48745, 204954.0156628439\n",
      "Epoch 7601, Training Loss: 49728, Validation Loss: 51707, 378157.84692225716\n",
      "Epoch 7701, Training Loss: 46727, Validation Loss: 50282, 277711.12808928435\n",
      "Epoch 7801, Training Loss: 49739, Validation Loss: 52424, 328561.04818137106\n",
      "Epoch 7901, Training Loss: 46340, Validation Loss: 49925, 203885.79831075517\n",
      "Epoch 8001, Training Loss: 49823, Validation Loss: 49755, 197057.22286719884\n",
      "Epoch 8101, Training Loss: 48415, Validation Loss: 49085, 196309.74243931906\n",
      "Epoch 8201, Training Loss: 46701, Validation Loss: 54155, 189630.01637750686\n",
      "Epoch 8301, Training Loss: 45170, Validation Loss: 51877, 182528.13354409288\n",
      "Epoch 8401, Training Loss: 44761, Validation Loss: 48834, 189706.06867798712\n",
      "Epoch 8501, Training Loss: 48225, Validation Loss: 49561, 249612.3139220867\n",
      "Epoch 8601, Training Loss: 50664, Validation Loss: 50108, 235235.42677396024\n",
      "Epoch 8701, Training Loss: 47191, Validation Loss: 50532, 116102.58377353929\n",
      "Epoch 8801, Training Loss: 47238, Validation Loss: 50981, 163080.89026391236\n",
      "Epoch 8901, Training Loss: 45426, Validation Loss: 48902, 173716.71058861053\n",
      "Epoch 9001, Training Loss: 48799, Validation Loss: 54391, 411293.353840403\n",
      "Epoch 9101, Training Loss: 45969, Validation Loss: 49215, 147916.17015759318\n",
      "Epoch 9201, Training Loss: 45703, Validation Loss: 50045, 173045.0992662995\n",
      "Epoch 9301, Training Loss: 45270, Validation Loss: 52085, 192983.24834970245\n",
      "Epoch 9401, Training Loss: 45680, Validation Loss: 50088, 195747.6642922707\n",
      "Epoch 9501, Training Loss: 48220, Validation Loss: 49088, 315291.1796772236\n",
      "Epoch 9601, Training Loss: 46989, Validation Loss: 48511, 240478.4791019214\n",
      "Epoch 9701, Training Loss: 45439, Validation Loss: 52686, 229389.56683178843\n",
      "Epoch 9801, Training Loss: 43657, Validation Loss: 50188, 229539.53598894426\n",
      "Epoch 9901, Training Loss: 45165, Validation Loss: 49325, 239740.20883207596\n",
      "Epoch 10001, Training Loss: 44001, Validation Loss: 50268, 146292.50612649857\n",
      "Epoch 10101, Training Loss: 48033, Validation Loss: 49554, 139027.54085183077\n",
      "Epoch 10201, Training Loss: 45664, Validation Loss: 49075, 255970.37341113467\n",
      "Epoch 10301, Training Loss: 44745, Validation Loss: 50136, 232859.28545994734\n",
      "Epoch 10401, Training Loss: 45823, Validation Loss: 50347, 162069.0915632357\n",
      "Epoch 10501, Training Loss: 48691, Validation Loss: 49941, 177740.4813054751\n",
      "Epoch 10601, Training Loss: 48492, Validation Loss: 50058, 142889.11008413573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10701, Training Loss: 45859, Validation Loss: 49896, 132964.26940115428\n",
      "Epoch 10801, Training Loss: 44340, Validation Loss: 52332, 206678.99649995603\n",
      "Epoch 10901, Training Loss: 47079, Validation Loss: 50035, 216446.3653580212\n",
      "Epoch 11001, Training Loss: 48553, Validation Loss: 52653, 316052.2452006159\n",
      "Epoch 11101, Training Loss: 43697, Validation Loss: 52440, 167922.52541597592\n",
      "Epoch 11201, Training Loss: 42063, Validation Loss: 49939, 165013.17514354948\n",
      "Epoch 11301, Training Loss: 44848, Validation Loss: 50325, 236126.87477939916\n",
      "Epoch 11401, Training Loss: 44080, Validation Loss: 49670, 132786.3012566334\n",
      "Epoch 11501, Training Loss: 47186, Validation Loss: 49480, 205966.24487423128\n",
      "Epoch 11601, Training Loss: 45393, Validation Loss: 49410, 165496.73429690753\n",
      "Epoch 11701, Training Loss: 45937, Validation Loss: 50320, 230090.06637714882\n",
      "Epoch 11801, Training Loss: 44264, Validation Loss: 49752, 124097.17667277617\n",
      "Epoch 11901, Training Loss: 47127, Validation Loss: 48432, 142587.12594536072\n",
      "Epoch 12001, Training Loss: 45764, Validation Loss: 52159, 156783.42998770325\n",
      "Epoch 12101, Training Loss: 43234, Validation Loss: 48279, 169819.74396199308\n",
      "Epoch 12201, Training Loss: 46236, Validation Loss: 51903, 184296.8943448478\n",
      "Epoch 12301, Training Loss: 48649, Validation Loss: 59486, 220716.22003061112\n",
      "Epoch 12401, Training Loss: 47938, Validation Loss: 50327, 157700.4938173828\n",
      "Epoch 12501, Training Loss: 46772, Validation Loss: 48744, 406108.898384661\n",
      "Epoch 12601, Training Loss: 43282, Validation Loss: 50818, 214371.42313652803\n",
      "Epoch 12701, Training Loss: 46342, Validation Loss: 51173, 230523.98906556494\n",
      "Epoch 12801, Training Loss: 45684, Validation Loss: 51797, 225441.20194888036\n",
      "Epoch 12901, Training Loss: 43225, Validation Loss: 51168, 174615.33904182087\n",
      "Epoch 13001, Training Loss: 44129, Validation Loss: 53621, 281957.7233560932\n",
      "Epoch 13101, Training Loss: 42229, Validation Loss: 48608, 125190.3969074128\n",
      "Epoch 13201, Training Loss: 46845, Validation Loss: 51322, 242040.01902971393\n",
      "Epoch 13301, Training Loss: 43451, Validation Loss: 48634, 234361.64716622155\n",
      "Epoch 13401, Training Loss: 43177, Validation Loss: 49466, 171465.6224647618\n",
      "Epoch 13501, Training Loss: 44391, Validation Loss: 49699, 267768.0212256336\n",
      "Epoch 13601, Training Loss: 43812, Validation Loss: 51538, 113568.87610306956\n",
      "Epoch 13701, Training Loss: 44735, Validation Loss: 50305, 207522.40304943817\n",
      "Epoch 13801, Training Loss: 42541, Validation Loss: 50596, 163210.4900929452\n",
      "Epoch 13901, Training Loss: 44536, Validation Loss: 49891, 164779.41936733964\n",
      "Epoch 14001, Training Loss: 47880, Validation Loss: 51454, 325394.5470981726\n",
      "Epoch 14101, Training Loss: 44410, Validation Loss: 53178, 265119.85947740293\n",
      "Epoch 14201, Training Loss: 42587, Validation Loss: 49580, 176721.14012264702\n",
      "Epoch 14301, Training Loss: 45852, Validation Loss: 49022, 157237.82309734888\n",
      "Epoch 14401, Training Loss: 40294, Validation Loss: 48901, 131185.95504280707\n",
      "Epoch 14501, Training Loss: 45322, Validation Loss: 48790, 235575.05675686695\n",
      "Epoch 14601, Training Loss: 42123, Validation Loss: 49816, 197038.0991923275\n",
      "Epoch 14701, Training Loss: 44724, Validation Loss: 49966, 196097.78743480667\n",
      "Epoch 14801, Training Loss: 44239, Validation Loss: 50908, 234935.85658600088\n",
      "Epoch 14901, Training Loss: 47789, Validation Loss: 52804, 226480.30303556952\n",
      "Epoch 15001, Training Loss: 43481, Validation Loss: 51349, 307808.29356034595\n",
      "Epoch 15101, Training Loss: 42605, Validation Loss: 50340, 137956.52528790745\n",
      "Epoch 15201, Training Loss: 42985, Validation Loss: 49881, 242723.8393044847\n",
      "Epoch 15301, Training Loss: 42084, Validation Loss: 49680, 170895.14678727195\n",
      "Epoch 15401, Training Loss: 43910, Validation Loss: 51246, 144924.7525757779\n",
      "Epoch 15501, Training Loss: 43890, Validation Loss: 49420, 232510.38460326058\n",
      "Epoch 15601, Training Loss: 42335, Validation Loss: 49410, 244412.00686094025\n",
      "Epoch 15701, Training Loss: 43563, Validation Loss: 50304, 109465.02939391171\n",
      "Epoch 15801, Training Loss: 43277, Validation Loss: 52442, 177601.07923561652\n",
      "Epoch 15901, Training Loss: 43777, Validation Loss: 54348, 233012.10699455705\n",
      "Epoch 16001, Training Loss: 45232, Validation Loss: 49388, 165290.37655782755\n",
      "Epoch 16101, Training Loss: 42258, Validation Loss: 51562, 204558.12693482218\n",
      "Epoch 16201, Training Loss: 43002, Validation Loss: 51142, 166146.00244606196\n",
      "Epoch 16301, Training Loss: 43875, Validation Loss: 51831, 145545.33315417203\n",
      "Epoch 16401, Training Loss: 41999, Validation Loss: 51284, 175601.22200926256\n",
      "Epoch 16501, Training Loss: 41999, Validation Loss: 48792, 177336.6408483629\n",
      "Epoch 16601, Training Loss: 40809, Validation Loss: 50429, 154161.93638855385\n",
      "Epoch 16701, Training Loss: 44113, Validation Loss: 52383, 182028.57375465255\n",
      "Epoch 16801, Training Loss: 46622, Validation Loss: 55467, 357435.83075720986\n",
      "Epoch 16901, Training Loss: 43774, Validation Loss: 53935, 255951.9547410547\n",
      "Epoch 17001, Training Loss: 42479, Validation Loss: 51496, 151895.48640298177\n",
      "Epoch 17101, Training Loss: 45908, Validation Loss: 49464, 193968.83873365805\n",
      "Epoch 17201, Training Loss: 45375, Validation Loss: 51862, 220862.79025870693\n",
      "Epoch 17301, Training Loss: 43693, Validation Loss: 52532, 216669.21357454607\n",
      "Epoch 17401, Training Loss: 45196, Validation Loss: 50670, 196641.08897843747\n",
      "Epoch 17501, Training Loss: 43446, Validation Loss: 50391, 160049.9582410876\n",
      "Epoch 17601, Training Loss: 43945, Validation Loss: 50367, 144851.81764640592\n",
      "Epoch 17701, Training Loss: 43564, Validation Loss: 52374, 246155.90096732703\n",
      "Epoch 17801, Training Loss: 43771, Validation Loss: 51180, 236173.56815945657\n",
      "Epoch 17901, Training Loss: 43563, Validation Loss: 52046, 173140.726836202\n",
      "Epoch 18001, Training Loss: 42693, Validation Loss: 51928, 276642.18702397175\n",
      "Epoch 18101, Training Loss: 42434, Validation Loss: 49022, 223769.08418705335\n",
      "Epoch 18201, Training Loss: 44502, Validation Loss: 49478, 223117.57271583425\n",
      "Epoch 18301, Training Loss: 43449, Validation Loss: 49458, 154448.24842758503\n",
      "Epoch 18401, Training Loss: 42429, Validation Loss: 49905, 158163.46907141316\n",
      "Epoch 18501, Training Loss: 42989, Validation Loss: 51645, 183536.78010662817\n",
      "Epoch 18601, Training Loss: 43636, Validation Loss: 51500, 192293.99785331017\n",
      "Epoch 18701, Training Loss: 44207, Validation Loss: 50706, 154141.11979189477\n",
      "Epoch 18801, Training Loss: 44136, Validation Loss: 49900, 134782.0403035486\n",
      "Epoch 18901, Training Loss: 43739, Validation Loss: 49291, 196155.87772999774\n",
      "Epoch 19001, Training Loss: 43954, Validation Loss: 49948, 208426.44054783325\n",
      "Epoch 19101, Training Loss: 41943, Validation Loss: 53496, 166622.20784929534\n",
      "Epoch 19201, Training Loss: 44505, Validation Loss: 51283, 241556.988178722\n",
      "Epoch 19301, Training Loss: 41592, Validation Loss: 49558, 178456.60419394154\n",
      "Epoch 19401, Training Loss: 41082, Validation Loss: 51395, 149184.70894263146\n",
      "Epoch 19501, Training Loss: 41913, Validation Loss: 52535, 151166.3966192069\n",
      "Epoch 19601, Training Loss: 47134, Validation Loss: 54680, 271256.74079650006\n",
      "Epoch 19701, Training Loss: 43010, Validation Loss: 50982, 215096.76537070939\n",
      "Epoch 19801, Training Loss: 42746, Validation Loss: 48558, 204082.29562183472\n",
      "Epoch 19901, Training Loss: 43247, Validation Loss: 55678, 181247.18571110093\n",
      "Epoch 20001, Training Loss: 40864, Validation Loss: 52882, 200793.45552239707\n",
      "Epoch 20101, Training Loss: 42081, Validation Loss: 52497, 158841.6098254477\n",
      "Epoch 20201, Training Loss: 41361, Validation Loss: 53256, 176652.97887822907\n",
      "Epoch 20301, Training Loss: 44194, Validation Loss: 51856, 247446.34699922559\n",
      "Epoch 20401, Training Loss: 44924, Validation Loss: 51579, 274486.149288171\n",
      "Epoch 20501, Training Loss: 44327, Validation Loss: 55474, 293393.4421095093\n",
      "Epoch 20601, Training Loss: 41757, Validation Loss: 53763, 268430.9060528541\n",
      "Epoch 20701, Training Loss: 43844, Validation Loss: 52411, 258293.41225825888\n",
      "Epoch 20801, Training Loss: 40930, Validation Loss: 49807, 189934.44905957335\n",
      "Epoch 20901, Training Loss: 43237, Validation Loss: 53615, 245831.70294510177\n",
      "Epoch 21001, Training Loss: 41381, Validation Loss: 48014, 215806.744081979\n",
      "Epoch 21101, Training Loss: 40976, Validation Loss: 51284, 162570.86133690123\n",
      "Epoch 21201, Training Loss: 43228, Validation Loss: 49542, 176928.32568638344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21301, Training Loss: 44869, Validation Loss: 53818, 198533.65504591414\n",
      "Epoch 21401, Training Loss: 41677, Validation Loss: 51083, 144417.58217667823\n",
      "Epoch 21501, Training Loss: 44230, Validation Loss: 52290, 196634.91423859564\n",
      "Epoch 21601, Training Loss: 41543, Validation Loss: 51293, 229770.64337665285\n",
      "Epoch 21701, Training Loss: 41602, Validation Loss: 51178, 191906.95763528708\n",
      "Epoch 21801, Training Loss: 40411, Validation Loss: 51138, 181618.666075225\n",
      "Epoch 21901, Training Loss: 43084, Validation Loss: 50271, 178611.2284980643\n",
      "Epoch 22001, Training Loss: 43056, Validation Loss: 54574, 191531.0951397495\n",
      "Epoch 22101, Training Loss: 47515, Validation Loss: 49760, 278020.88426418643\n",
      "Epoch 22201, Training Loss: 45112, Validation Loss: 50878, 242203.40468684013\n",
      "Epoch 22301, Training Loss: 42277, Validation Loss: 53898, 227706.66821297767\n",
      "Epoch 22401, Training Loss: 39244, Validation Loss: 50414, 132461.46542386003\n",
      "Epoch 22501, Training Loss: 44880, Validation Loss: 53885, 225791.7439956752\n",
      "Epoch 22601, Training Loss: 42206, Validation Loss: 50633, 210277.48366771088\n",
      "Epoch 22701, Training Loss: 41402, Validation Loss: 50180, 156214.2611852974\n",
      "Epoch 22801, Training Loss: 40557, Validation Loss: 49058, 174785.12790162442\n",
      "Epoch 22901, Training Loss: 42450, Validation Loss: 56271, 140469.3218186739\n",
      "Epoch 23001, Training Loss: 38224, Validation Loss: 50573, 179613.67383030299\n",
      "Epoch 23101, Training Loss: 43953, Validation Loss: 52240, 182879.4020722195\n",
      "Epoch 23201, Training Loss: 40122, Validation Loss: 52002, 188901.2596593787\n",
      "Epoch 23301, Training Loss: 41987, Validation Loss: 50730, 227373.35269381697\n",
      "Epoch 23401, Training Loss: 41827, Validation Loss: 52499, 213425.26343056202\n",
      "Epoch 23501, Training Loss: 42658, Validation Loss: 52951, 201297.82053508275\n",
      "Epoch 23601, Training Loss: 43087, Validation Loss: 59116, 172689.92721993526\n",
      "Epoch 23701, Training Loss: 41616, Validation Loss: 51539, 173064.45843617953\n",
      "Epoch 23801, Training Loss: 42577, Validation Loss: 49380, 234007.10654671688\n",
      "Epoch 23901, Training Loss: 43950, Validation Loss: 53278, 238846.09833517892\n",
      "Epoch 24001, Training Loss: 43725, Validation Loss: 49780, 273897.9914709134\n",
      "Epoch 24101, Training Loss: 41103, Validation Loss: 51156, 150053.56652645094\n",
      "Epoch 24201, Training Loss: 42207, Validation Loss: 51039, 168202.29136611367\n",
      "Epoch 24301, Training Loss: 42140, Validation Loss: 51172, 157111.5799453165\n",
      "Epoch 24401, Training Loss: 41654, Validation Loss: 53550, 196359.5471129699\n",
      "Epoch 24501, Training Loss: 41964, Validation Loss: 50002, 207167.27778861581\n",
      "Epoch 24601, Training Loss: 42924, Validation Loss: 50667, 210945.250567107\n",
      "Epoch 24701, Training Loss: 42005, Validation Loss: 54027, 234217.32470199082\n",
      "Epoch 24801, Training Loss: 41516, Validation Loss: 50795, 280128.96184457024\n",
      "Epoch 24901, Training Loss: 42720, Validation Loss: 51263, 123481.03994271818\n",
      "Epoch 25001, Training Loss: 42656, Validation Loss: 52596, 207864.7203625658\n",
      "Epoch 25101, Training Loss: 40736, Validation Loss: 52937, 175057.17896849624\n",
      "Epoch 25201, Training Loss: 39764, Validation Loss: 50241, 181345.33946408494\n",
      "Epoch 25301, Training Loss: 40302, Validation Loss: 50349, 158443.43179457568\n",
      "Epoch 25401, Training Loss: 40303, Validation Loss: 49766, 140413.37164080792\n",
      "Epoch 25501, Training Loss: 42378, Validation Loss: 51064, 207933.28399736527\n",
      "Epoch 25601, Training Loss: 43412, Validation Loss: 56136, 346478.95193320216\n",
      "Epoch 25701, Training Loss: 41524, Validation Loss: 50170, 166862.02037793628\n",
      "Epoch 25801, Training Loss: 44083, Validation Loss: 54343, 160321.5073836857\n",
      "Epoch 25901, Training Loss: 42626, Validation Loss: 51729, 208464.99857977746\n",
      "Epoch 26001, Training Loss: 43027, Validation Loss: 51770, 226670.58754606717\n",
      "Epoch 26101, Training Loss: 44884, Validation Loss: 48220, 265104.0782720154\n",
      "Epoch 26201, Training Loss: 44419, Validation Loss: 55639, 230867.27243371576\n",
      "Epoch 26301, Training Loss: 40895, Validation Loss: 56872, 258665.48805400697\n",
      "Epoch 26401, Training Loss: 42968, Validation Loss: 53422, 256646.51359827872\n",
      "Epoch 26501, Training Loss: 39674, Validation Loss: 51212, 179816.0373746126\n",
      "Epoch 26601, Training Loss: 39948, Validation Loss: 52375, 205397.99670805855\n",
      "Epoch 26701, Training Loss: 40430, Validation Loss: 54559, 139511.167178715\n",
      "Epoch 26801, Training Loss: 41790, Validation Loss: 51575, 203368.67632830795\n",
      "Epoch 26901, Training Loss: 39599, Validation Loss: 54974, 135248.71016015697\n",
      "Epoch 27001, Training Loss: 41785, Validation Loss: 54961, 191415.70408419622\n",
      "Epoch 27101, Training Loss: 41341, Validation Loss: 51936, 130489.25672536991\n",
      "Epoch 27201, Training Loss: 40307, Validation Loss: 50980, 147237.08575261317\n",
      "Epoch 27301, Training Loss: 41822, Validation Loss: 48400, 154863.18248208493\n",
      "Epoch 27401, Training Loss: 43816, Validation Loss: 52132, 188842.47467400588\n",
      "Epoch 27501, Training Loss: 39909, Validation Loss: 51362, 220511.1051147564\n",
      "Epoch 27601, Training Loss: 45274, Validation Loss: 50257, 359310.3925981489\n",
      "Epoch 27701, Training Loss: 41190, Validation Loss: 53449, 154802.53422508037\n",
      "Epoch 27801, Training Loss: 40464, Validation Loss: 56378, 139862.97626535533\n",
      "Epoch 27901, Training Loss: 41771, Validation Loss: 50845, 153579.51004846845\n",
      "Epoch 28001, Training Loss: 40267, Validation Loss: 52330, 234199.5159597367\n",
      "Epoch 28101, Training Loss: 40267, Validation Loss: 56414, 159559.34426159985\n",
      "Epoch 28201, Training Loss: 39749, Validation Loss: 53827, 207639.8030367324\n",
      "Epoch 28301, Training Loss: 38379, Validation Loss: 55296, 155826.4160061125\n",
      "Epoch 28401, Training Loss: 40259, Validation Loss: 56888, 191056.4281597403\n",
      "Epoch 28501, Training Loss: 40643, Validation Loss: 52867, 193130.85113976317\n",
      "Epoch 28601, Training Loss: 42868, Validation Loss: 56398, 192637.79665267342\n",
      "Epoch 28701, Training Loss: 40696, Validation Loss: 51662, 169037.2914437833\n",
      "Epoch 28801, Training Loss: 41601, Validation Loss: 50636, 209499.23670725\n",
      "Epoch 28901, Training Loss: 41891, Validation Loss: 51173, 184002.90330591402\n",
      "Epoch 29001, Training Loss: 42103, Validation Loss: 51511, 320108.6577884851\n",
      "Epoch 29101, Training Loss: 41909, Validation Loss: 51558, 214251.3596041893\n",
      "Epoch 29201, Training Loss: 39220, Validation Loss: 51559, 170436.95233998212\n",
      "Epoch 29301, Training Loss: 41978, Validation Loss: 55049, 208403.6289315239\n",
      "Epoch 29401, Training Loss: 42055, Validation Loss: 48732, 173080.7773862255\n",
      "Epoch 29501, Training Loss: 39612, Validation Loss: 51622, 133845.56010017914\n",
      "Epoch 29601, Training Loss: 40101, Validation Loss: 52080, 146085.49140358844\n",
      "Epoch 29701, Training Loss: 43282, Validation Loss: 50881, 309533.3418276961\n",
      "Epoch 29801, Training Loss: 39697, Validation Loss: 54718, 149978.35587905833\n",
      "Epoch 29901, Training Loss: 42684, Validation Loss: 52275, 213997.5256349116\n",
      "Epoch 30001, Training Loss: 39942, Validation Loss: 53339, 172473.0625928425\n",
      "Epoch 30101, Training Loss: 40552, Validation Loss: 52277, 209391.84212806937\n",
      "Epoch 30201, Training Loss: 37309, Validation Loss: 49742, 137735.90836054902\n",
      "Epoch 30301, Training Loss: 39500, Validation Loss: 50376, 151297.33485633627\n",
      "Epoch 30401, Training Loss: 41060, Validation Loss: 56872, 230028.71965666921\n",
      "Epoch 30501, Training Loss: 40160, Validation Loss: 53365, 263327.00665998034\n",
      "Epoch 30601, Training Loss: 41816, Validation Loss: 53809, 173728.21607998907\n",
      "Epoch 30701, Training Loss: 37052, Validation Loss: 55956, 214626.09401191375\n",
      "Epoch 30801, Training Loss: 40555, Validation Loss: 53432, 149056.57837429698\n",
      "Epoch 30901, Training Loss: 39815, Validation Loss: 53831, 175892.10897010923\n",
      "Epoch 31001, Training Loss: 39904, Validation Loss: 54536, 219924.43807355769\n",
      "Epoch 31101, Training Loss: 39712, Validation Loss: 56575, 196448.76540840417\n",
      "Epoch 31201, Training Loss: 40044, Validation Loss: 50100, 290533.2997004945\n",
      "Epoch 31301, Training Loss: 41404, Validation Loss: 50506, 239725.0000728648\n",
      "Epoch 31401, Training Loss: 39577, Validation Loss: 53503, 234531.54142508283\n",
      "Epoch 31501, Training Loss: 43289, Validation Loss: 50870, 182292.07394869637\n",
      "Epoch 31601, Training Loss: 37228, Validation Loss: 52347, 158724.85403115905\n",
      "Epoch 31701, Training Loss: 38092, Validation Loss: 50450, 176696.75235541118\n",
      "Epoch 31801, Training Loss: 40203, Validation Loss: 53099, 176381.7712909485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31901, Training Loss: 38624, Validation Loss: 51391, 152745.7045088301\n",
      "Epoch 32001, Training Loss: 40214, Validation Loss: 53070, 146896.98274764462\n",
      "Epoch 32101, Training Loss: 39879, Validation Loss: 49877, 131939.10090751084\n",
      "Epoch 32201, Training Loss: 37851, Validation Loss: 53936, 176709.15267024786\n",
      "Epoch 32301, Training Loss: 38627, Validation Loss: 51968, 184745.89902532267\n",
      "Epoch 32401, Training Loss: 38122, Validation Loss: 52349, 177716.78897844852\n",
      "Epoch 32501, Training Loss: 41250, Validation Loss: 51840, 119872.74013933638\n",
      "Epoch 32601, Training Loss: 37614, Validation Loss: 55046, 236010.46424295558\n",
      "Epoch 32701, Training Loss: 41274, Validation Loss: 50711, 187058.47492742888\n",
      "Epoch 32801, Training Loss: 40707, Validation Loss: 53979, 230352.5108681327\n",
      "Epoch 32901, Training Loss: 40867, Validation Loss: 52307, 233617.9858941563\n",
      "Epoch 33001, Training Loss: 38359, Validation Loss: 53183, 179126.7579034448\n",
      "Epoch 33101, Training Loss: 38256, Validation Loss: 50152, 159758.5938380025\n",
      "Epoch 33201, Training Loss: 39367, Validation Loss: 51017, 166567.99682788536\n",
      "Epoch 33301, Training Loss: 41033, Validation Loss: 50629, 181637.54792904985\n",
      "Epoch 33401, Training Loss: 40388, Validation Loss: 48658, 144080.76030521878\n",
      "Epoch 33501, Training Loss: 37699, Validation Loss: 52549, 195708.23552075264\n",
      "Epoch 33601, Training Loss: 39390, Validation Loss: 52355, 163102.69092299807\n",
      "Epoch 33701, Training Loss: 36902, Validation Loss: 49990, 162113.1159643981\n",
      "Epoch 33801, Training Loss: 41084, Validation Loss: 50136, 178287.03066362426\n",
      "Epoch 33901, Training Loss: 41720, Validation Loss: 53490, 255670.97654386293\n",
      "Epoch 34001, Training Loss: 40283, Validation Loss: 52387, 190778.71410259302\n",
      "Epoch 34101, Training Loss: 39133, Validation Loss: 51192, 196258.50886794567\n",
      "Epoch 34201, Training Loss: 37772, Validation Loss: 50359, 185306.956292895\n",
      "Epoch 34301, Training Loss: 37640, Validation Loss: 51870, 181713.6987975239\n",
      "Epoch 34401, Training Loss: 37497, Validation Loss: 54225, 165073.41930700958\n",
      "Epoch 34501, Training Loss: 40067, Validation Loss: 55190, 195416.83748659064\n",
      "Epoch 34601, Training Loss: 40012, Validation Loss: 52704, 208945.92183071157\n",
      "Epoch 34701, Training Loss: 39790, Validation Loss: 57582, 306104.378033564\n",
      "Epoch 34801, Training Loss: 41550, Validation Loss: 50416, 174974.18424956934\n",
      "Epoch 34901, Training Loss: 39851, Validation Loss: 53243, 134963.47972662354\n",
      "Epoch 35001, Training Loss: 43659, Validation Loss: 55041, 204594.82300953823\n",
      "Epoch 35101, Training Loss: 38645, Validation Loss: 59787, 171743.1994402676\n",
      "Epoch 35201, Training Loss: 37486, Validation Loss: 53086, 215778.63098653918\n",
      "Epoch 35301, Training Loss: 37232, Validation Loss: 49727, 200388.2588512037\n",
      "Epoch 35401, Training Loss: 38003, Validation Loss: 52403, 196463.98750992297\n",
      "Epoch 35501, Training Loss: 39790, Validation Loss: 49820, 230460.39162949202\n",
      "Epoch 35601, Training Loss: 39750, Validation Loss: 51550, 166699.41995386328\n",
      "Epoch 35701, Training Loss: 38649, Validation Loss: 50867, 194263.07109094682\n",
      "Epoch 35801, Training Loss: 37371, Validation Loss: 53850, 190192.2341310048\n",
      "Epoch 35901, Training Loss: 37429, Validation Loss: 57074, 192252.37752896393\n",
      "Epoch 36001, Training Loss: 43440, Validation Loss: 54527, 159605.98118581236\n",
      "Epoch 36101, Training Loss: 36381, Validation Loss: 55510, 177109.99396932634\n",
      "Epoch 36201, Training Loss: 38505, Validation Loss: 52261, 246068.52657118638\n",
      "Epoch 36301, Training Loss: 36360, Validation Loss: 54252, 182714.96578534693\n",
      "Epoch 36401, Training Loss: 38854, Validation Loss: 52267, 166875.18417017712\n",
      "Epoch 36501, Training Loss: 39541, Validation Loss: 51307, 179494.1771191527\n",
      "Epoch 36601, Training Loss: 37903, Validation Loss: 55603, 238564.157611422\n",
      "Epoch 36701, Training Loss: 37906, Validation Loss: 50097, 201809.55836218572\n",
      "Epoch 36801, Training Loss: 38540, Validation Loss: 48523, 251810.16556296105\n",
      "Epoch 36901, Training Loss: 41307, Validation Loss: 54788, 170054.8409414542\n",
      "Epoch 37001, Training Loss: 42696, Validation Loss: 51445, 194827.2813608572\n",
      "Epoch 37101, Training Loss: 40567, Validation Loss: 51118, 208228.11681921835\n",
      "Epoch 37201, Training Loss: 37768, Validation Loss: 48764, 197935.37970342557\n",
      "Epoch 37301, Training Loss: 38005, Validation Loss: 51004, 175478.3218454639\n",
      "Epoch 37401, Training Loss: 37430, Validation Loss: 51413, 210778.77891561494\n",
      "Epoch 37501, Training Loss: 40339, Validation Loss: 50394, 249116.0181683175\n",
      "Epoch 37601, Training Loss: 42533, Validation Loss: 55061, 179074.93550758777\n",
      "Epoch 37701, Training Loss: 36502, Validation Loss: 56484, 140725.0075918246\n",
      "Epoch 37801, Training Loss: 37661, Validation Loss: 53850, 190008.94447427057\n",
      "Epoch 37901, Training Loss: 36776, Validation Loss: 53528, 234638.97005858985\n",
      "Epoch 38001, Training Loss: 36424, Validation Loss: 55962, 197078.34653744838\n",
      "Epoch 38101, Training Loss: 42150, Validation Loss: 53087, 221952.9131599899\n",
      "Epoch 38201, Training Loss: 41672, Validation Loss: 52508, 179475.22141070603\n",
      "Epoch 38301, Training Loss: 39328, Validation Loss: 54083, 145837.71570448807\n",
      "Epoch 38401, Training Loss: 37682, Validation Loss: 54233, 161399.08125544628\n",
      "Epoch 38501, Training Loss: 41004, Validation Loss: 51869, 144100.80234794415\n",
      "Epoch 38601, Training Loss: 40042, Validation Loss: 50110, 203908.12023826176\n",
      "Epoch 38701, Training Loss: 43742, Validation Loss: 52866, 184653.9901295279\n",
      "Epoch 38801, Training Loss: 36478, Validation Loss: 52733, 158167.16201236108\n",
      "Epoch 38901, Training Loss: 37417, Validation Loss: 56431, 187416.93252770195\n",
      "Epoch 39001, Training Loss: 36873, Validation Loss: 52853, 156841.30297343424\n",
      "Epoch 39101, Training Loss: 39141, Validation Loss: 54446, 131845.6073328168\n",
      "Epoch 39201, Training Loss: 37263, Validation Loss: 54944, 187382.5483094623\n",
      "Epoch 39301, Training Loss: 37927, Validation Loss: 49780, 158411.90763366653\n",
      "Epoch 39401, Training Loss: 38090, Validation Loss: 52434, 230196.91178332665\n",
      "Epoch 39501, Training Loss: 36575, Validation Loss: 52994, 152999.01288559215\n",
      "Epoch 39601, Training Loss: 39858, Validation Loss: 52055, 172655.94905852733\n",
      "Epoch 39701, Training Loss: 38788, Validation Loss: 55927, 249440.76932663113\n",
      "Epoch 39801, Training Loss: 39590, Validation Loss: 52604, 144589.11696905212\n",
      "Epoch 39901, Training Loss: 35441, Validation Loss: 51442, 167663.55905631869\n",
      "Epoch 40001, Training Loss: 40029, Validation Loss: 51935, 187419.53654836226\n",
      "Epoch 40101, Training Loss: 39560, Validation Loss: 54561, 245569.25027033023\n",
      "Epoch 40201, Training Loss: 38963, Validation Loss: 50802, 239258.47002088814\n",
      "Epoch 40301, Training Loss: 39901, Validation Loss: 51917, 196683.38268994808\n",
      "Epoch 40401, Training Loss: 41794, Validation Loss: 56295, 288365.13212312304\n",
      "Epoch 40501, Training Loss: 36780, Validation Loss: 55682, 231973.83864601437\n",
      "Epoch 40601, Training Loss: 38759, Validation Loss: 53106, 251184.46587105733\n",
      "Epoch 40701, Training Loss: 37875, Validation Loss: 55117, 175882.32556673468\n",
      "Epoch 40801, Training Loss: 38981, Validation Loss: 56159, 147962.70591918193\n",
      "Epoch 40901, Training Loss: 38205, Validation Loss: 52155, 169328.11904650906\n",
      "Epoch 41001, Training Loss: 39168, Validation Loss: 49989, 263696.2480431035\n",
      "Epoch 41101, Training Loss: 37374, Validation Loss: 55570, 179273.74912651666\n",
      "Epoch 41201, Training Loss: 41513, Validation Loss: 57096, 239432.25125243305\n",
      "Epoch 41301, Training Loss: 39060, Validation Loss: 51425, 199463.43123010863\n",
      "Epoch 41401, Training Loss: 40581, Validation Loss: 60672, 190166.18459537896\n",
      "Epoch 41501, Training Loss: 36064, Validation Loss: 52700, 162008.86309284868\n",
      "Epoch 41601, Training Loss: 37872, Validation Loss: 51227, 191090.57737102182\n",
      "Epoch 41701, Training Loss: 37994, Validation Loss: 52270, 158313.99291657432\n",
      "Epoch 41801, Training Loss: 39912, Validation Loss: 51841, 198976.90036343178\n",
      "Epoch 41901, Training Loss: 38375, Validation Loss: 50883, 197334.0232190794\n",
      "Epoch 42001, Training Loss: 37695, Validation Loss: 53513, 199127.46510803187\n",
      "Epoch 42101, Training Loss: 39977, Validation Loss: 51948, 198520.91141897626\n",
      "Epoch 42201, Training Loss: 37222, Validation Loss: 55023, 204322.18346528683\n",
      "Epoch 42301, Training Loss: 40927, Validation Loss: 53496, 198956.44862718423\n",
      "Epoch 42401, Training Loss: 35936, Validation Loss: 54796, 152934.75292006088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42501, Training Loss: 38506, Validation Loss: 50614, 210102.2123925436\n",
      "Epoch 42601, Training Loss: 37612, Validation Loss: 48298, 224255.07188086247\n",
      "Epoch 42701, Training Loss: 39518, Validation Loss: 55452, 211934.534604772\n",
      "Epoch 42801, Training Loss: 37036, Validation Loss: 53536, 165038.28041661214\n",
      "Epoch 42901, Training Loss: 38846, Validation Loss: 52966, 268321.40293559065\n",
      "Epoch 43001, Training Loss: 38651, Validation Loss: 54019, 262841.5887730368\n",
      "Epoch 43101, Training Loss: 38043, Validation Loss: 53616, 126235.35632200162\n",
      "Epoch 43201, Training Loss: 39102, Validation Loss: 51452, 209899.8634001217\n",
      "Epoch 43301, Training Loss: 37683, Validation Loss: 49748, 184941.0690396769\n",
      "Epoch 43401, Training Loss: 36885, Validation Loss: 52482, 186088.58799084343\n",
      "Epoch 43501, Training Loss: 37863, Validation Loss: 52316, 236704.19014661582\n",
      "Epoch 43601, Training Loss: 38814, Validation Loss: 53450, 177812.84869787647\n",
      "Epoch 43701, Training Loss: 39340, Validation Loss: 56626, 207580.39538848746\n",
      "Epoch 43801, Training Loss: 39587, Validation Loss: 56394, 230176.6068056322\n",
      "Epoch 43901, Training Loss: 37426, Validation Loss: 53775, 146427.8245924189\n",
      "Epoch 44001, Training Loss: 40146, Validation Loss: 55942, 146474.73521659875\n",
      "Epoch 44101, Training Loss: 39670, Validation Loss: 53054, 151952.0659569182\n",
      "Epoch 44201, Training Loss: 40582, Validation Loss: 52681, 253063.4582868435\n",
      "Epoch 44301, Training Loss: 40856, Validation Loss: 55248, 344912.08150463854\n",
      "Epoch 44401, Training Loss: 38395, Validation Loss: 54139, 208300.92124737822\n",
      "Epoch 44501, Training Loss: 37229, Validation Loss: 53920, 172778.42795668906\n",
      "Epoch 44601, Training Loss: 38456, Validation Loss: 50365, 205945.28482523086\n",
      "Epoch 44701, Training Loss: 34937, Validation Loss: 51504, 185959.27549185968\n",
      "Epoch 44801, Training Loss: 34242, Validation Loss: 54416, 206697.33492836784\n",
      "Epoch 44901, Training Loss: 40018, Validation Loss: 53651, 146694.2142772795\n",
      "Epoch 45001, Training Loss: 36980, Validation Loss: 52146, 188829.52958010233\n",
      "Epoch 45101, Training Loss: 38273, Validation Loss: 53619, 198296.79721074077\n",
      "Epoch 45201, Training Loss: 36358, Validation Loss: 48883, 181539.98661740113\n",
      "Epoch 45301, Training Loss: 38819, Validation Loss: 49954, 140902.1624683387\n",
      "Epoch 45401, Training Loss: 38019, Validation Loss: 51518, 147111.21945943264\n",
      "Epoch 45501, Training Loss: 41660, Validation Loss: 54270, 193472.25604405472\n",
      "Epoch 45601, Training Loss: 38520, Validation Loss: 50752, 283757.1644427538\n",
      "Epoch 45701, Training Loss: 36979, Validation Loss: 51886, 136078.6816261483\n",
      "Epoch 45801, Training Loss: 36316, Validation Loss: 48440, 154965.79499922495\n",
      "Epoch 45901, Training Loss: 37523, Validation Loss: 55311, 168497.48590896348\n",
      "Epoch 46001, Training Loss: 38735, Validation Loss: 50240, 166652.27563383602\n",
      "Epoch 46101, Training Loss: 36187, Validation Loss: 49258, 211264.64437168426\n",
      "Epoch 46201, Training Loss: 36779, Validation Loss: 53849, 157683.2755510518\n",
      "Epoch 46301, Training Loss: 35074, Validation Loss: 51100, 261836.06404167644\n",
      "Epoch 46401, Training Loss: 35589, Validation Loss: 53849, 131894.67124631492\n",
      "Epoch 46501, Training Loss: 38010, Validation Loss: 55252, 202034.41837166794\n",
      "Epoch 46601, Training Loss: 40189, Validation Loss: 52578, 204552.13329280517\n",
      "Epoch 46701, Training Loss: 34191, Validation Loss: 57648, 185186.07190706264\n",
      "Epoch 46801, Training Loss: 34652, Validation Loss: 55396, 148718.1682913659\n",
      "Epoch 46901, Training Loss: 38069, Validation Loss: 52495, 213855.31337614197\n",
      "Epoch 47001, Training Loss: 36798, Validation Loss: 52562, 215582.89630341195\n",
      "Epoch 47101, Training Loss: 38662, Validation Loss: 55854, 224587.7759422825\n",
      "Epoch 47201, Training Loss: 38191, Validation Loss: 53695, 209102.02923536833\n",
      "Epoch 47301, Training Loss: 39731, Validation Loss: 53020, 278134.01080027036\n",
      "Epoch 47401, Training Loss: 39428, Validation Loss: 53816, 169978.14440115166\n",
      "Epoch 47501, Training Loss: 35877, Validation Loss: 51075, 162063.7698611999\n",
      "Epoch 47601, Training Loss: 40499, Validation Loss: 54861, 185904.83684080755\n",
      "Epoch 47701, Training Loss: 37555, Validation Loss: 52538, 188823.86420654715\n",
      "Epoch 47801, Training Loss: 37605, Validation Loss: 51697, 213170.77766843876\n",
      "Epoch 47901, Training Loss: 35920, Validation Loss: 52193, 135876.8084822379\n",
      "Epoch 48001, Training Loss: 40017, Validation Loss: 57650, 172180.27969017017\n",
      "Epoch 48101, Training Loss: 37676, Validation Loss: 53412, 201288.76738796875\n",
      "Epoch 48201, Training Loss: 34922, Validation Loss: 51719, 145572.49729203436\n",
      "Epoch 48301, Training Loss: 36417, Validation Loss: 54677, 216034.52599319196\n",
      "Epoch 48401, Training Loss: 37147, Validation Loss: 50844, 231789.5260028292\n",
      "Epoch 48501, Training Loss: 36574, Validation Loss: 55102, 220796.9492291337\n",
      "Epoch 48601, Training Loss: 37612, Validation Loss: 51789, 178650.6769931795\n",
      "Epoch 48701, Training Loss: 37354, Validation Loss: 50840, 163415.66458530937\n",
      "Epoch 48801, Training Loss: 36437, Validation Loss: 51300, 167686.1334666549\n",
      "Epoch 48901, Training Loss: 38561, Validation Loss: 52507, 206684.21965720263\n",
      "Epoch 49001, Training Loss: 36398, Validation Loss: 51670, 218393.746295878\n",
      "Epoch 49101, Training Loss: 34633, Validation Loss: 51486, 170249.42590526165\n",
      "Epoch 49201, Training Loss: 35483, Validation Loss: 51103, 196275.53702454994\n",
      "Epoch 49301, Training Loss: 37437, Validation Loss: 50660, 167307.6736280188\n",
      "Epoch 49401, Training Loss: 37870, Validation Loss: 50679, 172801.24866187104\n",
      "Epoch 49501, Training Loss: 38261, Validation Loss: 49463, 169795.33410381258\n",
      "Epoch 49601, Training Loss: 36858, Validation Loss: 53666, 204605.62604704054\n",
      "Epoch 49701, Training Loss: 38965, Validation Loss: 50060, 217494.23392973747\n",
      "Epoch 49801, Training Loss: 38033, Validation Loss: 54468, 157508.89524406093\n",
      "Epoch 49901, Training Loss: 37439, Validation Loss: 52582, 158554.03251351792\n",
      "Epoch 50001, Training Loss: 38587, Validation Loss: 52099, 200428.84847631422\n",
      "Epoch 50101, Training Loss: 36818, Validation Loss: 52599, 153301.04543042078\n",
      "Epoch 50201, Training Loss: 35384, Validation Loss: 51361, 177740.77668406107\n",
      "Epoch 50301, Training Loss: 36900, Validation Loss: 51014, 168966.3815477799\n",
      "Epoch 50401, Training Loss: 37994, Validation Loss: 54751, 240106.18438450867\n",
      "Epoch 50501, Training Loss: 35946, Validation Loss: 53169, 175979.2765530484\n",
      "Epoch 50601, Training Loss: 36479, Validation Loss: 53159, 155339.82781921796\n",
      "Epoch 50701, Training Loss: 33829, Validation Loss: 51897, 193249.36945091336\n",
      "Epoch 50801, Training Loss: 35266, Validation Loss: 51209, 167570.1079574811\n",
      "Epoch 50901, Training Loss: 36494, Validation Loss: 55562, 141989.30911158837\n",
      "Epoch 51001, Training Loss: 34793, Validation Loss: 52888, 155206.08029414914\n",
      "Epoch 51101, Training Loss: 38612, Validation Loss: 56670, 155340.76618354654\n",
      "Epoch 51201, Training Loss: 36864, Validation Loss: 53573, 154703.5743657835\n",
      "Epoch 51301, Training Loss: 35133, Validation Loss: 53036, 203039.23623465898\n",
      "Epoch 51401, Training Loss: 37590, Validation Loss: 54602, 197693.4668552319\n",
      "Epoch 51501, Training Loss: 37234, Validation Loss: 57917, 175607.16156419064\n",
      "Epoch 51601, Training Loss: 36452, Validation Loss: 55838, 132445.44683892914\n",
      "Epoch 51701, Training Loss: 37177, Validation Loss: 54687, 199615.28214365433\n",
      "Epoch 51801, Training Loss: 38551, Validation Loss: 52186, 219878.39538266519\n",
      "Epoch 51901, Training Loss: 37227, Validation Loss: 50937, 217394.37935593384\n",
      "Epoch 52001, Training Loss: 37128, Validation Loss: 57233, 196091.37191235585\n",
      "Epoch 52101, Training Loss: 36837, Validation Loss: 51245, 196363.47008726388\n",
      "Epoch 52201, Training Loss: 36986, Validation Loss: 52889, 141686.60885170536\n",
      "Epoch 52301, Training Loss: 41367, Validation Loss: 53398, 149941.8927535727\n",
      "Epoch 52401, Training Loss: 38899, Validation Loss: 53606, 199314.97835789024\n",
      "Epoch 52501, Training Loss: 39281, Validation Loss: 60315, 246290.55072970028\n",
      "Epoch 52601, Training Loss: 41808, Validation Loss: 55864, 240655.2192279376\n",
      "Epoch 52701, Training Loss: 37653, Validation Loss: 52354, 186898.94285459004\n",
      "Epoch 52801, Training Loss: 38247, Validation Loss: 51340, 129428.60484142935\n",
      "Epoch 52901, Training Loss: 37813, Validation Loss: 52396, 208225.5996538898\n",
      "Epoch 53001, Training Loss: 37293, Validation Loss: 55366, 168403.1565701304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53101, Training Loss: 36299, Validation Loss: 52727, 194622.45140569317\n",
      "Epoch 53201, Training Loss: 37850, Validation Loss: 53070, 127764.73659997864\n",
      "Epoch 53301, Training Loss: 39876, Validation Loss: 56351, 167648.81260453584\n",
      "Epoch 53401, Training Loss: 38286, Validation Loss: 51226, 227198.23809827157\n",
      "Epoch 53501, Training Loss: 36588, Validation Loss: 51137, 186030.57638293752\n",
      "Epoch 53601, Training Loss: 36687, Validation Loss: 53991, 203680.44512218263\n",
      "Epoch 53701, Training Loss: 39598, Validation Loss: 54945, 236827.39210624062\n",
      "Epoch 53801, Training Loss: 38593, Validation Loss: 52627, 134855.69208623294\n",
      "Epoch 53901, Training Loss: 37747, Validation Loss: 56209, 234262.11578799642\n",
      "Epoch 54001, Training Loss: 40541, Validation Loss: 53410, 192333.35105126115\n",
      "Epoch 54101, Training Loss: 36310, Validation Loss: 51999, 183682.698774077\n",
      "Epoch 54201, Training Loss: 38939, Validation Loss: 53249, 183334.68873215094\n",
      "Epoch 54301, Training Loss: 37624, Validation Loss: 54543, 121017.73338047219\n",
      "Epoch 54401, Training Loss: 36984, Validation Loss: 55314, 167451.730825784\n",
      "Epoch 54501, Training Loss: 36234, Validation Loss: 55407, 166764.20111907387\n",
      "Epoch 54601, Training Loss: 37916, Validation Loss: 54427, 208924.16989957044\n",
      "Epoch 54701, Training Loss: 36169, Validation Loss: 53204, 184872.0829314758\n",
      "Epoch 54801, Training Loss: 39191, Validation Loss: 58366, 145629.52522974546\n",
      "Epoch 54901, Training Loss: 39350, Validation Loss: 55030, 156753.48826419702\n",
      "Epoch 55001, Training Loss: 38404, Validation Loss: 52827, 186187.77007592397\n",
      "Epoch 55101, Training Loss: 36385, Validation Loss: 53199, 152003.95048263695\n",
      "Epoch 55201, Training Loss: 35108, Validation Loss: 51621, 122036.91922753617\n",
      "Epoch 55301, Training Loss: 38405, Validation Loss: 55869, 190347.85234754797\n",
      "Epoch 55401, Training Loss: 38082, Validation Loss: 52585, 243970.3924923373\n",
      "Epoch 55501, Training Loss: 38195, Validation Loss: 50392, 194451.6465270306\n",
      "Epoch 55601, Training Loss: 38313, Validation Loss: 57057, 197313.68675756795\n",
      "Epoch 55701, Training Loss: 37634, Validation Loss: 53712, 142993.8099977756\n",
      "Epoch 55801, Training Loss: 39017, Validation Loss: 50826, 180268.0555611342\n",
      "Epoch 55901, Training Loss: 36729, Validation Loss: 54684, 122946.36876818293\n",
      "Epoch 56001, Training Loss: 33501, Validation Loss: 54961, 157033.14137674405\n",
      "Epoch 56101, Training Loss: 37352, Validation Loss: 52933, 145395.02590960087\n",
      "Epoch 56201, Training Loss: 39049, Validation Loss: 53794, 152099.48139765175\n",
      "Epoch 56301, Training Loss: 37911, Validation Loss: 52464, 174375.32317828923\n",
      "Epoch 56401, Training Loss: 40345, Validation Loss: 50978, 162993.01886410304\n",
      "Epoch 56501, Training Loss: 37540, Validation Loss: 51785, 157422.3357385924\n",
      "Epoch 56601, Training Loss: 37255, Validation Loss: 57191, 146249.58361674694\n",
      "Epoch 56701, Training Loss: 37713, Validation Loss: 56332, 177036.6717250204\n",
      "Epoch 56801, Training Loss: 34817, Validation Loss: 51073, 186503.23619844104\n",
      "Epoch 56901, Training Loss: 37943, Validation Loss: 51552, 175657.59467160006\n",
      "Epoch 57001, Training Loss: 35562, Validation Loss: 53292, 148694.36543011514\n",
      "Epoch 57101, Training Loss: 36323, Validation Loss: 56524, 156779.4966189709\n",
      "Epoch 57201, Training Loss: 38064, Validation Loss: 55469, 186334.37280778424\n",
      "Epoch 57301, Training Loss: 37212, Validation Loss: 55528, 160051.59533150189\n",
      "Epoch 57401, Training Loss: 37414, Validation Loss: 50299, 149651.86974397374\n",
      "Epoch 57501, Training Loss: 38609, Validation Loss: 55945, 183192.2827625873\n",
      "Epoch 57601, Training Loss: 37224, Validation Loss: 52788, 237745.33477732612\n",
      "Epoch 57701, Training Loss: 37120, Validation Loss: 55119, 185714.23961288252\n",
      "Epoch 57801, Training Loss: 37000, Validation Loss: 53065, 231609.99091087005\n",
      "Epoch 57901, Training Loss: 38790, Validation Loss: 55207, 224782.61002606378\n",
      "Epoch 58001, Training Loss: 35709, Validation Loss: 52844, 143803.81765105433\n",
      "Epoch 58101, Training Loss: 38972, Validation Loss: 53606, 153582.31076671375\n",
      "Epoch 58201, Training Loss: 37103, Validation Loss: 52900, 217436.22202150896\n",
      "Epoch 58301, Training Loss: 35676, Validation Loss: 56965, 229028.57156681604\n",
      "Epoch 58401, Training Loss: 36647, Validation Loss: 54207, 182971.19853416653\n",
      "Epoch 58501, Training Loss: 33807, Validation Loss: 56925, 144110.46509252395\n",
      "Epoch 58601, Training Loss: 36623, Validation Loss: 54109, 148714.9279632645\n",
      "Epoch 58701, Training Loss: 36154, Validation Loss: 53777, 222663.78683418417\n",
      "Epoch 58801, Training Loss: 39494, Validation Loss: 55421, 209065.69947498478\n",
      "Epoch 58901, Training Loss: 38954, Validation Loss: 54092, 199366.13591195658\n",
      "Epoch 59001, Training Loss: 35183, Validation Loss: 54529, 179194.49906168936\n",
      "Epoch 59101, Training Loss: 36609, Validation Loss: 51345, 163777.4236613965\n",
      "Epoch 59201, Training Loss: 38083, Validation Loss: 54634, 149557.17082833863\n",
      "Epoch 59301, Training Loss: 35936, Validation Loss: 53088, 214287.0598849732\n",
      "Epoch 59401, Training Loss: 35522, Validation Loss: 57460, 160451.60348016463\n",
      "Epoch 59501, Training Loss: 39266, Validation Loss: 57036, 210628.40359859276\n",
      "Epoch 59601, Training Loss: 35489, Validation Loss: 56408, 155283.61591393233\n",
      "Epoch 59701, Training Loss: 36389, Validation Loss: 56409, 207135.37839426388\n",
      "Epoch 59801, Training Loss: 36924, Validation Loss: 53251, 163919.45409395764\n",
      "Epoch 59901, Training Loss: 37433, Validation Loss: 60523, 185036.5319153207\n",
      "Epoch 60001, Training Loss: 39062, Validation Loss: 51027, 159909.506867458\n",
      "Epoch 60101, Training Loss: 35106, Validation Loss: 51874, 150485.78368837477\n",
      "Epoch 60201, Training Loss: 39116, Validation Loss: 53161, 195858.615948579\n",
      "Epoch 60301, Training Loss: 38160, Validation Loss: 54476, 147116.5294710908\n",
      "Epoch 60401, Training Loss: 35132, Validation Loss: 56539, 153648.66893303258\n",
      "Epoch 60501, Training Loss: 36583, Validation Loss: 55538, 150848.06347292894\n",
      "Epoch 60601, Training Loss: 39966, Validation Loss: 54579, 224694.15394078684\n",
      "Epoch 60701, Training Loss: 40468, Validation Loss: 55726, 152181.0251127143\n",
      "Epoch 60801, Training Loss: 35924, Validation Loss: 52356, 163855.6423547985\n",
      "Epoch 60901, Training Loss: 36681, Validation Loss: 55962, 225076.43526373353\n",
      "Epoch 61001, Training Loss: 36351, Validation Loss: 57130, 168786.1119405873\n",
      "Epoch 61101, Training Loss: 38595, Validation Loss: 54921, 152905.00251731966\n",
      "Epoch 61201, Training Loss: 35886, Validation Loss: 52756, 155684.55647380068\n",
      "Epoch 61301, Training Loss: 36596, Validation Loss: 55233, 207723.63830633927\n",
      "Epoch 61401, Training Loss: 34793, Validation Loss: 54850, 195540.74650767221\n",
      "Epoch 61501, Training Loss: 36187, Validation Loss: 52336, 154633.87507978658\n",
      "Epoch 61601, Training Loss: 40892, Validation Loss: 53887, 186804.90052889203\n",
      "Epoch 61701, Training Loss: 38588, Validation Loss: 51732, 176412.09763727707\n",
      "Epoch 61801, Training Loss: 36189, Validation Loss: 51786, 215757.62444722615\n",
      "Epoch 61901, Training Loss: 38297, Validation Loss: 59941, 193871.3489585889\n",
      "Epoch 62001, Training Loss: 38546, Validation Loss: 56339, 149238.25995251024\n",
      "Epoch 62101, Training Loss: 37286, Validation Loss: 58364, 232299.10880165212\n",
      "Epoch 62201, Training Loss: 38951, Validation Loss: 54220, 132804.98969114988\n",
      "Epoch 62301, Training Loss: 37505, Validation Loss: 51945, 227223.0634429738\n",
      "Epoch 62401, Training Loss: 37989, Validation Loss: 57351, 214017.81901877708\n",
      "Epoch 62501, Training Loss: 37025, Validation Loss: 52246, 148552.4187658612\n",
      "Epoch 62601, Training Loss: 36597, Validation Loss: 56742, 167277.33266254628\n",
      "Epoch 62701, Training Loss: 38187, Validation Loss: 52694, 148625.65164313928\n",
      "Epoch 62801, Training Loss: 36806, Validation Loss: 52776, 183572.68858435843\n",
      "Epoch 62901, Training Loss: 34762, Validation Loss: 52979, 134347.58146290143\n",
      "Epoch 63001, Training Loss: 36150, Validation Loss: 56015, 152893.4244664285\n",
      "Epoch 63101, Training Loss: 38745, Validation Loss: 53854, 211583.98991767535\n",
      "Epoch 63201, Training Loss: 37823, Validation Loss: 56993, 193056.82342273285\n",
      "Epoch 63301, Training Loss: 37185, Validation Loss: 53012, 166064.2435512583\n",
      "Epoch 63401, Training Loss: 38082, Validation Loss: 53333, 139255.6826984751\n",
      "Epoch 63501, Training Loss: 37221, Validation Loss: 55808, 174503.00158537098\n",
      "Epoch 63601, Training Loss: 38554, Validation Loss: 57884, 280128.65012971795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63701, Training Loss: 38275, Validation Loss: 54070, 168993.2619621067\n",
      "Epoch 63801, Training Loss: 37671, Validation Loss: 53496, 218372.83878678977\n",
      "Epoch 63901, Training Loss: 37983, Validation Loss: 56236, 174775.64125121455\n",
      "Epoch 64001, Training Loss: 37217, Validation Loss: 53220, 134358.47815702602\n",
      "Epoch 64101, Training Loss: 38785, Validation Loss: 57074, 158094.13381659196\n",
      "Epoch 64201, Training Loss: 35990, Validation Loss: 50992, 138710.70251978596\n",
      "Epoch 64301, Training Loss: 37541, Validation Loss: 54467, 151101.12383394092\n",
      "Epoch 64401, Training Loss: 37072, Validation Loss: 54590, 195555.73656586884\n",
      "Epoch 64501, Training Loss: 35902, Validation Loss: 56515, 177567.8817210649\n",
      "Epoch 64601, Training Loss: 34517, Validation Loss: 55094, 201961.55136012522\n",
      "Epoch 64701, Training Loss: 36156, Validation Loss: 54644, 135103.1917616722\n",
      "Epoch 64801, Training Loss: 34872, Validation Loss: 56225, 179422.23087067457\n",
      "Epoch 64901, Training Loss: 37064, Validation Loss: 52217, 165473.4321129836\n",
      "Epoch 65001, Training Loss: 40062, Validation Loss: 54230, 150787.11989053825\n",
      "Epoch 65101, Training Loss: 37203, Validation Loss: 53261, 180927.99616029815\n",
      "Epoch 65201, Training Loss: 35908, Validation Loss: 53847, 123411.01475595318\n",
      "Epoch 65301, Training Loss: 35224, Validation Loss: 57059, 170504.51072232754\n",
      "Epoch 65401, Training Loss: 35232, Validation Loss: 58620, 128516.40933951472\n",
      "Epoch 65501, Training Loss: 36359, Validation Loss: 56056, 172280.19579726807\n",
      "Epoch 65601, Training Loss: 36849, Validation Loss: 55997, 194226.3072947144\n",
      "Epoch 65701, Training Loss: 37236, Validation Loss: 55177, 254947.34491525064\n",
      "Epoch 65801, Training Loss: 37518, Validation Loss: 56501, 180274.11912394883\n",
      "Epoch 65901, Training Loss: 39907, Validation Loss: 52729, 231425.59123679379\n",
      "Epoch 66001, Training Loss: 36214, Validation Loss: 57129, 168308.6615436563\n",
      "Epoch 66101, Training Loss: 37062, Validation Loss: 52363, 154123.68518558724\n",
      "Epoch 66201, Training Loss: 36055, Validation Loss: 54429, 144258.1528525245\n",
      "Epoch 66301, Training Loss: 38678, Validation Loss: 54930, 136031.5152842301\n",
      "Epoch 66401, Training Loss: 35785, Validation Loss: 54104, 146381.55923837025\n",
      "Epoch 66501, Training Loss: 36603, Validation Loss: 55944, 148684.41972194248\n",
      "Epoch 66601, Training Loss: 36237, Validation Loss: 49989, 186430.52707268912\n",
      "Epoch 66701, Training Loss: 34967, Validation Loss: 54260, 168918.16258868974\n",
      "Epoch 66801, Training Loss: 37781, Validation Loss: 54531, 184904.9218204033\n",
      "Epoch 66901, Training Loss: 37712, Validation Loss: 53699, 190748.27362807945\n",
      "Epoch 67001, Training Loss: 35269, Validation Loss: 54041, 215698.29998631464\n",
      "Epoch 67101, Training Loss: 34593, Validation Loss: 52932, 117755.6854619482\n",
      "Epoch 67201, Training Loss: 36733, Validation Loss: 52425, 180685.93995746892\n",
      "Epoch 67301, Training Loss: 39530, Validation Loss: 51949, 180675.93542320732\n",
      "Epoch 67401, Training Loss: 38921, Validation Loss: 52796, 166017.77035439204\n",
      "Epoch 67501, Training Loss: 34387, Validation Loss: 58002, 220901.3287114553\n",
      "Epoch 67601, Training Loss: 36781, Validation Loss: 51899, 144603.41614207265\n",
      "Epoch 67701, Training Loss: 37829, Validation Loss: 52529, 265041.6755483966\n",
      "Epoch 67801, Training Loss: 37864, Validation Loss: 55533, 181338.14370784684\n",
      "Epoch 67901, Training Loss: 35656, Validation Loss: 56992, 153223.87979317454\n",
      "Epoch 68001, Training Loss: 35356, Validation Loss: 51025, 212025.95094548946\n",
      "Epoch 68101, Training Loss: 36921, Validation Loss: 54342, 163885.73652015795\n",
      "Epoch 68201, Training Loss: 38012, Validation Loss: 53232, 254104.79608435734\n",
      "Epoch 68301, Training Loss: 39262, Validation Loss: 54660, 169915.3773717009\n",
      "Epoch 68401, Training Loss: 34345, Validation Loss: 53223, 147815.26199763315\n",
      "Epoch 68501, Training Loss: 35353, Validation Loss: 51794, 185520.70218756434\n",
      "Epoch 68601, Training Loss: 36508, Validation Loss: 49752, 172466.15326917532\n",
      "Epoch 68701, Training Loss: 36323, Validation Loss: 53193, 207170.96566902116\n",
      "Epoch 68801, Training Loss: 39983, Validation Loss: 54879, 213682.35528086385\n",
      "Epoch 68901, Training Loss: 35350, Validation Loss: 53684, 133682.32956418686\n",
      "Epoch 69001, Training Loss: 36205, Validation Loss: 50848, 178810.94781701418\n",
      "Epoch 69101, Training Loss: 37451, Validation Loss: 55446, 196831.70212167516\n",
      "Epoch 69201, Training Loss: 36794, Validation Loss: 52541, 129793.16764367271\n",
      "Epoch 69301, Training Loss: 38183, Validation Loss: 60086, 205450.4220766604\n",
      "Epoch 69401, Training Loss: 36051, Validation Loss: 53017, 176567.7167071188\n",
      "Epoch 69501, Training Loss: 35537, Validation Loss: 52759, 174509.55584437793\n",
      "Epoch 69601, Training Loss: 36539, Validation Loss: 53604, 173221.5234145609\n",
      "Epoch 69701, Training Loss: 39172, Validation Loss: 54029, 156659.52713585438\n",
      "Epoch 69801, Training Loss: 37464, Validation Loss: 53045, 145254.42433228274\n",
      "Epoch 69901, Training Loss: 37899, Validation Loss: 58239, 182304.1114551101\n",
      "Epoch 70001, Training Loss: 36491, Validation Loss: 54200, 229385.6582113063\n",
      "Epoch 70101, Training Loss: 36331, Validation Loss: 55601, 254842.91954488363\n",
      "Epoch 70201, Training Loss: 36905, Validation Loss: 58139, 145890.80816291645\n",
      "Epoch 70301, Training Loss: 38215, Validation Loss: 54279, 174406.38535689135\n",
      "Epoch 70401, Training Loss: 35960, Validation Loss: 55317, 139485.49702241065\n",
      "Epoch 70501, Training Loss: 36513, Validation Loss: 51469, 139906.0487256602\n",
      "Epoch 70601, Training Loss: 37805, Validation Loss: 51630, 191504.35422779815\n",
      "Epoch 70701, Training Loss: 36741, Validation Loss: 56809, 138776.26353648835\n",
      "Epoch 70801, Training Loss: 34502, Validation Loss: 53364, 140924.14063688205\n",
      "Epoch 70901, Training Loss: 38159, Validation Loss: 50878, 197690.07385732917\n",
      "Epoch 71001, Training Loss: 37981, Validation Loss: 52260, 193470.0749423609\n",
      "Epoch 71101, Training Loss: 35598, Validation Loss: 51004, 206973.79701591528\n",
      "Epoch 71201, Training Loss: 36575, Validation Loss: 52493, 147462.21682347727\n",
      "Epoch 71301, Training Loss: 37763, Validation Loss: 53247, 166800.56958080447\n",
      "Epoch 71401, Training Loss: 38829, Validation Loss: 58466, 195261.08150621448\n",
      "Epoch 71501, Training Loss: 39014, Validation Loss: 54140, 213318.38923818417\n",
      "Epoch 71601, Training Loss: 36601, Validation Loss: 57070, 154932.15268153485\n",
      "Epoch 71701, Training Loss: 38251, Validation Loss: 53162, 238239.44090719745\n",
      "Epoch 71801, Training Loss: 36261, Validation Loss: 54280, 155317.23605661487\n",
      "Epoch 71901, Training Loss: 36645, Validation Loss: 55124, 180619.8960016633\n",
      "Epoch 72001, Training Loss: 37675, Validation Loss: 52538, 229956.09827064464\n",
      "Epoch 72101, Training Loss: 39036, Validation Loss: 54265, 194284.66350803967\n",
      "Epoch 72201, Training Loss: 36737, Validation Loss: 53988, 174490.43015681484\n",
      "Epoch 72301, Training Loss: 35776, Validation Loss: 56245, 171390.28709941823\n",
      "Epoch 72401, Training Loss: 38262, Validation Loss: 56059, 179947.27283255718\n",
      "Epoch 72501, Training Loss: 37134, Validation Loss: 52170, 155238.20583762843\n",
      "Epoch 72601, Training Loss: 36435, Validation Loss: 52393, 182998.34289710177\n",
      "Epoch 72701, Training Loss: 35103, Validation Loss: 51386, 148331.38401173428\n",
      "Epoch 72801, Training Loss: 35498, Validation Loss: 53174, 172348.40890731008\n",
      "Epoch 72901, Training Loss: 38190, Validation Loss: 56457, 150535.96940815685\n",
      "Epoch 73001, Training Loss: 37860, Validation Loss: 51651, 132450.99378616284\n",
      "Epoch 73101, Training Loss: 36611, Validation Loss: 54315, 116038.85529603013\n",
      "Epoch 73201, Training Loss: 35047, Validation Loss: 53652, 163429.13614767345\n",
      "Epoch 73301, Training Loss: 39380, Validation Loss: 56669, 188567.88732713702\n",
      "Epoch 73401, Training Loss: 36308, Validation Loss: 54280, 143291.65704288185\n",
      "Epoch 73501, Training Loss: 36009, Validation Loss: 58053, 175898.01782084082\n",
      "Epoch 73601, Training Loss: 33926, Validation Loss: 52209, 167219.0451960809\n",
      "Epoch 73701, Training Loss: 38277, Validation Loss: 53483, 193358.07071813685\n",
      "Epoch 73801, Training Loss: 36762, Validation Loss: 54982, 207200.34852452236\n",
      "Epoch 73901, Training Loss: 35900, Validation Loss: 53789, 150659.51287029203\n",
      "Epoch 74001, Training Loss: 35536, Validation Loss: 54922, 190623.9256069212\n",
      "Epoch 74101, Training Loss: 35274, Validation Loss: 52946, 201780.73908713568\n",
      "Epoch 74201, Training Loss: 38317, Validation Loss: 53294, 157868.05863539068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74301, Training Loss: 35626, Validation Loss: 53872, 157258.90540202867\n",
      "Epoch 74401, Training Loss: 34068, Validation Loss: 54413, 176516.4601032883\n",
      "Epoch 74501, Training Loss: 38138, Validation Loss: 55926, 213355.86942492603\n",
      "Epoch 74601, Training Loss: 35639, Validation Loss: 53388, 177165.4532257476\n",
      "Epoch 74701, Training Loss: 36967, Validation Loss: 53656, 171871.93609879966\n",
      "Epoch 74801, Training Loss: 39544, Validation Loss: 54596, 202976.38990961903\n",
      "Epoch 74901, Training Loss: 35237, Validation Loss: 52994, 169833.38681457515\n",
      "Epoch 75001, Training Loss: 37451, Validation Loss: 55025, 185703.80090898994\n",
      "Epoch 75101, Training Loss: 37188, Validation Loss: 55375, 180809.15966988253\n",
      "Epoch 75201, Training Loss: 33964, Validation Loss: 54757, 157432.36394998283\n",
      "Epoch 75301, Training Loss: 34818, Validation Loss: 53269, 161960.66025914144\n",
      "Epoch 75401, Training Loss: 37037, Validation Loss: 52455, 154399.5465877788\n",
      "Epoch 75501, Training Loss: 35768, Validation Loss: 54814, 182516.7248129693\n",
      "Epoch 75601, Training Loss: 36012, Validation Loss: 54270, 231733.111758407\n",
      "Epoch 75701, Training Loss: 36519, Validation Loss: 53603, 185028.11281634154\n",
      "Epoch 75801, Training Loss: 37668, Validation Loss: 60518, 149791.34704147096\n",
      "Epoch 75901, Training Loss: 35604, Validation Loss: 53185, 159737.27317120405\n",
      "Epoch 76001, Training Loss: 35953, Validation Loss: 52616, 211367.0464537718\n",
      "Epoch 76101, Training Loss: 35587, Validation Loss: 51412, 192530.03832172658\n",
      "Epoch 76201, Training Loss: 37663, Validation Loss: 53978, 191909.1648976816\n",
      "Epoch 76301, Training Loss: 36681, Validation Loss: 54364, 180296.23942815917\n",
      "Epoch 76401, Training Loss: 35780, Validation Loss: 53648, 170745.26824890438\n",
      "Epoch 76501, Training Loss: 37081, Validation Loss: 55961, 180669.18167813882\n",
      "Epoch 76601, Training Loss: 34123, Validation Loss: 56467, 128344.93337398674\n",
      "Epoch 76701, Training Loss: 36895, Validation Loss: 54880, 254204.30582434373\n",
      "Epoch 76801, Training Loss: 34485, Validation Loss: 53773, 167521.84540438929\n",
      "Epoch 76901, Training Loss: 34747, Validation Loss: 55223, 132166.1739683738\n",
      "Epoch 77001, Training Loss: 33431, Validation Loss: 54332, 113619.11149188499\n",
      "Epoch 77101, Training Loss: 35525, Validation Loss: 51791, 163265.99484923948\n",
      "Epoch 77201, Training Loss: 39471, Validation Loss: 55273, 167689.72454854337\n",
      "Epoch 77301, Training Loss: 34785, Validation Loss: 53209, 134312.49589207416\n",
      "Epoch 77401, Training Loss: 37656, Validation Loss: 56809, 146018.5224179748\n",
      "Epoch 77501, Training Loss: 36108, Validation Loss: 52270, 181100.16015054053\n",
      "Epoch 77601, Training Loss: 35934, Validation Loss: 53649, 160086.93543300292\n",
      "Epoch 77701, Training Loss: 35854, Validation Loss: 54715, 152834.2469244429\n",
      "Epoch 77801, Training Loss: 35313, Validation Loss: 54966, 199029.82555455362\n",
      "Epoch 77901, Training Loss: 35578, Validation Loss: 56410, 176812.7999090116\n",
      "Epoch 78001, Training Loss: 36690, Validation Loss: 55782, 179729.60408635848\n",
      "Epoch 78101, Training Loss: 36819, Validation Loss: 54207, 200441.22273635012\n",
      "Epoch 78201, Training Loss: 36450, Validation Loss: 55542, 156896.8479886807\n",
      "Epoch 78301, Training Loss: 35148, Validation Loss: 54108, 144723.4813685104\n",
      "Epoch 78401, Training Loss: 35790, Validation Loss: 54880, 158137.61941959537\n",
      "Epoch 78501, Training Loss: 40530, Validation Loss: 53334, 169123.76587957426\n",
      "Epoch 78601, Training Loss: 34158, Validation Loss: 54592, 223773.07224440947\n",
      "Epoch 78701, Training Loss: 35276, Validation Loss: 52865, 174947.72793273965\n",
      "Epoch 78801, Training Loss: 38210, Validation Loss: 54669, 181368.17560771506\n",
      "Epoch 78901, Training Loss: 35841, Validation Loss: 51734, 199312.06967804045\n",
      "Epoch 79001, Training Loss: 38861, Validation Loss: 56818, 236485.84164287706\n",
      "Epoch 79101, Training Loss: 35430, Validation Loss: 56296, 167939.3631426686\n",
      "Epoch 79201, Training Loss: 35670, Validation Loss: 54433, 172121.71317054957\n",
      "Epoch 79301, Training Loss: 37581, Validation Loss: 55036, 211938.76476031507\n",
      "Epoch 79401, Training Loss: 37782, Validation Loss: 53389, 164051.95491248977\n",
      "Epoch 79501, Training Loss: 36587, Validation Loss: 55988, 213131.5358067609\n",
      "Epoch 79601, Training Loss: 38790, Validation Loss: 52618, 198712.18757361185\n",
      "Epoch 79701, Training Loss: 36278, Validation Loss: 52888, 158452.31916299602\n",
      "Epoch 79801, Training Loss: 36058, Validation Loss: 56973, 249193.18201269628\n",
      "Epoch 79901, Training Loss: 35311, Validation Loss: 55758, 248763.55733192852\n",
      "Epoch 80001, Training Loss: 35851, Validation Loss: 54381, 182081.80358140482\n",
      "Epoch 80101, Training Loss: 36779, Validation Loss: 53422, 216064.2358935116\n",
      "Epoch 80201, Training Loss: 37429, Validation Loss: 56925, 176865.6331900128\n",
      "Epoch 80301, Training Loss: 35185, Validation Loss: 52840, 147047.56662899617\n",
      "Epoch 80401, Training Loss: 36518, Validation Loss: 51987, 135094.3323209253\n",
      "Epoch 80501, Training Loss: 36412, Validation Loss: 55262, 185541.0854632714\n",
      "Epoch 80601, Training Loss: 34842, Validation Loss: 52781, 158924.662564911\n",
      "Epoch 80701, Training Loss: 36706, Validation Loss: 54178, 148023.76156059178\n",
      "Epoch 80801, Training Loss: 36202, Validation Loss: 51660, 134942.0710425901\n",
      "Epoch 80901, Training Loss: 38100, Validation Loss: 57191, 293782.440872454\n",
      "Epoch 81001, Training Loss: 34743, Validation Loss: 54945, 161869.06476335067\n",
      "Epoch 81101, Training Loss: 38229, Validation Loss: 55209, 172585.51218284733\n",
      "Epoch 81201, Training Loss: 36486, Validation Loss: 53993, 153948.95301701743\n",
      "Epoch 81301, Training Loss: 34682, Validation Loss: 52926, 174721.76075802857\n",
      "Epoch 81401, Training Loss: 39410, Validation Loss: 55033, 199778.2441189195\n",
      "Epoch 81501, Training Loss: 37461, Validation Loss: 51847, 163263.78932580355\n",
      "Epoch 81601, Training Loss: 35275, Validation Loss: 54116, 167671.85049961138\n",
      "Epoch 81701, Training Loss: 34255, Validation Loss: 59135, 191341.04149261967\n",
      "Epoch 81801, Training Loss: 36807, Validation Loss: 52368, 227416.78453849818\n",
      "Epoch 81901, Training Loss: 36489, Validation Loss: 55057, 150319.4787153857\n",
      "Epoch 82001, Training Loss: 35883, Validation Loss: 51843, 193081.28145255058\n",
      "Epoch 82101, Training Loss: 36533, Validation Loss: 55680, 119003.89922778803\n",
      "Epoch 82201, Training Loss: 36157, Validation Loss: 53696, 175550.2066722442\n",
      "Epoch 82301, Training Loss: 34439, Validation Loss: 53647, 181898.42597519234\n",
      "Epoch 82401, Training Loss: 38091, Validation Loss: 55499, 159770.4221328241\n",
      "Epoch 82501, Training Loss: 36748, Validation Loss: 57469, 206307.0086969391\n",
      "Epoch 82601, Training Loss: 36992, Validation Loss: 52987, 200221.5914623936\n",
      "Epoch 82701, Training Loss: 35152, Validation Loss: 53277, 178363.6480476531\n",
      "Epoch 82801, Training Loss: 37351, Validation Loss: 55946, 154203.00511462745\n",
      "Epoch 82901, Training Loss: 33868, Validation Loss: 58760, 115583.38354283548\n",
      "Epoch 83001, Training Loss: 36703, Validation Loss: 55537, 164197.4062623024\n",
      "Epoch 83101, Training Loss: 35898, Validation Loss: 54164, 216882.30598834297\n",
      "Epoch 83201, Training Loss: 35521, Validation Loss: 55484, 148662.86405103237\n",
      "Epoch 83301, Training Loss: 35232, Validation Loss: 52453, 199145.50448308015\n",
      "Epoch 83401, Training Loss: 34265, Validation Loss: 53998, 165983.18940066203\n",
      "Epoch 83501, Training Loss: 35285, Validation Loss: 55085, 148641.7264407468\n",
      "Epoch 83601, Training Loss: 36498, Validation Loss: 57457, 204674.82392688267\n",
      "Epoch 83701, Training Loss: 35376, Validation Loss: 53744, 192410.6748230356\n",
      "Epoch 83801, Training Loss: 32382, Validation Loss: 52323, 187443.60448751005\n",
      "Epoch 83901, Training Loss: 36604, Validation Loss: 58114, 175221.6679162602\n",
      "Epoch 84001, Training Loss: 35650, Validation Loss: 50899, 170533.92205628505\n",
      "Epoch 84101, Training Loss: 36405, Validation Loss: 55507, 202469.25083590404\n",
      "Epoch 84201, Training Loss: 36471, Validation Loss: 54697, 169888.86624748586\n",
      "Epoch 84301, Training Loss: 34337, Validation Loss: 49623, 168388.65359489256\n",
      "Epoch 84401, Training Loss: 34383, Validation Loss: 53284, 152645.98326259115\n",
      "Epoch 84501, Training Loss: 33975, Validation Loss: 51542, 156813.78608031807\n",
      "Epoch 84601, Training Loss: 37586, Validation Loss: 53190, 155807.79987686002\n",
      "Epoch 84701, Training Loss: 36321, Validation Loss: 51442, 154981.63942309655\n",
      "Epoch 84801, Training Loss: 34094, Validation Loss: 55700, 153848.49309109623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84901, Training Loss: 34625, Validation Loss: 60423, 141232.30456760607\n",
      "Epoch 85001, Training Loss: 35546, Validation Loss: 58886, 157388.69161117545\n",
      "Epoch 85101, Training Loss: 38406, Validation Loss: 54328, 171282.98958197143\n",
      "Epoch 85201, Training Loss: 35106, Validation Loss: 52811, 168054.22296936842\n",
      "Epoch 85301, Training Loss: 33834, Validation Loss: 54689, 138666.88887361516\n",
      "Epoch 85401, Training Loss: 36416, Validation Loss: 52072, 159447.38827732365\n",
      "Epoch 85501, Training Loss: 34559, Validation Loss: 53240, 135069.13698472813\n",
      "Epoch 85601, Training Loss: 37118, Validation Loss: 53367, 215293.21148745585\n",
      "Epoch 85701, Training Loss: 38469, Validation Loss: 53907, 230849.08327927996\n",
      "Epoch 85801, Training Loss: 35256, Validation Loss: 55221, 171947.6388851153\n",
      "Epoch 85901, Training Loss: 36528, Validation Loss: 53036, 153202.84201975117\n",
      "Epoch 86001, Training Loss: 36588, Validation Loss: 50638, 171047.4688776123\n",
      "Epoch 86101, Training Loss: 38221, Validation Loss: 55623, 166039.10442278025\n",
      "Epoch 86201, Training Loss: 36024, Validation Loss: 57653, 139559.51670662677\n",
      "Epoch 86301, Training Loss: 40954, Validation Loss: 57236, 230875.99734134844\n",
      "Epoch 86401, Training Loss: 33839, Validation Loss: 55786, 186772.1081536198\n",
      "Epoch 86501, Training Loss: 35011, Validation Loss: 56965, 166070.66054328423\n",
      "Epoch 86601, Training Loss: 37279, Validation Loss: 54108, 166001.50616193525\n",
      "Epoch 86701, Training Loss: 35431, Validation Loss: 57322, 148059.6718198943\n",
      "Epoch 86801, Training Loss: 37324, Validation Loss: 55876, 159247.2140759284\n",
      "Epoch 86901, Training Loss: 39429, Validation Loss: 53946, 175860.96391049333\n",
      "Epoch 87001, Training Loss: 36454, Validation Loss: 53257, 125986.96599859702\n",
      "Epoch 87101, Training Loss: 34064, Validation Loss: 53648, 136582.35380242288\n",
      "Epoch 87201, Training Loss: 35774, Validation Loss: 54759, 170617.9642230187\n",
      "Epoch 87301, Training Loss: 36366, Validation Loss: 55330, 163415.10777013307\n",
      "Epoch 87401, Training Loss: 34413, Validation Loss: 52452, 180629.9352131428\n",
      "Epoch 87501, Training Loss: 35310, Validation Loss: 55952, 179106.3347561174\n",
      "Epoch 87601, Training Loss: 37573, Validation Loss: 56362, 182163.57211360513\n",
      "Epoch 87701, Training Loss: 33089, Validation Loss: 54095, 145183.24978734186\n",
      "Epoch 87801, Training Loss: 36328, Validation Loss: 56222, 199310.2820663651\n",
      "Epoch 87901, Training Loss: 35247, Validation Loss: 57229, 190623.45204996946\n",
      "Epoch 88001, Training Loss: 38014, Validation Loss: 52219, 186980.27611926213\n",
      "Epoch 88101, Training Loss: 33606, Validation Loss: 55226, 151174.43284031373\n",
      "Epoch 88201, Training Loss: 35251, Validation Loss: 58921, 167395.28776726287\n",
      "Epoch 88301, Training Loss: 34638, Validation Loss: 55422, 172491.98031170052\n",
      "Epoch 88401, Training Loss: 32905, Validation Loss: 57835, 161480.34475091656\n",
      "Epoch 88501, Training Loss: 36492, Validation Loss: 56252, 142154.65482610578\n",
      "Epoch 88601, Training Loss: 33181, Validation Loss: 54857, 179682.8195202377\n",
      "Epoch 88701, Training Loss: 36228, Validation Loss: 54594, 227042.195901862\n",
      "Epoch 88801, Training Loss: 34200, Validation Loss: 58021, 146903.7168990903\n",
      "Epoch 88901, Training Loss: 36692, Validation Loss: 55433, 132527.8301033198\n",
      "Epoch 89001, Training Loss: 35985, Validation Loss: 51785, 159414.1248861329\n",
      "Epoch 89101, Training Loss: 36902, Validation Loss: 51943, 200180.8306665301\n",
      "Epoch 89201, Training Loss: 35996, Validation Loss: 54298, 186874.33094400595\n",
      "Epoch 89301, Training Loss: 34219, Validation Loss: 55656, 154995.4412901379\n",
      "Epoch 89401, Training Loss: 35366, Validation Loss: 55595, 202966.86365645626\n",
      "Epoch 89501, Training Loss: 37146, Validation Loss: 55351, 270530.2484073879\n",
      "Epoch 89601, Training Loss: 34442, Validation Loss: 52723, 168615.77980880946\n",
      "Epoch 89701, Training Loss: 32486, Validation Loss: 52409, 138305.54329122094\n",
      "Epoch 89801, Training Loss: 34073, Validation Loss: 56038, 141488.3678991671\n",
      "Epoch 89901, Training Loss: 35244, Validation Loss: 53289, 176537.86963853045\n",
      "Epoch 90001, Training Loss: 37982, Validation Loss: 54806, 169193.19464421252\n",
      "Epoch 90101, Training Loss: 37396, Validation Loss: 53258, 161421.32811314476\n",
      "Epoch 90201, Training Loss: 35589, Validation Loss: 50211, 113993.78925648557\n",
      "Epoch 90301, Training Loss: 34826, Validation Loss: 53011, 199246.75367546486\n",
      "Epoch 90401, Training Loss: 36695, Validation Loss: 57397, 170053.37611985372\n",
      "Epoch 90501, Training Loss: 37143, Validation Loss: 53949, 151974.43290989287\n",
      "Epoch 90601, Training Loss: 34074, Validation Loss: 57467, 210501.6152545047\n",
      "Epoch 90701, Training Loss: 36246, Validation Loss: 56321, 170928.6469079923\n",
      "Epoch 90801, Training Loss: 37456, Validation Loss: 53984, 180801.06686706946\n",
      "Epoch 90901, Training Loss: 36562, Validation Loss: 51836, 140203.90587953906\n",
      "Epoch 91001, Training Loss: 35548, Validation Loss: 57102, 216681.8168026531\n",
      "Epoch 91101, Training Loss: 34083, Validation Loss: 55186, 128073.0979242621\n",
      "Epoch 91201, Training Loss: 36951, Validation Loss: 53249, 184828.3545760091\n",
      "Epoch 91301, Training Loss: 35302, Validation Loss: 55276, 156162.5134497013\n",
      "Epoch 91401, Training Loss: 34932, Validation Loss: 55326, 212730.36928453608\n",
      "Epoch 91501, Training Loss: 35656, Validation Loss: 54633, 140220.87724679732\n",
      "Epoch 91601, Training Loss: 36943, Validation Loss: 53303, 178569.8865362175\n",
      "Epoch 91701, Training Loss: 36332, Validation Loss: 60079, 172934.98152842003\n",
      "Epoch 91801, Training Loss: 35851, Validation Loss: 53283, 150581.80944037248\n",
      "Epoch 91901, Training Loss: 34965, Validation Loss: 54666, 150841.6854058923\n",
      "Epoch 92001, Training Loss: 37268, Validation Loss: 54061, 189734.80899274335\n",
      "Epoch 92101, Training Loss: 36947, Validation Loss: 51659, 162171.53499611115\n",
      "Epoch 92201, Training Loss: 36433, Validation Loss: 53557, 181454.15967934649\n",
      "Epoch 92301, Training Loss: 34359, Validation Loss: 56900, 146858.54714392495\n",
      "Epoch 92401, Training Loss: 34847, Validation Loss: 56339, 195248.0803840831\n",
      "Epoch 92501, Training Loss: 35801, Validation Loss: 55362, 119473.70054665145\n",
      "Epoch 92601, Training Loss: 39060, Validation Loss: 58024, 178092.85623355207\n",
      "Epoch 92701, Training Loss: 36186, Validation Loss: 53468, 193543.4477504927\n",
      "Epoch 92801, Training Loss: 35737, Validation Loss: 52991, 168017.04371442462\n",
      "Epoch 92901, Training Loss: 35651, Validation Loss: 56020, 201897.2448259242\n",
      "Epoch 93001, Training Loss: 35617, Validation Loss: 52973, 192877.17426632383\n",
      "Epoch 93101, Training Loss: 33331, Validation Loss: 54236, 123911.63081546272\n",
      "Epoch 93201, Training Loss: 34127, Validation Loss: 55143, 133442.16940671936\n",
      "Epoch 93301, Training Loss: 37022, Validation Loss: 55967, 158438.09970184148\n",
      "Epoch 93401, Training Loss: 34519, Validation Loss: 54880, 142675.32523567113\n",
      "Epoch 93501, Training Loss: 37101, Validation Loss: 53472, 253362.3173752542\n",
      "Epoch 93601, Training Loss: 34539, Validation Loss: 55306, 189956.88811692552\n",
      "Epoch 93701, Training Loss: 35724, Validation Loss: 51396, 173017.0577873552\n",
      "Epoch 93801, Training Loss: 39233, Validation Loss: 55644, 179653.70115751168\n",
      "Epoch 93901, Training Loss: 35807, Validation Loss: 56313, 217587.83242905466\n",
      "Epoch 94001, Training Loss: 35787, Validation Loss: 51973, 155870.05023355832\n",
      "Epoch 94101, Training Loss: 38320, Validation Loss: 55109, 181189.65096622417\n",
      "Epoch 94201, Training Loss: 34851, Validation Loss: 58413, 133548.7078513869\n",
      "Epoch 94301, Training Loss: 36490, Validation Loss: 52145, 181004.1695347624\n",
      "Epoch 94401, Training Loss: 38646, Validation Loss: 51495, 219692.64436209245\n",
      "Epoch 94501, Training Loss: 35358, Validation Loss: 54870, 190756.58420058095\n",
      "Epoch 94601, Training Loss: 36030, Validation Loss: 53764, 148886.80567840228\n",
      "Epoch 94701, Training Loss: 35218, Validation Loss: 56829, 165981.15977849523\n",
      "Epoch 94801, Training Loss: 37042, Validation Loss: 56877, 145861.30166248512\n",
      "Epoch 94901, Training Loss: 34720, Validation Loss: 56692, 190997.2649668019\n",
      "Epoch 95001, Training Loss: 38617, Validation Loss: 53147, 178067.6391269531\n",
      "Epoch 95101, Training Loss: 38334, Validation Loss: 54505, 189806.29916125015\n",
      "Epoch 95201, Training Loss: 36478, Validation Loss: 56036, 163493.7176654697\n",
      "Epoch 95301, Training Loss: 34128, Validation Loss: 56509, 156131.81568595432\n",
      "Epoch 95401, Training Loss: 33940, Validation Loss: 55836, 190221.05400717515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95501, Training Loss: 34622, Validation Loss: 52354, 184212.63888434405\n",
      "Epoch 95601, Training Loss: 36579, Validation Loss: 54703, 192204.68462544915\n",
      "Epoch 95701, Training Loss: 37988, Validation Loss: 55694, 139116.9859180564\n",
      "Epoch 95801, Training Loss: 37403, Validation Loss: 54991, 159428.79782636746\n",
      "Epoch 95901, Training Loss: 38780, Validation Loss: 55057, 173854.12085162164\n",
      "Epoch 96001, Training Loss: 33702, Validation Loss: 56490, 116088.55977520232\n",
      "Epoch 96101, Training Loss: 33426, Validation Loss: 56091, 155574.2247841934\n",
      "Epoch 96201, Training Loss: 34601, Validation Loss: 54426, 133756.20357647285\n",
      "Epoch 96301, Training Loss: 36625, Validation Loss: 52653, 148447.89288219536\n",
      "Epoch 96401, Training Loss: 37110, Validation Loss: 54912, 210915.29436624734\n",
      "Epoch 96501, Training Loss: 40317, Validation Loss: 55090, 293602.10088234424\n",
      "Epoch 96601, Training Loss: 36029, Validation Loss: 55803, 182713.8671292487\n",
      "Epoch 96701, Training Loss: 35156, Validation Loss: 53870, 175008.50289911285\n",
      "Epoch 96801, Training Loss: 32319, Validation Loss: 50423, 175511.92484953662\n",
      "Epoch 96901, Training Loss: 35403, Validation Loss: 53217, 169904.3215314819\n",
      "Epoch 97001, Training Loss: 35699, Validation Loss: 54609, 154174.6762753064\n",
      "Epoch 97101, Training Loss: 35459, Validation Loss: 54512, 201928.12406757893\n",
      "Epoch 97201, Training Loss: 34489, Validation Loss: 54677, 167460.1293620443\n",
      "Epoch 97301, Training Loss: 37559, Validation Loss: 56261, 216669.17459980806\n",
      "Epoch 97401, Training Loss: 37613, Validation Loss: 53485, 198262.93912222292\n",
      "Epoch 97501, Training Loss: 37905, Validation Loss: 51751, 172832.95677141115\n",
      "Epoch 97601, Training Loss: 33729, Validation Loss: 55489, 152634.766704352\n",
      "Epoch 97701, Training Loss: 36735, Validation Loss: 57593, 163509.22929594858\n",
      "Epoch 97801, Training Loss: 37174, Validation Loss: 56910, 150470.35303620336\n",
      "Epoch 97901, Training Loss: 34274, Validation Loss: 55443, 176371.3264192435\n",
      "Epoch 98001, Training Loss: 31988, Validation Loss: 53036, 166060.89228671268\n",
      "Epoch 98101, Training Loss: 36033, Validation Loss: 52996, 153278.08674589478\n",
      "Epoch 98201, Training Loss: 34481, Validation Loss: 55872, 169311.18198173\n",
      "Epoch 98301, Training Loss: 36968, Validation Loss: 54433, 131331.82139402154\n",
      "Epoch 98401, Training Loss: 33301, Validation Loss: 56099, 148569.89445258447\n",
      "Epoch 98501, Training Loss: 36414, Validation Loss: 52504, 163149.426018421\n",
      "Epoch 98601, Training Loss: 34885, Validation Loss: 53604, 213113.44633857635\n",
      "Epoch 98701, Training Loss: 33867, Validation Loss: 54476, 156521.0667661829\n",
      "Epoch 98801, Training Loss: 35237, Validation Loss: 57565, 179133.54018526\n",
      "Epoch 98901, Training Loss: 37034, Validation Loss: 54929, 204163.57130925276\n",
      "Epoch 99001, Training Loss: 32849, Validation Loss: 59612, 177009.90850680848\n",
      "Epoch 99101, Training Loss: 36229, Validation Loss: 58621, 155290.8329835432\n",
      "Epoch 99201, Training Loss: 35446, Validation Loss: 52817, 185061.8418656413\n",
      "Epoch 99301, Training Loss: 32661, Validation Loss: 54938, 153261.16309626432\n",
      "Epoch 99401, Training Loss: 36805, Validation Loss: 54449, 148830.31121733517\n",
      "Epoch 99501, Training Loss: 37091, Validation Loss: 54635, 233467.913289725\n",
      "Epoch 99601, Training Loss: 35230, Validation Loss: 62034, 147078.0192546833\n",
      "Epoch 99701, Training Loss: 35606, Validation Loss: 56230, 217446.8662889714\n",
      "Epoch 99801, Training Loss: 36089, Validation Loss: 58401, 167009.38540921544\n",
      "Epoch 99901, Training Loss: 35387, Validation Loss: 53275, 178038.95370217625\n",
      "Epoch 100001, Training Loss: 35818, Validation Loss: 54358, 171783.42277817274\n",
      "Epoch 100101, Training Loss: 33303, Validation Loss: 54298, 164732.11011823747\n",
      "Epoch 100201, Training Loss: 35435, Validation Loss: 53149, 155755.8474753151\n",
      "Epoch 100301, Training Loss: 36846, Validation Loss: 53688, 129175.52042606189\n",
      "Epoch 100401, Training Loss: 35495, Validation Loss: 57354, 158919.71469008477\n",
      "Epoch 100501, Training Loss: 34586, Validation Loss: 56236, 134226.48593920033\n",
      "Epoch 100601, Training Loss: 33191, Validation Loss: 55257, 159521.3738787437\n",
      "Epoch 100701, Training Loss: 33717, Validation Loss: 56166, 167427.1176140216\n",
      "Epoch 100801, Training Loss: 35614, Validation Loss: 54465, 142002.77432605633\n",
      "Epoch 100901, Training Loss: 34885, Validation Loss: 56261, 154216.9318752082\n",
      "Epoch 101001, Training Loss: 35275, Validation Loss: 57880, 129943.83395681721\n",
      "Epoch 101101, Training Loss: 35727, Validation Loss: 56480, 132071.02202081608\n",
      "Epoch 101201, Training Loss: 35419, Validation Loss: 53186, 243669.3306116727\n",
      "Epoch 101301, Training Loss: 37308, Validation Loss: 56158, 157800.3127952804\n",
      "Epoch 101401, Training Loss: 33698, Validation Loss: 54570, 143498.38022224224\n",
      "Epoch 101501, Training Loss: 38451, Validation Loss: 52887, 120401.37314228497\n",
      "Epoch 101601, Training Loss: 35569, Validation Loss: 55935, 160928.90989038275\n",
      "Epoch 101701, Training Loss: 34474, Validation Loss: 55634, 159617.7483996688\n",
      "Epoch 101801, Training Loss: 35573, Validation Loss: 51975, 134319.87649028862\n",
      "Epoch 101901, Training Loss: 34167, Validation Loss: 54455, 159249.0786742584\n",
      "Epoch 102001, Training Loss: 35478, Validation Loss: 56377, 183746.41381945854\n",
      "Epoch 102101, Training Loss: 34445, Validation Loss: 53083, 171869.94671320857\n",
      "Epoch 102201, Training Loss: 36844, Validation Loss: 54100, 149876.4445578445\n",
      "Epoch 102301, Training Loss: 35200, Validation Loss: 55148, 149539.35762677938\n",
      "Epoch 102401, Training Loss: 35425, Validation Loss: 55091, 194503.24732138903\n",
      "Epoch 102501, Training Loss: 34674, Validation Loss: 52838, 180063.1637331252\n",
      "Epoch 102601, Training Loss: 35172, Validation Loss: 54225, 161365.5023511562\n",
      "Epoch 102701, Training Loss: 33910, Validation Loss: 53101, 198527.43583217086\n",
      "Epoch 102801, Training Loss: 34952, Validation Loss: 52788, 179186.48565006684\n",
      "Epoch 102901, Training Loss: 33258, Validation Loss: 55811, 163865.91074743294\n",
      "Epoch 103001, Training Loss: 34366, Validation Loss: 54172, 146817.3754389035\n",
      "Epoch 103101, Training Loss: 33973, Validation Loss: 55030, 153098.7405894651\n",
      "Epoch 103201, Training Loss: 35813, Validation Loss: 54964, 228328.4387983169\n",
      "Epoch 103301, Training Loss: 36197, Validation Loss: 54571, 160501.22826719168\n",
      "Epoch 103401, Training Loss: 36864, Validation Loss: 56026, 161522.1928406957\n",
      "Epoch 103501, Training Loss: 32746, Validation Loss: 54469, 163177.32083102353\n",
      "Epoch 103601, Training Loss: 37827, Validation Loss: 52733, 226453.4446950114\n",
      "Epoch 103701, Training Loss: 35011, Validation Loss: 53581, 173349.0316308453\n",
      "Epoch 103801, Training Loss: 33160, Validation Loss: 54841, 161302.17355288917\n",
      "Epoch 103901, Training Loss: 37927, Validation Loss: 56381, 196885.48451171233\n",
      "Epoch 104001, Training Loss: 35437, Validation Loss: 51608, 134924.57945018492\n",
      "Epoch 104101, Training Loss: 33475, Validation Loss: 54299, 155402.8738553057\n",
      "Epoch 104201, Training Loss: 36899, Validation Loss: 60732, 189644.8291289025\n",
      "Epoch 104301, Training Loss: 38617, Validation Loss: 55324, 187933.76690805051\n",
      "Epoch 104401, Training Loss: 34289, Validation Loss: 56114, 160443.36392425056\n",
      "Epoch 104501, Training Loss: 38636, Validation Loss: 55003, 197113.199519538\n",
      "Epoch 104601, Training Loss: 36243, Validation Loss: 56944, 128756.56964642073\n",
      "Epoch 104701, Training Loss: 35811, Validation Loss: 54150, 145253.05413418612\n",
      "Epoch 104801, Training Loss: 38460, Validation Loss: 55401, 186464.28148267348\n",
      "Epoch 104901, Training Loss: 33467, Validation Loss: 53605, 135722.32442529732\n",
      "Epoch 105001, Training Loss: 35168, Validation Loss: 53411, 167170.0437229632\n",
      "Epoch 105101, Training Loss: 33913, Validation Loss: 57040, 123967.44127420061\n",
      "Epoch 105201, Training Loss: 36214, Validation Loss: 56226, 176847.81652288057\n",
      "Epoch 105301, Training Loss: 39922, Validation Loss: 56922, 230455.08197172568\n",
      "Epoch 105401, Training Loss: 34752, Validation Loss: 53464, 195445.06268168622\n",
      "Epoch 105501, Training Loss: 34915, Validation Loss: 53128, 148006.5291060366\n",
      "Epoch 105601, Training Loss: 36459, Validation Loss: 55087, 151469.04079961506\n",
      "Epoch 105701, Training Loss: 33392, Validation Loss: 55268, 197071.62654546896\n",
      "Epoch 105801, Training Loss: 33511, Validation Loss: 54346, 169247.39217835764\n",
      "Epoch 105901, Training Loss: 35170, Validation Loss: 55518, 228327.05942793912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106001, Training Loss: 34121, Validation Loss: 56853, 185924.51092231402\n",
      "Epoch 106101, Training Loss: 36913, Validation Loss: 54081, 203022.19686855856\n",
      "Epoch 106201, Training Loss: 33560, Validation Loss: 53810, 217271.7425007976\n",
      "Epoch 106301, Training Loss: 35230, Validation Loss: 54787, 170475.1592440886\n",
      "Epoch 106401, Training Loss: 32247, Validation Loss: 57158, 158672.65834887236\n",
      "Epoch 106501, Training Loss: 37405, Validation Loss: 56643, 143621.1684397798\n",
      "Epoch 106601, Training Loss: 36638, Validation Loss: 54290, 177038.65696294303\n",
      "Epoch 106701, Training Loss: 35686, Validation Loss: 55095, 146171.13564334202\n",
      "Epoch 106801, Training Loss: 35037, Validation Loss: 54041, 178460.89833618325\n",
      "Epoch 106901, Training Loss: 35892, Validation Loss: 54397, 228919.99367147838\n",
      "Epoch 107001, Training Loss: 35187, Validation Loss: 54620, 196031.12394163234\n",
      "Epoch 107101, Training Loss: 37108, Validation Loss: 58065, 182803.14494541532\n",
      "Epoch 107201, Training Loss: 37884, Validation Loss: 53589, 170408.1418986389\n",
      "Epoch 107301, Training Loss: 37785, Validation Loss: 54176, 179362.7208473927\n",
      "Epoch 107401, Training Loss: 36761, Validation Loss: 55324, 166296.09936845384\n",
      "Epoch 107501, Training Loss: 35329, Validation Loss: 57431, 149093.89926266807\n",
      "Epoch 107601, Training Loss: 34852, Validation Loss: 56946, 171413.57156172485\n",
      "Epoch 107701, Training Loss: 37398, Validation Loss: 53379, 157377.9538055445\n",
      "Epoch 107801, Training Loss: 37015, Validation Loss: 56057, 150728.6585372889\n",
      "Epoch 107901, Training Loss: 36049, Validation Loss: 55209, 142380.354724398\n",
      "Epoch 108001, Training Loss: 35223, Validation Loss: 54522, 156968.4007106252\n",
      "Epoch 108101, Training Loss: 34001, Validation Loss: 54705, 157494.01942684103\n",
      "Epoch 108201, Training Loss: 31577, Validation Loss: 55196, 132755.81607872123\n",
      "Epoch 108301, Training Loss: 35301, Validation Loss: 58234, 168462.5468278024\n",
      "Epoch 108401, Training Loss: 34485, Validation Loss: 58229, 151003.3963526899\n",
      "Epoch 108501, Training Loss: 37668, Validation Loss: 55613, 200726.43886062174\n",
      "Epoch 108601, Training Loss: 34384, Validation Loss: 54869, 194200.28860260756\n",
      "Epoch 108701, Training Loss: 39529, Validation Loss: 53694, 198123.10449717336\n",
      "Epoch 108801, Training Loss: 37111, Validation Loss: 56970, 201419.64455181584\n",
      "Epoch 108901, Training Loss: 40525, Validation Loss: 57765, 250223.00769914314\n",
      "Epoch 109001, Training Loss: 35271, Validation Loss: 54984, 205936.9932513591\n",
      "Epoch 109101, Training Loss: 35243, Validation Loss: 52955, 161629.65205205334\n",
      "Epoch 109201, Training Loss: 35246, Validation Loss: 52627, 172056.50631325485\n",
      "Epoch 109301, Training Loss: 34407, Validation Loss: 53981, 162524.76233744368\n",
      "Epoch 109401, Training Loss: 34798, Validation Loss: 51773, 211229.55620407593\n",
      "Epoch 109501, Training Loss: 36225, Validation Loss: 57357, 175272.34220056524\n",
      "Epoch 109601, Training Loss: 34752, Validation Loss: 55461, 200104.1712814568\n",
      "Epoch 109701, Training Loss: 37070, Validation Loss: 55341, 207420.1031480939\n",
      "Epoch 109801, Training Loss: 37603, Validation Loss: 53360, 161217.58510266672\n",
      "Epoch 109901, Training Loss: 32390, Validation Loss: 57321, 144704.3343036756\n",
      "Epoch 110001, Training Loss: 34409, Validation Loss: 58613, 231582.31868205697\n",
      "Epoch 110101, Training Loss: 35410, Validation Loss: 54265, 183262.64251782556\n",
      "Epoch 110201, Training Loss: 36950, Validation Loss: 52976, 165597.30344376387\n",
      "Epoch 110301, Training Loss: 36714, Validation Loss: 58483, 193946.64888821178\n",
      "Epoch 110401, Training Loss: 35302, Validation Loss: 56091, 190073.7795888343\n",
      "Epoch 110501, Training Loss: 35043, Validation Loss: 54277, 194028.075715204\n",
      "Epoch 110601, Training Loss: 35361, Validation Loss: 59554, 141434.67192947655\n",
      "Epoch 110701, Training Loss: 37953, Validation Loss: 55899, 189707.43773196932\n",
      "Epoch 110801, Training Loss: 34299, Validation Loss: 51961, 129327.61527644376\n",
      "Epoch 110901, Training Loss: 37019, Validation Loss: 55839, 232270.05177929753\n",
      "Epoch 111001, Training Loss: 33560, Validation Loss: 53002, 166881.78438594853\n",
      "Epoch 111101, Training Loss: 37621, Validation Loss: 56416, 239074.95764973678\n",
      "Epoch 111201, Training Loss: 31823, Validation Loss: 54066, 126834.78347126958\n",
      "Epoch 111301, Training Loss: 35497, Validation Loss: 54489, 137289.80210119145\n",
      "Epoch 111401, Training Loss: 33639, Validation Loss: 54682, 182427.72788810427\n",
      "Epoch 111501, Training Loss: 36091, Validation Loss: 55552, 175817.64302584567\n",
      "Epoch 111601, Training Loss: 34942, Validation Loss: 57787, 175237.46977078854\n",
      "Epoch 111701, Training Loss: 35403, Validation Loss: 57055, 158524.93574338197\n",
      "Epoch 111801, Training Loss: 36283, Validation Loss: 52660, 152506.82642064028\n",
      "Epoch 111901, Training Loss: 39072, Validation Loss: 55627, 168048.5040969371\n",
      "Epoch 112001, Training Loss: 36257, Validation Loss: 56959, 193906.40350547605\n",
      "Epoch 112101, Training Loss: 37328, Validation Loss: 53346, 224785.97232151768\n",
      "Epoch 112201, Training Loss: 35563, Validation Loss: 54674, 169908.3433745294\n",
      "Epoch 112301, Training Loss: 36566, Validation Loss: 56509, 240636.24574393532\n",
      "Epoch 112401, Training Loss: 35226, Validation Loss: 55291, 127571.91513673849\n",
      "Epoch 112501, Training Loss: 33702, Validation Loss: 53320, 183356.56747665306\n",
      "Epoch 112601, Training Loss: 34545, Validation Loss: 54961, 133914.43158269906\n",
      "Epoch 112701, Training Loss: 36797, Validation Loss: 54047, 144085.06820709692\n",
      "Epoch 112801, Training Loss: 36137, Validation Loss: 56380, 116624.00943152474\n",
      "Epoch 112901, Training Loss: 34394, Validation Loss: 55213, 172400.90306569787\n",
      "Epoch 113001, Training Loss: 34626, Validation Loss: 55997, 242803.04816592197\n",
      "Epoch 113101, Training Loss: 35619, Validation Loss: 56305, 170823.32802554162\n",
      "Epoch 113201, Training Loss: 34961, Validation Loss: 56161, 172994.6283362093\n",
      "Epoch 113301, Training Loss: 35799, Validation Loss: 54136, 202937.97695560302\n",
      "Epoch 113401, Training Loss: 36527, Validation Loss: 57475, 205004.78475078195\n",
      "Epoch 113501, Training Loss: 33015, Validation Loss: 54579, 200021.45407384925\n",
      "Epoch 113601, Training Loss: 34864, Validation Loss: 57082, 150390.69887663535\n",
      "Epoch 113701, Training Loss: 34710, Validation Loss: 53719, 164747.82938338895\n",
      "Epoch 113801, Training Loss: 35503, Validation Loss: 55897, 150501.9507359175\n",
      "Epoch 113901, Training Loss: 34191, Validation Loss: 55667, 176040.85121081912\n",
      "Epoch 114001, Training Loss: 34431, Validation Loss: 54939, 154841.52958947318\n",
      "Epoch 114101, Training Loss: 34876, Validation Loss: 55249, 177003.38533171022\n",
      "Epoch 114201, Training Loss: 36835, Validation Loss: 57226, 157788.12026108472\n",
      "Epoch 114301, Training Loss: 34604, Validation Loss: 54548, 159719.4661569287\n",
      "Epoch 114401, Training Loss: 35740, Validation Loss: 55574, 184632.5401390413\n",
      "Epoch 114501, Training Loss: 39753, Validation Loss: 53673, 194586.28899789604\n",
      "Epoch 114601, Training Loss: 35314, Validation Loss: 57046, 207768.3340933992\n",
      "Epoch 114701, Training Loss: 32980, Validation Loss: 55351, 177932.52834901027\n",
      "Epoch 114801, Training Loss: 37621, Validation Loss: 55000, 248709.62337822095\n",
      "Epoch 114901, Training Loss: 34527, Validation Loss: 58029, 164661.90897508978\n",
      "Epoch 115001, Training Loss: 35441, Validation Loss: 54208, 169969.23552017054\n",
      "Epoch 115101, Training Loss: 35617, Validation Loss: 53100, 199929.18415423637\n",
      "Epoch 115201, Training Loss: 31216, Validation Loss: 56982, 115777.40716930432\n",
      "Epoch 115301, Training Loss: 32882, Validation Loss: 54542, 136971.51208260216\n",
      "Epoch 115401, Training Loss: 34679, Validation Loss: 54794, 202402.15380041688\n",
      "Epoch 115501, Training Loss: 36831, Validation Loss: 54245, 216709.02169991253\n",
      "Epoch 115601, Training Loss: 35470, Validation Loss: 56622, 154087.06440771802\n",
      "Epoch 115701, Training Loss: 34372, Validation Loss: 55176, 156415.5649451022\n",
      "Epoch 115801, Training Loss: 34893, Validation Loss: 53918, 188295.57692438643\n",
      "Epoch 115901, Training Loss: 33107, Validation Loss: 57949, 138147.67632672333\n",
      "Epoch 116001, Training Loss: 35603, Validation Loss: 55487, 190142.70292594485\n",
      "Epoch 116101, Training Loss: 32511, Validation Loss: 55675, 154856.8217034157\n",
      "Epoch 116201, Training Loss: 36740, Validation Loss: 54173, 195990.3175378431\n",
      "Epoch 116301, Training Loss: 33720, Validation Loss: 52855, 160178.45075906467\n",
      "Epoch 116401, Training Loss: 33701, Validation Loss: 53306, 189553.23130733438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116501, Training Loss: 33519, Validation Loss: 54693, 133233.57035458172\n",
      "Epoch 116601, Training Loss: 33920, Validation Loss: 58048, 174796.428326617\n",
      "Epoch 116701, Training Loss: 33968, Validation Loss: 55409, 223444.4115787011\n",
      "Epoch 116801, Training Loss: 34050, Validation Loss: 55402, 165494.97330491993\n",
      "Epoch 116901, Training Loss: 34999, Validation Loss: 53726, 147871.5630301922\n",
      "Epoch 117001, Training Loss: 32560, Validation Loss: 54665, 145908.98603043918\n",
      "Epoch 117101, Training Loss: 33065, Validation Loss: 55269, 117501.53930760403\n",
      "Epoch 117201, Training Loss: 34984, Validation Loss: 55699, 166569.40459159552\n",
      "Epoch 117301, Training Loss: 34481, Validation Loss: 56678, 198415.09957591063\n",
      "Epoch 117401, Training Loss: 35578, Validation Loss: 51671, 183696.40469544768\n",
      "Epoch 117501, Training Loss: 36899, Validation Loss: 54791, 218480.39323483044\n",
      "Epoch 117601, Training Loss: 35531, Validation Loss: 54709, 112121.5106060491\n",
      "Epoch 117701, Training Loss: 35689, Validation Loss: 55082, 159025.61385894174\n",
      "Epoch 117801, Training Loss: 33504, Validation Loss: 52322, 164392.32125174825\n",
      "Epoch 117901, Training Loss: 36964, Validation Loss: 54462, 143943.6838529279\n",
      "Epoch 118001, Training Loss: 35054, Validation Loss: 53301, 174109.63051804426\n",
      "Epoch 118101, Training Loss: 37069, Validation Loss: 52519, 190262.61229382444\n",
      "Epoch 118201, Training Loss: 34652, Validation Loss: 54809, 155931.59735790477\n",
      "Epoch 118301, Training Loss: 37272, Validation Loss: 59128, 187718.69555871378\n",
      "Epoch 118401, Training Loss: 34624, Validation Loss: 55568, 193742.2714044579\n",
      "Epoch 118501, Training Loss: 38605, Validation Loss: 55767, 174447.74719542678\n",
      "Epoch 118601, Training Loss: 35430, Validation Loss: 53756, 179637.6044696007\n",
      "Epoch 118701, Training Loss: 35127, Validation Loss: 55698, 159353.35087716707\n",
      "Epoch 118801, Training Loss: 35329, Validation Loss: 56585, 158587.8668125038\n",
      "Epoch 118901, Training Loss: 33458, Validation Loss: 54529, 159940.91435786465\n",
      "Epoch 119001, Training Loss: 34899, Validation Loss: 56325, 162936.49569137857\n",
      "Epoch 119101, Training Loss: 31927, Validation Loss: 55368, 195604.88881425626\n",
      "Epoch 119201, Training Loss: 33683, Validation Loss: 53552, 129059.89310139068\n",
      "Epoch 119301, Training Loss: 32708, Validation Loss: 56896, 169469.05688969637\n",
      "Epoch 119401, Training Loss: 36115, Validation Loss: 56425, 172308.13081341377\n",
      "Epoch 119501, Training Loss: 34581, Validation Loss: 56556, 167557.35498589676\n",
      "Epoch 119601, Training Loss: 36547, Validation Loss: 53267, 161845.1673037817\n",
      "Epoch 119701, Training Loss: 37923, Validation Loss: 50627, 172424.55322984766\n",
      "Epoch 119801, Training Loss: 31569, Validation Loss: 56818, 128550.81727267006\n",
      "Epoch 119901, Training Loss: 35154, Validation Loss: 54077, 198145.3211477363\n",
      "Epoch 120001, Training Loss: 34253, Validation Loss: 55089, 201398.17310754777\n",
      "Epoch 120101, Training Loss: 35190, Validation Loss: 57692, 157764.69761292057\n",
      "Epoch 120201, Training Loss: 33927, Validation Loss: 56350, 210644.55839437127\n",
      "Epoch 120301, Training Loss: 36203, Validation Loss: 54862, 168540.3015375913\n",
      "Epoch 120401, Training Loss: 35508, Validation Loss: 54028, 207379.4641718721\n",
      "Epoch 120501, Training Loss: 34678, Validation Loss: 52741, 201429.1295948414\n",
      "Epoch 120601, Training Loss: 35762, Validation Loss: 55148, 155835.91161174848\n",
      "Epoch 120701, Training Loss: 36666, Validation Loss: 55057, 169221.67027666932\n",
      "Epoch 120801, Training Loss: 35665, Validation Loss: 55053, 169035.7217796667\n",
      "Epoch 120901, Training Loss: 34774, Validation Loss: 55228, 216246.26839701273\n",
      "Epoch 121001, Training Loss: 36305, Validation Loss: 53310, 185038.20934837873\n",
      "Epoch 121101, Training Loss: 35717, Validation Loss: 54938, 177092.19482059192\n",
      "Epoch 121201, Training Loss: 33466, Validation Loss: 56363, 139587.2106473338\n",
      "Epoch 121301, Training Loss: 33290, Validation Loss: 54156, 133578.69114344296\n",
      "Epoch 121401, Training Loss: 38188, Validation Loss: 55004, 171615.05969667772\n",
      "Epoch 121501, Training Loss: 34345, Validation Loss: 55424, 168287.2849307845\n",
      "Epoch 121601, Training Loss: 35268, Validation Loss: 59176, 179697.95113127187\n",
      "Epoch 121701, Training Loss: 36187, Validation Loss: 55096, 183440.33325721408\n",
      "Epoch 121801, Training Loss: 33325, Validation Loss: 54863, 207391.7464706812\n",
      "Epoch 121901, Training Loss: 36971, Validation Loss: 60275, 216855.02826661276\n",
      "Epoch 122001, Training Loss: 35558, Validation Loss: 57875, 152369.46320866104\n",
      "Epoch 122101, Training Loss: 38077, Validation Loss: 58793, 146027.6330337179\n",
      "Epoch 122201, Training Loss: 36095, Validation Loss: 56347, 198678.1511563768\n",
      "Epoch 122301, Training Loss: 34659, Validation Loss: 55600, 156076.16132646252\n",
      "Epoch 122401, Training Loss: 37708, Validation Loss: 54122, 136427.6130795787\n",
      "Epoch 122501, Training Loss: 37415, Validation Loss: 52854, 182970.47529313606\n",
      "Epoch 122601, Training Loss: 35359, Validation Loss: 54019, 240961.6326462091\n",
      "Epoch 122701, Training Loss: 34997, Validation Loss: 54208, 167368.1402424042\n",
      "Epoch 122801, Training Loss: 37633, Validation Loss: 53211, 196370.39719785342\n",
      "Epoch 122901, Training Loss: 35123, Validation Loss: 55615, 126133.10493506782\n",
      "Epoch 123001, Training Loss: 34879, Validation Loss: 56107, 151188.12347779734\n",
      "Epoch 123101, Training Loss: 37328, Validation Loss: 54686, 159506.74968951754\n",
      "Epoch 123201, Training Loss: 35571, Validation Loss: 56840, 155514.79200956586\n",
      "Epoch 123301, Training Loss: 36479, Validation Loss: 56582, 164581.47313711394\n",
      "Epoch 123401, Training Loss: 36047, Validation Loss: 54890, 165813.4607757969\n",
      "Epoch 123501, Training Loss: 35463, Validation Loss: 56186, 235740.85307533236\n",
      "Epoch 123601, Training Loss: 35899, Validation Loss: 56947, 153236.18878852614\n",
      "Epoch 123701, Training Loss: 35639, Validation Loss: 57791, 191245.67143147264\n",
      "Epoch 123801, Training Loss: 34409, Validation Loss: 55896, 134167.63740395717\n",
      "Epoch 123901, Training Loss: 34789, Validation Loss: 57362, 141174.84015348754\n",
      "Epoch 124001, Training Loss: 33664, Validation Loss: 54247, 164439.78456705486\n",
      "Epoch 124101, Training Loss: 33947, Validation Loss: 56678, 195626.0309809445\n",
      "Epoch 124201, Training Loss: 35112, Validation Loss: 54060, 198003.52921206015\n",
      "Epoch 124301, Training Loss: 34811, Validation Loss: 55836, 179559.6706319128\n",
      "Epoch 124401, Training Loss: 34592, Validation Loss: 53413, 126629.73629496254\n",
      "Epoch 124501, Training Loss: 32493, Validation Loss: 54764, 197460.05593207394\n",
      "Epoch 124601, Training Loss: 34686, Validation Loss: 54285, 197385.8739353429\n",
      "Epoch 124701, Training Loss: 33671, Validation Loss: 56691, 175821.92257178\n",
      "Epoch 124801, Training Loss: 35462, Validation Loss: 54886, 166333.57841314873\n",
      "Epoch 124901, Training Loss: 35976, Validation Loss: 55521, 176949.39323834042\n",
      "Epoch 125001, Training Loss: 32507, Validation Loss: 54945, 152775.15242046185\n",
      "Epoch 125101, Training Loss: 33364, Validation Loss: 54930, 216552.18021417913\n",
      "Epoch 125201, Training Loss: 37380, Validation Loss: 55193, 203264.31425999352\n",
      "Epoch 125301, Training Loss: 36591, Validation Loss: 56827, 206316.80884330848\n",
      "Epoch 125401, Training Loss: 34629, Validation Loss: 53292, 171722.619310113\n",
      "Epoch 125501, Training Loss: 35404, Validation Loss: 57486, 140044.73751960558\n",
      "Epoch 125601, Training Loss: 33959, Validation Loss: 55007, 152483.2537291953\n",
      "Epoch 125701, Training Loss: 33471, Validation Loss: 54817, 129432.22802439761\n",
      "Epoch 125801, Training Loss: 32453, Validation Loss: 56227, 181851.55475726447\n",
      "Epoch 125901, Training Loss: 32586, Validation Loss: 56317, 152234.206599729\n",
      "Epoch 126001, Training Loss: 35153, Validation Loss: 53264, 165787.7515652565\n",
      "Epoch 126101, Training Loss: 32209, Validation Loss: 54954, 170195.3348526497\n",
      "Epoch 126201, Training Loss: 34432, Validation Loss: 56069, 178585.23549090617\n",
      "Epoch 126301, Training Loss: 37244, Validation Loss: 54721, 147630.57176482608\n",
      "Epoch 126401, Training Loss: 36911, Validation Loss: 54639, 147926.4595562923\n",
      "Epoch 126501, Training Loss: 34718, Validation Loss: 56386, 145926.1470026692\n",
      "Epoch 126601, Training Loss: 36343, Validation Loss: 60607, 178566.73832913535\n",
      "Epoch 126701, Training Loss: 36820, Validation Loss: 51924, 152980.1070793544\n",
      "Epoch 126801, Training Loss: 35862, Validation Loss: 56153, 160928.3220794273\n",
      "Epoch 126901, Training Loss: 35754, Validation Loss: 56563, 171200.49525101864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127001, Training Loss: 33016, Validation Loss: 56153, 162179.9615167108\n",
      "Epoch 127101, Training Loss: 38210, Validation Loss: 54616, 225672.68601055336\n",
      "Epoch 127201, Training Loss: 35014, Validation Loss: 57670, 176654.13068747878\n",
      "Epoch 127301, Training Loss: 39451, Validation Loss: 54912, 156077.03544613326\n",
      "Epoch 127401, Training Loss: 33360, Validation Loss: 56274, 154068.41096314494\n",
      "Epoch 127501, Training Loss: 35214, Validation Loss: 55766, 204174.12072051773\n",
      "Epoch 127601, Training Loss: 33933, Validation Loss: 56432, 138330.81983789522\n",
      "Epoch 127701, Training Loss: 35015, Validation Loss: 52901, 245857.88148125992\n",
      "Epoch 127801, Training Loss: 37329, Validation Loss: 58342, 151717.24861265274\n",
      "Epoch 127901, Training Loss: 35881, Validation Loss: 53273, 172510.5285050501\n",
      "Epoch 128001, Training Loss: 35336, Validation Loss: 56414, 170743.1829686278\n",
      "Epoch 128101, Training Loss: 36024, Validation Loss: 55137, 161714.42666182327\n",
      "Epoch 128201, Training Loss: 33749, Validation Loss: 56021, 167867.4217837782\n",
      "Epoch 128301, Training Loss: 35139, Validation Loss: 55603, 184445.74922650543\n",
      "Epoch 128401, Training Loss: 34686, Validation Loss: 52995, 139296.60880958664\n",
      "Epoch 128501, Training Loss: 35773, Validation Loss: 58206, 182229.10822022017\n",
      "Epoch 128601, Training Loss: 37379, Validation Loss: 54571, 224432.79726130815\n",
      "Epoch 128701, Training Loss: 34180, Validation Loss: 54696, 159360.42149028394\n",
      "Epoch 128801, Training Loss: 34204, Validation Loss: 58172, 165719.60044156178\n",
      "Epoch 128901, Training Loss: 34268, Validation Loss: 52761, 240146.353260036\n",
      "Epoch 129001, Training Loss: 35487, Validation Loss: 56857, 136541.34169816822\n",
      "Epoch 129101, Training Loss: 36715, Validation Loss: 54611, 225525.06674763086\n",
      "Epoch 129201, Training Loss: 33511, Validation Loss: 53603, 155465.9463345198\n",
      "Epoch 129301, Training Loss: 36398, Validation Loss: 56307, 163223.17953659222\n",
      "Epoch 129401, Training Loss: 32800, Validation Loss: 54474, 209331.5875469702\n",
      "Epoch 129501, Training Loss: 35449, Validation Loss: 54638, 195228.45178529588\n",
      "Epoch 129601, Training Loss: 36410, Validation Loss: 55395, 165384.0170117286\n",
      "Epoch 129701, Training Loss: 33441, Validation Loss: 54524, 206828.0509962386\n",
      "Epoch 129801, Training Loss: 35952, Validation Loss: 54776, 214921.15454598295\n",
      "Epoch 129901, Training Loss: 32762, Validation Loss: 54409, 145024.83953785215\n",
      "Epoch 130001, Training Loss: 34681, Validation Loss: 55683, 182665.85345615304\n",
      "Epoch 130101, Training Loss: 31306, Validation Loss: 57404, 183913.38805022324\n",
      "Epoch 130201, Training Loss: 36618, Validation Loss: 55143, 220244.54001274472\n",
      "Epoch 130301, Training Loss: 34485, Validation Loss: 57477, 222195.48769683615\n",
      "Epoch 130401, Training Loss: 31178, Validation Loss: 55256, 137510.12384062973\n",
      "Epoch 130501, Training Loss: 36195, Validation Loss: 53663, 178395.0443438062\n",
      "Epoch 130601, Training Loss: 33457, Validation Loss: 53840, 138946.9690714338\n",
      "Epoch 130701, Training Loss: 35023, Validation Loss: 55446, 136694.79607527936\n",
      "Epoch 130801, Training Loss: 34939, Validation Loss: 56778, 199100.50546865762\n",
      "Epoch 130901, Training Loss: 36409, Validation Loss: 57773, 163469.27055479444\n",
      "Epoch 131001, Training Loss: 33975, Validation Loss: 56158, 154382.0371258823\n",
      "Epoch 131101, Training Loss: 33669, Validation Loss: 56420, 150634.1340194442\n",
      "Epoch 131201, Training Loss: 34187, Validation Loss: 56264, 183409.18934403211\n",
      "Epoch 131301, Training Loss: 35290, Validation Loss: 56247, 151656.08935919293\n",
      "Epoch 131401, Training Loss: 35425, Validation Loss: 54563, 179023.27744103593\n",
      "Epoch 131501, Training Loss: 32538, Validation Loss: 56121, 192059.2510779909\n",
      "Epoch 131601, Training Loss: 36211, Validation Loss: 53262, 159082.23093698153\n",
      "Epoch 131701, Training Loss: 33799, Validation Loss: 55483, 121670.23872581283\n",
      "Epoch 131801, Training Loss: 34861, Validation Loss: 55072, 160421.22767129846\n",
      "Epoch 131901, Training Loss: 33642, Validation Loss: 52352, 141416.26252379976\n",
      "Epoch 132001, Training Loss: 37616, Validation Loss: 53720, 183182.80131317582\n",
      "Epoch 132101, Training Loss: 31844, Validation Loss: 57356, 158321.35035840314\n",
      "Epoch 132201, Training Loss: 35572, Validation Loss: 57468, 166234.03776967945\n",
      "Epoch 132301, Training Loss: 36078, Validation Loss: 52557, 173700.6007940879\n",
      "Epoch 132401, Training Loss: 34752, Validation Loss: 54983, 225012.60673484346\n",
      "Epoch 132501, Training Loss: 35206, Validation Loss: 53289, 107141.26546818206\n",
      "Epoch 132601, Training Loss: 36448, Validation Loss: 58242, 183771.94445839254\n",
      "Epoch 132701, Training Loss: 34038, Validation Loss: 54641, 129898.15718059249\n",
      "Epoch 132801, Training Loss: 34556, Validation Loss: 55992, 194846.20522293742\n",
      "Epoch 132901, Training Loss: 34344, Validation Loss: 56363, 154005.03374098497\n",
      "Epoch 133001, Training Loss: 35291, Validation Loss: 53322, 181300.40193496607\n",
      "Epoch 133101, Training Loss: 38747, Validation Loss: 54564, 162216.26679909963\n",
      "Epoch 133201, Training Loss: 30969, Validation Loss: 55636, 167179.70863916105\n",
      "Epoch 133301, Training Loss: 37454, Validation Loss: 56579, 152275.36852774626\n",
      "Epoch 133401, Training Loss: 36056, Validation Loss: 54025, 166080.1728112452\n",
      "Epoch 133501, Training Loss: 36975, Validation Loss: 55354, 183216.7917681965\n",
      "Epoch 133601, Training Loss: 35643, Validation Loss: 51367, 165090.9728136787\n",
      "Epoch 133701, Training Loss: 36732, Validation Loss: 55250, 146655.33922477436\n",
      "Epoch 133801, Training Loss: 35842, Validation Loss: 53436, 172661.87680094014\n",
      "Epoch 133901, Training Loss: 37122, Validation Loss: 54911, 174080.21342891434\n",
      "Epoch 134001, Training Loss: 34969, Validation Loss: 53974, 168350.6656364984\n",
      "Epoch 134101, Training Loss: 32924, Validation Loss: 57852, 116554.95331892796\n",
      "Epoch 134201, Training Loss: 35066, Validation Loss: 57306, 165593.34422859515\n",
      "Epoch 134301, Training Loss: 35542, Validation Loss: 56176, 154068.95339997316\n",
      "Epoch 134401, Training Loss: 34939, Validation Loss: 52568, 195518.4885033831\n",
      "Epoch 134501, Training Loss: 35412, Validation Loss: 54919, 161186.1504361992\n",
      "Epoch 134601, Training Loss: 33631, Validation Loss: 55128, 133299.69567804292\n",
      "Epoch 134701, Training Loss: 34865, Validation Loss: 55965, 203873.7328035884\n",
      "Epoch 134801, Training Loss: 36959, Validation Loss: 59674, 241626.8165086317\n",
      "Epoch 134901, Training Loss: 34026, Validation Loss: 53053, 116073.14117213164\n",
      "Epoch 135001, Training Loss: 35460, Validation Loss: 56100, 142728.74363006392\n",
      "Epoch 135101, Training Loss: 38488, Validation Loss: 54212, 155951.72518747614\n",
      "Epoch 135201, Training Loss: 35421, Validation Loss: 52831, 137734.3715808751\n",
      "Epoch 135301, Training Loss: 32121, Validation Loss: 55009, 142119.50604842216\n",
      "Epoch 135401, Training Loss: 36410, Validation Loss: 52424, 174567.72392710796\n",
      "Epoch 135501, Training Loss: 33673, Validation Loss: 57652, 147161.36359200164\n",
      "Epoch 135601, Training Loss: 33531, Validation Loss: 58124, 201761.26766825095\n",
      "Epoch 135701, Training Loss: 35440, Validation Loss: 55824, 182915.92311291923\n",
      "Epoch 135801, Training Loss: 35695, Validation Loss: 53694, 187763.5461275469\n",
      "Epoch 135901, Training Loss: 32782, Validation Loss: 56466, 157303.83583577749\n",
      "Epoch 136001, Training Loss: 36267, Validation Loss: 56559, 174816.31226804972\n",
      "Epoch 136101, Training Loss: 33255, Validation Loss: 54916, 165592.0923805087\n",
      "Epoch 136201, Training Loss: 36810, Validation Loss: 58091, 210798.16510603114\n",
      "Epoch 136301, Training Loss: 35827, Validation Loss: 56555, 142173.42798643684\n",
      "Epoch 136401, Training Loss: 33513, Validation Loss: 54758, 169677.20845115176\n",
      "Epoch 136501, Training Loss: 33021, Validation Loss: 55590, 150117.2171338156\n",
      "Epoch 136601, Training Loss: 36258, Validation Loss: 53233, 176559.8527722441\n",
      "Epoch 136701, Training Loss: 36296, Validation Loss: 58145, 182953.5825121562\n",
      "Epoch 136801, Training Loss: 36533, Validation Loss: 54833, 164531.1087765469\n",
      "Epoch 136901, Training Loss: 31107, Validation Loss: 54388, 130735.54335031183\n",
      "Epoch 137001, Training Loss: 36816, Validation Loss: 54151, 192245.87063702577\n",
      "Epoch 137101, Training Loss: 32377, Validation Loss: 56717, 156788.63530283704\n",
      "Epoch 137201, Training Loss: 35162, Validation Loss: 56130, 178318.31779712453\n",
      "Epoch 137301, Training Loss: 32619, Validation Loss: 53444, 121663.80148429728\n",
      "Epoch 137401, Training Loss: 35469, Validation Loss: 55382, 209112.61412469312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137501, Training Loss: 35034, Validation Loss: 55945, 190688.57616194125\n",
      "Epoch 137601, Training Loss: 32821, Validation Loss: 57351, 151335.36434822273\n",
      "Epoch 137701, Training Loss: 34234, Validation Loss: 52613, 195209.28576405635\n",
      "Epoch 137801, Training Loss: 35023, Validation Loss: 57639, 165539.9294724605\n",
      "Epoch 137901, Training Loss: 37019, Validation Loss: 56136, 304157.91148916114\n",
      "Epoch 138001, Training Loss: 34529, Validation Loss: 57662, 160143.9571994266\n",
      "Epoch 138101, Training Loss: 35382, Validation Loss: 55641, 173036.60369608004\n",
      "Epoch 138201, Training Loss: 34168, Validation Loss: 55872, 215769.71001771992\n",
      "Epoch 138301, Training Loss: 33753, Validation Loss: 54078, 136378.4117780556\n",
      "Epoch 138401, Training Loss: 35979, Validation Loss: 55642, 170642.27562378792\n",
      "Epoch 138501, Training Loss: 33052, Validation Loss: 54109, 127901.42599317594\n",
      "Epoch 138601, Training Loss: 36968, Validation Loss: 57003, 182636.8587096047\n",
      "Epoch 138701, Training Loss: 34058, Validation Loss: 56488, 147141.53864462036\n",
      "Epoch 138801, Training Loss: 37730, Validation Loss: 58458, 140421.0352939942\n",
      "Epoch 138901, Training Loss: 34939, Validation Loss: 58094, 149857.14573995044\n",
      "Epoch 139001, Training Loss: 37824, Validation Loss: 55143, 208974.61857960222\n",
      "Epoch 139101, Training Loss: 34413, Validation Loss: 54629, 181088.075017722\n",
      "Epoch 139201, Training Loss: 34028, Validation Loss: 55800, 140584.20822113843\n",
      "Epoch 139301, Training Loss: 34488, Validation Loss: 52373, 143432.85291239037\n",
      "Epoch 139401, Training Loss: 36305, Validation Loss: 56642, 161194.78837077555\n",
      "Epoch 139501, Training Loss: 35001, Validation Loss: 53116, 173330.77520680358\n",
      "Epoch 139601, Training Loss: 36606, Validation Loss: 56069, 143394.76083309882\n",
      "Epoch 139701, Training Loss: 33971, Validation Loss: 56237, 152425.8426613006\n",
      "Epoch 139801, Training Loss: 35358, Validation Loss: 56022, 137720.00066736314\n",
      "Epoch 139901, Training Loss: 34602, Validation Loss: 51493, 185406.86429237004\n",
      "Epoch 140001, Training Loss: 34276, Validation Loss: 56883, 215070.99158112402\n",
      "Epoch 140101, Training Loss: 37823, Validation Loss: 53383, 193045.00776735437\n",
      "Epoch 140201, Training Loss: 35028, Validation Loss: 54894, 163093.57729846725\n",
      "Epoch 140301, Training Loss: 34310, Validation Loss: 55209, 142022.319051708\n",
      "Epoch 140401, Training Loss: 35193, Validation Loss: 54842, 165883.725937328\n",
      "Epoch 140501, Training Loss: 35662, Validation Loss: 54574, 225912.8084476757\n",
      "Epoch 140601, Training Loss: 34262, Validation Loss: 52663, 166371.76471664585\n",
      "Epoch 140701, Training Loss: 34834, Validation Loss: 57404, 164339.52131097481\n",
      "Epoch 140801, Training Loss: 32066, Validation Loss: 58132, 115723.58294828799\n",
      "Epoch 140901, Training Loss: 36931, Validation Loss: 55814, 190051.2305574357\n",
      "Epoch 141001, Training Loss: 36327, Validation Loss: 56325, 147414.30691651345\n",
      "Epoch 141101, Training Loss: 35324, Validation Loss: 55122, 173609.28810240352\n",
      "Epoch 141201, Training Loss: 33323, Validation Loss: 55356, 164960.7543880873\n",
      "Epoch 141301, Training Loss: 34919, Validation Loss: 54152, 158209.39125256942\n",
      "Epoch 141401, Training Loss: 36020, Validation Loss: 52629, 165603.96204276267\n",
      "Epoch 141501, Training Loss: 33929, Validation Loss: 54149, 164720.0436537616\n",
      "Epoch 141601, Training Loss: 37254, Validation Loss: 52591, 203301.70967362414\n",
      "Epoch 141701, Training Loss: 31990, Validation Loss: 54965, 153035.92066627633\n",
      "Epoch 141801, Training Loss: 34859, Validation Loss: 54095, 168712.05572453394\n",
      "Epoch 141901, Training Loss: 34592, Validation Loss: 54460, 187103.04981353934\n",
      "Epoch 142001, Training Loss: 34129, Validation Loss: 57668, 131372.84046554894\n",
      "Epoch 142101, Training Loss: 33412, Validation Loss: 58084, 169474.48525021865\n",
      "Epoch 142201, Training Loss: 35099, Validation Loss: 54510, 179657.38202286183\n",
      "Epoch 142301, Training Loss: 36112, Validation Loss: 53479, 173448.3119236237\n",
      "Epoch 142401, Training Loss: 36608, Validation Loss: 57435, 191101.78817869825\n",
      "Epoch 142501, Training Loss: 33120, Validation Loss: 55334, 127842.43141420302\n",
      "Epoch 142601, Training Loss: 34865, Validation Loss: 53043, 144363.95397902533\n",
      "Epoch 142701, Training Loss: 34874, Validation Loss: 54253, 179767.82348636084\n",
      "Epoch 142801, Training Loss: 36841, Validation Loss: 55822, 195686.34628641242\n",
      "Epoch 142901, Training Loss: 33090, Validation Loss: 53443, 148979.4028600556\n",
      "Epoch 143001, Training Loss: 35646, Validation Loss: 56961, 166769.0051221335\n",
      "Epoch 143101, Training Loss: 35869, Validation Loss: 57020, 145532.62348587316\n",
      "Epoch 143201, Training Loss: 34744, Validation Loss: 54376, 187735.2076177036\n",
      "Epoch 143301, Training Loss: 32343, Validation Loss: 56522, 140243.6041888789\n",
      "Epoch 143401, Training Loss: 34161, Validation Loss: 56361, 174423.4178769912\n",
      "Epoch 143501, Training Loss: 35673, Validation Loss: 54024, 179466.0114544305\n",
      "Epoch 143601, Training Loss: 37933, Validation Loss: 54733, 144220.14346437607\n",
      "Epoch 143701, Training Loss: 35383, Validation Loss: 56362, 175696.22482463715\n",
      "Epoch 143801, Training Loss: 33037, Validation Loss: 56588, 140694.08285987066\n",
      "Epoch 143901, Training Loss: 35499, Validation Loss: 57265, 165936.18319617453\n",
      "Epoch 144001, Training Loss: 33753, Validation Loss: 57454, 161410.71038144766\n",
      "Epoch 144101, Training Loss: 38814, Validation Loss: 56965, 166144.94837892504\n",
      "Epoch 144201, Training Loss: 32771, Validation Loss: 59062, 181710.88827353786\n",
      "Epoch 144301, Training Loss: 38376, Validation Loss: 56454, 171756.6853984354\n",
      "Epoch 144401, Training Loss: 36848, Validation Loss: 57778, 268248.2091155004\n",
      "Epoch 144501, Training Loss: 34602, Validation Loss: 53208, 173795.39160135094\n",
      "Epoch 144601, Training Loss: 35540, Validation Loss: 57170, 180268.84533546574\n",
      "Epoch 144701, Training Loss: 32815, Validation Loss: 57707, 127025.91720618622\n",
      "Epoch 144801, Training Loss: 35657, Validation Loss: 55248, 153835.2033879143\n",
      "Epoch 144901, Training Loss: 35257, Validation Loss: 56106, 204807.0039080582\n",
      "Epoch 145001, Training Loss: 35365, Validation Loss: 51804, 152040.33910149802\n",
      "Epoch 145101, Training Loss: 33854, Validation Loss: 56217, 148023.93993105358\n",
      "Epoch 145201, Training Loss: 34338, Validation Loss: 56440, 184032.41149832378\n",
      "Epoch 145301, Training Loss: 33378, Validation Loss: 55940, 152312.78856173318\n",
      "Epoch 145401, Training Loss: 34074, Validation Loss: 58100, 182182.8792310387\n",
      "Epoch 145501, Training Loss: 32823, Validation Loss: 55708, 164714.24147144682\n",
      "Epoch 145601, Training Loss: 32575, Validation Loss: 55326, 124321.81551317243\n",
      "Epoch 145701, Training Loss: 32599, Validation Loss: 56436, 141723.8606228314\n",
      "Epoch 145801, Training Loss: 34858, Validation Loss: 54855, 176559.48083124557\n",
      "Epoch 145901, Training Loss: 34605, Validation Loss: 56169, 154073.66389440527\n",
      "Epoch 146001, Training Loss: 32942, Validation Loss: 56229, 168782.69448137702\n",
      "Epoch 146101, Training Loss: 33290, Validation Loss: 57014, 213504.11295574447\n",
      "Epoch 146201, Training Loss: 33360, Validation Loss: 56256, 187558.05291182362\n",
      "Epoch 146301, Training Loss: 35447, Validation Loss: 58395, 179757.48502572594\n",
      "Epoch 146401, Training Loss: 33115, Validation Loss: 53515, 157211.95870799117\n",
      "Epoch 146501, Training Loss: 34072, Validation Loss: 55480, 165105.84464180222\n",
      "Epoch 146601, Training Loss: 35146, Validation Loss: 56292, 140376.46780135206\n",
      "Epoch 146701, Training Loss: 36222, Validation Loss: 55781, 245905.19289827335\n",
      "Epoch 146801, Training Loss: 32837, Validation Loss: 56706, 144784.83828771274\n",
      "Epoch 146901, Training Loss: 33998, Validation Loss: 54652, 148755.02000561345\n",
      "Epoch 147001, Training Loss: 33500, Validation Loss: 54994, 169967.7754829523\n",
      "Epoch 147101, Training Loss: 35238, Validation Loss: 59020, 180254.1610062451\n",
      "Epoch 147201, Training Loss: 32630, Validation Loss: 56126, 200447.9126382065\n",
      "Epoch 147301, Training Loss: 34311, Validation Loss: 55381, 165413.7223668389\n",
      "Epoch 147401, Training Loss: 37050, Validation Loss: 53342, 155809.0877919893\n",
      "Epoch 147501, Training Loss: 36486, Validation Loss: 57590, 137184.48094953565\n",
      "Epoch 147601, Training Loss: 34336, Validation Loss: 53852, 138092.44970132993\n",
      "Epoch 147701, Training Loss: 31712, Validation Loss: 57781, 120314.46263365408\n",
      "Epoch 147801, Training Loss: 33792, Validation Loss: 57609, 148000.25159879323\n",
      "Epoch 147901, Training Loss: 34671, Validation Loss: 56651, 150346.7645896765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148001, Training Loss: 34135, Validation Loss: 57117, 145195.45067414994\n",
      "Epoch 148101, Training Loss: 35295, Validation Loss: 55283, 153872.03815209263\n",
      "Epoch 148201, Training Loss: 33367, Validation Loss: 54762, 138062.10486432665\n",
      "Epoch 148301, Training Loss: 34112, Validation Loss: 57387, 129422.25707404804\n",
      "Epoch 148401, Training Loss: 33199, Validation Loss: 56007, 169115.17063201082\n",
      "Epoch 148501, Training Loss: 35265, Validation Loss: 58359, 195049.03853510585\n",
      "Epoch 148601, Training Loss: 35721, Validation Loss: 54362, 178147.88977858948\n",
      "Epoch 148701, Training Loss: 37041, Validation Loss: 54171, 151369.1188831291\n",
      "Epoch 148801, Training Loss: 34673, Validation Loss: 55661, 142290.40405535986\n",
      "Epoch 148901, Training Loss: 32200, Validation Loss: 56055, 155626.75540702534\n",
      "Epoch 149001, Training Loss: 33688, Validation Loss: 55644, 137825.67966383244\n",
      "Epoch 149101, Training Loss: 34780, Validation Loss: 57893, 169388.01788809884\n",
      "Epoch 149201, Training Loss: 34540, Validation Loss: 54489, 199386.8801894243\n",
      "Epoch 149301, Training Loss: 35498, Validation Loss: 54982, 176709.92677911712\n",
      "Epoch 149401, Training Loss: 33254, Validation Loss: 57236, 135801.37371460107\n",
      "Epoch 149501, Training Loss: 34729, Validation Loss: 55779, 129258.35077385663\n",
      "Epoch 149601, Training Loss: 37413, Validation Loss: 56388, 153669.96547534483\n",
      "Epoch 149701, Training Loss: 35606, Validation Loss: 55865, 187092.90517775062\n",
      "Epoch 149801, Training Loss: 34489, Validation Loss: 56273, 189229.5884728158\n",
      "Epoch 149901, Training Loss: 34863, Validation Loss: 55206, 170901.9304859811\n",
      "Epoch 150001, Training Loss: 33563, Validation Loss: 52636, 149564.05742879168\n",
      "Epoch 150101, Training Loss: 32735, Validation Loss: 54272, 153988.8907760665\n",
      "Epoch 150201, Training Loss: 33311, Validation Loss: 52268, 138229.31776641417\n",
      "Epoch 150301, Training Loss: 35917, Validation Loss: 57789, 145790.70601186974\n",
      "Epoch 150401, Training Loss: 36573, Validation Loss: 52150, 133087.8850026528\n",
      "Epoch 150501, Training Loss: 34030, Validation Loss: 53996, 125987.87077001203\n",
      "Epoch 150601, Training Loss: 37389, Validation Loss: 55365, 180048.0823015643\n",
      "Epoch 150701, Training Loss: 32521, Validation Loss: 56182, 144742.6701866316\n",
      "Epoch 150801, Training Loss: 34860, Validation Loss: 53509, 221869.27122496147\n",
      "Epoch 150901, Training Loss: 36514, Validation Loss: 56632, 187444.25725395628\n",
      "Epoch 151001, Training Loss: 33659, Validation Loss: 59855, 225707.67530310588\n",
      "Epoch 151101, Training Loss: 35636, Validation Loss: 58169, 132671.78364214586\n",
      "Epoch 151201, Training Loss: 37792, Validation Loss: 54573, 187026.0122101357\n",
      "Epoch 151301, Training Loss: 35546, Validation Loss: 55024, 161918.60094831462\n",
      "Epoch 151401, Training Loss: 36225, Validation Loss: 56686, 155761.5690883008\n",
      "Epoch 151501, Training Loss: 34084, Validation Loss: 56988, 148423.01062898387\n",
      "Epoch 151601, Training Loss: 35187, Validation Loss: 57025, 149452.72437887083\n",
      "Epoch 151701, Training Loss: 34932, Validation Loss: 57064, 229681.43228061486\n",
      "Epoch 151801, Training Loss: 35113, Validation Loss: 55447, 154987.40996722152\n",
      "Epoch 151901, Training Loss: 34674, Validation Loss: 54703, 164414.62848910314\n",
      "Epoch 152001, Training Loss: 33480, Validation Loss: 54570, 198656.74212596484\n",
      "Epoch 152101, Training Loss: 35747, Validation Loss: 57024, 205303.17549253334\n",
      "Epoch 152201, Training Loss: 33366, Validation Loss: 56972, 147096.77508997472\n",
      "Epoch 152301, Training Loss: 35832, Validation Loss: 55340, 195055.01361584142\n",
      "Epoch 152401, Training Loss: 33426, Validation Loss: 58100, 195168.01148382513\n",
      "Epoch 152501, Training Loss: 36807, Validation Loss: 55594, 154304.90180059554\n",
      "Epoch 152601, Training Loss: 34646, Validation Loss: 54648, 166876.37844853222\n",
      "Epoch 152701, Training Loss: 34985, Validation Loss: 57515, 211281.04531690455\n",
      "Epoch 152801, Training Loss: 35089, Validation Loss: 55498, 205332.02545520526\n",
      "Epoch 152901, Training Loss: 33629, Validation Loss: 53984, 165532.42928288088\n",
      "Epoch 153001, Training Loss: 32659, Validation Loss: 52643, 134003.90281068822\n",
      "Epoch 153101, Training Loss: 36398, Validation Loss: 54224, 159237.31115476755\n",
      "Epoch 153201, Training Loss: 36125, Validation Loss: 52556, 166686.94348968944\n",
      "Epoch 153301, Training Loss: 34016, Validation Loss: 54287, 156238.2876600947\n",
      "Epoch 153401, Training Loss: 34240, Validation Loss: 59033, 178361.75322724087\n",
      "Epoch 153501, Training Loss: 32646, Validation Loss: 54768, 149547.6684079375\n",
      "Epoch 153601, Training Loss: 34948, Validation Loss: 53334, 142624.7868015114\n",
      "Epoch 153701, Training Loss: 35122, Validation Loss: 54669, 134052.66716845342\n",
      "Epoch 153801, Training Loss: 34706, Validation Loss: 55123, 151998.9812657181\n",
      "Epoch 153901, Training Loss: 33883, Validation Loss: 55882, 152530.64853683146\n",
      "Epoch 154001, Training Loss: 34788, Validation Loss: 56286, 137294.5261473033\n",
      "Epoch 154101, Training Loss: 33652, Validation Loss: 57306, 148538.26989937384\n",
      "Epoch 154201, Training Loss: 35634, Validation Loss: 54846, 179124.42875481732\n",
      "Epoch 154301, Training Loss: 34493, Validation Loss: 57863, 130561.33742844984\n",
      "Epoch 154401, Training Loss: 36666, Validation Loss: 56073, 191323.61930447444\n",
      "Epoch 154501, Training Loss: 33073, Validation Loss: 55009, 164592.38946043563\n",
      "Epoch 154601, Training Loss: 34399, Validation Loss: 54816, 155795.54769954967\n",
      "Epoch 154701, Training Loss: 35369, Validation Loss: 55465, 167121.96396062596\n",
      "Epoch 154801, Training Loss: 35568, Validation Loss: 58224, 158343.83302876932\n",
      "Epoch 154901, Training Loss: 35945, Validation Loss: 55880, 166941.39325832305\n",
      "Epoch 155001, Training Loss: 33537, Validation Loss: 53275, 175201.729203149\n",
      "Epoch 155101, Training Loss: 33954, Validation Loss: 58408, 164590.60375122784\n",
      "Epoch 155201, Training Loss: 35650, Validation Loss: 55152, 152702.34879012662\n",
      "Epoch 155301, Training Loss: 32768, Validation Loss: 52509, 168926.3412969185\n",
      "Epoch 155401, Training Loss: 36458, Validation Loss: 54527, 178836.14204438557\n",
      "Epoch 155501, Training Loss: 35586, Validation Loss: 54870, 134225.00046366025\n",
      "Epoch 155601, Training Loss: 32972, Validation Loss: 56583, 117654.94366035529\n",
      "Epoch 155701, Training Loss: 36158, Validation Loss: 52093, 182021.97156239403\n",
      "Epoch 155801, Training Loss: 34934, Validation Loss: 56017, 134240.00753105385\n",
      "Epoch 155901, Training Loss: 32001, Validation Loss: 56770, 142460.612804181\n",
      "Epoch 156001, Training Loss: 33187, Validation Loss: 55026, 124915.21006999847\n",
      "Epoch 156101, Training Loss: 33660, Validation Loss: 54524, 175534.42049597125\n",
      "Epoch 156201, Training Loss: 35554, Validation Loss: 53561, 234113.0078211523\n",
      "Epoch 156301, Training Loss: 33254, Validation Loss: 55331, 168501.7223212348\n",
      "Epoch 156401, Training Loss: 35934, Validation Loss: 53926, 196090.4158930444\n",
      "Epoch 156501, Training Loss: 35455, Validation Loss: 51729, 202157.92126631885\n",
      "Epoch 156601, Training Loss: 33900, Validation Loss: 55486, 132421.0647930574\n",
      "Epoch 156701, Training Loss: 34217, Validation Loss: 55809, 270286.5261319136\n",
      "Epoch 156801, Training Loss: 33639, Validation Loss: 53695, 175101.3961472741\n",
      "Epoch 156901, Training Loss: 32909, Validation Loss: 54194, 151796.1972522191\n",
      "Epoch 157001, Training Loss: 36025, Validation Loss: 56728, 250688.58580989102\n",
      "Epoch 157101, Training Loss: 34110, Validation Loss: 55440, 208007.74546297872\n",
      "Epoch 157201, Training Loss: 35535, Validation Loss: 53618, 171762.76443934665\n",
      "Epoch 157301, Training Loss: 39104, Validation Loss: 53361, 208025.61985514863\n",
      "Epoch 157401, Training Loss: 36168, Validation Loss: 59654, 193376.88351371035\n",
      "Epoch 157501, Training Loss: 35060, Validation Loss: 57469, 128884.4556770067\n",
      "Epoch 157601, Training Loss: 33051, Validation Loss: 53618, 174494.16213010563\n",
      "Epoch 157701, Training Loss: 34875, Validation Loss: 53188, 128086.91829798883\n",
      "Epoch 157801, Training Loss: 34348, Validation Loss: 52675, 180347.86990255755\n",
      "Epoch 157901, Training Loss: 34304, Validation Loss: 57449, 190911.37912535926\n",
      "Epoch 158001, Training Loss: 34763, Validation Loss: 54899, 155175.7059927548\n",
      "Epoch 158101, Training Loss: 39327, Validation Loss: 62231, 192368.09498250353\n",
      "Epoch 158201, Training Loss: 32969, Validation Loss: 54969, 206903.50459081432\n",
      "Epoch 158301, Training Loss: 36765, Validation Loss: 55771, 170130.22145126355\n",
      "Epoch 158401, Training Loss: 34201, Validation Loss: 55364, 161880.4770534021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158501, Training Loss: 32551, Validation Loss: 56339, 246852.12715698392\n",
      "Epoch 158601, Training Loss: 34187, Validation Loss: 55763, 146831.18218937877\n",
      "Epoch 158701, Training Loss: 34466, Validation Loss: 53241, 173353.95971166552\n",
      "Epoch 158801, Training Loss: 33971, Validation Loss: 55960, 132820.94147097904\n",
      "Epoch 158901, Training Loss: 35488, Validation Loss: 52370, 210668.5990451672\n",
      "Epoch 159001, Training Loss: 31838, Validation Loss: 55155, 137079.02678673956\n",
      "Epoch 159101, Training Loss: 33887, Validation Loss: 54139, 162643.42648265345\n",
      "Epoch 159201, Training Loss: 33006, Validation Loss: 53560, 114282.59980337632\n",
      "Epoch 159301, Training Loss: 33293, Validation Loss: 56476, 124204.41512558966\n",
      "Epoch 159401, Training Loss: 32336, Validation Loss: 55254, 129261.54887993581\n",
      "Epoch 159501, Training Loss: 36214, Validation Loss: 52825, 254822.61710940977\n",
      "Epoch 159601, Training Loss: 35715, Validation Loss: 55197, 135974.3931970007\n",
      "Epoch 159701, Training Loss: 35832, Validation Loss: 55991, 146988.68369165494\n",
      "Epoch 159801, Training Loss: 34715, Validation Loss: 55800, 175288.51929800547\n",
      "Epoch 159901, Training Loss: 35824, Validation Loss: 57972, 202532.96484620453\n",
      "Epoch 160001, Training Loss: 34035, Validation Loss: 53970, 138455.3045669197\n",
      "Epoch 160101, Training Loss: 34191, Validation Loss: 58637, 125651.79509418132\n",
      "Epoch 160201, Training Loss: 34961, Validation Loss: 55721, 141007.6684864269\n",
      "Epoch 160301, Training Loss: 33159, Validation Loss: 54567, 144999.58011575896\n",
      "Epoch 160401, Training Loss: 35729, Validation Loss: 56194, 136064.75870479527\n",
      "Epoch 160501, Training Loss: 33102, Validation Loss: 52768, 172895.35731917666\n",
      "Epoch 160601, Training Loss: 34749, Validation Loss: 57847, 176722.51425479833\n",
      "Epoch 160701, Training Loss: 31430, Validation Loss: 53478, 166161.08744312785\n",
      "Epoch 160801, Training Loss: 36489, Validation Loss: 54269, 139652.65561423937\n",
      "Epoch 160901, Training Loss: 35194, Validation Loss: 55364, 193006.38923001057\n",
      "Epoch 161001, Training Loss: 32130, Validation Loss: 52884, 157413.54513163262\n",
      "Epoch 161101, Training Loss: 34069, Validation Loss: 57780, 149376.16672813173\n",
      "Epoch 161201, Training Loss: 31552, Validation Loss: 56136, 143042.84731065427\n",
      "Epoch 161301, Training Loss: 35279, Validation Loss: 57212, 151828.6020812\n",
      "Epoch 161401, Training Loss: 33812, Validation Loss: 53194, 153380.47316297833\n",
      "Epoch 161501, Training Loss: 37948, Validation Loss: 56926, 161905.70524143538\n",
      "Epoch 161601, Training Loss: 35901, Validation Loss: 55048, 197225.51021094286\n",
      "Epoch 161701, Training Loss: 34155, Validation Loss: 59396, 178536.92110586085\n",
      "Epoch 161801, Training Loss: 34300, Validation Loss: 54873, 156818.81287303718\n",
      "Epoch 161901, Training Loss: 33227, Validation Loss: 55604, 171167.00452167462\n",
      "Epoch 162001, Training Loss: 36781, Validation Loss: 54583, 155571.9757604104\n",
      "Epoch 162101, Training Loss: 34333, Validation Loss: 54829, 181902.0742214589\n",
      "Epoch 162201, Training Loss: 34224, Validation Loss: 55661, 171732.68402568184\n",
      "Epoch 162301, Training Loss: 33591, Validation Loss: 52119, 150802.3578278712\n",
      "Epoch 162401, Training Loss: 33596, Validation Loss: 54277, 174179.71895360682\n",
      "Epoch 162501, Training Loss: 36766, Validation Loss: 56359, 220841.41158717233\n",
      "Epoch 162601, Training Loss: 36297, Validation Loss: 54786, 149974.28544195145\n",
      "Epoch 162701, Training Loss: 33299, Validation Loss: 53355, 153225.2344993157\n",
      "Epoch 162801, Training Loss: 34195, Validation Loss: 54061, 177809.92155466136\n",
      "Epoch 162901, Training Loss: 35902, Validation Loss: 54891, 177691.0529018601\n",
      "Epoch 163001, Training Loss: 32205, Validation Loss: 54768, 142385.2129155669\n",
      "Epoch 163101, Training Loss: 33366, Validation Loss: 54607, 145433.9216383301\n",
      "Epoch 163201, Training Loss: 34425, Validation Loss: 57274, 156628.4585873464\n",
      "Epoch 163301, Training Loss: 32365, Validation Loss: 57673, 186982.15741096518\n",
      "Epoch 163401, Training Loss: 35959, Validation Loss: 54205, 173933.57845311784\n",
      "Epoch 163501, Training Loss: 35928, Validation Loss: 57405, 279212.97453440976\n",
      "Epoch 163601, Training Loss: 34034, Validation Loss: 54136, 131107.94777339968\n",
      "Epoch 163701, Training Loss: 33783, Validation Loss: 56682, 134004.22207396707\n",
      "Epoch 163801, Training Loss: 33793, Validation Loss: 54969, 167310.77435192818\n",
      "Epoch 163901, Training Loss: 32684, Validation Loss: 54697, 157825.42440455113\n",
      "Epoch 164001, Training Loss: 34865, Validation Loss: 56546, 203512.16463961801\n",
      "Epoch 164101, Training Loss: 34886, Validation Loss: 56196, 155540.96641427546\n",
      "Epoch 164201, Training Loss: 34624, Validation Loss: 54054, 139897.90550002144\n",
      "Epoch 164301, Training Loss: 34777, Validation Loss: 54749, 161206.2125522153\n",
      "Epoch 164401, Training Loss: 36416, Validation Loss: 54102, 184952.8402791766\n",
      "Epoch 164501, Training Loss: 35583, Validation Loss: 57416, 186219.33069136328\n",
      "Epoch 164601, Training Loss: 36123, Validation Loss: 55097, 167898.82757293756\n",
      "Epoch 164701, Training Loss: 37854, Validation Loss: 57918, 163899.29859261945\n",
      "Epoch 164801, Training Loss: 36455, Validation Loss: 54687, 153248.27935893202\n",
      "Epoch 164901, Training Loss: 33912, Validation Loss: 59655, 164734.57447977443\n",
      "Epoch 165001, Training Loss: 35399, Validation Loss: 55787, 156742.61299493993\n",
      "Epoch 165101, Training Loss: 33350, Validation Loss: 55252, 125933.09489583608\n",
      "Epoch 165201, Training Loss: 35040, Validation Loss: 53868, 177654.29398128772\n",
      "Epoch 165301, Training Loss: 35037, Validation Loss: 55785, 121768.95790871214\n",
      "Epoch 165401, Training Loss: 35754, Validation Loss: 57753, 143156.6563459656\n",
      "Epoch 165501, Training Loss: 32134, Validation Loss: 58881, 167928.99048702887\n",
      "Epoch 165601, Training Loss: 33872, Validation Loss: 54015, 126723.35952636239\n",
      "Epoch 165701, Training Loss: 34418, Validation Loss: 54839, 129309.1234378101\n",
      "Epoch 165801, Training Loss: 32737, Validation Loss: 55783, 166182.87519841088\n",
      "Epoch 165901, Training Loss: 33728, Validation Loss: 55111, 204558.98198378016\n",
      "Epoch 166001, Training Loss: 32521, Validation Loss: 54193, 166595.99834302603\n",
      "Epoch 166101, Training Loss: 33487, Validation Loss: 57018, 172450.21536510167\n",
      "Epoch 166201, Training Loss: 31945, Validation Loss: 56579, 212180.65454035834\n",
      "Epoch 166301, Training Loss: 34201, Validation Loss: 56214, 182758.93294135167\n",
      "Epoch 166401, Training Loss: 37066, Validation Loss: 53632, 169504.25111740976\n",
      "Epoch 166501, Training Loss: 36267, Validation Loss: 53628, 188776.97060111817\n",
      "Epoch 166601, Training Loss: 35216, Validation Loss: 52728, 155048.23693747132\n",
      "Epoch 166701, Training Loss: 33592, Validation Loss: 52321, 149921.84631403963\n",
      "Epoch 166801, Training Loss: 33973, Validation Loss: 53969, 171347.08365391102\n",
      "Epoch 166901, Training Loss: 32262, Validation Loss: 55724, 165727.34822583108\n",
      "Epoch 167001, Training Loss: 35460, Validation Loss: 57462, 137897.8178180859\n",
      "Epoch 167101, Training Loss: 33266, Validation Loss: 53037, 170063.6070315363\n",
      "Epoch 167201, Training Loss: 34116, Validation Loss: 53797, 152381.4061430128\n",
      "Epoch 167301, Training Loss: 33896, Validation Loss: 53263, 178042.11647433625\n",
      "Epoch 167401, Training Loss: 32996, Validation Loss: 57373, 145186.7610009672\n",
      "Epoch 167501, Training Loss: 36539, Validation Loss: 55816, 215200.18402665682\n",
      "Epoch 167601, Training Loss: 34054, Validation Loss: 57124, 153665.39604306582\n",
      "Epoch 167701, Training Loss: 34279, Validation Loss: 54867, 124558.01433799409\n",
      "Epoch 167801, Training Loss: 36936, Validation Loss: 57043, 166036.38986762823\n",
      "Epoch 167901, Training Loss: 34058, Validation Loss: 54569, 158120.13535075317\n",
      "Epoch 168001, Training Loss: 34483, Validation Loss: 56198, 169651.05167714568\n",
      "Epoch 168101, Training Loss: 35682, Validation Loss: 53118, 199096.48510171767\n",
      "Epoch 168201, Training Loss: 34860, Validation Loss: 53990, 195347.1711196576\n",
      "Epoch 168301, Training Loss: 32892, Validation Loss: 55341, 152742.55515457664\n",
      "Epoch 168401, Training Loss: 35199, Validation Loss: 54020, 169466.7973097446\n",
      "Epoch 168501, Training Loss: 36898, Validation Loss: 55709, 154031.28516358684\n",
      "Epoch 168601, Training Loss: 33818, Validation Loss: 56148, 175555.04497987812\n",
      "Epoch 168701, Training Loss: 33238, Validation Loss: 56617, 170633.78930914996\n",
      "Epoch 168801, Training Loss: 33182, Validation Loss: 56538, 151367.42389660864\n",
      "Epoch 168901, Training Loss: 33851, Validation Loss: 55555, 206591.8996218394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169001, Training Loss: 36280, Validation Loss: 54747, 139614.70289350854\n",
      "Epoch 169101, Training Loss: 34675, Validation Loss: 53585, 148068.54008560578\n",
      "Epoch 169201, Training Loss: 36265, Validation Loss: 56681, 206653.75218791506\n",
      "Epoch 169301, Training Loss: 35352, Validation Loss: 59443, 207803.3643835551\n",
      "Epoch 169401, Training Loss: 34567, Validation Loss: 53433, 172802.2696349162\n",
      "Epoch 169501, Training Loss: 34103, Validation Loss: 55633, 187682.90041086162\n",
      "Epoch 169601, Training Loss: 33813, Validation Loss: 55223, 170508.66189081877\n",
      "Epoch 169701, Training Loss: 31760, Validation Loss: 58085, 168166.3779487676\n",
      "Epoch 169801, Training Loss: 34414, Validation Loss: 55934, 166435.6672194472\n",
      "Epoch 169901, Training Loss: 33880, Validation Loss: 56454, 124865.85126245006\n",
      "Epoch 170001, Training Loss: 36726, Validation Loss: 54380, 143799.36290868852\n",
      "Epoch 170101, Training Loss: 35106, Validation Loss: 55052, 184175.95167277\n",
      "Epoch 170201, Training Loss: 33231, Validation Loss: 55869, 162182.63317927773\n",
      "Epoch 170301, Training Loss: 38127, Validation Loss: 55499, 193391.20690786466\n",
      "Epoch 170401, Training Loss: 34333, Validation Loss: 55726, 166495.76680036154\n",
      "Epoch 170501, Training Loss: 35552, Validation Loss: 54066, 182770.76965157\n",
      "Epoch 170601, Training Loss: 36693, Validation Loss: 53536, 146963.840325385\n",
      "Epoch 170701, Training Loss: 32076, Validation Loss: 56991, 193975.8629499762\n",
      "Epoch 170801, Training Loss: 34867, Validation Loss: 54993, 178892.94214996172\n",
      "Epoch 170901, Training Loss: 35453, Validation Loss: 56896, 173794.78879985408\n",
      "Epoch 171001, Training Loss: 33930, Validation Loss: 57411, 145669.38390267765\n",
      "Epoch 171101, Training Loss: 35198, Validation Loss: 53969, 156258.1948068523\n",
      "Epoch 171201, Training Loss: 32815, Validation Loss: 59484, 128474.4565087806\n",
      "Epoch 171301, Training Loss: 32959, Validation Loss: 54955, 142601.06039440818\n",
      "Epoch 171401, Training Loss: 32727, Validation Loss: 53981, 160457.02630655418\n",
      "Epoch 171501, Training Loss: 35341, Validation Loss: 54333, 162605.77921052245\n",
      "Epoch 171601, Training Loss: 35198, Validation Loss: 54997, 159726.65193433172\n",
      "Epoch 171701, Training Loss: 33105, Validation Loss: 56530, 204003.32174763712\n",
      "Epoch 171801, Training Loss: 35163, Validation Loss: 53918, 188918.6150572945\n",
      "Epoch 171901, Training Loss: 36732, Validation Loss: 55071, 170271.2251729401\n",
      "Epoch 172001, Training Loss: 34596, Validation Loss: 54999, 192909.56272221662\n",
      "Epoch 172101, Training Loss: 32679, Validation Loss: 59273, 186308.72086954387\n",
      "Epoch 172201, Training Loss: 32042, Validation Loss: 54200, 171386.1889965844\n",
      "Epoch 172301, Training Loss: 34621, Validation Loss: 54568, 151668.21091616576\n",
      "Epoch 172401, Training Loss: 32439, Validation Loss: 55952, 152406.24720937034\n",
      "Epoch 172501, Training Loss: 33975, Validation Loss: 57910, 177163.34547042043\n",
      "Epoch 172601, Training Loss: 31591, Validation Loss: 58012, 146023.92985024967\n",
      "Epoch 172701, Training Loss: 34407, Validation Loss: 53695, 128486.29005902114\n",
      "Epoch 172801, Training Loss: 36416, Validation Loss: 55264, 200013.612208061\n",
      "Epoch 172901, Training Loss: 36257, Validation Loss: 54669, 210847.74744069122\n",
      "Epoch 173001, Training Loss: 34003, Validation Loss: 54821, 152065.02045697728\n",
      "Epoch 173101, Training Loss: 35232, Validation Loss: 56148, 174850.71612314752\n",
      "Epoch 173201, Training Loss: 34023, Validation Loss: 56961, 152107.9831353325\n",
      "Epoch 173301, Training Loss: 33881, Validation Loss: 54169, 146662.68502785356\n",
      "Epoch 173401, Training Loss: 33572, Validation Loss: 52852, 171084.52517606432\n",
      "Epoch 173501, Training Loss: 34193, Validation Loss: 56118, 183300.22137073995\n",
      "Epoch 173601, Training Loss: 34403, Validation Loss: 55730, 183870.40684378706\n",
      "Epoch 173701, Training Loss: 34471, Validation Loss: 55818, 195760.10818142406\n",
      "Epoch 173801, Training Loss: 32368, Validation Loss: 54812, 123427.44168101905\n",
      "Epoch 173901, Training Loss: 33917, Validation Loss: 56426, 210656.64118463566\n",
      "Epoch 174001, Training Loss: 33227, Validation Loss: 54013, 154083.359627835\n",
      "Epoch 174101, Training Loss: 32683, Validation Loss: 58480, 144386.42637664743\n",
      "Epoch 174201, Training Loss: 35525, Validation Loss: 54289, 253392.25536686744\n",
      "Epoch 174301, Training Loss: 32354, Validation Loss: 55046, 148126.44618171683\n",
      "Epoch 174401, Training Loss: 34103, Validation Loss: 56220, 157684.08922435765\n",
      "Epoch 174501, Training Loss: 34994, Validation Loss: 54272, 190255.78206963325\n",
      "Epoch 174601, Training Loss: 32826, Validation Loss: 56162, 150039.39672192282\n",
      "Epoch 174701, Training Loss: 34110, Validation Loss: 56213, 222513.7669024833\n",
      "Epoch 174801, Training Loss: 34447, Validation Loss: 54623, 180622.16403048384\n",
      "Epoch 174901, Training Loss: 33960, Validation Loss: 57267, 177793.93457656456\n",
      "Epoch 175001, Training Loss: 34532, Validation Loss: 52782, 151738.92115913882\n",
      "Epoch 175101, Training Loss: 36373, Validation Loss: 55147, 222116.2035802089\n",
      "Epoch 175201, Training Loss: 33804, Validation Loss: 56159, 193703.9585553511\n",
      "Epoch 175301, Training Loss: 36921, Validation Loss: 57539, 155303.81728921874\n",
      "Epoch 175401, Training Loss: 34909, Validation Loss: 54791, 195907.00918126098\n",
      "Epoch 175501, Training Loss: 34481, Validation Loss: 57975, 151582.94565958623\n",
      "Epoch 175601, Training Loss: 35320, Validation Loss: 54003, 152587.8297458546\n",
      "Epoch 175701, Training Loss: 31903, Validation Loss: 56511, 159014.98593824636\n",
      "Epoch 175801, Training Loss: 36177, Validation Loss: 57252, 271921.6512403852\n",
      "Epoch 175901, Training Loss: 37718, Validation Loss: 58037, 157598.5660363125\n",
      "Epoch 176001, Training Loss: 38128, Validation Loss: 56955, 193665.01832250867\n",
      "Epoch 176101, Training Loss: 36092, Validation Loss: 54224, 202343.9139370322\n",
      "Epoch 176201, Training Loss: 32890, Validation Loss: 54637, 138350.35165241756\n",
      "Epoch 176301, Training Loss: 37001, Validation Loss: 52096, 197767.73820760145\n",
      "Epoch 176401, Training Loss: 34789, Validation Loss: 56876, 127005.78132510021\n",
      "Epoch 176501, Training Loss: 36569, Validation Loss: 57424, 173862.94395494554\n",
      "Epoch 176601, Training Loss: 35878, Validation Loss: 53956, 153036.52140046554\n",
      "Epoch 176701, Training Loss: 37421, Validation Loss: 55602, 173484.16919793308\n",
      "Epoch 176801, Training Loss: 35902, Validation Loss: 51485, 165417.5629341683\n",
      "Epoch 176901, Training Loss: 35040, Validation Loss: 56462, 185621.65683457314\n",
      "Epoch 177001, Training Loss: 33334, Validation Loss: 56923, 123948.83886737253\n",
      "Epoch 177101, Training Loss: 32971, Validation Loss: 54936, 148625.4165059637\n",
      "Epoch 177201, Training Loss: 33868, Validation Loss: 54395, 169318.55503103894\n",
      "Epoch 177301, Training Loss: 35241, Validation Loss: 55585, 139107.90940683451\n",
      "Epoch 177401, Training Loss: 33176, Validation Loss: 57576, 197154.80941600088\n",
      "Epoch 177501, Training Loss: 36895, Validation Loss: 54246, 148670.6641981676\n",
      "Epoch 177601, Training Loss: 34873, Validation Loss: 53650, 148583.2517152136\n",
      "Epoch 177701, Training Loss: 33822, Validation Loss: 53638, 165718.55282549575\n",
      "Epoch 177801, Training Loss: 34850, Validation Loss: 52748, 174661.9557663208\n",
      "Epoch 177901, Training Loss: 33201, Validation Loss: 53224, 132296.34967082692\n",
      "Epoch 178001, Training Loss: 33521, Validation Loss: 58075, 130188.59634198691\n",
      "Epoch 178101, Training Loss: 36768, Validation Loss: 55345, 168284.26563322684\n",
      "Epoch 178201, Training Loss: 35588, Validation Loss: 55834, 115800.79267128154\n",
      "Epoch 178301, Training Loss: 37260, Validation Loss: 55939, 192032.6736803518\n",
      "Epoch 178401, Training Loss: 33099, Validation Loss: 54869, 139805.8625099271\n",
      "Epoch 178501, Training Loss: 35874, Validation Loss: 57334, 176913.6898399664\n",
      "Epoch 178601, Training Loss: 38836, Validation Loss: 57708, 169252.9584565538\n",
      "Epoch 178701, Training Loss: 35004, Validation Loss: 57555, 119929.09961212227\n",
      "Epoch 178801, Training Loss: 36239, Validation Loss: 54705, 175242.68744664788\n",
      "Epoch 178901, Training Loss: 35408, Validation Loss: 54474, 168117.1810724309\n",
      "Epoch 179001, Training Loss: 34377, Validation Loss: 54852, 148859.53637800124\n",
      "Epoch 179101, Training Loss: 34588, Validation Loss: 53329, 135537.7081830336\n",
      "Epoch 179201, Training Loss: 40567, Validation Loss: 54386, 171200.28862807603\n",
      "Epoch 179301, Training Loss: 35074, Validation Loss: 54884, 160281.9836106947\n",
      "Epoch 179401, Training Loss: 34136, Validation Loss: 56779, 183633.27213767372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179501, Training Loss: 33055, Validation Loss: 57728, 153984.06878993285\n",
      "Epoch 179601, Training Loss: 35381, Validation Loss: 55068, 146033.5953833525\n",
      "Epoch 179701, Training Loss: 34056, Validation Loss: 55636, 162303.70031102397\n",
      "Epoch 179801, Training Loss: 35215, Validation Loss: 53730, 213168.0273215597\n",
      "Epoch 179901, Training Loss: 34362, Validation Loss: 56113, 175873.71088524917\n",
      "Epoch 180001, Training Loss: 37568, Validation Loss: 54870, 199268.46988982058\n",
      "Epoch 180101, Training Loss: 35597, Validation Loss: 58216, 208737.43694143847\n",
      "Epoch 180201, Training Loss: 34250, Validation Loss: 52806, 125702.30112071364\n",
      "Epoch 180301, Training Loss: 34745, Validation Loss: 56626, 166914.67399596775\n",
      "Epoch 180401, Training Loss: 37040, Validation Loss: 55966, 168312.99382172013\n",
      "Epoch 180501, Training Loss: 34609, Validation Loss: 54196, 156217.6751404321\n",
      "Epoch 180601, Training Loss: 38400, Validation Loss: 55315, 177325.54729235618\n",
      "Epoch 180701, Training Loss: 33968, Validation Loss: 56587, 167819.44702948516\n",
      "Epoch 180801, Training Loss: 37204, Validation Loss: 51398, 166790.0935248358\n",
      "Epoch 180901, Training Loss: 34070, Validation Loss: 56290, 165951.34275170325\n",
      "Epoch 181001, Training Loss: 35371, Validation Loss: 55175, 130593.86834196914\n",
      "Epoch 181101, Training Loss: 35278, Validation Loss: 54420, 209309.54264077544\n",
      "Epoch 181201, Training Loss: 32370, Validation Loss: 53212, 148553.31376879834\n",
      "Epoch 181301, Training Loss: 34562, Validation Loss: 56940, 153805.98018545148\n",
      "Epoch 181401, Training Loss: 36072, Validation Loss: 58276, 173285.56415266544\n",
      "Epoch 181501, Training Loss: 35097, Validation Loss: 53417, 169487.63195195992\n",
      "Epoch 181601, Training Loss: 33597, Validation Loss: 53958, 162859.73857027697\n",
      "Epoch 181701, Training Loss: 33924, Validation Loss: 54537, 138600.4574365815\n",
      "Epoch 181801, Training Loss: 36368, Validation Loss: 55587, 141496.34130850036\n",
      "Epoch 181901, Training Loss: 35084, Validation Loss: 54357, 168386.70135744358\n",
      "Epoch 182001, Training Loss: 35005, Validation Loss: 54446, 184614.51431909393\n",
      "Epoch 182101, Training Loss: 33406, Validation Loss: 55048, 139351.48412628865\n",
      "Epoch 182201, Training Loss: 35522, Validation Loss: 55199, 166100.70573867913\n",
      "Epoch 182301, Training Loss: 32785, Validation Loss: 56423, 135507.12612605226\n",
      "Epoch 182401, Training Loss: 33298, Validation Loss: 54752, 171984.8458244735\n",
      "Epoch 182501, Training Loss: 35345, Validation Loss: 54963, 128629.14774058636\n",
      "Epoch 182601, Training Loss: 37689, Validation Loss: 58946, 238659.4109256505\n",
      "Epoch 182701, Training Loss: 33475, Validation Loss: 58539, 128331.56958658004\n",
      "Epoch 182801, Training Loss: 33609, Validation Loss: 53200, 125363.74660042278\n",
      "Epoch 182901, Training Loss: 34077, Validation Loss: 55593, 134574.0781731959\n",
      "Epoch 183001, Training Loss: 35275, Validation Loss: 58148, 197921.22726599686\n",
      "Epoch 183101, Training Loss: 36164, Validation Loss: 61223, 188743.11657218484\n",
      "Epoch 183201, Training Loss: 32379, Validation Loss: 54060, 160285.0385840522\n",
      "Epoch 183301, Training Loss: 35353, Validation Loss: 56358, 161038.69845044674\n",
      "Epoch 183401, Training Loss: 37024, Validation Loss: 53890, 173531.86441303874\n",
      "Epoch 183501, Training Loss: 36079, Validation Loss: 54353, 191406.7709011774\n",
      "Epoch 183601, Training Loss: 33624, Validation Loss: 58445, 121541.21639799148\n",
      "Epoch 183701, Training Loss: 34168, Validation Loss: 53652, 140000.6054041741\n",
      "Epoch 183801, Training Loss: 36594, Validation Loss: 54945, 151065.87409373958\n",
      "Epoch 183901, Training Loss: 36423, Validation Loss: 58931, 143034.99845606208\n",
      "Epoch 184001, Training Loss: 35255, Validation Loss: 56457, 184650.08261073954\n",
      "Epoch 184101, Training Loss: 35441, Validation Loss: 55911, 99139.46559798073\n",
      "Epoch 184201, Training Loss: 37339, Validation Loss: 54067, 135688.8532612086\n",
      "Epoch 184301, Training Loss: 39361, Validation Loss: 55014, 225347.1106492896\n",
      "Epoch 184401, Training Loss: 34042, Validation Loss: 56899, 130798.95139339795\n",
      "Epoch 184501, Training Loss: 36475, Validation Loss: 56654, 132078.92492990618\n",
      "Epoch 184601, Training Loss: 34381, Validation Loss: 55975, 146696.03199027502\n",
      "Epoch 184701, Training Loss: 31721, Validation Loss: 53055, 116267.10274347996\n",
      "Epoch 184801, Training Loss: 34901, Validation Loss: 54687, 164335.6249617425\n",
      "Epoch 184901, Training Loss: 34884, Validation Loss: 54714, 155626.7335026134\n",
      "Epoch 185001, Training Loss: 35799, Validation Loss: 54355, 187284.2758026025\n",
      "Epoch 185101, Training Loss: 35403, Validation Loss: 54804, 175740.71686420814\n",
      "Epoch 185201, Training Loss: 33368, Validation Loss: 53880, 135395.43857718227\n",
      "Epoch 185301, Training Loss: 33809, Validation Loss: 59341, 172470.11701603592\n",
      "Epoch 185401, Training Loss: 35282, Validation Loss: 55861, 182400.71677107454\n",
      "Epoch 185501, Training Loss: 37741, Validation Loss: 55714, 161390.7760582009\n",
      "Epoch 185601, Training Loss: 36716, Validation Loss: 57004, 166279.717256968\n",
      "Epoch 185701, Training Loss: 35480, Validation Loss: 54253, 200663.1182840759\n",
      "Epoch 185801, Training Loss: 35880, Validation Loss: 54813, 191509.73973596274\n",
      "Epoch 185901, Training Loss: 35525, Validation Loss: 56634, 151544.31377513535\n",
      "Epoch 186001, Training Loss: 37218, Validation Loss: 53974, 208692.276541614\n",
      "Epoch 186101, Training Loss: 33405, Validation Loss: 55703, 151235.30502297723\n",
      "Epoch 186201, Training Loss: 32555, Validation Loss: 55820, 146673.69994045913\n",
      "Epoch 186301, Training Loss: 34832, Validation Loss: 58942, 162309.3690383384\n",
      "Epoch 186401, Training Loss: 35199, Validation Loss: 55208, 160402.25979454163\n",
      "Epoch 186501, Training Loss: 35666, Validation Loss: 55552, 144066.67337515138\n",
      "Epoch 186601, Training Loss: 36370, Validation Loss: 54841, 134032.11139456197\n",
      "Epoch 186701, Training Loss: 35062, Validation Loss: 54512, 130606.24001890449\n",
      "Epoch 186801, Training Loss: 32733, Validation Loss: 55396, 124053.51980844217\n",
      "Epoch 186901, Training Loss: 35089, Validation Loss: 59039, 143802.37161242738\n",
      "Epoch 187001, Training Loss: 32911, Validation Loss: 55591, 161230.7457174263\n",
      "Epoch 187101, Training Loss: 37431, Validation Loss: 54856, 204064.77565787142\n",
      "Epoch 187201, Training Loss: 35944, Validation Loss: 54759, 136449.50852429256\n",
      "Epoch 187301, Training Loss: 35404, Validation Loss: 55341, 161448.59294221792\n",
      "Epoch 187401, Training Loss: 33260, Validation Loss: 55066, 169528.7154205298\n",
      "Epoch 187501, Training Loss: 36007, Validation Loss: 55992, 131977.282273547\n",
      "Epoch 187601, Training Loss: 33979, Validation Loss: 54895, 155813.61628747117\n",
      "Epoch 187701, Training Loss: 33052, Validation Loss: 55110, 145899.95866512295\n",
      "Epoch 187801, Training Loss: 35149, Validation Loss: 56015, 239676.97326056028\n",
      "Epoch 187901, Training Loss: 35919, Validation Loss: 52855, 168047.1390047949\n",
      "Epoch 188001, Training Loss: 36695, Validation Loss: 55712, 155048.5098163271\n",
      "Epoch 188101, Training Loss: 33639, Validation Loss: 54648, 136559.30768754272\n",
      "Epoch 188201, Training Loss: 32722, Validation Loss: 55727, 145326.17547758235\n",
      "Epoch 188301, Training Loss: 36353, Validation Loss: 55513, 133995.49498337143\n",
      "Epoch 188401, Training Loss: 34742, Validation Loss: 55243, 121239.39193360323\n",
      "Epoch 188501, Training Loss: 32416, Validation Loss: 58310, 134868.65497997735\n",
      "Epoch 188601, Training Loss: 36606, Validation Loss: 55633, 136688.68662803434\n",
      "Epoch 188701, Training Loss: 36626, Validation Loss: 54397, 166239.0773154065\n",
      "Epoch 188801, Training Loss: 35825, Validation Loss: 54939, 189385.23924504223\n",
      "Epoch 188901, Training Loss: 34867, Validation Loss: 54046, 176437.30350246848\n",
      "Epoch 189001, Training Loss: 35920, Validation Loss: 55910, 175139.58036609786\n",
      "Epoch 189101, Training Loss: 35469, Validation Loss: 58572, 174073.09246616086\n",
      "Epoch 189201, Training Loss: 33882, Validation Loss: 58757, 123558.6148610507\n",
      "Epoch 189301, Training Loss: 33788, Validation Loss: 56535, 153430.56387157468\n",
      "Epoch 189401, Training Loss: 33505, Validation Loss: 56101, 138276.37734189912\n",
      "Epoch 189501, Training Loss: 33629, Validation Loss: 57213, 161369.84342836394\n",
      "Epoch 189601, Training Loss: 31770, Validation Loss: 54749, 132790.94977400554\n",
      "Epoch 189701, Training Loss: 34430, Validation Loss: 56086, 159640.00067950177\n",
      "Epoch 189801, Training Loss: 34284, Validation Loss: 57714, 179684.869539893\n",
      "Epoch 189901, Training Loss: 34777, Validation Loss: 55992, 158872.0257193107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190001, Training Loss: 36486, Validation Loss: 53305, 145649.09521149602\n",
      "Epoch 190101, Training Loss: 38098, Validation Loss: 55907, 146362.12948352157\n",
      "Epoch 190201, Training Loss: 34427, Validation Loss: 54655, 136787.7993302584\n",
      "Epoch 190301, Training Loss: 34739, Validation Loss: 54303, 159205.6134921692\n",
      "Epoch 190401, Training Loss: 35757, Validation Loss: 54227, 165632.6126118708\n",
      "Epoch 190501, Training Loss: 34630, Validation Loss: 56727, 126975.32505242973\n",
      "Epoch 190601, Training Loss: 36826, Validation Loss: 54192, 135933.26609628045\n",
      "Epoch 190701, Training Loss: 38143, Validation Loss: 55864, 223422.69601314855\n",
      "Epoch 190801, Training Loss: 34630, Validation Loss: 56085, 155219.2274365793\n",
      "Epoch 190901, Training Loss: 33704, Validation Loss: 55894, 169321.54560166935\n",
      "Epoch 191001, Training Loss: 34539, Validation Loss: 59752, 176684.53839065935\n",
      "Epoch 191101, Training Loss: 33927, Validation Loss: 52973, 168461.5110158401\n",
      "Epoch 191201, Training Loss: 34997, Validation Loss: 54691, 150679.08933620714\n",
      "Epoch 191301, Training Loss: 35881, Validation Loss: 59633, 174560.47938833045\n",
      "Epoch 191401, Training Loss: 34106, Validation Loss: 58023, 188439.30995204113\n",
      "Epoch 191501, Training Loss: 33929, Validation Loss: 55743, 143418.9582271586\n",
      "Epoch 191601, Training Loss: 34045, Validation Loss: 54652, 146393.29288589364\n",
      "Epoch 191701, Training Loss: 33779, Validation Loss: 52645, 142116.82049713295\n",
      "Epoch 191801, Training Loss: 35492, Validation Loss: 54157, 175928.06617341485\n",
      "Epoch 191901, Training Loss: 34310, Validation Loss: 55229, 179586.716332339\n",
      "Epoch 192001, Training Loss: 32983, Validation Loss: 54157, 170959.69841117822\n",
      "Epoch 192101, Training Loss: 34912, Validation Loss: 52662, 143136.27036007438\n",
      "Epoch 192201, Training Loss: 36610, Validation Loss: 55988, 130156.27337905014\n",
      "Epoch 192301, Training Loss: 36966, Validation Loss: 56405, 202460.5392822091\n",
      "Epoch 192401, Training Loss: 32084, Validation Loss: 55279, 192575.5942404305\n",
      "Epoch 192501, Training Loss: 35679, Validation Loss: 56109, 160896.51750819993\n",
      "Epoch 192601, Training Loss: 33784, Validation Loss: 54854, 175696.81024085227\n",
      "Epoch 192701, Training Loss: 34782, Validation Loss: 53302, 153349.28856406713\n",
      "Epoch 192801, Training Loss: 32851, Validation Loss: 53569, 145807.85455813282\n",
      "Epoch 192901, Training Loss: 36043, Validation Loss: 57611, 149371.54102662505\n",
      "Epoch 193001, Training Loss: 34347, Validation Loss: 54573, 175000.05299183316\n",
      "Epoch 193101, Training Loss: 35764, Validation Loss: 57436, 140197.59019219907\n",
      "Epoch 193201, Training Loss: 36024, Validation Loss: 56994, 123356.70386938895\n",
      "Epoch 193301, Training Loss: 35033, Validation Loss: 55951, 148186.33366304348\n",
      "Epoch 193401, Training Loss: 37423, Validation Loss: 56498, 140772.28646251126\n",
      "Epoch 193501, Training Loss: 33181, Validation Loss: 56491, 163767.15356881716\n",
      "Epoch 193601, Training Loss: 33348, Validation Loss: 55319, 144971.03554759276\n",
      "Epoch 193701, Training Loss: 36440, Validation Loss: 53952, 128423.60386682606\n",
      "Epoch 193801, Training Loss: 34536, Validation Loss: 57145, 165824.9068361337\n",
      "Epoch 193901, Training Loss: 33687, Validation Loss: 54916, 150951.0089946362\n",
      "Epoch 194001, Training Loss: 33914, Validation Loss: 55204, 100444.984240992\n",
      "Epoch 194101, Training Loss: 37427, Validation Loss: 52256, 140900.81260180334\n",
      "Epoch 194201, Training Loss: 37047, Validation Loss: 53271, 175370.51236293116\n",
      "Epoch 194301, Training Loss: 36441, Validation Loss: 59213, 200857.98980250084\n",
      "Epoch 194401, Training Loss: 34435, Validation Loss: 57288, 177086.5974872789\n",
      "Epoch 194501, Training Loss: 35041, Validation Loss: 54491, 167293.94243250633\n",
      "Epoch 194601, Training Loss: 31957, Validation Loss: 56649, 111759.86615469074\n",
      "Epoch 194701, Training Loss: 33924, Validation Loss: 54478, 129498.94561974086\n",
      "Epoch 194801, Training Loss: 35817, Validation Loss: 54477, 143313.6890671584\n",
      "Epoch 194901, Training Loss: 35180, Validation Loss: 56112, 195333.16326318696\n",
      "Epoch 195001, Training Loss: 33644, Validation Loss: 57390, 148238.14549772986\n",
      "Epoch 195101, Training Loss: 33600, Validation Loss: 55879, 151003.73221404356\n",
      "Epoch 195201, Training Loss: 33131, Validation Loss: 56891, 156316.9211051035\n",
      "Epoch 195301, Training Loss: 35020, Validation Loss: 55902, 142436.67304298378\n",
      "Epoch 195401, Training Loss: 33172, Validation Loss: 55390, 127016.37290533628\n",
      "Epoch 195501, Training Loss: 37773, Validation Loss: 55211, 160701.13211562092\n",
      "Epoch 195601, Training Loss: 33978, Validation Loss: 54361, 102758.94216810814\n",
      "Epoch 195701, Training Loss: 34301, Validation Loss: 56211, 137742.7727882143\n",
      "Epoch 195801, Training Loss: 34945, Validation Loss: 55033, 221374.7251718609\n",
      "Epoch 195901, Training Loss: 34324, Validation Loss: 55545, 165596.7794644147\n",
      "Epoch 196001, Training Loss: 33644, Validation Loss: 55415, 127194.68269695586\n",
      "Epoch 196101, Training Loss: 37177, Validation Loss: 57200, 165241.35592608442\n",
      "Epoch 196201, Training Loss: 33771, Validation Loss: 52386, 135395.7204457881\n",
      "Epoch 196301, Training Loss: 36570, Validation Loss: 57107, 176971.73419685542\n",
      "Epoch 196401, Training Loss: 33301, Validation Loss: 55883, 124719.09606871907\n",
      "Epoch 196501, Training Loss: 34598, Validation Loss: 54527, 177747.99060269646\n",
      "Epoch 196601, Training Loss: 34426, Validation Loss: 56594, 126177.58408795681\n",
      "Epoch 196701, Training Loss: 34564, Validation Loss: 56207, 156776.12324292993\n",
      "Epoch 196801, Training Loss: 35444, Validation Loss: 56709, 124453.62764296052\n",
      "Epoch 196901, Training Loss: 34816, Validation Loss: 54280, 156583.40945041913\n",
      "Epoch 197001, Training Loss: 35306, Validation Loss: 53208, 179393.1202588156\n",
      "Epoch 197101, Training Loss: 36096, Validation Loss: 58576, 151787.04873698138\n",
      "Epoch 197201, Training Loss: 32199, Validation Loss: 55537, 120678.19470625388\n",
      "Epoch 197301, Training Loss: 37058, Validation Loss: 53247, 194998.77163247592\n",
      "Epoch 197401, Training Loss: 33722, Validation Loss: 55371, 161875.48285951276\n",
      "Epoch 197501, Training Loss: 34566, Validation Loss: 54903, 140397.8522468809\n",
      "Epoch 197601, Training Loss: 37404, Validation Loss: 52967, 140766.2075512509\n",
      "Epoch 197701, Training Loss: 36802, Validation Loss: 54837, 199596.5762430204\n",
      "Epoch 197801, Training Loss: 35508, Validation Loss: 55635, 153846.12653575453\n",
      "Epoch 197901, Training Loss: 35332, Validation Loss: 53698, 144509.99782786434\n",
      "Epoch 198001, Training Loss: 36399, Validation Loss: 52135, 138644.51229039006\n",
      "Epoch 198101, Training Loss: 35032, Validation Loss: 56526, 162204.1585284525\n",
      "Epoch 198201, Training Loss: 35420, Validation Loss: 57812, 133955.2218153612\n",
      "Epoch 198301, Training Loss: 34747, Validation Loss: 56606, 154584.38560752966\n",
      "Epoch 198401, Training Loss: 36309, Validation Loss: 54100, 108482.42282378439\n",
      "Epoch 198501, Training Loss: 37057, Validation Loss: 55993, 160975.74657959642\n",
      "Epoch 198601, Training Loss: 34661, Validation Loss: 55913, 216647.23471689117\n",
      "Epoch 198701, Training Loss: 35378, Validation Loss: 57081, 148253.59860470527\n",
      "Epoch 198801, Training Loss: 35672, Validation Loss: 54836, 133278.16486656285\n",
      "Epoch 198901, Training Loss: 33601, Validation Loss: 54462, 165371.24165103934\n",
      "Epoch 199001, Training Loss: 34144, Validation Loss: 57291, 125784.86053390615\n",
      "Epoch 199101, Training Loss: 34974, Validation Loss: 57523, 207798.89963277185\n",
      "Epoch 199201, Training Loss: 32231, Validation Loss: 54998, 149465.5364102693\n",
      "Epoch 199301, Training Loss: 34071, Validation Loss: 55748, 114941.9616990896\n",
      "Epoch 199401, Training Loss: 34051, Validation Loss: 59433, 154295.433483675\n",
      "Epoch 199501, Training Loss: 34584, Validation Loss: 53634, 166169.21349018865\n",
      "Epoch 199601, Training Loss: 34730, Validation Loss: 53088, 163367.2642327059\n",
      "Epoch 199701, Training Loss: 37002, Validation Loss: 52891, 183253.41814733096\n",
      "Epoch 199801, Training Loss: 36758, Validation Loss: 51451, 209987.18335891745\n",
      "Epoch 199901, Training Loss: 34461, Validation Loss: 56124, 167771.55724841045\n",
      "Epoch 200001, Training Loss: 36143, Validation Loss: 52612, 163844.96243015517\n",
      "Epoch 200101, Training Loss: 35656, Validation Loss: 54403, 207038.23078929304\n",
      "Epoch 200201, Training Loss: 32730, Validation Loss: 51094, 149925.1388191747\n",
      "Epoch 200301, Training Loss: 33552, Validation Loss: 55283, 129527.51261854447\n",
      "Epoch 200401, Training Loss: 34548, Validation Loss: 53053, 178373.20862150818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200501, Training Loss: 32359, Validation Loss: 53502, 123181.47729321546\n",
      "Epoch 200601, Training Loss: 34409, Validation Loss: 59200, 135565.47140781445\n",
      "Epoch 200701, Training Loss: 37763, Validation Loss: 54243, 105533.16277578923\n",
      "Epoch 200801, Training Loss: 32678, Validation Loss: 56895, 137537.10800856352\n",
      "Epoch 200901, Training Loss: 33642, Validation Loss: 54212, 163364.6709218851\n",
      "Epoch 201001, Training Loss: 35973, Validation Loss: 58738, 201075.83800767464\n",
      "Epoch 201101, Training Loss: 35174, Validation Loss: 55743, 150054.46379246304\n",
      "Epoch 201201, Training Loss: 34498, Validation Loss: 53345, 163625.58586351134\n",
      "Epoch 201301, Training Loss: 36624, Validation Loss: 55692, 134655.64023204133\n",
      "Epoch 201401, Training Loss: 36239, Validation Loss: 54167, 130118.64600290272\n",
      "Epoch 201501, Training Loss: 33798, Validation Loss: 59122, 124964.01652327315\n",
      "Epoch 201601, Training Loss: 32866, Validation Loss: 54213, 120763.04682348784\n",
      "Epoch 201701, Training Loss: 37546, Validation Loss: 58690, 185538.50559564665\n",
      "Epoch 201801, Training Loss: 34466, Validation Loss: 53370, 127606.66533156075\n",
      "Epoch 201901, Training Loss: 34466, Validation Loss: 58389, 144747.99413898724\n",
      "Epoch 202001, Training Loss: 35790, Validation Loss: 53414, 108166.36006660845\n",
      "Epoch 202101, Training Loss: 35588, Validation Loss: 58229, 239734.15626505032\n",
      "Epoch 202201, Training Loss: 34178, Validation Loss: 56825, 161897.8684629191\n",
      "Epoch 202301, Training Loss: 37019, Validation Loss: 55767, 153207.3877141431\n",
      "Epoch 202401, Training Loss: 36933, Validation Loss: 52872, 216336.46622500275\n",
      "Epoch 202501, Training Loss: 33946, Validation Loss: 56052, 145130.10323199764\n",
      "Epoch 202601, Training Loss: 35826, Validation Loss: 54207, 197947.9818580534\n",
      "Epoch 202701, Training Loss: 35455, Validation Loss: 54915, 166018.15363435517\n",
      "Epoch 202801, Training Loss: 35287, Validation Loss: 54244, 189697.17779686025\n",
      "Epoch 202901, Training Loss: 34832, Validation Loss: 57077, 141019.72900859022\n",
      "Epoch 203001, Training Loss: 33177, Validation Loss: 53821, 187352.22774623267\n",
      "Epoch 203101, Training Loss: 35648, Validation Loss: 58781, 139167.8370986914\n",
      "Epoch 203201, Training Loss: 34556, Validation Loss: 58056, 156750.7231991171\n",
      "Epoch 203301, Training Loss: 36002, Validation Loss: 57077, 147528.96720259372\n",
      "Epoch 203401, Training Loss: 33573, Validation Loss: 55946, 178376.8755179436\n",
      "Epoch 203501, Training Loss: 34886, Validation Loss: 53664, 156947.77371154356\n",
      "Epoch 203601, Training Loss: 33928, Validation Loss: 55615, 127945.84023298637\n",
      "Epoch 203701, Training Loss: 35287, Validation Loss: 52953, 195832.6517666507\n",
      "Epoch 203801, Training Loss: 37834, Validation Loss: 54533, 147603.84322253728\n",
      "Epoch 203901, Training Loss: 34749, Validation Loss: 54544, 170849.79341789638\n",
      "Epoch 204001, Training Loss: 35206, Validation Loss: 55340, 198543.26606777994\n",
      "Epoch 204101, Training Loss: 33964, Validation Loss: 54226, 171207.57303033062\n",
      "Epoch 204201, Training Loss: 32481, Validation Loss: 53902, 111546.46812362007\n",
      "Epoch 204301, Training Loss: 33450, Validation Loss: 59021, 128094.1386211611\n",
      "Epoch 204401, Training Loss: 31319, Validation Loss: 54158, 121114.34721500902\n",
      "Epoch 204501, Training Loss: 36909, Validation Loss: 56263, 198975.31284103906\n",
      "Epoch 204601, Training Loss: 35966, Validation Loss: 54020, 130201.59952756175\n",
      "Epoch 204701, Training Loss: 34716, Validation Loss: 55963, 153391.84319738182\n",
      "Epoch 204801, Training Loss: 34276, Validation Loss: 53315, 163934.5290244365\n",
      "Epoch 204901, Training Loss: 34284, Validation Loss: 54220, 138867.28187530098\n",
      "Epoch 205001, Training Loss: 32999, Validation Loss: 56990, 138734.4061474428\n",
      "Epoch 205101, Training Loss: 34986, Validation Loss: 53565, 159426.20893136357\n",
      "Epoch 205201, Training Loss: 35230, Validation Loss: 55735, 162360.2228147754\n",
      "Epoch 205301, Training Loss: 34980, Validation Loss: 53064, 156797.0841890771\n",
      "Epoch 205401, Training Loss: 34312, Validation Loss: 52365, 162997.53484549763\n",
      "Epoch 205501, Training Loss: 33545, Validation Loss: 54454, 125724.13970879519\n",
      "Epoch 205601, Training Loss: 32903, Validation Loss: 55122, 205021.65545299998\n",
      "Epoch 205701, Training Loss: 35330, Validation Loss: 57287, 149628.4796509082\n",
      "Epoch 205801, Training Loss: 35715, Validation Loss: 53574, 148002.62546839268\n",
      "Epoch 205901, Training Loss: 33021, Validation Loss: 58225, 145663.58983698158\n",
      "Epoch 206001, Training Loss: 33593, Validation Loss: 55880, 165689.84594848115\n",
      "Epoch 206101, Training Loss: 34973, Validation Loss: 59833, 158886.6366242613\n",
      "Epoch 206201, Training Loss: 34079, Validation Loss: 54125, 166293.154217326\n",
      "Epoch 206301, Training Loss: 34543, Validation Loss: 53330, 206711.61072065507\n",
      "Epoch 206401, Training Loss: 33106, Validation Loss: 57619, 147371.39154202337\n",
      "Epoch 206501, Training Loss: 31389, Validation Loss: 58689, 153685.21429473648\n",
      "Epoch 206601, Training Loss: 36205, Validation Loss: 57063, 159325.51005697134\n",
      "Epoch 206701, Training Loss: 34639, Validation Loss: 58058, 155783.98785842778\n",
      "Epoch 206801, Training Loss: 36486, Validation Loss: 57025, 150291.97364898826\n",
      "Epoch 206901, Training Loss: 31835, Validation Loss: 56349, 121660.77508908817\n",
      "Epoch 207001, Training Loss: 32754, Validation Loss: 52695, 176878.2523313243\n",
      "Epoch 207101, Training Loss: 35151, Validation Loss: 53327, 133393.08399641808\n",
      "Epoch 207201, Training Loss: 35126, Validation Loss: 56202, 159772.1914889533\n",
      "Epoch 207301, Training Loss: 34491, Validation Loss: 58491, 139057.83170939388\n",
      "Epoch 207401, Training Loss: 36249, Validation Loss: 53283, 171180.41208062365\n",
      "Epoch 207501, Training Loss: 31609, Validation Loss: 54900, 139716.25283154493\n",
      "Epoch 207601, Training Loss: 34424, Validation Loss: 54026, 106506.21975441607\n",
      "Epoch 207701, Training Loss: 34913, Validation Loss: 53633, 155931.02113972334\n",
      "Epoch 207801, Training Loss: 35706, Validation Loss: 57113, 177498.37203618852\n",
      "Epoch 207901, Training Loss: 34171, Validation Loss: 56918, 116672.98881594841\n",
      "Epoch 208001, Training Loss: 34915, Validation Loss: 56330, 116737.3206403415\n",
      "Epoch 208101, Training Loss: 32669, Validation Loss: 56433, 150005.71956528258\n",
      "Epoch 208201, Training Loss: 35646, Validation Loss: 56684, 196224.88950322338\n",
      "Epoch 208301, Training Loss: 36342, Validation Loss: 57073, 154581.4233177869\n",
      "Epoch 208401, Training Loss: 34654, Validation Loss: 57597, 148326.89344579433\n",
      "Epoch 208501, Training Loss: 34766, Validation Loss: 56073, 151334.66375457324\n",
      "Epoch 208601, Training Loss: 34612, Validation Loss: 53952, 121690.7945374364\n",
      "Epoch 208701, Training Loss: 34229, Validation Loss: 55923, 185148.30984271827\n",
      "Epoch 208801, Training Loss: 33485, Validation Loss: 55348, 159678.94973534593\n",
      "Epoch 208901, Training Loss: 35651, Validation Loss: 56699, 182626.72210726445\n",
      "Epoch 209001, Training Loss: 32550, Validation Loss: 54747, 186582.24284118033\n",
      "Epoch 209101, Training Loss: 36834, Validation Loss: 56274, 208753.7575632087\n",
      "Epoch 209201, Training Loss: 33917, Validation Loss: 57711, 150911.6508797226\n",
      "Epoch 209301, Training Loss: 37059, Validation Loss: 54051, 174192.45563139857\n",
      "Epoch 209401, Training Loss: 34413, Validation Loss: 53812, 197748.61370279567\n",
      "Epoch 209501, Training Loss: 35213, Validation Loss: 53851, 173067.6321084116\n",
      "Epoch 209601, Training Loss: 35934, Validation Loss: 54934, 199940.42595518133\n",
      "Epoch 209701, Training Loss: 36019, Validation Loss: 55966, 162853.4568573138\n",
      "Epoch 209801, Training Loss: 34007, Validation Loss: 54912, 146778.63830181464\n",
      "Epoch 209901, Training Loss: 35662, Validation Loss: 55354, 125803.17214243137\n",
      "Epoch 210001, Training Loss: 31877, Validation Loss: 56893, 118577.33372606112\n",
      "Epoch 210101, Training Loss: 35282, Validation Loss: 52932, 190122.29510548245\n",
      "Epoch 210201, Training Loss: 33848, Validation Loss: 54631, 159250.2273299991\n",
      "Epoch 210301, Training Loss: 34273, Validation Loss: 56734, 177731.35956944874\n",
      "Epoch 210401, Training Loss: 34070, Validation Loss: 54278, 227005.21628943537\n",
      "Epoch 210501, Training Loss: 34155, Validation Loss: 57490, 122672.0148523234\n",
      "Epoch 210601, Training Loss: 36362, Validation Loss: 54684, 130908.8803621806\n",
      "Epoch 210701, Training Loss: 36838, Validation Loss: 52024, 176381.93355128434\n",
      "Epoch 210801, Training Loss: 35583, Validation Loss: 54210, 135811.36128846518\n",
      "Epoch 210901, Training Loss: 35453, Validation Loss: 54999, 153179.0088137289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211001, Training Loss: 37221, Validation Loss: 56029, 169129.8636263977\n",
      "Epoch 211101, Training Loss: 35760, Validation Loss: 55246, 132015.6593676748\n",
      "Epoch 211201, Training Loss: 36522, Validation Loss: 53900, 152487.22464536165\n",
      "Epoch 211301, Training Loss: 36502, Validation Loss: 58059, 208369.12135635418\n",
      "Epoch 211401, Training Loss: 36685, Validation Loss: 56199, 204834.71934289878\n",
      "Epoch 211501, Training Loss: 36180, Validation Loss: 56479, 172619.47068962283\n",
      "Epoch 211601, Training Loss: 39337, Validation Loss: 59800, 204715.97684352053\n",
      "Epoch 211701, Training Loss: 35200, Validation Loss: 57596, 159094.4755310939\n",
      "Epoch 211801, Training Loss: 32633, Validation Loss: 55416, 129629.76950053248\n",
      "Epoch 211901, Training Loss: 32750, Validation Loss: 55877, 170710.11675496903\n",
      "Epoch 212001, Training Loss: 34031, Validation Loss: 54652, 144396.2355942193\n",
      "Epoch 212101, Training Loss: 37000, Validation Loss: 55042, 112776.12692980653\n",
      "Epoch 212201, Training Loss: 36869, Validation Loss: 54002, 152918.6916369296\n",
      "Epoch 212301, Training Loss: 34553, Validation Loss: 55658, 148294.06385983335\n",
      "Epoch 212401, Training Loss: 33834, Validation Loss: 55484, 179658.10082181962\n",
      "Epoch 212501, Training Loss: 33374, Validation Loss: 53624, 109868.23234485784\n",
      "Epoch 212601, Training Loss: 33801, Validation Loss: 53410, 132812.74438942134\n",
      "Epoch 212701, Training Loss: 39827, Validation Loss: 54325, 221457.56808930906\n",
      "Epoch 212801, Training Loss: 33812, Validation Loss: 56206, 148948.45366743274\n",
      "Epoch 212901, Training Loss: 34426, Validation Loss: 51133, 198332.10281561574\n",
      "Epoch 213001, Training Loss: 33784, Validation Loss: 57503, 229329.1544212352\n",
      "Epoch 213101, Training Loss: 35895, Validation Loss: 59679, 184348.45912201784\n",
      "Epoch 213201, Training Loss: 36441, Validation Loss: 55856, 165278.6434057413\n",
      "Epoch 213301, Training Loss: 34094, Validation Loss: 56822, 180002.5295057801\n",
      "Epoch 213401, Training Loss: 32540, Validation Loss: 56845, 143625.15250840408\n",
      "Epoch 213501, Training Loss: 34565, Validation Loss: 58087, 190907.059580007\n",
      "Epoch 213601, Training Loss: 36368, Validation Loss: 57358, 132412.24920613584\n",
      "Epoch 213701, Training Loss: 37355, Validation Loss: 58580, 170823.0287707666\n",
      "Epoch 213801, Training Loss: 32871, Validation Loss: 56454, 138290.81634896298\n",
      "Epoch 213901, Training Loss: 35088, Validation Loss: 55337, 145698.07974455282\n",
      "Epoch 214001, Training Loss: 36202, Validation Loss: 52087, 99164.27681297739\n",
      "Epoch 214101, Training Loss: 33626, Validation Loss: 56078, 130864.88585947939\n",
      "Epoch 214201, Training Loss: 34716, Validation Loss: 56457, 167673.36287363488\n",
      "Epoch 214301, Training Loss: 36077, Validation Loss: 53640, 154181.64799250965\n",
      "Epoch 214401, Training Loss: 33764, Validation Loss: 54823, 152706.0752166239\n",
      "Epoch 214501, Training Loss: 34986, Validation Loss: 52639, 134065.97304931356\n",
      "Epoch 214601, Training Loss: 35936, Validation Loss: 55257, 168163.4495700061\n",
      "Epoch 214701, Training Loss: 34825, Validation Loss: 55942, 144026.42919213607\n",
      "Epoch 214801, Training Loss: 35070, Validation Loss: 58291, 149337.71836288893\n",
      "Epoch 214901, Training Loss: 33892, Validation Loss: 56031, 153425.83127416667\n",
      "Epoch 215001, Training Loss: 38144, Validation Loss: 53796, 172367.58082365178\n",
      "Epoch 215101, Training Loss: 38270, Validation Loss: 57511, 158677.83547896807\n",
      "Epoch 215201, Training Loss: 35616, Validation Loss: 60083, 127464.81846540805\n",
      "Epoch 215301, Training Loss: 34255, Validation Loss: 56040, 126464.1517271562\n",
      "Epoch 215401, Training Loss: 34739, Validation Loss: 55124, 153582.13091282788\n",
      "Epoch 215501, Training Loss: 34971, Validation Loss: 53556, 127527.38557822538\n",
      "Epoch 215601, Training Loss: 33545, Validation Loss: 53452, 154488.51166687883\n",
      "Epoch 215701, Training Loss: 33608, Validation Loss: 52004, 150519.08549589428\n",
      "Epoch 215801, Training Loss: 36356, Validation Loss: 56509, 160590.9871922504\n",
      "Epoch 215901, Training Loss: 35466, Validation Loss: 55464, 183199.63819942056\n",
      "Epoch 216001, Training Loss: 34431, Validation Loss: 54446, 143908.4941357378\n",
      "Epoch 216101, Training Loss: 33170, Validation Loss: 57226, 147480.86582145924\n",
      "Epoch 216201, Training Loss: 36469, Validation Loss: 55300, 148459.33384078275\n",
      "Epoch 216301, Training Loss: 37176, Validation Loss: 55109, 176023.28568101508\n",
      "Epoch 216401, Training Loss: 38587, Validation Loss: 57020, 235802.7245596985\n",
      "Epoch 216501, Training Loss: 33952, Validation Loss: 56529, 129340.04512874557\n",
      "Epoch 216601, Training Loss: 34887, Validation Loss: 55076, 157018.18075457343\n",
      "Epoch 216701, Training Loss: 33154, Validation Loss: 54558, 163349.06539178835\n",
      "Epoch 216801, Training Loss: 35677, Validation Loss: 54213, 144158.5704926743\n",
      "Epoch 216901, Training Loss: 37066, Validation Loss: 56563, 179951.58299040425\n",
      "Epoch 217001, Training Loss: 35197, Validation Loss: 57663, 165833.19296212308\n",
      "Epoch 217101, Training Loss: 35208, Validation Loss: 56972, 227047.17650860533\n",
      "Epoch 217201, Training Loss: 36223, Validation Loss: 54645, 168551.88332926694\n",
      "Epoch 217301, Training Loss: 35678, Validation Loss: 54967, 134085.7073654354\n",
      "Epoch 217401, Training Loss: 34772, Validation Loss: 55737, 131356.74306265442\n",
      "Epoch 217501, Training Loss: 33988, Validation Loss: 55890, 178656.3130520773\n",
      "Epoch 217601, Training Loss: 36998, Validation Loss: 57770, 145527.72704724982\n",
      "Epoch 217701, Training Loss: 34356, Validation Loss: 53530, 186223.70757789095\n",
      "Epoch 217801, Training Loss: 34357, Validation Loss: 57288, 187052.5726182435\n",
      "Epoch 217901, Training Loss: 34021, Validation Loss: 59293, 159984.54107693947\n",
      "Epoch 218001, Training Loss: 37908, Validation Loss: 56819, 144744.63925599828\n",
      "Epoch 218101, Training Loss: 35003, Validation Loss: 56292, 170338.89052655196\n",
      "Epoch 218201, Training Loss: 36182, Validation Loss: 56959, 152405.39637157856\n",
      "Epoch 218301, Training Loss: 33056, Validation Loss: 57589, 154347.5236110705\n",
      "Epoch 218401, Training Loss: 34134, Validation Loss: 55679, 110448.39715376997\n",
      "Epoch 218501, Training Loss: 33035, Validation Loss: 54668, 125611.21364559636\n",
      "Epoch 218601, Training Loss: 37577, Validation Loss: 56307, 185958.50954850102\n",
      "Epoch 218701, Training Loss: 35593, Validation Loss: 56941, 145602.00736163952\n",
      "Epoch 218801, Training Loss: 33194, Validation Loss: 56770, 130066.01008587942\n",
      "Epoch 218901, Training Loss: 32859, Validation Loss: 57414, 163732.5927844363\n",
      "Epoch 219001, Training Loss: 34160, Validation Loss: 55217, 151069.40908891146\n",
      "Epoch 219101, Training Loss: 33472, Validation Loss: 56137, 110500.7411607891\n",
      "Epoch 219201, Training Loss: 36257, Validation Loss: 54865, 166945.87943799558\n",
      "Epoch 219301, Training Loss: 34761, Validation Loss: 53516, 165237.5917825942\n",
      "Epoch 219401, Training Loss: 33548, Validation Loss: 54971, 132223.7632722743\n",
      "Epoch 219501, Training Loss: 33384, Validation Loss: 53947, 171489.9205476645\n",
      "Epoch 219601, Training Loss: 33102, Validation Loss: 55175, 194659.9222365252\n",
      "Epoch 219701, Training Loss: 34968, Validation Loss: 56851, 192697.9574960776\n",
      "Epoch 219801, Training Loss: 32175, Validation Loss: 55710, 147920.1927627228\n",
      "Epoch 219901, Training Loss: 36567, Validation Loss: 53074, 138112.35607893232\n",
      "Epoch 220001, Training Loss: 35344, Validation Loss: 56542, 164420.21466948424\n",
      "Epoch 220101, Training Loss: 37274, Validation Loss: 55712, 175897.63508755862\n",
      "Epoch 220201, Training Loss: 37346, Validation Loss: 58806, 167116.60924469455\n",
      "Epoch 220301, Training Loss: 35290, Validation Loss: 56041, 161332.655467489\n",
      "Epoch 220401, Training Loss: 34379, Validation Loss: 56346, 130856.24889991268\n",
      "Epoch 220501, Training Loss: 34196, Validation Loss: 56551, 146953.89067236325\n",
      "Epoch 220601, Training Loss: 35398, Validation Loss: 55262, 142324.1316064418\n",
      "Epoch 220701, Training Loss: 36652, Validation Loss: 55829, 178325.04006990776\n",
      "Epoch 220801, Training Loss: 33334, Validation Loss: 53808, 141962.9039938139\n",
      "Epoch 220901, Training Loss: 34464, Validation Loss: 55713, 204296.59735908188\n",
      "Epoch 221001, Training Loss: 34444, Validation Loss: 55401, 162847.33407843646\n",
      "Epoch 221101, Training Loss: 34609, Validation Loss: 55440, 130073.28246674302\n",
      "Epoch 221201, Training Loss: 36145, Validation Loss: 53630, 137756.0872518004\n",
      "Epoch 221301, Training Loss: 34067, Validation Loss: 56249, 125861.96920763655\n",
      "Epoch 221401, Training Loss: 36236, Validation Loss: 57890, 215647.8341150238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221501, Training Loss: 38293, Validation Loss: 55009, 199584.88964926233\n",
      "Epoch 221601, Training Loss: 34930, Validation Loss: 53616, 132572.09945254304\n",
      "Epoch 221701, Training Loss: 32341, Validation Loss: 55607, 139097.31443380503\n",
      "Epoch 221801, Training Loss: 34608, Validation Loss: 58916, 153172.4871474266\n",
      "Epoch 221901, Training Loss: 34565, Validation Loss: 56328, 130619.48549077903\n",
      "Epoch 222001, Training Loss: 33788, Validation Loss: 57945, 172993.01098832008\n",
      "Epoch 222101, Training Loss: 37188, Validation Loss: 56664, 188517.73300579193\n",
      "Epoch 222201, Training Loss: 36222, Validation Loss: 58368, 159415.2751738952\n",
      "Epoch 222301, Training Loss: 36580, Validation Loss: 54975, 168866.3608128678\n",
      "Epoch 222401, Training Loss: 37688, Validation Loss: 58986, 183153.34204761553\n",
      "Epoch 222501, Training Loss: 37460, Validation Loss: 53604, 120714.91408519998\n",
      "Epoch 222601, Training Loss: 36843, Validation Loss: 55530, 183643.72592472238\n",
      "Epoch 222701, Training Loss: 36356, Validation Loss: 54164, 144982.8902011548\n",
      "Epoch 222801, Training Loss: 37938, Validation Loss: 56104, 162952.35323727643\n",
      "Epoch 222901, Training Loss: 32368, Validation Loss: 55195, 160956.3530554272\n",
      "Epoch 223001, Training Loss: 35273, Validation Loss: 54158, 140680.3794250239\n",
      "Epoch 223101, Training Loss: 35208, Validation Loss: 58286, 171817.47372011803\n",
      "Epoch 223201, Training Loss: 32969, Validation Loss: 55093, 154129.02315946584\n",
      "Epoch 223301, Training Loss: 35595, Validation Loss: 54143, 142846.97232566573\n",
      "Epoch 223401, Training Loss: 34885, Validation Loss: 55607, 176798.93833503118\n",
      "Epoch 223501, Training Loss: 37718, Validation Loss: 56250, 181763.2439395703\n",
      "Epoch 223601, Training Loss: 34995, Validation Loss: 59000, 157472.59878116575\n",
      "Epoch 223701, Training Loss: 34030, Validation Loss: 58148, 128518.63987960662\n",
      "Epoch 223801, Training Loss: 37119, Validation Loss: 53468, 166978.4998449032\n",
      "Epoch 223901, Training Loss: 34677, Validation Loss: 53676, 121443.5633798776\n",
      "Epoch 224001, Training Loss: 34068, Validation Loss: 53981, 111974.12706401804\n",
      "Epoch 224101, Training Loss: 34040, Validation Loss: 55492, 141549.258273055\n",
      "Epoch 224201, Training Loss: 37783, Validation Loss: 53388, 132444.54753634363\n",
      "Epoch 224301, Training Loss: 34372, Validation Loss: 52731, 159478.45502670258\n",
      "Epoch 224401, Training Loss: 33672, Validation Loss: 56925, 139697.99179564914\n",
      "Epoch 224501, Training Loss: 34278, Validation Loss: 53412, 168599.71746451128\n",
      "Epoch 224601, Training Loss: 33994, Validation Loss: 56463, 162896.90058911737\n",
      "Epoch 224701, Training Loss: 35254, Validation Loss: 53501, 147245.18180986124\n",
      "Epoch 224801, Training Loss: 33185, Validation Loss: 54731, 133566.94544981324\n",
      "Epoch 224901, Training Loss: 33542, Validation Loss: 54998, 132265.88252380176\n",
      "Epoch 225001, Training Loss: 36308, Validation Loss: 55762, 178218.07421282958\n",
      "Epoch 225101, Training Loss: 34961, Validation Loss: 56846, 148150.21630971742\n",
      "Epoch 225201, Training Loss: 33027, Validation Loss: 53533, 115918.82173765612\n",
      "Epoch 225301, Training Loss: 36434, Validation Loss: 55223, 158799.27147764727\n",
      "Epoch 225401, Training Loss: 35141, Validation Loss: 53710, 146622.41624997606\n",
      "Epoch 225501, Training Loss: 35186, Validation Loss: 55477, 168786.63478637996\n",
      "Epoch 225601, Training Loss: 35204, Validation Loss: 54806, 170321.3755853334\n",
      "Epoch 225701, Training Loss: 34902, Validation Loss: 56561, 167608.17235897807\n",
      "Epoch 225801, Training Loss: 33078, Validation Loss: 57459, 126637.65073235457\n",
      "Epoch 225901, Training Loss: 34469, Validation Loss: 53579, 224472.0111804499\n",
      "Epoch 226001, Training Loss: 35793, Validation Loss: 53778, 172488.1577690111\n",
      "Epoch 226101, Training Loss: 34057, Validation Loss: 54245, 157692.63042836872\n",
      "Epoch 226201, Training Loss: 36033, Validation Loss: 57174, 150710.1183186214\n",
      "Epoch 226301, Training Loss: 34353, Validation Loss: 56072, 155337.39793716432\n",
      "Epoch 226401, Training Loss: 34965, Validation Loss: 56681, 164799.23022941596\n",
      "Epoch 226501, Training Loss: 34955, Validation Loss: 56893, 173576.18719976116\n",
      "Epoch 226601, Training Loss: 34902, Validation Loss: 56830, 138736.38098540506\n",
      "Epoch 226701, Training Loss: 35886, Validation Loss: 55332, 163494.07195599386\n",
      "Epoch 226801, Training Loss: 35370, Validation Loss: 56232, 161092.25447964048\n",
      "Epoch 226901, Training Loss: 35737, Validation Loss: 54576, 169015.48837634502\n",
      "Epoch 227001, Training Loss: 34644, Validation Loss: 57159, 152943.95291466865\n",
      "Epoch 227101, Training Loss: 33618, Validation Loss: 54816, 139059.64044915713\n",
      "Epoch 227201, Training Loss: 32547, Validation Loss: 56047, 127398.60838105412\n",
      "Epoch 227301, Training Loss: 35499, Validation Loss: 56105, 136045.52663520153\n",
      "Epoch 227401, Training Loss: 35601, Validation Loss: 55416, 131639.01247949153\n",
      "Epoch 227501, Training Loss: 34698, Validation Loss: 58499, 124681.5702719972\n",
      "Epoch 227601, Training Loss: 33896, Validation Loss: 58311, 202374.00488899834\n",
      "Epoch 227701, Training Loss: 34925, Validation Loss: 52199, 159219.8920044473\n",
      "Epoch 227801, Training Loss: 35706, Validation Loss: 53318, 199891.62846212814\n",
      "Epoch 227901, Training Loss: 33022, Validation Loss: 58403, 142028.01434064194\n",
      "Epoch 228001, Training Loss: 35049, Validation Loss: 56039, 189419.00156788892\n",
      "Epoch 228101, Training Loss: 33665, Validation Loss: 52053, 158027.87338431925\n",
      "Epoch 228201, Training Loss: 35054, Validation Loss: 56150, 130757.48014585266\n",
      "Epoch 228301, Training Loss: 37081, Validation Loss: 57547, 121408.54886302621\n",
      "Epoch 228401, Training Loss: 38249, Validation Loss: 56226, 188626.31837518222\n",
      "Epoch 228501, Training Loss: 37753, Validation Loss: 55311, 200423.44101913212\n",
      "Epoch 228601, Training Loss: 34343, Validation Loss: 54930, 128674.98517934822\n",
      "Epoch 228701, Training Loss: 33439, Validation Loss: 55465, 146342.98424818742\n",
      "Epoch 228801, Training Loss: 34853, Validation Loss: 58252, 168699.58881046585\n",
      "Epoch 228901, Training Loss: 34503, Validation Loss: 53454, 150203.0189570979\n",
      "Epoch 229001, Training Loss: 34039, Validation Loss: 57282, 160190.66920944216\n",
      "Epoch 229101, Training Loss: 37042, Validation Loss: 55056, 135427.50951105542\n",
      "Epoch 229201, Training Loss: 34815, Validation Loss: 53755, 131769.31286174603\n",
      "Epoch 229301, Training Loss: 33769, Validation Loss: 56877, 153949.14026809885\n",
      "Epoch 229401, Training Loss: 33011, Validation Loss: 57282, 122192.94842814798\n",
      "Epoch 229501, Training Loss: 34838, Validation Loss: 55592, 208238.48308477877\n",
      "Epoch 229601, Training Loss: 34939, Validation Loss: 57546, 183120.45624532294\n",
      "Epoch 229701, Training Loss: 33539, Validation Loss: 52850, 148478.65976712696\n",
      "Epoch 229801, Training Loss: 34254, Validation Loss: 55761, 160420.73957899533\n",
      "Epoch 229901, Training Loss: 35881, Validation Loss: 55346, 145801.9673041346\n",
      "Epoch 230001, Training Loss: 32450, Validation Loss: 57048, 139582.55113340134\n",
      "Epoch 230101, Training Loss: 34876, Validation Loss: 54081, 128816.31182085513\n",
      "Epoch 230201, Training Loss: 35134, Validation Loss: 58510, 205871.0023047667\n",
      "Epoch 230301, Training Loss: 35995, Validation Loss: 56208, 159452.55867425058\n",
      "Epoch 230401, Training Loss: 34657, Validation Loss: 55517, 165732.86102764364\n",
      "Epoch 230501, Training Loss: 36313, Validation Loss: 54690, 209139.4198056215\n",
      "Epoch 230601, Training Loss: 35654, Validation Loss: 51870, 174145.77908075156\n",
      "Epoch 230701, Training Loss: 34492, Validation Loss: 55846, 147965.631803122\n",
      "Epoch 230801, Training Loss: 36390, Validation Loss: 55578, 162296.96596597807\n",
      "Epoch 230901, Training Loss: 36187, Validation Loss: 54975, 153271.01137127497\n",
      "Epoch 231001, Training Loss: 35422, Validation Loss: 55327, 149397.84091723515\n",
      "Epoch 231101, Training Loss: 34916, Validation Loss: 53011, 142994.24940344394\n",
      "Epoch 231201, Training Loss: 35477, Validation Loss: 52588, 160021.87766557687\n",
      "Epoch 231301, Training Loss: 36381, Validation Loss: 54184, 188732.9826952479\n",
      "Epoch 231401, Training Loss: 33437, Validation Loss: 53450, 174899.22616689932\n",
      "Epoch 231501, Training Loss: 33413, Validation Loss: 57746, 133541.65935100816\n",
      "Epoch 231601, Training Loss: 34001, Validation Loss: 55241, 182675.56340341773\n",
      "Epoch 231701, Training Loss: 33799, Validation Loss: 52503, 142591.35919331698\n",
      "Epoch 231801, Training Loss: 32700, Validation Loss: 53003, 145984.03050143344\n",
      "Epoch 231901, Training Loss: 35675, Validation Loss: 52769, 249441.57644326487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232001, Training Loss: 35699, Validation Loss: 54905, 149671.65957747053\n",
      "Epoch 232101, Training Loss: 34228, Validation Loss: 55684, 146443.52051103205\n",
      "Epoch 232201, Training Loss: 35988, Validation Loss: 54825, 169092.31931691302\n",
      "Epoch 232301, Training Loss: 35645, Validation Loss: 55281, 146406.08312573147\n",
      "Epoch 232401, Training Loss: 33673, Validation Loss: 55731, 167249.12094256532\n",
      "Epoch 232501, Training Loss: 31978, Validation Loss: 56856, 154759.77392575666\n",
      "Epoch 232601, Training Loss: 32658, Validation Loss: 55068, 144997.80471321932\n",
      "Epoch 232701, Training Loss: 37480, Validation Loss: 55984, 225841.96032448622\n",
      "Epoch 232801, Training Loss: 33923, Validation Loss: 55625, 128453.75951197512\n",
      "Epoch 232901, Training Loss: 34983, Validation Loss: 55481, 162600.8785969861\n",
      "Epoch 233001, Training Loss: 33637, Validation Loss: 56159, 113757.45519142128\n",
      "Epoch 233101, Training Loss: 32626, Validation Loss: 58899, 188855.85508181495\n",
      "Epoch 233201, Training Loss: 32640, Validation Loss: 56091, 109390.15616023877\n",
      "Epoch 233301, Training Loss: 37588, Validation Loss: 55816, 145031.9642352527\n",
      "Epoch 233401, Training Loss: 33454, Validation Loss: 56481, 118334.87677569345\n",
      "Epoch 233501, Training Loss: 34835, Validation Loss: 56932, 161363.8299604708\n",
      "Epoch 233601, Training Loss: 33122, Validation Loss: 55497, 149655.50231954988\n",
      "Epoch 233701, Training Loss: 33361, Validation Loss: 57747, 166068.52767079207\n",
      "Epoch 233801, Training Loss: 33978, Validation Loss: 54579, 178857.38535673628\n",
      "Epoch 233901, Training Loss: 36470, Validation Loss: 56149, 207892.37987902167\n",
      "Epoch 234001, Training Loss: 34462, Validation Loss: 55675, 155876.88445698822\n",
      "Epoch 234101, Training Loss: 34081, Validation Loss: 58222, 129191.56940441801\n",
      "Epoch 234201, Training Loss: 31349, Validation Loss: 58258, 157078.48046870957\n",
      "Epoch 234301, Training Loss: 35562, Validation Loss: 54369, 176169.10331785306\n",
      "Epoch 234401, Training Loss: 32246, Validation Loss: 53378, 154566.15779992632\n",
      "Epoch 234501, Training Loss: 32060, Validation Loss: 55198, 150395.05411122364\n",
      "Epoch 234601, Training Loss: 34768, Validation Loss: 54302, 127937.34244388186\n",
      "Epoch 234701, Training Loss: 34065, Validation Loss: 58436, 163832.05314345777\n",
      "Epoch 234801, Training Loss: 35518, Validation Loss: 57067, 214557.8866115229\n",
      "Epoch 234901, Training Loss: 33868, Validation Loss: 55211, 111074.61865864015\n",
      "Epoch 235001, Training Loss: 35025, Validation Loss: 57090, 141409.00334933228\n",
      "Epoch 235101, Training Loss: 34144, Validation Loss: 55984, 123529.61973565386\n",
      "Epoch 235201, Training Loss: 32246, Validation Loss: 53965, 145787.93158053458\n",
      "Epoch 235301, Training Loss: 35235, Validation Loss: 59351, 181284.5794193352\n",
      "Epoch 235401, Training Loss: 33575, Validation Loss: 54616, 191519.76117616045\n",
      "Epoch 235501, Training Loss: 32909, Validation Loss: 56135, 173410.0344631682\n",
      "Epoch 235601, Training Loss: 31925, Validation Loss: 52607, 161034.9416140604\n",
      "Epoch 235701, Training Loss: 37322, Validation Loss: 55008, 138154.11963823982\n",
      "Epoch 235801, Training Loss: 36238, Validation Loss: 55324, 156977.44325690868\n",
      "Epoch 235901, Training Loss: 35490, Validation Loss: 57769, 158537.08948178255\n",
      "Epoch 236001, Training Loss: 35143, Validation Loss: 56708, 132815.90792081974\n",
      "Epoch 236101, Training Loss: 35329, Validation Loss: 53122, 183350.38613714487\n",
      "Epoch 236201, Training Loss: 35516, Validation Loss: 59503, 139759.7184044208\n",
      "Epoch 236301, Training Loss: 36281, Validation Loss: 56370, 211218.86936796363\n",
      "Epoch 236401, Training Loss: 33603, Validation Loss: 57130, 184491.74083532128\n",
      "Epoch 236501, Training Loss: 32637, Validation Loss: 57365, 143113.6290331854\n",
      "Epoch 236601, Training Loss: 34553, Validation Loss: 57349, 110487.60923967096\n",
      "Epoch 236701, Training Loss: 35464, Validation Loss: 56772, 172746.9139749162\n",
      "Epoch 236801, Training Loss: 33359, Validation Loss: 55188, 138193.50880734777\n",
      "Epoch 236901, Training Loss: 34787, Validation Loss: 55193, 146187.32595622996\n",
      "Epoch 237001, Training Loss: 35142, Validation Loss: 59949, 201488.47061448847\n",
      "Epoch 237101, Training Loss: 34820, Validation Loss: 57482, 128960.94152870332\n",
      "Epoch 237201, Training Loss: 34810, Validation Loss: 56846, 197479.62988593374\n",
      "Epoch 237301, Training Loss: 34829, Validation Loss: 53528, 178416.33911805763\n",
      "Epoch 237401, Training Loss: 35023, Validation Loss: 52760, 151471.82007296904\n",
      "Epoch 237501, Training Loss: 36209, Validation Loss: 58562, 300970.2638581889\n",
      "Epoch 237601, Training Loss: 33310, Validation Loss: 55488, 153917.97996301664\n",
      "Epoch 237701, Training Loss: 31644, Validation Loss: 54801, 128528.46060228477\n",
      "Epoch 237801, Training Loss: 32874, Validation Loss: 53433, 170963.92777159144\n",
      "Epoch 237901, Training Loss: 34180, Validation Loss: 57384, 149987.6007210306\n",
      "Epoch 238001, Training Loss: 36498, Validation Loss: 58446, 162168.08522174755\n",
      "Epoch 238101, Training Loss: 33532, Validation Loss: 56349, 132351.41153003962\n",
      "Epoch 238201, Training Loss: 33671, Validation Loss: 53779, 156601.21809929705\n",
      "Epoch 238301, Training Loss: 34724, Validation Loss: 56030, 188102.91315648388\n",
      "Epoch 238401, Training Loss: 33690, Validation Loss: 54520, 156866.90981778628\n",
      "Epoch 238501, Training Loss: 34987, Validation Loss: 57138, 157196.18350203693\n",
      "Epoch 238601, Training Loss: 34740, Validation Loss: 58922, 165356.02059551582\n",
      "Epoch 238701, Training Loss: 35066, Validation Loss: 55420, 155572.69875155805\n",
      "Epoch 238801, Training Loss: 33581, Validation Loss: 55411, 122349.02966102741\n",
      "Epoch 238901, Training Loss: 35970, Validation Loss: 54666, 151746.43914356627\n",
      "Epoch 239001, Training Loss: 32861, Validation Loss: 54889, 141165.14379527353\n",
      "Epoch 239101, Training Loss: 36091, Validation Loss: 52771, 181766.61018024696\n",
      "Epoch 239201, Training Loss: 35154, Validation Loss: 57873, 179381.5113129895\n",
      "Epoch 239301, Training Loss: 34013, Validation Loss: 58013, 155980.31555028082\n",
      "Epoch 239401, Training Loss: 33629, Validation Loss: 57082, 170306.98106653994\n",
      "Epoch 239501, Training Loss: 35804, Validation Loss: 57244, 208009.1238442883\n",
      "Epoch 239601, Training Loss: 34656, Validation Loss: 57653, 135112.5195281209\n",
      "Epoch 239701, Training Loss: 36689, Validation Loss: 53770, 147699.9383841744\n",
      "Epoch 239801, Training Loss: 33763, Validation Loss: 54172, 139327.85582685075\n",
      "Epoch 239901, Training Loss: 34894, Validation Loss: 54433, 188585.8642048439\n",
      "Epoch 240001, Training Loss: 33823, Validation Loss: 54310, 130292.76074194163\n",
      "Epoch 240101, Training Loss: 34752, Validation Loss: 54511, 147369.94006608898\n",
      "Epoch 240201, Training Loss: 32753, Validation Loss: 53697, 149008.14471956788\n",
      "Epoch 240301, Training Loss: 34473, Validation Loss: 54120, 153208.0805967274\n",
      "Epoch 240401, Training Loss: 33514, Validation Loss: 59987, 183352.50390627782\n",
      "Epoch 240501, Training Loss: 35188, Validation Loss: 55919, 160770.30981837944\n",
      "Epoch 240601, Training Loss: 35170, Validation Loss: 54395, 149314.77592082034\n",
      "Epoch 240701, Training Loss: 36072, Validation Loss: 56691, 138070.67977679512\n",
      "Epoch 240801, Training Loss: 33350, Validation Loss: 54996, 172808.02024764408\n",
      "Epoch 240901, Training Loss: 34544, Validation Loss: 58677, 186853.43080110624\n",
      "Epoch 241001, Training Loss: 37287, Validation Loss: 55016, 175007.33148433943\n",
      "Epoch 241101, Training Loss: 32500, Validation Loss: 55730, 137945.0590170883\n",
      "Epoch 241201, Training Loss: 35306, Validation Loss: 56429, 93605.62901772502\n",
      "Epoch 241301, Training Loss: 35428, Validation Loss: 53952, 128092.07912693352\n",
      "Epoch 241401, Training Loss: 33723, Validation Loss: 57479, 133270.59461603707\n",
      "Epoch 241501, Training Loss: 36192, Validation Loss: 60102, 310079.0475040697\n",
      "Epoch 241601, Training Loss: 31767, Validation Loss: 55815, 125511.85027817059\n",
      "Epoch 241701, Training Loss: 34294, Validation Loss: 55187, 223814.8429053334\n",
      "Epoch 241801, Training Loss: 38968, Validation Loss: 53831, 148823.5123373173\n",
      "Epoch 241901, Training Loss: 35463, Validation Loss: 54416, 164486.00586661653\n",
      "Epoch 242001, Training Loss: 34212, Validation Loss: 56908, 155978.6874326199\n",
      "Epoch 242101, Training Loss: 34448, Validation Loss: 54861, 159187.6757681648\n",
      "Epoch 242201, Training Loss: 34533, Validation Loss: 56612, 151694.58418945185\n",
      "Epoch 242301, Training Loss: 35589, Validation Loss: 53405, 168452.79732407408\n",
      "Epoch 242401, Training Loss: 31781, Validation Loss: 56141, 172992.09212187133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242501, Training Loss: 35218, Validation Loss: 55368, 168719.32230121037\n",
      "Epoch 242601, Training Loss: 34188, Validation Loss: 56184, 183772.4130398465\n",
      "Epoch 242701, Training Loss: 34825, Validation Loss: 58200, 173387.57583365173\n",
      "Epoch 242801, Training Loss: 34107, Validation Loss: 55965, 140211.88154726804\n",
      "Epoch 242901, Training Loss: 34907, Validation Loss: 52968, 174832.79390451722\n",
      "Epoch 243001, Training Loss: 31784, Validation Loss: 58519, 155869.77175650242\n",
      "Epoch 243101, Training Loss: 35197, Validation Loss: 54889, 177499.14700574125\n",
      "Epoch 243201, Training Loss: 35897, Validation Loss: 54061, 184991.58617727147\n",
      "Epoch 243301, Training Loss: 34871, Validation Loss: 52808, 119758.65213619564\n",
      "Epoch 243401, Training Loss: 31506, Validation Loss: 54210, 133321.80940617502\n",
      "Epoch 243501, Training Loss: 34112, Validation Loss: 57699, 133284.49387649566\n",
      "Epoch 243601, Training Loss: 34237, Validation Loss: 54776, 177306.51154593177\n",
      "Epoch 243701, Training Loss: 33373, Validation Loss: 54573, 118000.68692803232\n",
      "Epoch 243801, Training Loss: 34184, Validation Loss: 57193, 169023.37565236446\n",
      "Epoch 243901, Training Loss: 33423, Validation Loss: 56847, 149042.6642006123\n",
      "Epoch 244001, Training Loss: 33035, Validation Loss: 51096, 134879.62631376705\n",
      "Epoch 244101, Training Loss: 31997, Validation Loss: 57047, 151735.17139409538\n",
      "Epoch 244201, Training Loss: 32629, Validation Loss: 55552, 174073.039888197\n",
      "Epoch 244301, Training Loss: 37298, Validation Loss: 54691, 136706.05231622164\n",
      "Epoch 244401, Training Loss: 35039, Validation Loss: 54040, 154220.84781561617\n",
      "Epoch 244501, Training Loss: 34676, Validation Loss: 56403, 174407.20600168002\n",
      "Epoch 244601, Training Loss: 34145, Validation Loss: 56653, 137705.3240834719\n",
      "Epoch 244701, Training Loss: 34391, Validation Loss: 56449, 142435.26319440771\n",
      "Epoch 244801, Training Loss: 34931, Validation Loss: 55294, 123102.28653255466\n",
      "Epoch 244901, Training Loss: 34593, Validation Loss: 54252, 154755.26077202146\n",
      "Epoch 245001, Training Loss: 34749, Validation Loss: 54374, 142513.4581465375\n",
      "Epoch 245101, Training Loss: 34471, Validation Loss: 55347, 132929.8510315855\n",
      "Epoch 245201, Training Loss: 34077, Validation Loss: 55724, 169709.37368665813\n",
      "Epoch 245301, Training Loss: 34066, Validation Loss: 56393, 133620.94507198952\n",
      "Epoch 245401, Training Loss: 35220, Validation Loss: 54748, 130881.89874675067\n",
      "Epoch 245501, Training Loss: 36563, Validation Loss: 54887, 165509.9695551972\n",
      "Epoch 245601, Training Loss: 33856, Validation Loss: 54752, 198899.63206580977\n",
      "Epoch 245701, Training Loss: 31808, Validation Loss: 60797, 138239.16268815682\n",
      "Epoch 245801, Training Loss: 33539, Validation Loss: 56396, 184067.94220114461\n",
      "Epoch 245901, Training Loss: 32884, Validation Loss: 55510, 163028.4056084508\n",
      "Epoch 246001, Training Loss: 30825, Validation Loss: 56364, 164016.56008129727\n",
      "Epoch 246101, Training Loss: 33003, Validation Loss: 57642, 164593.36165215916\n",
      "Epoch 246201, Training Loss: 34503, Validation Loss: 54059, 136362.45490760086\n",
      "Epoch 246301, Training Loss: 35939, Validation Loss: 58017, 214860.65638228072\n",
      "Epoch 246401, Training Loss: 33182, Validation Loss: 56626, 170012.66210046774\n",
      "Epoch 246501, Training Loss: 34715, Validation Loss: 57716, 125426.30711027452\n",
      "Epoch 246601, Training Loss: 34712, Validation Loss: 55316, 120571.2103715508\n",
      "Epoch 246701, Training Loss: 34223, Validation Loss: 57800, 144695.60566362456\n",
      "Epoch 246801, Training Loss: 33436, Validation Loss: 53408, 133587.73338113\n",
      "Epoch 246901, Training Loss: 35016, Validation Loss: 54436, 205116.15828917883\n",
      "Epoch 247001, Training Loss: 33678, Validation Loss: 57114, 143577.47911304285\n",
      "Epoch 247101, Training Loss: 34018, Validation Loss: 56391, 163099.2319672369\n",
      "Epoch 247201, Training Loss: 33674, Validation Loss: 55974, 120295.79300575449\n",
      "Epoch 247301, Training Loss: 32610, Validation Loss: 56162, 171866.5294041485\n",
      "Epoch 247401, Training Loss: 35218, Validation Loss: 55754, 147563.0857708193\n",
      "Epoch 247501, Training Loss: 32735, Validation Loss: 57991, 157125.79554401347\n",
      "Epoch 247601, Training Loss: 33995, Validation Loss: 54627, 152503.06900179392\n",
      "Epoch 247701, Training Loss: 34495, Validation Loss: 56957, 162428.40502436765\n",
      "Epoch 247801, Training Loss: 35553, Validation Loss: 55739, 151478.65384292207\n",
      "Epoch 247901, Training Loss: 32661, Validation Loss: 54598, 171613.03358338287\n",
      "Epoch 248001, Training Loss: 35621, Validation Loss: 57096, 194929.9313228324\n",
      "Epoch 248101, Training Loss: 34744, Validation Loss: 55791, 126575.37797443528\n",
      "Epoch 248201, Training Loss: 35363, Validation Loss: 53989, 187557.5352433706\n",
      "Epoch 248301, Training Loss: 38087, Validation Loss: 54888, 191824.1075321605\n",
      "Epoch 248401, Training Loss: 33806, Validation Loss: 56342, 154228.0383739005\n",
      "Epoch 248501, Training Loss: 35719, Validation Loss: 52753, 119726.78158736393\n",
      "Epoch 248601, Training Loss: 32998, Validation Loss: 51832, 191839.49161379645\n",
      "Epoch 248701, Training Loss: 34027, Validation Loss: 55027, 147680.93377070304\n",
      "Epoch 248801, Training Loss: 34893, Validation Loss: 58262, 205948.18018072925\n",
      "Epoch 248901, Training Loss: 33494, Validation Loss: 53229, 146436.41305842137\n",
      "Epoch 249001, Training Loss: 35002, Validation Loss: 56039, 146080.03395612296\n",
      "Epoch 249101, Training Loss: 34169, Validation Loss: 55993, 170910.25338767885\n",
      "Epoch 249201, Training Loss: 34521, Validation Loss: 54400, 142025.21470303903\n",
      "Epoch 249301, Training Loss: 34612, Validation Loss: 56358, 132122.28404379543\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m     grad_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_gradient_norm(model)\n\u001b[0;32m---> 32\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     36\u001b[0m grad_norms\u001b[38;5;241m.\u001b[39mappend(grad_norm \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:384\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    381\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    385\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=9e-3,\n",
    "    weight_decay=3e-4\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999999, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
