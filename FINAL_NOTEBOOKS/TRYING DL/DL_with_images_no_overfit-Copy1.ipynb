{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 64\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "#             nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 230736, Validation Loss: 225692, 19371.189133565545\n",
      "Epoch 101, Training Loss: 67212, Validation Loss: 65115, 70026.8113734025\n",
      "Epoch 201, Training Loss: 62527, Validation Loss: 57177, 207634.27056806735\n",
      "Epoch 301, Training Loss: 61059, Validation Loss: 58635, 270514.6441880083\n",
      "Epoch 401, Training Loss: 56367, Validation Loss: 54806, 183101.93715866373\n",
      "Epoch 501, Training Loss: 55409, Validation Loss: 54445, 202596.1800139094\n",
      "Epoch 601, Training Loss: 53169, Validation Loss: 53696, 163139.04122935576\n",
      "Epoch 701, Training Loss: 55917, Validation Loss: 59549, 227824.1912576616\n",
      "Epoch 801, Training Loss: 51337, Validation Loss: 52265, 175816.96591024828\n",
      "Epoch 901, Training Loss: 53174, Validation Loss: 50838, 184761.95120466087\n",
      "Epoch 1001, Training Loss: 50331, Validation Loss: 52518, 160121.75680060982\n",
      "Epoch 1101, Training Loss: 52326, Validation Loss: 51275, 210400.9281382421\n",
      "Epoch 1201, Training Loss: 50204, Validation Loss: 50022, 162976.73072395526\n",
      "Epoch 1301, Training Loss: 49354, Validation Loss: 49996, 141732.81524797628\n",
      "Epoch 1401, Training Loss: 52656, Validation Loss: 49164, 306559.1591016399\n",
      "Epoch 1501, Training Loss: 49031, Validation Loss: 48937, 96945.35700765457\n",
      "Epoch 1601, Training Loss: 50582, Validation Loss: 49059, 187322.37433367348\n",
      "Epoch 1701, Training Loss: 49891, Validation Loss: 48858, 215368.51518614468\n",
      "Epoch 1801, Training Loss: 48170, Validation Loss: 49018, 136812.31153439038\n",
      "Epoch 1901, Training Loss: 49362, Validation Loss: 48463, 345247.71549728827\n",
      "Epoch 2001, Training Loss: 47915, Validation Loss: 47874, 189021.8656022925\n",
      "Epoch 2101, Training Loss: 50295, Validation Loss: 52191, 204937.42151869292\n",
      "Epoch 2201, Training Loss: 47877, Validation Loss: 50352, 128777.11443977256\n",
      "Epoch 2301, Training Loss: 48235, Validation Loss: 50866, 163713.46562918904\n",
      "Epoch 2401, Training Loss: 47265, Validation Loss: 48514, 137642.09037515806\n",
      "Epoch 2501, Training Loss: 49381, Validation Loss: 50221, 239945.6138964632\n",
      "Epoch 2601, Training Loss: 47266, Validation Loss: 47764, 139939.84022333\n",
      "Epoch 2701, Training Loss: 47666, Validation Loss: 47175, 209035.9784366188\n",
      "Epoch 2801, Training Loss: 47488, Validation Loss: 46892, 275150.0706339915\n",
      "Epoch 2901, Training Loss: 46943, Validation Loss: 47204, 212542.88740286464\n",
      "Epoch 3001, Training Loss: 47084, Validation Loss: 53934, 189221.20499566162\n",
      "Epoch 3101, Training Loss: 48301, Validation Loss: 47424, 314699.9008659144\n",
      "Epoch 3201, Training Loss: 47087, Validation Loss: 51700, 263158.3228743988\n",
      "Epoch 3301, Training Loss: 46400, Validation Loss: 47094, 173517.80313016337\n",
      "Epoch 3401, Training Loss: 44781, Validation Loss: 49545, 202750.74190723902\n",
      "Epoch 3501, Training Loss: 45961, Validation Loss: 46684, 240886.42337480173\n",
      "Epoch 3601, Training Loss: 47968, Validation Loss: 46680, 246884.50792294828\n",
      "Epoch 3701, Training Loss: 45747, Validation Loss: 51667, 197914.8000897306\n",
      "Epoch 3801, Training Loss: 46332, Validation Loss: 48328, 249277.244006722\n",
      "Epoch 3901, Training Loss: 46657, Validation Loss: 47312, 131958.24412472133\n",
      "Epoch 4001, Training Loss: 45701, Validation Loss: 45882, 265885.08304092573\n",
      "Epoch 4101, Training Loss: 49265, Validation Loss: 51404, 288763.0163607724\n",
      "Epoch 4201, Training Loss: 44990, Validation Loss: 46469, 171331.2935575913\n",
      "Epoch 4301, Training Loss: 46951, Validation Loss: 49543, 229149.38569665706\n",
      "Epoch 4401, Training Loss: 47070, Validation Loss: 49079, 177811.08106147806\n",
      "Epoch 4501, Training Loss: 47694, Validation Loss: 48037, 222260.66665084026\n",
      "Epoch 4601, Training Loss: 46269, Validation Loss: 48234, 258400.54593441339\n",
      "Epoch 4701, Training Loss: 44509, Validation Loss: 46115, 172934.18809419073\n",
      "Epoch 4801, Training Loss: 46708, Validation Loss: 47400, 200205.8544515813\n",
      "Epoch 4901, Training Loss: 44634, Validation Loss: 47292, 207626.30370464324\n",
      "Epoch 5001, Training Loss: 44695, Validation Loss: 47179, 197285.1958231723\n",
      "Epoch 5101, Training Loss: 45562, Validation Loss: 47028, 196780.78549103235\n",
      "Epoch 5201, Training Loss: 44188, Validation Loss: 47697, 246221.84324100227\n",
      "Epoch 5301, Training Loss: 44260, Validation Loss: 47479, 141713.23099221138\n",
      "Epoch 5401, Training Loss: 45933, Validation Loss: 48004, 215658.48357384978\n",
      "Epoch 5501, Training Loss: 43473, Validation Loss: 46547, 151543.758050197\n",
      "Epoch 5601, Training Loss: 45386, Validation Loss: 46884, 209634.59026404694\n",
      "Epoch 5701, Training Loss: 45787, Validation Loss: 46421, 268421.80931992613\n",
      "Epoch 5801, Training Loss: 43806, Validation Loss: 46959, 144652.37831929582\n",
      "Epoch 5901, Training Loss: 45861, Validation Loss: 51183, 225850.4891549456\n",
      "Epoch 6001, Training Loss: 46346, Validation Loss: 47614, 170798.54693217864\n",
      "Epoch 6101, Training Loss: 43817, Validation Loss: 47154, 183392.80943448492\n",
      "Epoch 6201, Training Loss: 43408, Validation Loss: 50446, 167008.23939727625\n",
      "Epoch 6301, Training Loss: 46013, Validation Loss: 47487, 256261.11512478007\n",
      "Epoch 6401, Training Loss: 45321, Validation Loss: 49565, 292519.58192607586\n",
      "Epoch 6501, Training Loss: 43546, Validation Loss: 46843, 205051.31348459306\n",
      "Epoch 6601, Training Loss: 42849, Validation Loss: 49205, 169325.02491896052\n",
      "Epoch 6701, Training Loss: 45548, Validation Loss: 46785, 233002.21699259462\n",
      "Epoch 6801, Training Loss: 43139, Validation Loss: 46323, 222294.48804137055\n",
      "Epoch 6901, Training Loss: 43858, Validation Loss: 46404, 179304.1886040479\n",
      "Epoch 7001, Training Loss: 45569, Validation Loss: 46555, 291162.0309046399\n",
      "Epoch 7101, Training Loss: 44125, Validation Loss: 46977, 248887.19900877035\n",
      "Epoch 7201, Training Loss: 46420, Validation Loss: 46411, 261625.76557006134\n",
      "Epoch 7301, Training Loss: 43586, Validation Loss: 46566, 141368.6687657662\n",
      "Epoch 7401, Training Loss: 45119, Validation Loss: 47986, 252192.98465284618\n",
      "Epoch 7501, Training Loss: 44829, Validation Loss: 49363, 259325.8178027013\n",
      "Epoch 7601, Training Loss: 46305, Validation Loss: 55909, 316294.5231323564\n",
      "Epoch 7701, Training Loss: 44730, Validation Loss: 47712, 236767.29633709107\n",
      "Epoch 7801, Training Loss: 43930, Validation Loss: 46335, 232510.87631460195\n",
      "Epoch 7901, Training Loss: 42087, Validation Loss: 47225, 159544.59933898383\n",
      "Epoch 8001, Training Loss: 44370, Validation Loss: 46107, 155756.01612321395\n",
      "Epoch 8101, Training Loss: 43982, Validation Loss: 46588, 188969.42738747958\n",
      "Epoch 8201, Training Loss: 43310, Validation Loss: 46887, 207873.58778105164\n",
      "Epoch 8301, Training Loss: 43631, Validation Loss: 48586, 144358.1314423912\n",
      "Epoch 8401, Training Loss: 41426, Validation Loss: 46899, 181778.81310751842\n",
      "Epoch 8501, Training Loss: 46563, Validation Loss: 50499, 335708.9383308189\n",
      "Epoch 8601, Training Loss: 44231, Validation Loss: 46982, 214589.22710092776\n",
      "Epoch 8701, Training Loss: 41448, Validation Loss: 47821, 147091.57407759008\n",
      "Epoch 8801, Training Loss: 44364, Validation Loss: 51234, 267718.5655465574\n",
      "Epoch 8901, Training Loss: 43209, Validation Loss: 46934, 171208.64983699116\n",
      "Epoch 9001, Training Loss: 41563, Validation Loss: 46090, 170402.44043831865\n",
      "Epoch 9101, Training Loss: 43608, Validation Loss: 46828, 119934.31814660059\n",
      "Epoch 9201, Training Loss: 42081, Validation Loss: 47552, 173120.70007280348\n",
      "Epoch 9301, Training Loss: 44024, Validation Loss: 46790, 258075.3824721998\n",
      "Epoch 9401, Training Loss: 43962, Validation Loss: 48991, 305961.00205807417\n",
      "Epoch 9501, Training Loss: 41396, Validation Loss: 50253, 148288.94642855116\n",
      "Epoch 9601, Training Loss: 42571, Validation Loss: 48884, 158156.89058029745\n",
      "Epoch 9701, Training Loss: 42885, Validation Loss: 46549, 214307.00292919792\n",
      "Epoch 9801, Training Loss: 41812, Validation Loss: 49625, 194911.642963772\n",
      "Epoch 9901, Training Loss: 42432, Validation Loss: 46889, 203866.9924300113\n",
      "Epoch 10001, Training Loss: 40493, Validation Loss: 50547, 139077.26330730162\n",
      "Epoch 10101, Training Loss: 43551, Validation Loss: 46510, 166982.85632247612\n",
      "Epoch 10201, Training Loss: 43269, Validation Loss: 47444, 186097.76274506247\n",
      "Epoch 10301, Training Loss: 43224, Validation Loss: 48338, 189247.20420287384\n",
      "Epoch 10401, Training Loss: 42861, Validation Loss: 47287, 196337.47896114425\n",
      "Epoch 10501, Training Loss: 41945, Validation Loss: 46047, 180704.2242505623\n",
      "Epoch 10601, Training Loss: 42003, Validation Loss: 47482, 142029.97003247956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10701, Training Loss: 42801, Validation Loss: 48305, 234718.21013679882\n",
      "Epoch 10801, Training Loss: 42598, Validation Loss: 48647, 212113.01640916493\n",
      "Epoch 10901, Training Loss: 43044, Validation Loss: 47061, 162461.85865630332\n",
      "Epoch 11001, Training Loss: 43525, Validation Loss: 47148, 226463.10818906347\n",
      "Epoch 11101, Training Loss: 42767, Validation Loss: 47989, 265387.82284088753\n",
      "Epoch 11201, Training Loss: 42766, Validation Loss: 46528, 189016.6977162146\n",
      "Epoch 11301, Training Loss: 40362, Validation Loss: 47489, 184881.8865567253\n",
      "Epoch 11401, Training Loss: 40015, Validation Loss: 50748, 202757.30455332817\n",
      "Epoch 11501, Training Loss: 42698, Validation Loss: 47399, 261187.98224435732\n",
      "Epoch 11601, Training Loss: 43187, Validation Loss: 48022, 297730.02481382777\n",
      "Epoch 11701, Training Loss: 40036, Validation Loss: 47085, 157341.45460652845\n",
      "Epoch 11801, Training Loss: 42512, Validation Loss: 49529, 183813.6816338934\n",
      "Epoch 11901, Training Loss: 43449, Validation Loss: 51218, 249046.86923447502\n",
      "Epoch 12001, Training Loss: 40076, Validation Loss: 49234, 200702.24529643558\n",
      "Epoch 12101, Training Loss: 41684, Validation Loss: 48609, 154196.89787660673\n",
      "Epoch 12201, Training Loss: 42822, Validation Loss: 49558, 185867.49218175793\n",
      "Epoch 12301, Training Loss: 40709, Validation Loss: 46177, 204934.38748007794\n",
      "Epoch 12401, Training Loss: 39801, Validation Loss: 46878, 147457.1039276505\n",
      "Epoch 12501, Training Loss: 39751, Validation Loss: 46695, 189911.40482440987\n",
      "Epoch 12601, Training Loss: 42649, Validation Loss: 46374, 150823.73837042088\n",
      "Epoch 12701, Training Loss: 43472, Validation Loss: 47717, 232446.90235786414\n",
      "Epoch 12801, Training Loss: 41296, Validation Loss: 47562, 174600.30785679823\n",
      "Epoch 12901, Training Loss: 41533, Validation Loss: 48234, 215544.93199928757\n",
      "Epoch 13001, Training Loss: 40373, Validation Loss: 47776, 208851.67034825397\n",
      "Epoch 13101, Training Loss: 41435, Validation Loss: 52180, 211343.02678781707\n",
      "Epoch 13201, Training Loss: 41752, Validation Loss: 47160, 142860.96332675117\n",
      "Epoch 13301, Training Loss: 42231, Validation Loss: 45846, 305052.3965784344\n",
      "Epoch 13401, Training Loss: 40688, Validation Loss: 46658, 176483.89218035256\n",
      "Epoch 13501, Training Loss: 41404, Validation Loss: 45752, 142905.74052454304\n",
      "Epoch 13601, Training Loss: 40852, Validation Loss: 48358, 222742.36313301936\n",
      "Epoch 13701, Training Loss: 41284, Validation Loss: 47313, 136122.1905811215\n",
      "Epoch 13801, Training Loss: 43426, Validation Loss: 46937, 172106.50307639298\n",
      "Epoch 13901, Training Loss: 41680, Validation Loss: 46253, 193288.39056206765\n",
      "Epoch 14001, Training Loss: 42257, Validation Loss: 45580, 165743.5535965879\n",
      "Epoch 14101, Training Loss: 43633, Validation Loss: 46092, 221608.00063820454\n",
      "Epoch 14201, Training Loss: 43283, Validation Loss: 50865, 214020.56384835616\n",
      "Epoch 14301, Training Loss: 40569, Validation Loss: 47851, 193424.24888635208\n",
      "Epoch 14401, Training Loss: 40373, Validation Loss: 48310, 150082.6099130038\n",
      "Epoch 14501, Training Loss: 41430, Validation Loss: 47067, 168375.8260216142\n",
      "Epoch 14601, Training Loss: 41183, Validation Loss: 46391, 212453.21521601445\n",
      "Epoch 14701, Training Loss: 40893, Validation Loss: 47521, 246693.75662060207\n",
      "Epoch 14801, Training Loss: 39451, Validation Loss: 50472, 295624.1038324288\n",
      "Epoch 14901, Training Loss: 40938, Validation Loss: 47120, 143192.44339131372\n",
      "Epoch 15001, Training Loss: 41327, Validation Loss: 45339, 176079.2810811131\n",
      "Epoch 15101, Training Loss: 40735, Validation Loss: 48655, 186530.44827366067\n",
      "Epoch 15201, Training Loss: 40611, Validation Loss: 47446, 164579.5049898447\n",
      "Epoch 15301, Training Loss: 41553, Validation Loss: 45359, 251326.01784709055\n",
      "Epoch 15401, Training Loss: 41417, Validation Loss: 50287, 228697.42082844715\n",
      "Epoch 15501, Training Loss: 40300, Validation Loss: 46986, 126976.93131080313\n",
      "Epoch 15601, Training Loss: 40018, Validation Loss: 48809, 222260.66807209724\n",
      "Epoch 15701, Training Loss: 40515, Validation Loss: 47687, 197198.09351667436\n",
      "Epoch 15801, Training Loss: 41036, Validation Loss: 46261, 182712.43475175332\n",
      "Epoch 15901, Training Loss: 40863, Validation Loss: 47381, 188347.66500596184\n",
      "Epoch 16001, Training Loss: 40484, Validation Loss: 47886, 253504.5769133239\n",
      "Epoch 16101, Training Loss: 38675, Validation Loss: 46368, 176138.8061690033\n",
      "Epoch 16201, Training Loss: 40303, Validation Loss: 46304, 165715.4795919916\n",
      "Epoch 16301, Training Loss: 40103, Validation Loss: 47281, 214201.52226268346\n",
      "Epoch 16401, Training Loss: 40060, Validation Loss: 45798, 176931.74138080218\n",
      "Epoch 16501, Training Loss: 40333, Validation Loss: 48635, 180064.06651585788\n",
      "Epoch 16601, Training Loss: 40630, Validation Loss: 43556, 171893.83910339052\n",
      "Epoch 16701, Training Loss: 38958, Validation Loss: 47830, 214961.59795284164\n",
      "Epoch 16801, Training Loss: 40303, Validation Loss: 48331, 183134.5070766251\n",
      "Epoch 16901, Training Loss: 39403, Validation Loss: 47422, 212930.86760560464\n",
      "Epoch 17001, Training Loss: 40504, Validation Loss: 48108, 198558.2279542995\n",
      "Epoch 17101, Training Loss: 40502, Validation Loss: 47666, 159390.92266612002\n",
      "Epoch 17201, Training Loss: 40550, Validation Loss: 51178, 196082.2836457674\n",
      "Epoch 17301, Training Loss: 39771, Validation Loss: 45495, 191427.7518855684\n",
      "Epoch 17401, Training Loss: 41786, Validation Loss: 51274, 212413.164079462\n",
      "Epoch 17501, Training Loss: 39393, Validation Loss: 51728, 258656.70250950422\n",
      "Epoch 17601, Training Loss: 39406, Validation Loss: 48570, 199226.18415974377\n",
      "Epoch 17701, Training Loss: 39944, Validation Loss: 46924, 208896.8031220702\n",
      "Epoch 17801, Training Loss: 40289, Validation Loss: 48437, 200161.4162636198\n",
      "Epoch 17901, Training Loss: 41460, Validation Loss: 45290, 183772.94974370324\n",
      "Epoch 18001, Training Loss: 40963, Validation Loss: 46442, 211078.8419335191\n",
      "Epoch 18101, Training Loss: 40465, Validation Loss: 50893, 191476.90319812586\n",
      "Epoch 18201, Training Loss: 37630, Validation Loss: 50148, 224210.193610162\n",
      "Epoch 18301, Training Loss: 39647, Validation Loss: 45703, 196860.63140696025\n",
      "Epoch 18401, Training Loss: 38201, Validation Loss: 46928, 180878.4865110796\n",
      "Epoch 18501, Training Loss: 39729, Validation Loss: 49654, 196653.48402935272\n",
      "Epoch 18601, Training Loss: 38875, Validation Loss: 48073, 199133.86430157357\n",
      "Epoch 18701, Training Loss: 41308, Validation Loss: 45829, 186878.223616387\n",
      "Epoch 18801, Training Loss: 38524, Validation Loss: 48462, 147686.13977217078\n",
      "Epoch 18901, Training Loss: 38927, Validation Loss: 51089, 247639.60088018916\n",
      "Epoch 19001, Training Loss: 37122, Validation Loss: 49084, 154273.0651262484\n",
      "Epoch 19101, Training Loss: 40890, Validation Loss: 45552, 284733.7424678852\n",
      "Epoch 19201, Training Loss: 37808, Validation Loss: 47453, 154726.81542315986\n",
      "Epoch 19301, Training Loss: 38478, Validation Loss: 47225, 194083.71462294296\n",
      "Epoch 19401, Training Loss: 39427, Validation Loss: 46245, 170990.37185704816\n",
      "Epoch 19501, Training Loss: 37681, Validation Loss: 47764, 122753.61406351572\n",
      "Epoch 19601, Training Loss: 39193, Validation Loss: 48519, 215158.58132287144\n",
      "Epoch 19701, Training Loss: 38795, Validation Loss: 47182, 167428.28350522785\n",
      "Epoch 19801, Training Loss: 37582, Validation Loss: 47845, 164198.67213870352\n",
      "Epoch 19901, Training Loss: 39009, Validation Loss: 47055, 212742.47402741775\n",
      "Epoch 20001, Training Loss: 41701, Validation Loss: 49707, 218812.06354931634\n",
      "Epoch 20101, Training Loss: 39689, Validation Loss: 46982, 210829.04685262134\n",
      "Epoch 20201, Training Loss: 37602, Validation Loss: 48746, 214385.8274334153\n",
      "Epoch 20301, Training Loss: 37536, Validation Loss: 48009, 211186.1593936695\n",
      "Epoch 20401, Training Loss: 37452, Validation Loss: 45981, 174384.8009827197\n",
      "Epoch 20501, Training Loss: 36577, Validation Loss: 46048, 193437.56590005726\n",
      "Epoch 20601, Training Loss: 38877, Validation Loss: 47487, 197969.03256598822\n",
      "Epoch 20701, Training Loss: 38254, Validation Loss: 48884, 180287.72422511145\n",
      "Epoch 20801, Training Loss: 38931, Validation Loss: 47510, 214877.72172630957\n",
      "Epoch 20901, Training Loss: 37769, Validation Loss: 47786, 208795.43261049385\n",
      "Epoch 21001, Training Loss: 36455, Validation Loss: 47471, 161495.98406905733\n",
      "Epoch 21101, Training Loss: 38178, Validation Loss: 46926, 160649.42179328107\n",
      "Epoch 21201, Training Loss: 37794, Validation Loss: 51234, 251896.01816595858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21301, Training Loss: 36921, Validation Loss: 48466, 155698.4815156714\n",
      "Epoch 21401, Training Loss: 38829, Validation Loss: 48422, 158750.5767319329\n",
      "Epoch 21501, Training Loss: 38321, Validation Loss: 45796, 195561.165235524\n",
      "Epoch 21601, Training Loss: 39108, Validation Loss: 46810, 245672.3920987829\n",
      "Epoch 21701, Training Loss: 38135, Validation Loss: 48022, 180692.0871357278\n",
      "Epoch 21801, Training Loss: 37850, Validation Loss: 50412, 249558.9757838807\n",
      "Epoch 21901, Training Loss: 39110, Validation Loss: 46696, 213000.6706742379\n",
      "Epoch 22001, Training Loss: 35403, Validation Loss: 47364, 182303.63036908623\n",
      "Epoch 22101, Training Loss: 38145, Validation Loss: 48269, 178233.27094000304\n",
      "Epoch 22201, Training Loss: 37631, Validation Loss: 45799, 151620.31677556835\n",
      "Epoch 22301, Training Loss: 36273, Validation Loss: 51091, 225697.3985339425\n",
      "Epoch 22401, Training Loss: 40794, Validation Loss: 45990, 203612.97812654174\n",
      "Epoch 22501, Training Loss: 38761, Validation Loss: 48188, 191825.77265427186\n",
      "Epoch 22601, Training Loss: 38109, Validation Loss: 45775, 172964.89707613603\n",
      "Epoch 22701, Training Loss: 38550, Validation Loss: 47895, 165358.1923541716\n",
      "Epoch 22801, Training Loss: 37350, Validation Loss: 49848, 137417.60885021408\n",
      "Epoch 22901, Training Loss: 37802, Validation Loss: 48499, 189226.24549770003\n",
      "Epoch 23001, Training Loss: 37930, Validation Loss: 47557, 234802.85277370014\n",
      "Epoch 23101, Training Loss: 37883, Validation Loss: 48120, 230503.59802447158\n",
      "Epoch 23201, Training Loss: 37989, Validation Loss: 45833, 213578.45956691244\n",
      "Epoch 23301, Training Loss: 38552, Validation Loss: 48846, 243049.52457273792\n",
      "Epoch 23401, Training Loss: 38923, Validation Loss: 46244, 232315.38269847416\n",
      "Epoch 23501, Training Loss: 36323, Validation Loss: 50636, 187548.6904514044\n",
      "Epoch 23601, Training Loss: 37671, Validation Loss: 45694, 224584.04989612632\n",
      "Epoch 23701, Training Loss: 38729, Validation Loss: 47526, 289288.6932592694\n",
      "Epoch 23801, Training Loss: 37463, Validation Loss: 50598, 235374.06661939976\n",
      "Epoch 23901, Training Loss: 36905, Validation Loss: 50102, 194606.42753948297\n",
      "Epoch 24001, Training Loss: 38837, Validation Loss: 52209, 213157.15266762296\n",
      "Epoch 24101, Training Loss: 38410, Validation Loss: 47501, 181868.24945703897\n",
      "Epoch 24201, Training Loss: 37055, Validation Loss: 53178, 199661.57627589122\n",
      "Epoch 24301, Training Loss: 37824, Validation Loss: 51074, 213965.6509461821\n",
      "Epoch 24401, Training Loss: 36114, Validation Loss: 47790, 182568.13421154398\n",
      "Epoch 24501, Training Loss: 36669, Validation Loss: 48674, 154813.42798059146\n",
      "Epoch 24601, Training Loss: 39199, Validation Loss: 52605, 247768.32666003495\n",
      "Epoch 24701, Training Loss: 36712, Validation Loss: 50315, 160249.33885159556\n",
      "Epoch 24801, Training Loss: 37612, Validation Loss: 48498, 215467.25818882586\n",
      "Epoch 24901, Training Loss: 37563, Validation Loss: 48503, 214082.97211041691\n",
      "Epoch 25001, Training Loss: 36805, Validation Loss: 47481, 177669.28381260548\n",
      "Epoch 25101, Training Loss: 38202, Validation Loss: 47555, 195236.41208348345\n",
      "Epoch 25201, Training Loss: 36836, Validation Loss: 52852, 185864.08163917586\n",
      "Epoch 25301, Training Loss: 38064, Validation Loss: 49030, 234657.9387979731\n",
      "Epoch 25401, Training Loss: 36560, Validation Loss: 47892, 202077.51352992427\n",
      "Epoch 25501, Training Loss: 35627, Validation Loss: 48883, 156657.90975748858\n",
      "Epoch 25601, Training Loss: 35247, Validation Loss: 48290, 161627.84961282622\n",
      "Epoch 25701, Training Loss: 39118, Validation Loss: 51412, 186149.9880783011\n",
      "Epoch 25801, Training Loss: 36664, Validation Loss: 47202, 167676.38140129478\n",
      "Epoch 25901, Training Loss: 38152, Validation Loss: 51316, 250926.40239290879\n",
      "Epoch 26001, Training Loss: 37657, Validation Loss: 49536, 198553.3869838946\n",
      "Epoch 26101, Training Loss: 35821, Validation Loss: 49330, 178007.14024342626\n",
      "Epoch 26201, Training Loss: 36952, Validation Loss: 51113, 172118.08462064472\n",
      "Epoch 26301, Training Loss: 37035, Validation Loss: 47466, 161298.97019519296\n",
      "Epoch 26401, Training Loss: 38000, Validation Loss: 48263, 198769.17928993638\n",
      "Epoch 26501, Training Loss: 36301, Validation Loss: 50664, 254993.24661626265\n",
      "Epoch 26601, Training Loss: 37096, Validation Loss: 48089, 170163.6870208439\n",
      "Epoch 26701, Training Loss: 34792, Validation Loss: 50992, 228275.13488707654\n",
      "Epoch 26801, Training Loss: 37803, Validation Loss: 51945, 197063.67749436054\n",
      "Epoch 26901, Training Loss: 39371, Validation Loss: 50008, 246376.48062649817\n",
      "Epoch 27001, Training Loss: 36450, Validation Loss: 49752, 176635.99634255172\n",
      "Epoch 27101, Training Loss: 36121, Validation Loss: 51752, 174647.85819786575\n",
      "Epoch 27201, Training Loss: 35986, Validation Loss: 53048, 211826.3132608953\n",
      "Epoch 27301, Training Loss: 38626, Validation Loss: 50240, 249942.7852913984\n",
      "Epoch 27401, Training Loss: 36380, Validation Loss: 47793, 165802.80270693917\n",
      "Epoch 27501, Training Loss: 35503, Validation Loss: 48537, 160274.31230450448\n",
      "Epoch 27601, Training Loss: 36941, Validation Loss: 48964, 158531.30413964594\n",
      "Epoch 27701, Training Loss: 36529, Validation Loss: 48089, 159144.90844536037\n",
      "Epoch 27801, Training Loss: 36287, Validation Loss: 49351, 189202.2669217614\n",
      "Epoch 27901, Training Loss: 38968, Validation Loss: 47926, 206835.65026584463\n",
      "Epoch 28001, Training Loss: 37330, Validation Loss: 48603, 183291.13229232383\n",
      "Epoch 28101, Training Loss: 38427, Validation Loss: 53231, 201615.86608873314\n",
      "Epoch 28201, Training Loss: 37442, Validation Loss: 49197, 209760.17979729167\n",
      "Epoch 28301, Training Loss: 38247, Validation Loss: 46667, 225206.29835207373\n",
      "Epoch 28401, Training Loss: 35534, Validation Loss: 49256, 233829.48355538287\n",
      "Epoch 28501, Training Loss: 35641, Validation Loss: 45857, 228141.41337137469\n",
      "Epoch 28601, Training Loss: 37881, Validation Loss: 50827, 186405.96348935834\n",
      "Epoch 28701, Training Loss: 37821, Validation Loss: 48499, 233987.92567792805\n",
      "Epoch 28801, Training Loss: 36278, Validation Loss: 48091, 183692.6726955731\n",
      "Epoch 28901, Training Loss: 35766, Validation Loss: 50679, 134040.92892137603\n",
      "Epoch 29001, Training Loss: 35727, Validation Loss: 50029, 151187.29424873219\n",
      "Epoch 29101, Training Loss: 35652, Validation Loss: 50028, 237133.36342406948\n",
      "Epoch 29201, Training Loss: 35892, Validation Loss: 48992, 157697.46672728338\n",
      "Epoch 29301, Training Loss: 38116, Validation Loss: 48830, 235491.83037350173\n",
      "Epoch 29401, Training Loss: 35995, Validation Loss: 48507, 148286.24912446097\n",
      "Epoch 29501, Training Loss: 38754, Validation Loss: 46243, 214104.17122174986\n",
      "Epoch 29601, Training Loss: 36491, Validation Loss: 49849, 148184.02646586922\n",
      "Epoch 29701, Training Loss: 36550, Validation Loss: 53668, 227865.34676014987\n",
      "Epoch 29801, Training Loss: 37019, Validation Loss: 51788, 209236.031788472\n",
      "Epoch 29901, Training Loss: 34832, Validation Loss: 47520, 152433.85801748614\n",
      "Epoch 30001, Training Loss: 36244, Validation Loss: 49203, 146628.5228911057\n",
      "Epoch 30101, Training Loss: 36682, Validation Loss: 47963, 194904.17374449054\n",
      "Epoch 30201, Training Loss: 36360, Validation Loss: 51358, 194786.66623412282\n",
      "Epoch 30301, Training Loss: 36202, Validation Loss: 46732, 191614.84359938122\n",
      "Epoch 30401, Training Loss: 37415, Validation Loss: 49140, 157035.13534426797\n",
      "Epoch 30501, Training Loss: 36648, Validation Loss: 48876, 138756.81223436721\n",
      "Epoch 30601, Training Loss: 36406, Validation Loss: 47705, 164813.58458193697\n",
      "Epoch 30701, Training Loss: 34872, Validation Loss: 49047, 190655.05923820115\n",
      "Epoch 30801, Training Loss: 37731, Validation Loss: 49208, 232469.8032326018\n",
      "Epoch 30901, Training Loss: 36740, Validation Loss: 48039, 157617.74340656935\n",
      "Epoch 31001, Training Loss: 36045, Validation Loss: 50870, 148276.10228429525\n",
      "Epoch 31101, Training Loss: 34652, Validation Loss: 46236, 161318.16773240324\n",
      "Epoch 31201, Training Loss: 36723, Validation Loss: 48507, 154411.02059588593\n",
      "Epoch 31301, Training Loss: 36293, Validation Loss: 47665, 161921.95542670865\n",
      "Epoch 31401, Training Loss: 37926, Validation Loss: 48674, 215937.77442716117\n",
      "Epoch 31501, Training Loss: 37181, Validation Loss: 50877, 197581.02069601248\n",
      "Epoch 31601, Training Loss: 35967, Validation Loss: 49507, 153194.97367101195\n",
      "Epoch 31701, Training Loss: 35784, Validation Loss: 48241, 198572.82898886845\n",
      "Epoch 31801, Training Loss: 35169, Validation Loss: 51054, 181044.3233207665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31901, Training Loss: 36518, Validation Loss: 47928, 178128.6523636084\n",
      "Epoch 32001, Training Loss: 38010, Validation Loss: 52108, 248350.55282867188\n",
      "Epoch 32101, Training Loss: 36776, Validation Loss: 48954, 171532.47359580584\n",
      "Epoch 32201, Training Loss: 35499, Validation Loss: 47323, 243195.48622529604\n",
      "Epoch 32301, Training Loss: 35963, Validation Loss: 49342, 181387.62526868525\n",
      "Epoch 32401, Training Loss: 35104, Validation Loss: 47462, 174308.82183638276\n",
      "Epoch 32501, Training Loss: 36201, Validation Loss: 47058, 171646.8791167444\n",
      "Epoch 32601, Training Loss: 37006, Validation Loss: 46813, 155401.0245834302\n",
      "Epoch 32701, Training Loss: 37149, Validation Loss: 47177, 167871.6232418012\n",
      "Epoch 32801, Training Loss: 36699, Validation Loss: 48499, 182880.28658559997\n",
      "Epoch 32901, Training Loss: 35250, Validation Loss: 51650, 167472.99106420172\n",
      "Epoch 33001, Training Loss: 38158, Validation Loss: 47479, 189010.5169048269\n",
      "Epoch 33101, Training Loss: 37683, Validation Loss: 50206, 223534.1900936529\n",
      "Epoch 33201, Training Loss: 37639, Validation Loss: 48831, 141625.85261458665\n",
      "Epoch 33301, Training Loss: 37332, Validation Loss: 51456, 170156.43928875352\n",
      "Epoch 33401, Training Loss: 36936, Validation Loss: 50097, 157373.02087705545\n",
      "Epoch 33501, Training Loss: 40101, Validation Loss: 51358, 264624.4479673134\n",
      "Epoch 33601, Training Loss: 38614, Validation Loss: 48071, 191159.61090712826\n",
      "Epoch 33701, Training Loss: 37217, Validation Loss: 51350, 162648.97765909115\n",
      "Epoch 33801, Training Loss: 37907, Validation Loss: 50212, 154113.2010897727\n",
      "Epoch 33901, Training Loss: 38853, Validation Loss: 47311, 203154.78489478718\n",
      "Epoch 34001, Training Loss: 37501, Validation Loss: 48709, 187092.41414891713\n",
      "Epoch 34101, Training Loss: 37495, Validation Loss: 49205, 145636.0418254314\n",
      "Epoch 34201, Training Loss: 38788, Validation Loss: 51949, 183451.0313228062\n",
      "Epoch 34301, Training Loss: 34342, Validation Loss: 51242, 155919.93137336988\n",
      "Epoch 34401, Training Loss: 35305, Validation Loss: 46997, 187321.64208609602\n",
      "Epoch 34501, Training Loss: 33574, Validation Loss: 49847, 169369.8163110917\n",
      "Epoch 34601, Training Loss: 35245, Validation Loss: 51316, 161295.5696309419\n",
      "Epoch 34701, Training Loss: 37505, Validation Loss: 50262, 176307.81767424685\n",
      "Epoch 34801, Training Loss: 34038, Validation Loss: 48904, 154170.82532926422\n",
      "Epoch 34901, Training Loss: 34852, Validation Loss: 50658, 158895.93389822295\n",
      "Epoch 35001, Training Loss: 35386, Validation Loss: 50531, 140478.89975115852\n",
      "Epoch 35101, Training Loss: 34391, Validation Loss: 53734, 195822.68670835093\n",
      "Epoch 35201, Training Loss: 37014, Validation Loss: 52806, 205542.81382904033\n",
      "Epoch 35301, Training Loss: 34386, Validation Loss: 51033, 178668.598583164\n",
      "Epoch 35401, Training Loss: 35722, Validation Loss: 54228, 198506.48078063893\n",
      "Epoch 35501, Training Loss: 33674, Validation Loss: 49687, 151468.93505547446\n",
      "Epoch 35601, Training Loss: 34845, Validation Loss: 51330, 189733.793227957\n",
      "Epoch 35701, Training Loss: 37572, Validation Loss: 51842, 174931.9830782299\n",
      "Epoch 35801, Training Loss: 35656, Validation Loss: 47584, 154637.98321359933\n",
      "Epoch 35901, Training Loss: 35729, Validation Loss: 47098, 208556.17620138807\n",
      "Epoch 36001, Training Loss: 35412, Validation Loss: 49956, 151476.39592239275\n",
      "Epoch 36101, Training Loss: 37549, Validation Loss: 50829, 209311.66777139244\n",
      "Epoch 36201, Training Loss: 37260, Validation Loss: 48909, 184047.07018212127\n",
      "Epoch 36301, Training Loss: 35265, Validation Loss: 50993, 179701.12268283172\n",
      "Epoch 36401, Training Loss: 34680, Validation Loss: 50862, 144681.8843232048\n",
      "Epoch 36501, Training Loss: 35389, Validation Loss: 50339, 150091.60616269647\n",
      "Epoch 36601, Training Loss: 35616, Validation Loss: 49706, 166220.95722382338\n",
      "Epoch 36701, Training Loss: 35748, Validation Loss: 50915, 200729.12435216602\n",
      "Epoch 36801, Training Loss: 36444, Validation Loss: 49614, 123786.08241199702\n",
      "Epoch 36901, Training Loss: 34577, Validation Loss: 47403, 222766.96763418667\n",
      "Epoch 37001, Training Loss: 35930, Validation Loss: 54518, 147709.9504492703\n",
      "Epoch 37101, Training Loss: 35396, Validation Loss: 48304, 189722.4693613679\n",
      "Epoch 37201, Training Loss: 35686, Validation Loss: 47949, 157223.08117932567\n",
      "Epoch 37301, Training Loss: 36230, Validation Loss: 49216, 179569.8604627871\n",
      "Epoch 37401, Training Loss: 34781, Validation Loss: 51034, 139151.97457811848\n",
      "Epoch 37501, Training Loss: 33654, Validation Loss: 49227, 145571.1276075261\n",
      "Epoch 37601, Training Loss: 35143, Validation Loss: 50316, 185045.10041817374\n",
      "Epoch 37701, Training Loss: 35598, Validation Loss: 50788, 157464.6389530726\n",
      "Epoch 37801, Training Loss: 35907, Validation Loss: 49117, 185776.71326918402\n",
      "Epoch 37901, Training Loss: 34550, Validation Loss: 51046, 183758.30756404373\n",
      "Epoch 38001, Training Loss: 34203, Validation Loss: 51076, 155219.15343662823\n",
      "Epoch 38101, Training Loss: 35586, Validation Loss: 49714, 195087.46858969252\n",
      "Epoch 38201, Training Loss: 35989, Validation Loss: 48415, 132548.5626400625\n",
      "Epoch 38301, Training Loss: 34438, Validation Loss: 49528, 145100.6019546649\n",
      "Epoch 38401, Training Loss: 34954, Validation Loss: 49879, 175746.73012792328\n",
      "Epoch 38501, Training Loss: 35604, Validation Loss: 50736, 210431.38928299263\n",
      "Epoch 38601, Training Loss: 34593, Validation Loss: 50151, 175444.96377381653\n",
      "Epoch 38701, Training Loss: 36701, Validation Loss: 51022, 191157.49636213467\n",
      "Epoch 38801, Training Loss: 35162, Validation Loss: 48884, 153298.21948522519\n",
      "Epoch 38901, Training Loss: 34625, Validation Loss: 49413, 155783.27687725474\n",
      "Epoch 39001, Training Loss: 34596, Validation Loss: 50423, 169702.73062486688\n",
      "Epoch 39101, Training Loss: 36142, Validation Loss: 48065, 168876.77378645272\n",
      "Epoch 39201, Training Loss: 34265, Validation Loss: 48737, 167532.21829691625\n",
      "Epoch 39301, Training Loss: 36475, Validation Loss: 49557, 175701.15152827644\n",
      "Epoch 39401, Training Loss: 36006, Validation Loss: 48498, 125499.42270277618\n",
      "Epoch 39501, Training Loss: 33790, Validation Loss: 48112, 157522.23898051944\n",
      "Epoch 39601, Training Loss: 35613, Validation Loss: 49722, 176543.30124763324\n",
      "Epoch 39701, Training Loss: 36348, Validation Loss: 50405, 144692.68054155735\n",
      "Epoch 39801, Training Loss: 34484, Validation Loss: 52855, 199416.13285652158\n",
      "Epoch 39901, Training Loss: 33479, Validation Loss: 51581, 157471.4931395128\n",
      "Epoch 40001, Training Loss: 33999, Validation Loss: 50573, 167134.43483288438\n",
      "Epoch 40101, Training Loss: 34282, Validation Loss: 50576, 180667.67982116307\n",
      "Epoch 40201, Training Loss: 35786, Validation Loss: 50956, 173839.08073914656\n",
      "Epoch 40301, Training Loss: 35230, Validation Loss: 47199, 151120.0321945122\n",
      "Epoch 40401, Training Loss: 34318, Validation Loss: 49352, 156585.74511532616\n",
      "Epoch 40501, Training Loss: 36713, Validation Loss: 50324, 160106.46559257066\n",
      "Epoch 40601, Training Loss: 35281, Validation Loss: 52566, 147196.68952736925\n",
      "Epoch 40701, Training Loss: 34798, Validation Loss: 49114, 180722.46929372352\n",
      "Epoch 40801, Training Loss: 33248, Validation Loss: 48304, 141320.25197164985\n",
      "Epoch 40901, Training Loss: 34902, Validation Loss: 49997, 133641.3443998647\n",
      "Epoch 41001, Training Loss: 34107, Validation Loss: 49586, 158788.3322871654\n",
      "Epoch 41101, Training Loss: 34690, Validation Loss: 49534, 194286.2987388791\n",
      "Epoch 41201, Training Loss: 34763, Validation Loss: 53199, 206678.54013917467\n",
      "Epoch 41301, Training Loss: 38539, Validation Loss: 51643, 171191.50461531745\n",
      "Epoch 41401, Training Loss: 37116, Validation Loss: 52566, 239371.09780543437\n",
      "Epoch 41501, Training Loss: 34354, Validation Loss: 51871, 160618.84971992546\n",
      "Epoch 41601, Training Loss: 34208, Validation Loss: 51307, 155084.57046434592\n",
      "Epoch 41701, Training Loss: 33650, Validation Loss: 49248, 140787.549752842\n",
      "Epoch 41801, Training Loss: 36740, Validation Loss: 52377, 162090.4425053279\n",
      "Epoch 41901, Training Loss: 36162, Validation Loss: 52235, 187793.70772438767\n",
      "Epoch 42001, Training Loss: 35528, Validation Loss: 51282, 138415.8856742434\n",
      "Epoch 42101, Training Loss: 35266, Validation Loss: 49623, 150178.8505845729\n",
      "Epoch 42201, Training Loss: 34830, Validation Loss: 54106, 189513.10141957214\n",
      "Epoch 42301, Training Loss: 35035, Validation Loss: 50891, 185040.30797211413\n",
      "Epoch 42401, Training Loss: 36011, Validation Loss: 48537, 131657.6496049899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42501, Training Loss: 34079, Validation Loss: 48927, 142818.81138279528\n",
      "Epoch 42601, Training Loss: 36139, Validation Loss: 48177, 160374.21196130474\n",
      "Epoch 42701, Training Loss: 33859, Validation Loss: 48842, 201690.35683040167\n",
      "Epoch 42801, Training Loss: 35689, Validation Loss: 47507, 206507.6467519372\n",
      "Epoch 42901, Training Loss: 32914, Validation Loss: 51910, 166584.25252658865\n",
      "Epoch 43001, Training Loss: 34222, Validation Loss: 48595, 198996.8294189878\n",
      "Epoch 43101, Training Loss: 35475, Validation Loss: 49768, 198596.81854036817\n",
      "Epoch 43201, Training Loss: 36464, Validation Loss: 50271, 214518.75180510897\n",
      "Epoch 43301, Training Loss: 33737, Validation Loss: 51642, 172814.17847997946\n",
      "Epoch 43401, Training Loss: 34646, Validation Loss: 50186, 136046.40981319683\n",
      "Epoch 43501, Training Loss: 33668, Validation Loss: 52009, 159146.48019874512\n",
      "Epoch 43601, Training Loss: 33230, Validation Loss: 47807, 147922.03980703995\n",
      "Epoch 43701, Training Loss: 34315, Validation Loss: 49249, 165057.01268799076\n",
      "Epoch 43801, Training Loss: 37045, Validation Loss: 54775, 222406.20743530424\n",
      "Epoch 43901, Training Loss: 33246, Validation Loss: 51931, 131731.94120440562\n",
      "Epoch 44001, Training Loss: 33955, Validation Loss: 49058, 145539.04424905352\n",
      "Epoch 44101, Training Loss: 35058, Validation Loss: 48752, 186516.91684364682\n",
      "Epoch 44201, Training Loss: 36308, Validation Loss: 50847, 236436.66764026965\n",
      "Epoch 44301, Training Loss: 34694, Validation Loss: 47908, 180402.28754754332\n",
      "Epoch 44401, Training Loss: 35679, Validation Loss: 52208, 172438.48762430777\n",
      "Epoch 44501, Training Loss: 35339, Validation Loss: 50705, 173941.84524062503\n",
      "Epoch 44601, Training Loss: 34678, Validation Loss: 50944, 170863.99768664347\n",
      "Epoch 44701, Training Loss: 34475, Validation Loss: 49847, 129648.45772454678\n",
      "Epoch 44801, Training Loss: 34486, Validation Loss: 50442, 144068.81029488193\n",
      "Epoch 44901, Training Loss: 35105, Validation Loss: 48444, 134072.26345901773\n",
      "Epoch 45001, Training Loss: 33697, Validation Loss: 50196, 166628.87137786657\n",
      "Epoch 45101, Training Loss: 35645, Validation Loss: 50829, 153654.5600804341\n",
      "Epoch 45201, Training Loss: 34468, Validation Loss: 51758, 143385.3904733584\n",
      "Epoch 45301, Training Loss: 35521, Validation Loss: 47453, 170920.99231100283\n",
      "Epoch 45401, Training Loss: 35272, Validation Loss: 49696, 268755.5556267309\n",
      "Epoch 45501, Training Loss: 34254, Validation Loss: 50650, 157360.6062115691\n",
      "Epoch 45601, Training Loss: 34609, Validation Loss: 51624, 205792.8762906929\n",
      "Epoch 45701, Training Loss: 35448, Validation Loss: 50455, 180075.2769747818\n",
      "Epoch 45801, Training Loss: 34199, Validation Loss: 48515, 154499.16950476507\n",
      "Epoch 45901, Training Loss: 37330, Validation Loss: 48756, 136820.3907030435\n",
      "Epoch 46001, Training Loss: 34866, Validation Loss: 51031, 176252.0070405523\n",
      "Epoch 46101, Training Loss: 33301, Validation Loss: 50235, 172391.26746717477\n",
      "Epoch 46201, Training Loss: 34240, Validation Loss: 51568, 166033.31261817267\n",
      "Epoch 46301, Training Loss: 33638, Validation Loss: 47539, 155499.85986793268\n",
      "Epoch 46401, Training Loss: 34933, Validation Loss: 53677, 155066.61454980198\n",
      "Epoch 46501, Training Loss: 33562, Validation Loss: 49385, 160994.31527385436\n",
      "Epoch 46601, Training Loss: 34667, Validation Loss: 52407, 173171.28314140197\n",
      "Epoch 46701, Training Loss: 34882, Validation Loss: 52297, 181350.31826915377\n",
      "Epoch 46801, Training Loss: 33144, Validation Loss: 50559, 135344.02315798806\n",
      "Epoch 46901, Training Loss: 33046, Validation Loss: 50067, 155099.68312728044\n",
      "Epoch 47001, Training Loss: 32271, Validation Loss: 47703, 156467.3156964669\n",
      "Epoch 47101, Training Loss: 34149, Validation Loss: 51044, 129244.62128108836\n",
      "Epoch 47201, Training Loss: 33677, Validation Loss: 49890, 174391.88001397374\n",
      "Epoch 47301, Training Loss: 36820, Validation Loss: 51309, 199549.10658828274\n",
      "Epoch 47401, Training Loss: 34465, Validation Loss: 49240, 153457.6249105236\n",
      "Epoch 47501, Training Loss: 34220, Validation Loss: 51709, 146691.62044685293\n",
      "Epoch 47601, Training Loss: 35624, Validation Loss: 51790, 156256.802042111\n",
      "Epoch 47701, Training Loss: 32846, Validation Loss: 49981, 154593.6517846212\n",
      "Epoch 47801, Training Loss: 34212, Validation Loss: 49691, 146999.85737869056\n",
      "Epoch 47901, Training Loss: 34686, Validation Loss: 51553, 174814.4169099722\n",
      "Epoch 48001, Training Loss: 34014, Validation Loss: 50590, 197042.85138312256\n",
      "Epoch 48101, Training Loss: 33686, Validation Loss: 49904, 151536.56767393817\n",
      "Epoch 48201, Training Loss: 34847, Validation Loss: 54855, 181032.5872966677\n",
      "Epoch 48301, Training Loss: 33418, Validation Loss: 49369, 145741.40480741992\n",
      "Epoch 48401, Training Loss: 34736, Validation Loss: 50185, 193207.2686871466\n",
      "Epoch 48501, Training Loss: 34351, Validation Loss: 49613, 143438.2842952156\n",
      "Epoch 48601, Training Loss: 33278, Validation Loss: 49008, 155389.0568076393\n",
      "Epoch 48701, Training Loss: 32446, Validation Loss: 53909, 131525.31922724922\n",
      "Epoch 48801, Training Loss: 34709, Validation Loss: 50999, 177898.86918552284\n",
      "Epoch 48901, Training Loss: 32213, Validation Loss: 49524, 133429.92597081914\n",
      "Epoch 49001, Training Loss: 34970, Validation Loss: 51145, 175411.15936775226\n",
      "Epoch 49101, Training Loss: 33158, Validation Loss: 52436, 145620.15930882844\n",
      "Epoch 49201, Training Loss: 32964, Validation Loss: 51538, 141612.50810554984\n",
      "Epoch 49301, Training Loss: 33899, Validation Loss: 51610, 158921.5201426604\n",
      "Epoch 49401, Training Loss: 34258, Validation Loss: 49493, 159056.0438868625\n",
      "Epoch 49501, Training Loss: 34413, Validation Loss: 50649, 131882.7132387206\n",
      "Epoch 49601, Training Loss: 31922, Validation Loss: 52804, 161279.08944456867\n",
      "Epoch 49701, Training Loss: 32962, Validation Loss: 50814, 137216.94296576304\n",
      "Epoch 49801, Training Loss: 34098, Validation Loss: 51464, 140232.18448201974\n",
      "Epoch 49901, Training Loss: 34860, Validation Loss: 52600, 174239.1401030169\n",
      "Epoch 50001, Training Loss: 35372, Validation Loss: 48038, 187048.63025717525\n",
      "Epoch 50101, Training Loss: 33031, Validation Loss: 48711, 162609.1418634207\n",
      "Epoch 50201, Training Loss: 33609, Validation Loss: 49771, 196864.30323937576\n",
      "Epoch 50301, Training Loss: 36514, Validation Loss: 50432, 158144.00852450627\n",
      "Epoch 50401, Training Loss: 35938, Validation Loss: 49559, 154234.534462204\n",
      "Epoch 50501, Training Loss: 33832, Validation Loss: 53995, 167611.61158419988\n",
      "Epoch 50601, Training Loss: 32772, Validation Loss: 50335, 137729.8277600513\n",
      "Epoch 50701, Training Loss: 34858, Validation Loss: 49874, 166739.6822651792\n",
      "Epoch 50801, Training Loss: 33091, Validation Loss: 52982, 157521.7723633708\n",
      "Epoch 50901, Training Loss: 33223, Validation Loss: 50416, 155738.29069490187\n",
      "Epoch 51001, Training Loss: 32995, Validation Loss: 50188, 158132.6803386757\n",
      "Epoch 51101, Training Loss: 34630, Validation Loss: 55194, 193301.69781385397\n",
      "Epoch 51201, Training Loss: 33033, Validation Loss: 51970, 160280.03911144874\n",
      "Epoch 51301, Training Loss: 33054, Validation Loss: 51880, 149266.4429006054\n",
      "Epoch 51401, Training Loss: 33129, Validation Loss: 49200, 126849.18606227264\n",
      "Epoch 51501, Training Loss: 34317, Validation Loss: 49708, 165147.42784353177\n",
      "Epoch 51601, Training Loss: 33625, Validation Loss: 52516, 148782.13480577813\n",
      "Epoch 51701, Training Loss: 34676, Validation Loss: 52780, 196949.88357194763\n",
      "Epoch 51801, Training Loss: 34152, Validation Loss: 51590, 133668.55494915997\n",
      "Epoch 51901, Training Loss: 34001, Validation Loss: 49095, 170103.2132127135\n",
      "Epoch 52001, Training Loss: 34803, Validation Loss: 46976, 181770.01235119655\n",
      "Epoch 52101, Training Loss: 35198, Validation Loss: 48960, 166632.63873064023\n",
      "Epoch 52201, Training Loss: 33015, Validation Loss: 50243, 152716.511342628\n",
      "Epoch 52301, Training Loss: 34923, Validation Loss: 52235, 142659.90189022085\n",
      "Epoch 52401, Training Loss: 32901, Validation Loss: 50197, 156460.9593298443\n",
      "Epoch 52501, Training Loss: 33234, Validation Loss: 52361, 160324.51092409564\n",
      "Epoch 52601, Training Loss: 33066, Validation Loss: 49434, 161392.40025035426\n",
      "Epoch 52701, Training Loss: 32034, Validation Loss: 50042, 133341.73067815634\n",
      "Epoch 52801, Training Loss: 32680, Validation Loss: 51078, 178378.17808265708\n",
      "Epoch 52901, Training Loss: 33498, Validation Loss: 49202, 133889.17978909338\n",
      "Epoch 53001, Training Loss: 34311, Validation Loss: 51651, 159148.1275680814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53101, Training Loss: 33802, Validation Loss: 51373, 147120.73762528092\n",
      "Epoch 53201, Training Loss: 34140, Validation Loss: 51610, 158135.19213974668\n",
      "Epoch 53301, Training Loss: 33116, Validation Loss: 52402, 135671.81228436955\n",
      "Epoch 53401, Training Loss: 33219, Validation Loss: 51591, 164135.3036070006\n",
      "Epoch 53501, Training Loss: 33669, Validation Loss: 49846, 131314.75512940943\n",
      "Epoch 53601, Training Loss: 35350, Validation Loss: 53842, 148167.96108678877\n",
      "Epoch 53701, Training Loss: 33144, Validation Loss: 49716, 158157.17003683012\n",
      "Epoch 53801, Training Loss: 30474, Validation Loss: 54193, 160341.08968067664\n",
      "Epoch 53901, Training Loss: 33939, Validation Loss: 49641, 137617.4054469911\n",
      "Epoch 54001, Training Loss: 34384, Validation Loss: 53341, 155194.98897147103\n",
      "Epoch 54101, Training Loss: 35938, Validation Loss: 48993, 183184.73450646596\n",
      "Epoch 54201, Training Loss: 33504, Validation Loss: 51057, 142476.7143587282\n",
      "Epoch 54301, Training Loss: 31950, Validation Loss: 49991, 156597.47436418335\n",
      "Epoch 54401, Training Loss: 35572, Validation Loss: 49487, 149065.77270674627\n",
      "Epoch 54501, Training Loss: 31750, Validation Loss: 51593, 143851.7360214253\n",
      "Epoch 54601, Training Loss: 32015, Validation Loss: 49267, 177373.93528000178\n",
      "Epoch 54701, Training Loss: 31723, Validation Loss: 51633, 152169.22959028682\n",
      "Epoch 54801, Training Loss: 32390, Validation Loss: 52881, 140271.17565746658\n",
      "Epoch 54901, Training Loss: 32798, Validation Loss: 48999, 151243.2158705696\n",
      "Epoch 55001, Training Loss: 32398, Validation Loss: 49042, 160661.14684246577\n",
      "Epoch 55101, Training Loss: 33312, Validation Loss: 51901, 149099.29992690188\n",
      "Epoch 55201, Training Loss: 32378, Validation Loss: 51277, 165600.04217802547\n",
      "Epoch 55301, Training Loss: 35392, Validation Loss: 51373, 165632.79143418203\n",
      "Epoch 55401, Training Loss: 34648, Validation Loss: 47947, 202102.35468175434\n",
      "Epoch 55501, Training Loss: 33025, Validation Loss: 51631, 125222.24409388207\n",
      "Epoch 55601, Training Loss: 33733, Validation Loss: 49694, 178387.01433788845\n",
      "Epoch 55701, Training Loss: 32201, Validation Loss: 51435, 172464.36917911167\n",
      "Epoch 55801, Training Loss: 32194, Validation Loss: 50716, 137817.8464731277\n",
      "Epoch 55901, Training Loss: 32733, Validation Loss: 49743, 155295.86147819512\n",
      "Epoch 56001, Training Loss: 33812, Validation Loss: 49737, 160241.9790575739\n",
      "Epoch 56101, Training Loss: 33685, Validation Loss: 51615, 161321.65804398712\n",
      "Epoch 56201, Training Loss: 35317, Validation Loss: 53943, 208523.4815708254\n",
      "Epoch 56301, Training Loss: 33220, Validation Loss: 50849, 152987.42090669973\n",
      "Epoch 56401, Training Loss: 34204, Validation Loss: 51555, 167893.373057652\n",
      "Epoch 56501, Training Loss: 32180, Validation Loss: 53676, 147962.6086272256\n",
      "Epoch 56601, Training Loss: 33229, Validation Loss: 48786, 172003.75294027597\n",
      "Epoch 56701, Training Loss: 34023, Validation Loss: 52358, 128594.21168679143\n",
      "Epoch 56801, Training Loss: 33277, Validation Loss: 50995, 167858.14518817092\n",
      "Epoch 56901, Training Loss: 31508, Validation Loss: 52476, 167851.39114051446\n",
      "Epoch 57001, Training Loss: 33569, Validation Loss: 51978, 135504.8303375585\n",
      "Epoch 57101, Training Loss: 32311, Validation Loss: 49939, 181542.92665484635\n",
      "Epoch 57201, Training Loss: 32053, Validation Loss: 50881, 157666.1036152846\n",
      "Epoch 57301, Training Loss: 32497, Validation Loss: 52002, 164130.201094719\n",
      "Epoch 57401, Training Loss: 33824, Validation Loss: 50851, 169763.73140456126\n",
      "Epoch 57501, Training Loss: 33547, Validation Loss: 50240, 180540.7671298456\n",
      "Epoch 57601, Training Loss: 32854, Validation Loss: 49949, 131500.06462719626\n",
      "Epoch 57701, Training Loss: 33465, Validation Loss: 53243, 173870.0533558182\n",
      "Epoch 57801, Training Loss: 33441, Validation Loss: 53255, 149119.53646876122\n",
      "Epoch 57901, Training Loss: 34953, Validation Loss: 50303, 119009.92763480381\n",
      "Epoch 58001, Training Loss: 34701, Validation Loss: 50339, 145836.48670776933\n",
      "Epoch 58101, Training Loss: 32284, Validation Loss: 51728, 147998.42963000844\n",
      "Epoch 58201, Training Loss: 32979, Validation Loss: 51513, 132973.17065341558\n",
      "Epoch 58301, Training Loss: 33831, Validation Loss: 50570, 135588.96103644356\n",
      "Epoch 58401, Training Loss: 33599, Validation Loss: 50761, 152032.20791300022\n",
      "Epoch 58501, Training Loss: 32321, Validation Loss: 53027, 149846.02050049655\n",
      "Epoch 58601, Training Loss: 31898, Validation Loss: 50609, 142573.55716245633\n",
      "Epoch 58701, Training Loss: 33089, Validation Loss: 51047, 155290.93432174763\n",
      "Epoch 58801, Training Loss: 32184, Validation Loss: 52779, 144893.33529561825\n",
      "Epoch 58901, Training Loss: 33197, Validation Loss: 52327, 160540.84754085998\n",
      "Epoch 59001, Training Loss: 31780, Validation Loss: 53500, 134167.35800540756\n",
      "Epoch 59101, Training Loss: 33853, Validation Loss: 49952, 127174.37511341872\n",
      "Epoch 59201, Training Loss: 33173, Validation Loss: 50061, 162707.50722702924\n",
      "Epoch 59301, Training Loss: 31719, Validation Loss: 49803, 136204.4068534475\n",
      "Epoch 59401, Training Loss: 33709, Validation Loss: 49915, 183537.9875475967\n",
      "Epoch 59501, Training Loss: 34614, Validation Loss: 47924, 224095.9433061569\n",
      "Epoch 59601, Training Loss: 34542, Validation Loss: 49968, 157400.25094431455\n",
      "Epoch 59701, Training Loss: 31695, Validation Loss: 50034, 144309.56143619696\n",
      "Epoch 59801, Training Loss: 30873, Validation Loss: 50814, 145210.1854063164\n",
      "Epoch 59901, Training Loss: 33584, Validation Loss: 52542, 161065.5239534131\n",
      "Epoch 60001, Training Loss: 34569, Validation Loss: 49806, 169831.28163045642\n",
      "Epoch 60101, Training Loss: 32346, Validation Loss: 48839, 144836.24959917826\n",
      "Epoch 60201, Training Loss: 34634, Validation Loss: 49314, 152200.75531413598\n",
      "Epoch 60301, Training Loss: 33637, Validation Loss: 51787, 179648.82819916058\n",
      "Epoch 60401, Training Loss: 32544, Validation Loss: 50571, 137651.99784726268\n",
      "Epoch 60501, Training Loss: 32130, Validation Loss: 51640, 125175.73818649646\n",
      "Epoch 60601, Training Loss: 33203, Validation Loss: 51331, 152743.41624053693\n",
      "Epoch 60701, Training Loss: 31322, Validation Loss: 51862, 142902.97931797794\n",
      "Epoch 60801, Training Loss: 31807, Validation Loss: 54680, 159089.02856697622\n",
      "Epoch 60901, Training Loss: 32966, Validation Loss: 51980, 198469.9929133407\n",
      "Epoch 61001, Training Loss: 31009, Validation Loss: 51732, 138321.12753627478\n",
      "Epoch 61101, Training Loss: 33389, Validation Loss: 53023, 154181.51684190365\n",
      "Epoch 61201, Training Loss: 34082, Validation Loss: 50583, 163732.1667403724\n",
      "Epoch 61301, Training Loss: 33292, Validation Loss: 52544, 223470.40617569862\n",
      "Epoch 61401, Training Loss: 32369, Validation Loss: 50456, 146707.434501368\n",
      "Epoch 61501, Training Loss: 32712, Validation Loss: 52147, 118081.23923256304\n",
      "Epoch 61601, Training Loss: 33545, Validation Loss: 51017, 153646.13229353368\n",
      "Epoch 61701, Training Loss: 33661, Validation Loss: 50021, 145780.15956940776\n",
      "Epoch 61801, Training Loss: 33092, Validation Loss: 52580, 146230.711534811\n",
      "Epoch 61901, Training Loss: 32689, Validation Loss: 53336, 144723.32888816053\n",
      "Epoch 62001, Training Loss: 32264, Validation Loss: 53071, 135230.24225560235\n",
      "Epoch 62101, Training Loss: 33296, Validation Loss: 52183, 149426.38158678255\n",
      "Epoch 62201, Training Loss: 33895, Validation Loss: 53099, 178169.93738463012\n",
      "Epoch 62301, Training Loss: 32768, Validation Loss: 52911, 126188.88274396867\n",
      "Epoch 62401, Training Loss: 32708, Validation Loss: 48264, 146316.16516911733\n",
      "Epoch 62501, Training Loss: 32332, Validation Loss: 52908, 146642.57393473905\n",
      "Epoch 62601, Training Loss: 32665, Validation Loss: 53087, 147433.65009540264\n",
      "Epoch 62701, Training Loss: 30757, Validation Loss: 51589, 153948.77812073356\n",
      "Epoch 62801, Training Loss: 33251, Validation Loss: 52033, 155935.19601922628\n",
      "Epoch 62901, Training Loss: 33693, Validation Loss: 52969, 208048.09076074144\n",
      "Epoch 63001, Training Loss: 34779, Validation Loss: 51001, 149375.2262448332\n",
      "Epoch 63101, Training Loss: 31162, Validation Loss: 50669, 179472.05846675034\n",
      "Epoch 63201, Training Loss: 32278, Validation Loss: 50684, 146255.4621825212\n",
      "Epoch 63301, Training Loss: 33615, Validation Loss: 53857, 145449.64155992738\n",
      "Epoch 63401, Training Loss: 32918, Validation Loss: 49517, 151227.2356486104\n",
      "Epoch 63501, Training Loss: 34518, Validation Loss: 52807, 138993.0117763892\n",
      "Epoch 63601, Training Loss: 31001, Validation Loss: 50073, 124959.970126425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63701, Training Loss: 33313, Validation Loss: 52412, 146021.1912587088\n",
      "Epoch 63801, Training Loss: 33963, Validation Loss: 50105, 128001.10597934667\n",
      "Epoch 63901, Training Loss: 32861, Validation Loss: 50056, 139984.21529803867\n",
      "Epoch 64001, Training Loss: 33187, Validation Loss: 51537, 184823.5703610128\n",
      "Epoch 64101, Training Loss: 32083, Validation Loss: 53084, 150448.3584210595\n",
      "Epoch 64201, Training Loss: 32113, Validation Loss: 50739, 145390.45541679763\n",
      "Epoch 64301, Training Loss: 33809, Validation Loss: 52472, 181871.16481139336\n",
      "Epoch 64401, Training Loss: 32639, Validation Loss: 50338, 131669.7793199067\n",
      "Epoch 64501, Training Loss: 31701, Validation Loss: 52656, 144642.62714535242\n",
      "Epoch 64601, Training Loss: 32715, Validation Loss: 53738, 160576.07717117347\n",
      "Epoch 64701, Training Loss: 32068, Validation Loss: 51172, 153107.54943612654\n",
      "Epoch 64801, Training Loss: 33669, Validation Loss: 52494, 165056.2342437982\n",
      "Epoch 64901, Training Loss: 32605, Validation Loss: 52191, 191104.19385676557\n",
      "Epoch 65001, Training Loss: 32035, Validation Loss: 49504, 156926.82584245683\n",
      "Epoch 65101, Training Loss: 36582, Validation Loss: 48375, 202273.6921579529\n",
      "Epoch 65201, Training Loss: 33064, Validation Loss: 49445, 178255.92937119064\n",
      "Epoch 65301, Training Loss: 32923, Validation Loss: 51904, 140615.69901828893\n",
      "Epoch 65401, Training Loss: 33258, Validation Loss: 53112, 143688.1466374229\n",
      "Epoch 65501, Training Loss: 32072, Validation Loss: 53668, 137225.13357239508\n",
      "Epoch 65601, Training Loss: 31945, Validation Loss: 52633, 131316.58566983734\n",
      "Epoch 65701, Training Loss: 32362, Validation Loss: 51018, 124379.70617954496\n",
      "Epoch 65801, Training Loss: 33171, Validation Loss: 50118, 127753.57568058118\n",
      "Epoch 65901, Training Loss: 31787, Validation Loss: 51654, 152385.85422206152\n",
      "Epoch 66001, Training Loss: 32314, Validation Loss: 49419, 173586.9318239485\n",
      "Epoch 66101, Training Loss: 31751, Validation Loss: 51487, 166243.66147907785\n",
      "Epoch 66201, Training Loss: 33144, Validation Loss: 52568, 160962.35264915417\n",
      "Epoch 66301, Training Loss: 33021, Validation Loss: 51983, 180251.77775389823\n",
      "Epoch 66401, Training Loss: 34423, Validation Loss: 50829, 132394.35283590478\n",
      "Epoch 66501, Training Loss: 33612, Validation Loss: 54243, 155842.38850944227\n",
      "Epoch 66601, Training Loss: 30633, Validation Loss: 51876, 160734.0119652784\n",
      "Epoch 66701, Training Loss: 34972, Validation Loss: 48581, 144639.90934933457\n",
      "Epoch 66801, Training Loss: 33134, Validation Loss: 51274, 166746.5913197699\n",
      "Epoch 66901, Training Loss: 31968, Validation Loss: 51763, 150448.1771908294\n",
      "Epoch 67001, Training Loss: 32127, Validation Loss: 52271, 145917.71782286995\n",
      "Epoch 67101, Training Loss: 34114, Validation Loss: 52136, 171213.19698378947\n",
      "Epoch 67201, Training Loss: 32642, Validation Loss: 49920, 154795.14331454536\n",
      "Epoch 67301, Training Loss: 33261, Validation Loss: 50696, 147033.3893934202\n",
      "Epoch 67401, Training Loss: 31331, Validation Loss: 52763, 159391.39702159437\n",
      "Epoch 67501, Training Loss: 31518, Validation Loss: 49905, 157936.26303454995\n",
      "Epoch 67601, Training Loss: 32756, Validation Loss: 52891, 142806.79691645293\n",
      "Epoch 67701, Training Loss: 32585, Validation Loss: 52507, 157374.42888669734\n",
      "Epoch 67801, Training Loss: 32032, Validation Loss: 51769, 139169.91695124004\n",
      "Epoch 67901, Training Loss: 31228, Validation Loss: 51042, 159989.40959143866\n",
      "Epoch 68001, Training Loss: 30776, Validation Loss: 50641, 124172.32651884164\n",
      "Epoch 68101, Training Loss: 32983, Validation Loss: 50042, 119232.3251826884\n",
      "Epoch 68201, Training Loss: 32649, Validation Loss: 54128, 151208.08345893055\n",
      "Epoch 68301, Training Loss: 32382, Validation Loss: 52091, 134974.96380614548\n",
      "Epoch 68401, Training Loss: 32359, Validation Loss: 49833, 153447.37052527722\n",
      "Epoch 68501, Training Loss: 32759, Validation Loss: 49919, 150664.19938975055\n",
      "Epoch 68601, Training Loss: 33425, Validation Loss: 52123, 189474.63656769673\n",
      "Epoch 68701, Training Loss: 31986, Validation Loss: 51401, 158794.6290809954\n",
      "Epoch 68801, Training Loss: 33386, Validation Loss: 51930, 144912.34856088986\n",
      "Epoch 68901, Training Loss: 32280, Validation Loss: 51442, 150949.423646416\n",
      "Epoch 69001, Training Loss: 32582, Validation Loss: 51212, 162612.64947858473\n",
      "Epoch 69101, Training Loss: 32201, Validation Loss: 52292, 146223.53528529874\n",
      "Epoch 69201, Training Loss: 32453, Validation Loss: 52045, 200166.64335115493\n",
      "Epoch 69301, Training Loss: 30944, Validation Loss: 53354, 138403.44670900932\n",
      "Epoch 69401, Training Loss: 33638, Validation Loss: 51189, 152850.05124394695\n",
      "Epoch 69501, Training Loss: 32688, Validation Loss: 52406, 181044.51317646742\n",
      "Epoch 69601, Training Loss: 32061, Validation Loss: 51213, 158526.78678215685\n",
      "Epoch 69701, Training Loss: 32125, Validation Loss: 50964, 164200.2066595527\n",
      "Epoch 69801, Training Loss: 33245, Validation Loss: 52608, 143335.79824046747\n",
      "Epoch 69901, Training Loss: 31775, Validation Loss: 50561, 123949.48049300474\n",
      "Epoch 70001, Training Loss: 32845, Validation Loss: 49815, 155744.56358089144\n",
      "Epoch 70101, Training Loss: 32581, Validation Loss: 52458, 140445.57636189292\n",
      "Epoch 70201, Training Loss: 32551, Validation Loss: 51392, 139113.36058822993\n",
      "Epoch 70301, Training Loss: 32065, Validation Loss: 53578, 191793.58871555288\n",
      "Epoch 70401, Training Loss: 32620, Validation Loss: 52695, 164388.45457924498\n",
      "Epoch 70501, Training Loss: 32863, Validation Loss: 50011, 150263.39983763048\n",
      "Epoch 70601, Training Loss: 34035, Validation Loss: 50651, 159750.27186243588\n",
      "Epoch 70701, Training Loss: 33525, Validation Loss: 55950, 175390.11357205414\n",
      "Epoch 70801, Training Loss: 32796, Validation Loss: 50470, 136604.66389727063\n",
      "Epoch 70901, Training Loss: 33102, Validation Loss: 50217, 140579.33555083344\n",
      "Epoch 71001, Training Loss: 31266, Validation Loss: 50016, 139468.01543850137\n",
      "Epoch 71101, Training Loss: 31406, Validation Loss: 51572, 134294.80291722645\n",
      "Epoch 71201, Training Loss: 32671, Validation Loss: 51461, 172516.2533882259\n",
      "Epoch 71301, Training Loss: 34032, Validation Loss: 52220, 162782.88255975308\n",
      "Epoch 71401, Training Loss: 32496, Validation Loss: 51500, 132230.3776544857\n",
      "Epoch 71501, Training Loss: 30627, Validation Loss: 52429, 134611.5509122931\n",
      "Epoch 71601, Training Loss: 30793, Validation Loss: 52476, 142273.2624280564\n",
      "Epoch 71701, Training Loss: 31187, Validation Loss: 51300, 152291.65664125467\n",
      "Epoch 71801, Training Loss: 34171, Validation Loss: 52665, 153575.4125776238\n",
      "Epoch 71901, Training Loss: 33164, Validation Loss: 51659, 169732.83651731184\n",
      "Epoch 72001, Training Loss: 33640, Validation Loss: 50084, 144109.02197225872\n",
      "Epoch 72101, Training Loss: 32624, Validation Loss: 51245, 150643.21966591862\n",
      "Epoch 72201, Training Loss: 33076, Validation Loss: 51472, 132044.78788508696\n",
      "Epoch 72301, Training Loss: 31827, Validation Loss: 51017, 136774.19625325763\n",
      "Epoch 72401, Training Loss: 35124, Validation Loss: 49959, 160579.49031457095\n",
      "Epoch 72501, Training Loss: 32043, Validation Loss: 52272, 145387.1382757469\n",
      "Epoch 72601, Training Loss: 32098, Validation Loss: 53118, 137393.88513181295\n",
      "Epoch 72701, Training Loss: 31258, Validation Loss: 50297, 165966.9680047079\n",
      "Epoch 72801, Training Loss: 32908, Validation Loss: 52427, 132235.40334284195\n",
      "Epoch 72901, Training Loss: 31246, Validation Loss: 51961, 144117.37826801994\n",
      "Epoch 73001, Training Loss: 31659, Validation Loss: 51334, 157779.58543651144\n",
      "Epoch 73101, Training Loss: 33012, Validation Loss: 50417, 117202.60652161665\n",
      "Epoch 73201, Training Loss: 31132, Validation Loss: 54481, 147829.57910706315\n",
      "Epoch 73301, Training Loss: 29650, Validation Loss: 51624, 135045.1343961379\n",
      "Epoch 73401, Training Loss: 31945, Validation Loss: 52296, 154793.3442743525\n",
      "Epoch 73501, Training Loss: 31848, Validation Loss: 51666, 135606.53896107586\n",
      "Epoch 73601, Training Loss: 32813, Validation Loss: 51802, 169500.03583225934\n",
      "Epoch 73701, Training Loss: 32179, Validation Loss: 51825, 144732.4917382591\n",
      "Epoch 73801, Training Loss: 31314, Validation Loss: 50074, 128657.35658986382\n",
      "Epoch 73901, Training Loss: 32482, Validation Loss: 51774, 133336.80187563936\n",
      "Epoch 74001, Training Loss: 31978, Validation Loss: 50805, 135878.09016295715\n",
      "Epoch 74101, Training Loss: 32180, Validation Loss: 53330, 138084.67555613728\n",
      "Epoch 74201, Training Loss: 33593, Validation Loss: 49475, 175004.6486126656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74301, Training Loss: 32065, Validation Loss: 52185, 150590.0803769656\n",
      "Epoch 74401, Training Loss: 31485, Validation Loss: 52324, 131205.2609913147\n",
      "Epoch 74501, Training Loss: 31478, Validation Loss: 53411, 182918.5912035278\n",
      "Epoch 74601, Training Loss: 35329, Validation Loss: 54191, 237532.285657476\n",
      "Epoch 74701, Training Loss: 32663, Validation Loss: 51046, 185448.8138605556\n",
      "Epoch 74801, Training Loss: 31370, Validation Loss: 54666, 140721.08387989324\n",
      "Epoch 74901, Training Loss: 32080, Validation Loss: 50435, 153396.0978389814\n",
      "Epoch 75001, Training Loss: 33630, Validation Loss: 52605, 153109.1399270321\n",
      "Epoch 75101, Training Loss: 30534, Validation Loss: 52254, 142881.47112354552\n",
      "Epoch 75201, Training Loss: 33355, Validation Loss: 50015, 125587.19308520874\n",
      "Epoch 75301, Training Loss: 31355, Validation Loss: 51283, 156399.95150262516\n",
      "Epoch 75401, Training Loss: 32902, Validation Loss: 52326, 148505.85644177167\n",
      "Epoch 75501, Training Loss: 32136, Validation Loss: 50073, 138810.72778596435\n",
      "Epoch 75601, Training Loss: 33019, Validation Loss: 49056, 147491.76049477892\n",
      "Epoch 75701, Training Loss: 32079, Validation Loss: 52529, 199828.697462762\n",
      "Epoch 75801, Training Loss: 31133, Validation Loss: 48904, 166875.81401983745\n",
      "Epoch 75901, Training Loss: 33118, Validation Loss: 52406, 154248.24455237814\n",
      "Epoch 76001, Training Loss: 33199, Validation Loss: 54742, 186289.38094141995\n",
      "Epoch 76101, Training Loss: 32568, Validation Loss: 52809, 152083.6718096531\n",
      "Epoch 76201, Training Loss: 31202, Validation Loss: 49711, 142304.24207437184\n",
      "Epoch 76301, Training Loss: 33194, Validation Loss: 49134, 141709.22961171632\n",
      "Epoch 76401, Training Loss: 31714, Validation Loss: 52090, 120838.64409709074\n",
      "Epoch 76501, Training Loss: 31781, Validation Loss: 52086, 150380.32654322794\n",
      "Epoch 76601, Training Loss: 33510, Validation Loss: 49508, 171674.55146726454\n",
      "Epoch 76701, Training Loss: 33179, Validation Loss: 50446, 138342.2397481209\n",
      "Epoch 76801, Training Loss: 30884, Validation Loss: 50170, 144594.46540597285\n",
      "Epoch 76901, Training Loss: 31592, Validation Loss: 51886, 146349.9584968753\n",
      "Epoch 77001, Training Loss: 30900, Validation Loss: 48819, 134049.5255114798\n",
      "Epoch 77101, Training Loss: 33696, Validation Loss: 50836, 159430.60625171813\n",
      "Epoch 77201, Training Loss: 31930, Validation Loss: 50126, 150853.34389880527\n",
      "Epoch 77301, Training Loss: 30268, Validation Loss: 50010, 121927.56144434088\n",
      "Epoch 77401, Training Loss: 29163, Validation Loss: 51334, 151330.98043166613\n",
      "Epoch 77501, Training Loss: 31996, Validation Loss: 51194, 159757.0132694196\n",
      "Epoch 77601, Training Loss: 31165, Validation Loss: 53272, 144962.35150035366\n",
      "Epoch 77701, Training Loss: 31443, Validation Loss: 53094, 139940.57624161724\n",
      "Epoch 77801, Training Loss: 31531, Validation Loss: 50988, 146027.57613587112\n",
      "Epoch 77901, Training Loss: 31705, Validation Loss: 54447, 168982.576482133\n",
      "Epoch 78001, Training Loss: 32867, Validation Loss: 49909, 161095.87514652807\n",
      "Epoch 78101, Training Loss: 32264, Validation Loss: 52428, 152501.64502314248\n",
      "Epoch 78201, Training Loss: 32740, Validation Loss: 52405, 137531.30159539325\n",
      "Epoch 78301, Training Loss: 32324, Validation Loss: 52750, 154557.89128608216\n",
      "Epoch 78401, Training Loss: 31710, Validation Loss: 52430, 154960.62129373042\n",
      "Epoch 78501, Training Loss: 31360, Validation Loss: 51220, 149225.5991527005\n",
      "Epoch 78601, Training Loss: 33085, Validation Loss: 50184, 133340.47401532662\n",
      "Epoch 78701, Training Loss: 31442, Validation Loss: 50322, 129161.13328294356\n",
      "Epoch 78801, Training Loss: 30581, Validation Loss: 53979, 174875.4548966444\n",
      "Epoch 78901, Training Loss: 32085, Validation Loss: 51668, 125113.6742337967\n",
      "Epoch 79001, Training Loss: 33654, Validation Loss: 52567, 146456.45771396815\n",
      "Epoch 79101, Training Loss: 32305, Validation Loss: 50545, 119918.11590777166\n",
      "Epoch 79201, Training Loss: 32176, Validation Loss: 50755, 169591.16825371297\n",
      "Epoch 79301, Training Loss: 32526, Validation Loss: 54711, 164907.5318229357\n",
      "Epoch 79401, Training Loss: 31874, Validation Loss: 52043, 136103.09876070073\n",
      "Epoch 79501, Training Loss: 31005, Validation Loss: 52228, 148217.06442519458\n",
      "Epoch 79601, Training Loss: 32316, Validation Loss: 53800, 151934.49260232243\n",
      "Epoch 79701, Training Loss: 30627, Validation Loss: 51765, 144653.44070469038\n",
      "Epoch 79801, Training Loss: 32227, Validation Loss: 50029, 158270.47232215694\n",
      "Epoch 79901, Training Loss: 31985, Validation Loss: 50236, 140016.90524337807\n",
      "Epoch 80001, Training Loss: 31199, Validation Loss: 51474, 154708.99064354482\n",
      "Epoch 80101, Training Loss: 33533, Validation Loss: 56873, 169703.452634303\n",
      "Epoch 80201, Training Loss: 32515, Validation Loss: 53307, 144806.23948419938\n",
      "Epoch 80301, Training Loss: 32666, Validation Loss: 52204, 146405.0062346027\n",
      "Epoch 80401, Training Loss: 31977, Validation Loss: 53188, 126502.53129378535\n",
      "Epoch 80501, Training Loss: 31057, Validation Loss: 50544, 130590.39678232613\n",
      "Epoch 80601, Training Loss: 31974, Validation Loss: 50340, 125781.6200567679\n",
      "Epoch 80701, Training Loss: 31512, Validation Loss: 55246, 146276.84403668105\n",
      "Epoch 80801, Training Loss: 32606, Validation Loss: 52492, 161320.42321750527\n",
      "Epoch 80901, Training Loss: 30401, Validation Loss: 51003, 136665.93185610452\n",
      "Epoch 81001, Training Loss: 32856, Validation Loss: 50970, 170148.31565602886\n",
      "Epoch 81101, Training Loss: 31783, Validation Loss: 52975, 139460.6307833147\n",
      "Epoch 81201, Training Loss: 33455, Validation Loss: 52685, 118439.66525593423\n",
      "Epoch 81301, Training Loss: 31611, Validation Loss: 50526, 128412.05622701025\n",
      "Epoch 81401, Training Loss: 30865, Validation Loss: 51880, 151999.50174248902\n",
      "Epoch 81501, Training Loss: 31070, Validation Loss: 53859, 129834.37599125858\n",
      "Epoch 81601, Training Loss: 32420, Validation Loss: 49552, 167731.1052672284\n",
      "Epoch 81701, Training Loss: 32238, Validation Loss: 51269, 164021.28946388484\n",
      "Epoch 81801, Training Loss: 30243, Validation Loss: 51165, 126689.58517443007\n",
      "Epoch 81901, Training Loss: 33247, Validation Loss: 51414, 153535.44505477077\n",
      "Epoch 82001, Training Loss: 32014, Validation Loss: 51239, 151428.09748616995\n",
      "Epoch 82101, Training Loss: 32440, Validation Loss: 52155, 152905.5364955633\n",
      "Epoch 82201, Training Loss: 31964, Validation Loss: 50336, 160063.88301096793\n",
      "Epoch 82301, Training Loss: 30614, Validation Loss: 52306, 133524.31941692563\n",
      "Epoch 82401, Training Loss: 30796, Validation Loss: 53277, 137985.33895610907\n",
      "Epoch 82501, Training Loss: 32789, Validation Loss: 51884, 149461.29120195867\n",
      "Epoch 82601, Training Loss: 33975, Validation Loss: 54267, 167040.94709859206\n",
      "Epoch 82701, Training Loss: 30630, Validation Loss: 52442, 125182.2304996826\n",
      "Epoch 82801, Training Loss: 31044, Validation Loss: 54150, 144424.35752395104\n",
      "Epoch 82901, Training Loss: 30438, Validation Loss: 51804, 140870.1781859225\n",
      "Epoch 83001, Training Loss: 31062, Validation Loss: 53077, 161017.57377385485\n",
      "Epoch 83101, Training Loss: 30046, Validation Loss: 50945, 153604.01140289172\n",
      "Epoch 83201, Training Loss: 30944, Validation Loss: 52000, 141639.4853268591\n",
      "Epoch 83301, Training Loss: 32380, Validation Loss: 53012, 164765.50760709876\n",
      "Epoch 83401, Training Loss: 33389, Validation Loss: 52062, 137106.84565014835\n",
      "Epoch 83501, Training Loss: 31019, Validation Loss: 52507, 156797.5865225722\n",
      "Epoch 83601, Training Loss: 30602, Validation Loss: 53317, 151800.2830407974\n",
      "Epoch 83701, Training Loss: 30172, Validation Loss: 52868, 125798.52436648088\n",
      "Epoch 83801, Training Loss: 30812, Validation Loss: 53934, 138950.43706919072\n",
      "Epoch 83901, Training Loss: 34439, Validation Loss: 58040, 192031.2900245795\n",
      "Epoch 84001, Training Loss: 30680, Validation Loss: 53633, 118545.6986848234\n",
      "Epoch 84101, Training Loss: 33922, Validation Loss: 49705, 157628.66115803187\n",
      "Epoch 84201, Training Loss: 32971, Validation Loss: 53618, 193027.85921913767\n",
      "Epoch 84301, Training Loss: 32420, Validation Loss: 53322, 117017.10336386408\n",
      "Epoch 84401, Training Loss: 32886, Validation Loss: 50208, 149844.46638543037\n",
      "Epoch 84501, Training Loss: 31890, Validation Loss: 52343, 149595.74541054846\n",
      "Epoch 84601, Training Loss: 32271, Validation Loss: 52204, 151306.3087217494\n",
      "Epoch 84701, Training Loss: 32130, Validation Loss: 51658, 169378.29683458564\n",
      "Epoch 84801, Training Loss: 32337, Validation Loss: 50326, 136998.36868632442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84901, Training Loss: 31948, Validation Loss: 48731, 152706.66361101755\n",
      "Epoch 85001, Training Loss: 31316, Validation Loss: 51480, 152741.9011628359\n",
      "Epoch 85101, Training Loss: 32223, Validation Loss: 50591, 139995.7107492456\n",
      "Epoch 85201, Training Loss: 30868, Validation Loss: 52096, 146773.5556713338\n",
      "Epoch 85301, Training Loss: 31055, Validation Loss: 52824, 135261.6725110883\n",
      "Epoch 85401, Training Loss: 32068, Validation Loss: 49606, 139769.77997263643\n",
      "Epoch 85501, Training Loss: 32234, Validation Loss: 53312, 156149.1514790379\n",
      "Epoch 85601, Training Loss: 31978, Validation Loss: 52360, 163177.76341409684\n",
      "Epoch 85701, Training Loss: 32170, Validation Loss: 52204, 146413.1989415758\n",
      "Epoch 85801, Training Loss: 30940, Validation Loss: 49338, 124416.44693623681\n",
      "Epoch 85901, Training Loss: 32783, Validation Loss: 52135, 174730.54279553035\n",
      "Epoch 86001, Training Loss: 30104, Validation Loss: 51809, 118637.95770356509\n",
      "Epoch 86101, Training Loss: 34430, Validation Loss: 51694, 173354.82125509036\n",
      "Epoch 86201, Training Loss: 32460, Validation Loss: 52314, 138081.75990888398\n",
      "Epoch 86301, Training Loss: 30379, Validation Loss: 50749, 107719.44710695965\n",
      "Epoch 86401, Training Loss: 32703, Validation Loss: 53872, 155691.1341632192\n",
      "Epoch 86501, Training Loss: 30193, Validation Loss: 50284, 122586.99947583873\n",
      "Epoch 86601, Training Loss: 31303, Validation Loss: 53559, 148468.44627879595\n",
      "Epoch 86701, Training Loss: 31584, Validation Loss: 50054, 137626.33244826645\n",
      "Epoch 86801, Training Loss: 32087, Validation Loss: 50646, 170219.1559262562\n",
      "Epoch 86901, Training Loss: 31434, Validation Loss: 51599, 151740.05481092568\n",
      "Epoch 87001, Training Loss: 32558, Validation Loss: 52925, 150058.54664306476\n",
      "Epoch 87101, Training Loss: 31087, Validation Loss: 51752, 154284.5634867371\n",
      "Epoch 87201, Training Loss: 31933, Validation Loss: 51620, 128117.31425712588\n",
      "Epoch 87301, Training Loss: 29644, Validation Loss: 52256, 140094.65749358822\n",
      "Epoch 87401, Training Loss: 31653, Validation Loss: 51579, 148898.46553533053\n",
      "Epoch 87501, Training Loss: 32113, Validation Loss: 50999, 142098.85403894997\n",
      "Epoch 87601, Training Loss: 31750, Validation Loss: 52335, 157234.28831051078\n",
      "Epoch 87701, Training Loss: 31283, Validation Loss: 54080, 145010.85870340216\n",
      "Epoch 87801, Training Loss: 32249, Validation Loss: 50424, 124822.63945789875\n",
      "Epoch 87901, Training Loss: 30235, Validation Loss: 50627, 117952.92112707505\n",
      "Epoch 88001, Training Loss: 30515, Validation Loss: 50902, 150502.3941788907\n",
      "Epoch 88101, Training Loss: 30428, Validation Loss: 52405, 152497.72722098156\n",
      "Epoch 88201, Training Loss: 29571, Validation Loss: 53153, 112197.51858885688\n",
      "Epoch 88301, Training Loss: 30973, Validation Loss: 51365, 155402.73174369597\n",
      "Epoch 88401, Training Loss: 32235, Validation Loss: 52931, 144417.1167169576\n",
      "Epoch 88501, Training Loss: 30586, Validation Loss: 50198, 134244.65947950695\n",
      "Epoch 88601, Training Loss: 31226, Validation Loss: 54121, 188744.20911691777\n",
      "Epoch 88701, Training Loss: 33694, Validation Loss: 52907, 139836.6599116157\n",
      "Epoch 88801, Training Loss: 30307, Validation Loss: 51385, 166820.7693823247\n",
      "Epoch 88901, Training Loss: 31997, Validation Loss: 50343, 133396.77453057776\n",
      "Epoch 89001, Training Loss: 30234, Validation Loss: 51750, 136483.04175324517\n",
      "Epoch 89101, Training Loss: 31955, Validation Loss: 51154, 161540.7308486982\n",
      "Epoch 89201, Training Loss: 31158, Validation Loss: 51018, 135350.36455724208\n",
      "Epoch 89301, Training Loss: 30139, Validation Loss: 52106, 131001.79857614198\n",
      "Epoch 89401, Training Loss: 32738, Validation Loss: 50900, 148905.60174019943\n",
      "Epoch 89501, Training Loss: 34692, Validation Loss: 49709, 156226.906666299\n",
      "Epoch 89601, Training Loss: 32973, Validation Loss: 52516, 184496.61530952554\n",
      "Epoch 89701, Training Loss: 33500, Validation Loss: 51527, 151700.8369705361\n",
      "Epoch 89801, Training Loss: 31770, Validation Loss: 52618, 135039.86973366898\n",
      "Epoch 89901, Training Loss: 31969, Validation Loss: 52234, 137353.35139842625\n",
      "Epoch 90001, Training Loss: 32810, Validation Loss: 50128, 172025.5906881111\n",
      "Epoch 90101, Training Loss: 32809, Validation Loss: 53861, 159865.54768886013\n",
      "Epoch 90201, Training Loss: 31556, Validation Loss: 52545, 137067.89447453764\n",
      "Epoch 90301, Training Loss: 31729, Validation Loss: 50637, 160450.4867920712\n",
      "Epoch 90401, Training Loss: 30302, Validation Loss: 52281, 129158.02432518441\n",
      "Epoch 90501, Training Loss: 31853, Validation Loss: 51853, 134396.2581913916\n",
      "Epoch 90601, Training Loss: 33873, Validation Loss: 50248, 171016.364378699\n",
      "Epoch 90701, Training Loss: 31548, Validation Loss: 50524, 135764.76551106566\n",
      "Epoch 90801, Training Loss: 31815, Validation Loss: 51012, 122995.65957237458\n",
      "Epoch 90901, Training Loss: 35606, Validation Loss: 52188, 168838.13411613237\n",
      "Epoch 91001, Training Loss: 32738, Validation Loss: 51337, 121619.322820326\n",
      "Epoch 91101, Training Loss: 32323, Validation Loss: 52639, 190610.7739608306\n",
      "Epoch 91201, Training Loss: 34283, Validation Loss: 52019, 152954.4115545847\n",
      "Epoch 91301, Training Loss: 31959, Validation Loss: 51001, 137236.4470163404\n",
      "Epoch 91401, Training Loss: 30682, Validation Loss: 54186, 173167.18003658165\n",
      "Epoch 91501, Training Loss: 32807, Validation Loss: 54904, 190098.46481231618\n",
      "Epoch 91601, Training Loss: 30958, Validation Loss: 49346, 138515.36184633375\n",
      "Epoch 91701, Training Loss: 32622, Validation Loss: 54138, 167230.20883313357\n",
      "Epoch 91801, Training Loss: 31625, Validation Loss: 52938, 173006.86045581097\n",
      "Epoch 91901, Training Loss: 33590, Validation Loss: 54077, 156731.67324182528\n",
      "Epoch 92001, Training Loss: 32376, Validation Loss: 50586, 147354.77434825036\n",
      "Epoch 92101, Training Loss: 32664, Validation Loss: 51192, 176319.60774467618\n",
      "Epoch 92201, Training Loss: 32683, Validation Loss: 51172, 146064.43534261917\n",
      "Epoch 92301, Training Loss: 30559, Validation Loss: 49211, 136727.5290506671\n",
      "Epoch 92401, Training Loss: 31734, Validation Loss: 53795, 157472.0852256483\n",
      "Epoch 92501, Training Loss: 31656, Validation Loss: 55062, 175514.60072069382\n",
      "Epoch 92601, Training Loss: 32052, Validation Loss: 50364, 179501.41560107414\n",
      "Epoch 92701, Training Loss: 33250, Validation Loss: 53901, 147423.82893787982\n",
      "Epoch 92801, Training Loss: 30664, Validation Loss: 52170, 150630.37793237812\n",
      "Epoch 92901, Training Loss: 32993, Validation Loss: 53791, 196289.7168217975\n",
      "Epoch 93001, Training Loss: 31557, Validation Loss: 50735, 127088.6307348155\n",
      "Epoch 93101, Training Loss: 32327, Validation Loss: 50889, 160012.512471378\n",
      "Epoch 93201, Training Loss: 31840, Validation Loss: 49961, 135769.39141215794\n",
      "Epoch 93301, Training Loss: 32786, Validation Loss: 57352, 210290.73191408528\n",
      "Epoch 93401, Training Loss: 32419, Validation Loss: 51422, 133103.65094716338\n",
      "Epoch 93501, Training Loss: 32606, Validation Loss: 51856, 122199.135316939\n",
      "Epoch 93601, Training Loss: 33072, Validation Loss: 52509, 141224.63227083307\n",
      "Epoch 93701, Training Loss: 31392, Validation Loss: 50382, 141206.94655009988\n",
      "Epoch 93801, Training Loss: 33284, Validation Loss: 53814, 146559.89725428712\n",
      "Epoch 93901, Training Loss: 32340, Validation Loss: 51951, 145236.93572827376\n",
      "Epoch 94001, Training Loss: 32929, Validation Loss: 54970, 144725.25166075284\n",
      "Epoch 94101, Training Loss: 31627, Validation Loss: 52723, 114096.82403708599\n",
      "Epoch 94201, Training Loss: 31348, Validation Loss: 52328, 130322.06849123635\n",
      "Epoch 94301, Training Loss: 32690, Validation Loss: 58269, 188900.48770762858\n",
      "Epoch 94401, Training Loss: 31553, Validation Loss: 51979, 148898.39890992714\n",
      "Epoch 94501, Training Loss: 31337, Validation Loss: 50519, 113293.69946756357\n",
      "Epoch 94601, Training Loss: 33148, Validation Loss: 51950, 177416.05447369142\n",
      "Epoch 94701, Training Loss: 31097, Validation Loss: 52987, 159594.3890242766\n",
      "Epoch 94801, Training Loss: 33043, Validation Loss: 51162, 157359.67171465926\n",
      "Epoch 94901, Training Loss: 32798, Validation Loss: 52523, 140137.76575379586\n",
      "Epoch 95001, Training Loss: 31984, Validation Loss: 50833, 154527.21460745818\n",
      "Epoch 95101, Training Loss: 31496, Validation Loss: 51362, 160426.6905055536\n",
      "Epoch 95201, Training Loss: 31652, Validation Loss: 50757, 162643.9945191957\n",
      "Epoch 95301, Training Loss: 32390, Validation Loss: 49547, 143655.31035371317\n",
      "Epoch 95401, Training Loss: 31883, Validation Loss: 51335, 160707.0563447461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95501, Training Loss: 30378, Validation Loss: 53260, 135574.3597107656\n",
      "Epoch 95601, Training Loss: 31718, Validation Loss: 52000, 140306.97918874669\n",
      "Epoch 95701, Training Loss: 30160, Validation Loss: 53364, 142847.39436308743\n",
      "Epoch 95801, Training Loss: 32122, Validation Loss: 50666, 176674.3501751932\n",
      "Epoch 95901, Training Loss: 33517, Validation Loss: 54140, 169429.93395415234\n",
      "Epoch 96001, Training Loss: 31980, Validation Loss: 50330, 165320.22891741042\n",
      "Epoch 96101, Training Loss: 31574, Validation Loss: 51865, 136754.72670149757\n",
      "Epoch 96201, Training Loss: 31455, Validation Loss: 53141, 148979.76433422725\n",
      "Epoch 96301, Training Loss: 32748, Validation Loss: 53342, 138248.55016798663\n",
      "Epoch 96401, Training Loss: 31702, Validation Loss: 51467, 197371.04324477626\n",
      "Epoch 96501, Training Loss: 32285, Validation Loss: 52270, 123605.4969012707\n",
      "Epoch 96601, Training Loss: 31374, Validation Loss: 52917, 175345.13003774793\n",
      "Epoch 96701, Training Loss: 32797, Validation Loss: 52335, 142786.76540219036\n",
      "Epoch 96801, Training Loss: 32809, Validation Loss: 54100, 153239.05575220758\n",
      "Epoch 96901, Training Loss: 32137, Validation Loss: 50078, 158554.31014645548\n",
      "Epoch 97001, Training Loss: 31308, Validation Loss: 53461, 138452.80046214347\n",
      "Epoch 97101, Training Loss: 32127, Validation Loss: 54643, 158564.64737810282\n",
      "Epoch 97201, Training Loss: 32662, Validation Loss: 51685, 164462.68270357102\n",
      "Epoch 97301, Training Loss: 29446, Validation Loss: 51395, 136408.41500005353\n",
      "Epoch 97401, Training Loss: 31730, Validation Loss: 51670, 120138.19667897075\n",
      "Epoch 97501, Training Loss: 31419, Validation Loss: 52643, 148750.2670105805\n",
      "Epoch 97601, Training Loss: 29913, Validation Loss: 52756, 144024.10309849758\n",
      "Epoch 97701, Training Loss: 31688, Validation Loss: 53239, 134283.23753995443\n",
      "Epoch 97801, Training Loss: 33367, Validation Loss: 50064, 162612.36200448466\n",
      "Epoch 97901, Training Loss: 32179, Validation Loss: 51939, 157082.91773992573\n",
      "Epoch 98001, Training Loss: 31408, Validation Loss: 53069, 154143.00398182002\n",
      "Epoch 98101, Training Loss: 31501, Validation Loss: 53102, 163733.58883836557\n",
      "Epoch 98201, Training Loss: 31382, Validation Loss: 52064, 147915.20791912894\n",
      "Epoch 98301, Training Loss: 31219, Validation Loss: 52745, 165246.02054645488\n",
      "Epoch 98401, Training Loss: 30686, Validation Loss: 52545, 150972.34073080742\n",
      "Epoch 98501, Training Loss: 31914, Validation Loss: 52781, 139931.73064064002\n",
      "Epoch 98601, Training Loss: 31615, Validation Loss: 51268, 141836.9424181938\n",
      "Epoch 98701, Training Loss: 32170, Validation Loss: 50395, 171097.21395301394\n",
      "Epoch 98801, Training Loss: 32100, Validation Loss: 51042, 150540.13809694035\n",
      "Epoch 98901, Training Loss: 31689, Validation Loss: 53407, 165243.0653609145\n",
      "Epoch 99001, Training Loss: 32872, Validation Loss: 52612, 139894.94225133574\n",
      "Epoch 99101, Training Loss: 31942, Validation Loss: 54305, 138666.9398807264\n",
      "Epoch 99201, Training Loss: 29153, Validation Loss: 54044, 132090.4939469843\n",
      "Epoch 99301, Training Loss: 31845, Validation Loss: 51104, 155720.91078467434\n",
      "Epoch 99401, Training Loss: 31148, Validation Loss: 51991, 142625.39554241858\n",
      "Epoch 99501, Training Loss: 30336, Validation Loss: 49864, 130379.08052274254\n",
      "Epoch 99601, Training Loss: 30507, Validation Loss: 53081, 137067.10627263592\n",
      "Epoch 99701, Training Loss: 31396, Validation Loss: 51131, 140243.89551683803\n",
      "Epoch 99801, Training Loss: 31109, Validation Loss: 51700, 126875.75483656205\n",
      "Epoch 99901, Training Loss: 32455, Validation Loss: 51624, 204431.33826737228\n",
      "Epoch 100001, Training Loss: 33489, Validation Loss: 53273, 142280.68624723377\n",
      "Epoch 100101, Training Loss: 33743, Validation Loss: 51676, 138106.2364518096\n",
      "Epoch 100201, Training Loss: 32558, Validation Loss: 51984, 168742.56356065636\n",
      "Epoch 100301, Training Loss: 30577, Validation Loss: 52190, 132719.19065862824\n",
      "Epoch 100401, Training Loss: 33507, Validation Loss: 54187, 150761.55469952297\n",
      "Epoch 100501, Training Loss: 33197, Validation Loss: 53868, 144414.49617575775\n",
      "Epoch 100601, Training Loss: 32439, Validation Loss: 52265, 149543.91109895057\n",
      "Epoch 100701, Training Loss: 32391, Validation Loss: 55181, 178032.0207309862\n",
      "Epoch 100801, Training Loss: 30187, Validation Loss: 51932, 148266.4439605167\n",
      "Epoch 100901, Training Loss: 32206, Validation Loss: 52924, 127551.81753177964\n",
      "Epoch 101001, Training Loss: 29656, Validation Loss: 52460, 158934.77107291753\n",
      "Epoch 101101, Training Loss: 31838, Validation Loss: 50519, 154591.57166615204\n",
      "Epoch 101201, Training Loss: 31664, Validation Loss: 51418, 139764.089002387\n",
      "Epoch 101301, Training Loss: 30410, Validation Loss: 51463, 133896.2402217626\n",
      "Epoch 101401, Training Loss: 32994, Validation Loss: 51852, 141231.39326158314\n",
      "Epoch 101501, Training Loss: 31075, Validation Loss: 53312, 181917.23990483352\n",
      "Epoch 101601, Training Loss: 31220, Validation Loss: 54298, 136918.1537685856\n",
      "Epoch 101701, Training Loss: 30834, Validation Loss: 54751, 152397.7448087472\n",
      "Epoch 101801, Training Loss: 31686, Validation Loss: 52363, 135691.81509527276\n",
      "Epoch 101901, Training Loss: 31769, Validation Loss: 52849, 155325.7616621544\n",
      "Epoch 102001, Training Loss: 30976, Validation Loss: 50425, 140527.65117193616\n",
      "Epoch 102101, Training Loss: 32627, Validation Loss: 52369, 164011.9618069577\n",
      "Epoch 102201, Training Loss: 30710, Validation Loss: 50863, 152492.08653774555\n",
      "Epoch 102301, Training Loss: 31574, Validation Loss: 50978, 124848.64758042041\n",
      "Epoch 102401, Training Loss: 32521, Validation Loss: 52171, 158452.88949842603\n",
      "Epoch 102501, Training Loss: 30201, Validation Loss: 52239, 142527.15822498058\n",
      "Epoch 102601, Training Loss: 31561, Validation Loss: 52736, 134691.12851455773\n",
      "Epoch 102701, Training Loss: 30226, Validation Loss: 52950, 126571.36772390427\n",
      "Epoch 102801, Training Loss: 30310, Validation Loss: 51492, 146381.00142159878\n",
      "Epoch 102901, Training Loss: 30051, Validation Loss: 52875, 139127.40153087504\n",
      "Epoch 103001, Training Loss: 32118, Validation Loss: 55459, 163311.47742636906\n",
      "Epoch 103101, Training Loss: 30876, Validation Loss: 54997, 164866.91468894272\n",
      "Epoch 103201, Training Loss: 33681, Validation Loss: 51669, 147094.90596365795\n",
      "Epoch 103301, Training Loss: 33116, Validation Loss: 54657, 133664.8728978802\n",
      "Epoch 103401, Training Loss: 31107, Validation Loss: 52255, 129084.61703770724\n",
      "Epoch 103501, Training Loss: 31977, Validation Loss: 51708, 178529.22975581078\n",
      "Epoch 103601, Training Loss: 31563, Validation Loss: 53493, 157613.7114752321\n",
      "Epoch 103701, Training Loss: 32493, Validation Loss: 52286, 150239.47296362906\n",
      "Epoch 103801, Training Loss: 31099, Validation Loss: 53442, 144813.05605709867\n",
      "Epoch 103901, Training Loss: 32693, Validation Loss: 53212, 139589.27603599345\n",
      "Epoch 104001, Training Loss: 30855, Validation Loss: 50333, 152460.1579942134\n",
      "Epoch 104101, Training Loss: 32174, Validation Loss: 52365, 144720.01198993486\n",
      "Epoch 104201, Training Loss: 33045, Validation Loss: 52873, 107065.79298602518\n",
      "Epoch 104301, Training Loss: 29834, Validation Loss: 53540, 151723.60839151248\n",
      "Epoch 104401, Training Loss: 32784, Validation Loss: 53442, 153602.45489119203\n",
      "Epoch 104501, Training Loss: 31863, Validation Loss: 52279, 165363.97716417778\n",
      "Epoch 104601, Training Loss: 30153, Validation Loss: 52043, 152059.75551022962\n",
      "Epoch 104701, Training Loss: 31276, Validation Loss: 51923, 151483.83042146714\n",
      "Epoch 104801, Training Loss: 30901, Validation Loss: 51865, 135626.41034591725\n",
      "Epoch 104901, Training Loss: 33436, Validation Loss: 53367, 145682.00148187243\n",
      "Epoch 105001, Training Loss: 30992, Validation Loss: 52140, 160745.58905883788\n",
      "Epoch 105101, Training Loss: 33495, Validation Loss: 55866, 170229.4838711207\n",
      "Epoch 105201, Training Loss: 29976, Validation Loss: 53722, 141870.99608069294\n",
      "Epoch 105301, Training Loss: 30460, Validation Loss: 52828, 135238.29012589488\n",
      "Epoch 105401, Training Loss: 31303, Validation Loss: 48632, 149342.97691478612\n",
      "Epoch 105501, Training Loss: 30946, Validation Loss: 51632, 141732.28881075786\n",
      "Epoch 105601, Training Loss: 30916, Validation Loss: 53779, 164274.2512373071\n",
      "Epoch 105701, Training Loss: 29950, Validation Loss: 51240, 120080.8190573244\n",
      "Epoch 105801, Training Loss: 31944, Validation Loss: 51155, 161596.11689823034\n",
      "Epoch 105901, Training Loss: 30543, Validation Loss: 51007, 124768.57853826186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106001, Training Loss: 31454, Validation Loss: 51544, 152032.6010142792\n",
      "Epoch 106101, Training Loss: 32617, Validation Loss: 52406, 131940.4633030273\n",
      "Epoch 106201, Training Loss: 31729, Validation Loss: 51603, 158995.35006577737\n",
      "Epoch 106301, Training Loss: 31051, Validation Loss: 49973, 134157.42795973393\n",
      "Epoch 106401, Training Loss: 32336, Validation Loss: 50877, 174711.76280677697\n",
      "Epoch 106501, Training Loss: 32022, Validation Loss: 52608, 125086.9840731237\n",
      "Epoch 106601, Training Loss: 31043, Validation Loss: 52175, 146323.3771307456\n",
      "Epoch 106701, Training Loss: 31196, Validation Loss: 51442, 122597.886259986\n",
      "Epoch 106801, Training Loss: 32769, Validation Loss: 51504, 172771.30996549092\n",
      "Epoch 106901, Training Loss: 32100, Validation Loss: 52591, 171043.47042516756\n",
      "Epoch 107001, Training Loss: 31751, Validation Loss: 52553, 142322.3887985452\n",
      "Epoch 107101, Training Loss: 32117, Validation Loss: 51460, 146450.27889787348\n",
      "Epoch 107201, Training Loss: 31552, Validation Loss: 53403, 158012.73722350373\n",
      "Epoch 107301, Training Loss: 31638, Validation Loss: 51163, 134020.72033427423\n",
      "Epoch 107401, Training Loss: 31590, Validation Loss: 53580, 121436.84846216184\n",
      "Epoch 107501, Training Loss: 31740, Validation Loss: 54264, 149301.91261806563\n",
      "Epoch 107601, Training Loss: 31927, Validation Loss: 55164, 155609.0618196829\n",
      "Epoch 107701, Training Loss: 30421, Validation Loss: 53312, 140564.45797349693\n",
      "Epoch 107801, Training Loss: 33253, Validation Loss: 52518, 140888.9890969234\n",
      "Epoch 107901, Training Loss: 31368, Validation Loss: 51819, 130477.82221403843\n",
      "Epoch 108001, Training Loss: 30015, Validation Loss: 52938, 151506.14763071775\n",
      "Epoch 108101, Training Loss: 30133, Validation Loss: 52942, 133852.36709726506\n",
      "Epoch 108201, Training Loss: 32476, Validation Loss: 52500, 141014.68341401513\n",
      "Epoch 108301, Training Loss: 31816, Validation Loss: 53598, 151593.45807074563\n",
      "Epoch 108401, Training Loss: 32048, Validation Loss: 52158, 157215.70527108276\n",
      "Epoch 108501, Training Loss: 30862, Validation Loss: 52543, 134786.9569560749\n",
      "Epoch 108601, Training Loss: 29599, Validation Loss: 50337, 142455.9409677577\n",
      "Epoch 108701, Training Loss: 30212, Validation Loss: 54299, 132418.4638408665\n",
      "Epoch 108801, Training Loss: 31848, Validation Loss: 52676, 190509.71542928513\n",
      "Epoch 108901, Training Loss: 32503, Validation Loss: 51777, 145784.49273323765\n",
      "Epoch 109001, Training Loss: 31018, Validation Loss: 51384, 166476.46059591565\n",
      "Epoch 109101, Training Loss: 31532, Validation Loss: 53550, 147339.6580920244\n",
      "Epoch 109201, Training Loss: 32138, Validation Loss: 52453, 148141.5424254837\n",
      "Epoch 109301, Training Loss: 32015, Validation Loss: 57444, 188098.24500130545\n",
      "Epoch 109401, Training Loss: 32383, Validation Loss: 52453, 133983.27965236272\n",
      "Epoch 109501, Training Loss: 31271, Validation Loss: 52163, 161778.89503867074\n",
      "Epoch 109601, Training Loss: 31416, Validation Loss: 51855, 145145.05040691327\n",
      "Epoch 109701, Training Loss: 31308, Validation Loss: 51831, 144158.29464039108\n",
      "Epoch 109801, Training Loss: 32060, Validation Loss: 53583, 130698.30680448911\n",
      "Epoch 109901, Training Loss: 31250, Validation Loss: 51825, 126953.52191480486\n",
      "Epoch 110001, Training Loss: 31622, Validation Loss: 51405, 156606.59121825956\n",
      "Epoch 110101, Training Loss: 31803, Validation Loss: 51603, 174708.6831290288\n",
      "Epoch 110201, Training Loss: 32706, Validation Loss: 50554, 170137.11866921364\n",
      "Epoch 110301, Training Loss: 30766, Validation Loss: 53343, 139317.69302746124\n",
      "Epoch 110401, Training Loss: 30426, Validation Loss: 52306, 142407.752517914\n",
      "Epoch 110501, Training Loss: 29545, Validation Loss: 53176, 151814.32715608488\n",
      "Epoch 110601, Training Loss: 31736, Validation Loss: 54132, 135381.946457172\n",
      "Epoch 110701, Training Loss: 30855, Validation Loss: 52117, 153319.43221634132\n",
      "Epoch 110801, Training Loss: 31877, Validation Loss: 52252, 138782.56899022934\n",
      "Epoch 110901, Training Loss: 32275, Validation Loss: 54783, 158256.27231862914\n",
      "Epoch 111001, Training Loss: 31538, Validation Loss: 51922, 130400.4336066925\n",
      "Epoch 111101, Training Loss: 30238, Validation Loss: 51217, 125867.89126988803\n",
      "Epoch 111201, Training Loss: 30448, Validation Loss: 51024, 153764.8577708756\n",
      "Epoch 111301, Training Loss: 33112, Validation Loss: 51752, 164758.38012708165\n",
      "Epoch 111401, Training Loss: 31858, Validation Loss: 52000, 138550.66016095423\n",
      "Epoch 111501, Training Loss: 31091, Validation Loss: 52438, 148223.41192828337\n",
      "Epoch 111601, Training Loss: 32362, Validation Loss: 53252, 165681.71181150555\n",
      "Epoch 111701, Training Loss: 32000, Validation Loss: 51039, 184067.37228727792\n",
      "Epoch 111801, Training Loss: 30858, Validation Loss: 53873, 146984.4683344486\n",
      "Epoch 111901, Training Loss: 33278, Validation Loss: 54996, 157918.5916286453\n",
      "Epoch 112001, Training Loss: 31287, Validation Loss: 55581, 183181.07962100257\n",
      "Epoch 112101, Training Loss: 30524, Validation Loss: 54237, 143588.8800422069\n",
      "Epoch 112201, Training Loss: 32457, Validation Loss: 51232, 168964.1374194796\n",
      "Epoch 112301, Training Loss: 31578, Validation Loss: 52502, 146201.57810100404\n",
      "Epoch 112401, Training Loss: 30789, Validation Loss: 50484, 118645.4961801386\n",
      "Epoch 112501, Training Loss: 31998, Validation Loss: 51312, 164596.01621314164\n",
      "Epoch 112601, Training Loss: 29992, Validation Loss: 52785, 132151.79932947175\n",
      "Epoch 112701, Training Loss: 31587, Validation Loss: 51368, 150887.3384960283\n",
      "Epoch 112801, Training Loss: 31442, Validation Loss: 53344, 163670.97840830465\n",
      "Epoch 112901, Training Loss: 30919, Validation Loss: 49500, 147138.42839591362\n",
      "Epoch 113001, Training Loss: 31333, Validation Loss: 54144, 144879.0557957468\n",
      "Epoch 113101, Training Loss: 30468, Validation Loss: 53367, 137876.53078469442\n",
      "Epoch 113201, Training Loss: 32177, Validation Loss: 52083, 129205.97609238564\n",
      "Epoch 113301, Training Loss: 31654, Validation Loss: 52635, 139737.1315426237\n",
      "Epoch 113401, Training Loss: 33593, Validation Loss: 52412, 171513.66651432705\n",
      "Epoch 113501, Training Loss: 33810, Validation Loss: 49728, 135183.3097968589\n",
      "Epoch 113601, Training Loss: 31752, Validation Loss: 49945, 136851.35260575652\n",
      "Epoch 113701, Training Loss: 29707, Validation Loss: 49523, 140677.76975359395\n",
      "Epoch 113801, Training Loss: 30468, Validation Loss: 51286, 166325.7718562548\n",
      "Epoch 113901, Training Loss: 33232, Validation Loss: 51233, 155844.72387021786\n",
      "Epoch 114001, Training Loss: 33584, Validation Loss: 50685, 187147.56983199387\n",
      "Epoch 114101, Training Loss: 33153, Validation Loss: 55176, 142691.00852761534\n",
      "Epoch 114201, Training Loss: 31686, Validation Loss: 53481, 124377.55011873851\n",
      "Epoch 114301, Training Loss: 33456, Validation Loss: 51419, 136137.9017483339\n",
      "Epoch 114401, Training Loss: 30809, Validation Loss: 53108, 147493.3759339216\n",
      "Epoch 114501, Training Loss: 32974, Validation Loss: 51444, 158952.13277382014\n",
      "Epoch 114601, Training Loss: 31464, Validation Loss: 53722, 141885.55105317762\n",
      "Epoch 114701, Training Loss: 32758, Validation Loss: 54755, 174226.15297847707\n",
      "Epoch 114801, Training Loss: 32602, Validation Loss: 53157, 181773.5883804808\n",
      "Epoch 114901, Training Loss: 31181, Validation Loss: 51614, 138174.0071264893\n",
      "Epoch 115001, Training Loss: 31654, Validation Loss: 53895, 151396.52553056675\n",
      "Epoch 115101, Training Loss: 31049, Validation Loss: 51640, 151665.44726952878\n",
      "Epoch 115201, Training Loss: 32190, Validation Loss: 52824, 149660.15793574127\n",
      "Epoch 115301, Training Loss: 34277, Validation Loss: 52757, 172269.23552699195\n",
      "Epoch 115401, Training Loss: 31117, Validation Loss: 50476, 173546.08264535328\n",
      "Epoch 115501, Training Loss: 32378, Validation Loss: 52344, 153326.6907581222\n",
      "Epoch 115601, Training Loss: 31049, Validation Loss: 52017, 172129.8328872836\n",
      "Epoch 115701, Training Loss: 31634, Validation Loss: 51331, 134957.09358382854\n",
      "Epoch 115801, Training Loss: 30142, Validation Loss: 50508, 119396.77454724442\n",
      "Epoch 115901, Training Loss: 31627, Validation Loss: 53869, 152822.6812047829\n",
      "Epoch 116001, Training Loss: 30892, Validation Loss: 52655, 166551.46083687292\n",
      "Epoch 116101, Training Loss: 30461, Validation Loss: 51025, 153863.061307677\n",
      "Epoch 116201, Training Loss: 31534, Validation Loss: 52330, 117315.86041504522\n",
      "Epoch 116301, Training Loss: 30605, Validation Loss: 50830, 149479.48838807075\n",
      "Epoch 116401, Training Loss: 33231, Validation Loss: 48801, 179186.98636284776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116501, Training Loss: 31188, Validation Loss: 51926, 114995.88541767592\n",
      "Epoch 116601, Training Loss: 31605, Validation Loss: 51201, 149857.48950940697\n",
      "Epoch 116701, Training Loss: 32035, Validation Loss: 51160, 140685.6825718923\n",
      "Epoch 116801, Training Loss: 31621, Validation Loss: 52072, 142158.86393555408\n",
      "Epoch 116901, Training Loss: 30590, Validation Loss: 52741, 177311.7964770468\n",
      "Epoch 117001, Training Loss: 30927, Validation Loss: 53370, 143339.25712358113\n",
      "Epoch 117101, Training Loss: 31169, Validation Loss: 53397, 123161.83414343838\n",
      "Epoch 117201, Training Loss: 33090, Validation Loss: 52456, 150806.21069094897\n",
      "Epoch 117301, Training Loss: 32695, Validation Loss: 55213, 146260.79970141884\n",
      "Epoch 117401, Training Loss: 31423, Validation Loss: 54953, 166313.13460389816\n",
      "Epoch 117501, Training Loss: 31238, Validation Loss: 53687, 133693.53536126998\n",
      "Epoch 117601, Training Loss: 30026, Validation Loss: 53659, 176633.57495417402\n",
      "Epoch 117701, Training Loss: 31405, Validation Loss: 52264, 118691.31104263279\n",
      "Epoch 117801, Training Loss: 31285, Validation Loss: 51263, 143156.92091640187\n",
      "Epoch 117901, Training Loss: 31965, Validation Loss: 55755, 140056.13009155187\n",
      "Epoch 118001, Training Loss: 31211, Validation Loss: 50002, 145377.91048828064\n",
      "Epoch 118101, Training Loss: 31015, Validation Loss: 52107, 152673.14388455992\n",
      "Epoch 118201, Training Loss: 36128, Validation Loss: 51954, 167700.614076082\n",
      "Epoch 118301, Training Loss: 31688, Validation Loss: 53648, 142510.00355740104\n",
      "Epoch 118401, Training Loss: 31422, Validation Loss: 51912, 157364.2393786391\n",
      "Epoch 118501, Training Loss: 30352, Validation Loss: 52119, 159740.34196194593\n",
      "Epoch 118601, Training Loss: 32531, Validation Loss: 53109, 178299.4772590502\n",
      "Epoch 118701, Training Loss: 32835, Validation Loss: 50609, 142490.8431158018\n",
      "Epoch 118801, Training Loss: 31267, Validation Loss: 51102, 129401.00487314098\n",
      "Epoch 118901, Training Loss: 31033, Validation Loss: 52559, 164239.13505237858\n",
      "Epoch 119001, Training Loss: 30627, Validation Loss: 52677, 167228.7625481077\n",
      "Epoch 119101, Training Loss: 32521, Validation Loss: 52244, 147625.3770270458\n",
      "Epoch 119201, Training Loss: 31405, Validation Loss: 54443, 132275.8907927345\n",
      "Epoch 119301, Training Loss: 32797, Validation Loss: 54123, 142184.36540193175\n",
      "Epoch 119401, Training Loss: 33778, Validation Loss: 51836, 118758.34723849516\n",
      "Epoch 119501, Training Loss: 30946, Validation Loss: 51114, 123629.73376605807\n",
      "Epoch 119601, Training Loss: 32381, Validation Loss: 54642, 120989.06230275026\n",
      "Epoch 119701, Training Loss: 32182, Validation Loss: 51665, 171362.01809278497\n",
      "Epoch 119801, Training Loss: 31653, Validation Loss: 52274, 135014.61040448563\n",
      "Epoch 119901, Training Loss: 32846, Validation Loss: 51757, 148172.3044906396\n",
      "Epoch 120001, Training Loss: 30279, Validation Loss: 52691, 131854.1364563513\n",
      "Epoch 120101, Training Loss: 31291, Validation Loss: 54449, 123866.17220577874\n",
      "Epoch 120201, Training Loss: 30478, Validation Loss: 52249, 177829.3447340178\n",
      "Epoch 120301, Training Loss: 30451, Validation Loss: 51274, 157383.12510925374\n",
      "Epoch 120401, Training Loss: 31546, Validation Loss: 52599, 133755.75401630957\n",
      "Epoch 120501, Training Loss: 30795, Validation Loss: 52129, 125687.30037410065\n",
      "Epoch 120601, Training Loss: 31488, Validation Loss: 51586, 172430.5837662174\n",
      "Epoch 120701, Training Loss: 31649, Validation Loss: 51960, 143835.24606753627\n",
      "Epoch 120801, Training Loss: 31086, Validation Loss: 52807, 141176.30675017124\n",
      "Epoch 120901, Training Loss: 29736, Validation Loss: 52509, 116232.14525029871\n",
      "Epoch 121001, Training Loss: 30077, Validation Loss: 51998, 164433.10582569236\n",
      "Epoch 121101, Training Loss: 33001, Validation Loss: 54680, 160869.13374392776\n",
      "Epoch 121201, Training Loss: 30321, Validation Loss: 53788, 141458.6862121997\n",
      "Epoch 121301, Training Loss: 31073, Validation Loss: 52600, 163579.92147866823\n",
      "Epoch 121401, Training Loss: 30604, Validation Loss: 52642, 126529.87909288755\n",
      "Epoch 121501, Training Loss: 33018, Validation Loss: 54784, 161227.8899095437\n",
      "Epoch 121601, Training Loss: 29780, Validation Loss: 52810, 163037.1470198794\n",
      "Epoch 121701, Training Loss: 30824, Validation Loss: 52235, 147726.43525038238\n",
      "Epoch 121801, Training Loss: 31368, Validation Loss: 52984, 112961.34086042689\n",
      "Epoch 121901, Training Loss: 32828, Validation Loss: 57057, 164550.32911607475\n",
      "Epoch 122001, Training Loss: 30885, Validation Loss: 51336, 127366.663844482\n",
      "Epoch 122101, Training Loss: 30293, Validation Loss: 50594, 161807.8665931186\n",
      "Epoch 122201, Training Loss: 32671, Validation Loss: 55351, 154617.6201802115\n",
      "Epoch 122301, Training Loss: 30762, Validation Loss: 54300, 170528.29800567642\n",
      "Epoch 122401, Training Loss: 32041, Validation Loss: 52982, 143574.18978722332\n",
      "Epoch 122501, Training Loss: 30221, Validation Loss: 53237, 154645.80506290082\n",
      "Epoch 122601, Training Loss: 31921, Validation Loss: 51880, 153813.61609348102\n",
      "Epoch 122701, Training Loss: 32039, Validation Loss: 53222, 146957.50484426046\n",
      "Epoch 122801, Training Loss: 32726, Validation Loss: 54325, 150658.74934262637\n",
      "Epoch 122901, Training Loss: 30908, Validation Loss: 51785, 132338.59884986514\n",
      "Epoch 123001, Training Loss: 30216, Validation Loss: 52609, 130262.81193894465\n",
      "Epoch 123101, Training Loss: 30926, Validation Loss: 53324, 152632.23422893422\n",
      "Epoch 123201, Training Loss: 30250, Validation Loss: 53011, 152283.7972562607\n",
      "Epoch 123301, Training Loss: 31378, Validation Loss: 53240, 124687.483709174\n",
      "Epoch 123401, Training Loss: 31315, Validation Loss: 51366, 159051.25417524655\n",
      "Epoch 123501, Training Loss: 30428, Validation Loss: 52667, 150508.45866899745\n",
      "Epoch 123601, Training Loss: 31663, Validation Loss: 53533, 135262.157602628\n",
      "Epoch 123701, Training Loss: 30952, Validation Loss: 52339, 127253.48229335556\n",
      "Epoch 123801, Training Loss: 31542, Validation Loss: 54082, 153212.95699383997\n",
      "Epoch 123901, Training Loss: 29928, Validation Loss: 53807, 129429.75493478698\n",
      "Epoch 124001, Training Loss: 33139, Validation Loss: 52220, 150736.21881303555\n",
      "Epoch 124101, Training Loss: 32229, Validation Loss: 52111, 136405.655075514\n",
      "Epoch 124201, Training Loss: 30163, Validation Loss: 54712, 152897.22799584502\n",
      "Epoch 124301, Training Loss: 28617, Validation Loss: 54787, 134174.51198463803\n",
      "Epoch 124401, Training Loss: 33064, Validation Loss: 54593, 176126.11038635642\n",
      "Epoch 124501, Training Loss: 30581, Validation Loss: 52695, 134618.0164831398\n",
      "Epoch 124601, Training Loss: 30103, Validation Loss: 53153, 129086.12898036856\n",
      "Epoch 124701, Training Loss: 32103, Validation Loss: 53916, 141091.1445937304\n",
      "Epoch 124801, Training Loss: 32008, Validation Loss: 54429, 172449.52881569543\n",
      "Epoch 124901, Training Loss: 31160, Validation Loss: 55359, 155300.20074540647\n",
      "Epoch 125001, Training Loss: 36222, Validation Loss: 51983, 194668.232025171\n",
      "Epoch 125101, Training Loss: 30369, Validation Loss: 54547, 127943.67929578717\n",
      "Epoch 125201, Training Loss: 32382, Validation Loss: 50776, 141595.9229869602\n",
      "Epoch 125301, Training Loss: 30123, Validation Loss: 53003, 162056.08712980553\n",
      "Epoch 125401, Training Loss: 30863, Validation Loss: 51337, 135620.30679837253\n",
      "Epoch 125501, Training Loss: 29253, Validation Loss: 50258, 135050.7527110844\n",
      "Epoch 125601, Training Loss: 30976, Validation Loss: 52745, 144455.68025466963\n",
      "Epoch 125701, Training Loss: 29868, Validation Loss: 54521, 133651.43947117077\n",
      "Epoch 125801, Training Loss: 30596, Validation Loss: 53007, 140452.1428658408\n",
      "Epoch 125901, Training Loss: 32087, Validation Loss: 53232, 181331.19603814185\n",
      "Epoch 126001, Training Loss: 32250, Validation Loss: 51157, 136479.0345337139\n",
      "Epoch 126101, Training Loss: 30851, Validation Loss: 51857, 125332.22460566575\n",
      "Epoch 126201, Training Loss: 31083, Validation Loss: 53175, 144759.037133589\n",
      "Epoch 126301, Training Loss: 30424, Validation Loss: 53804, 147959.47526611784\n",
      "Epoch 126401, Training Loss: 30142, Validation Loss: 53742, 145390.57833146024\n",
      "Epoch 126501, Training Loss: 32993, Validation Loss: 58008, 179504.64054907247\n",
      "Epoch 126601, Training Loss: 30482, Validation Loss: 52511, 118895.725425222\n",
      "Epoch 126701, Training Loss: 29366, Validation Loss: 51880, 137385.48952868202\n",
      "Epoch 126801, Training Loss: 30798, Validation Loss: 53309, 133640.94448095947\n",
      "Epoch 126901, Training Loss: 31212, Validation Loss: 53181, 160497.96531331944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127001, Training Loss: 31361, Validation Loss: 52634, 188381.4218406352\n",
      "Epoch 127101, Training Loss: 29874, Validation Loss: 53165, 145242.7896565393\n",
      "Epoch 127201, Training Loss: 32429, Validation Loss: 52581, 136281.09289246835\n",
      "Epoch 127301, Training Loss: 31190, Validation Loss: 52372, 135161.1098115302\n",
      "Epoch 127401, Training Loss: 31415, Validation Loss: 52183, 141789.50818634024\n",
      "Epoch 127501, Training Loss: 32011, Validation Loss: 55578, 146245.8813645429\n",
      "Epoch 127601, Training Loss: 29992, Validation Loss: 54928, 154358.1602506766\n",
      "Epoch 127701, Training Loss: 30053, Validation Loss: 53721, 157629.19888536018\n",
      "Epoch 127801, Training Loss: 30676, Validation Loss: 53586, 165900.074969945\n",
      "Epoch 127901, Training Loss: 31527, Validation Loss: 53869, 164297.7369281279\n",
      "Epoch 128001, Training Loss: 31778, Validation Loss: 52232, 145302.66641237322\n",
      "Epoch 128101, Training Loss: 30565, Validation Loss: 52167, 151495.81993414284\n",
      "Epoch 128201, Training Loss: 32257, Validation Loss: 54138, 154130.6474959141\n",
      "Epoch 128301, Training Loss: 30560, Validation Loss: 53820, 163359.43637852088\n",
      "Epoch 128401, Training Loss: 30712, Validation Loss: 51576, 149223.69884325608\n",
      "Epoch 128501, Training Loss: 31182, Validation Loss: 54137, 191417.48441482143\n",
      "Epoch 128601, Training Loss: 31113, Validation Loss: 51982, 185611.4648324527\n",
      "Epoch 128701, Training Loss: 33127, Validation Loss: 53065, 184071.06895465552\n",
      "Epoch 128801, Training Loss: 32298, Validation Loss: 54176, 151359.22887644617\n",
      "Epoch 128901, Training Loss: 29970, Validation Loss: 52638, 125552.44226525756\n",
      "Epoch 129001, Training Loss: 29500, Validation Loss: 52729, 143031.15369540337\n",
      "Epoch 129101, Training Loss: 29845, Validation Loss: 54695, 141471.41155073518\n",
      "Epoch 129201, Training Loss: 29570, Validation Loss: 54492, 172973.04010524\n",
      "Epoch 129301, Training Loss: 29446, Validation Loss: 53185, 141127.61169503236\n",
      "Epoch 129401, Training Loss: 31226, Validation Loss: 51435, 138009.74928802944\n",
      "Epoch 129501, Training Loss: 31876, Validation Loss: 54091, 156435.19195221798\n",
      "Epoch 129601, Training Loss: 31353, Validation Loss: 55093, 139135.47145963862\n",
      "Epoch 129701, Training Loss: 30619, Validation Loss: 52878, 139896.4835657218\n",
      "Epoch 129801, Training Loss: 31125, Validation Loss: 55237, 155827.9567308848\n",
      "Epoch 129901, Training Loss: 31206, Validation Loss: 54208, 171736.49301098954\n",
      "Epoch 130001, Training Loss: 32098, Validation Loss: 54049, 163003.40445860336\n",
      "Epoch 130101, Training Loss: 29965, Validation Loss: 53576, 132721.92071304686\n",
      "Epoch 130201, Training Loss: 30503, Validation Loss: 52936, 155044.16608780762\n",
      "Epoch 130301, Training Loss: 33466, Validation Loss: 53455, 169880.31146887134\n",
      "Epoch 130401, Training Loss: 31758, Validation Loss: 52716, 157935.0176674075\n",
      "Epoch 130501, Training Loss: 31550, Validation Loss: 54365, 143944.11249305136\n",
      "Epoch 130601, Training Loss: 29905, Validation Loss: 54846, 131265.14973923695\n",
      "Epoch 130701, Training Loss: 30539, Validation Loss: 52262, 142232.69659487158\n",
      "Epoch 130801, Training Loss: 30253, Validation Loss: 51898, 173351.1057777198\n",
      "Epoch 130901, Training Loss: 31552, Validation Loss: 53578, 142800.94318129666\n",
      "Epoch 131001, Training Loss: 29580, Validation Loss: 54910, 154131.58601904564\n",
      "Epoch 131101, Training Loss: 32332, Validation Loss: 55425, 157177.19536583323\n",
      "Epoch 131201, Training Loss: 30863, Validation Loss: 51604, 150840.69731127418\n",
      "Epoch 131301, Training Loss: 31309, Validation Loss: 52757, 147179.15679555037\n",
      "Epoch 131401, Training Loss: 31969, Validation Loss: 52877, 132097.2245697907\n",
      "Epoch 131501, Training Loss: 29201, Validation Loss: 52506, 157429.7192198296\n",
      "Epoch 131601, Training Loss: 29059, Validation Loss: 52559, 133118.45203422906\n",
      "Epoch 131701, Training Loss: 32619, Validation Loss: 53315, 182641.61835657113\n",
      "Epoch 131801, Training Loss: 30048, Validation Loss: 54581, 155105.33620485786\n",
      "Epoch 131901, Training Loss: 33289, Validation Loss: 53981, 168836.20416648148\n",
      "Epoch 132001, Training Loss: 33146, Validation Loss: 52153, 154916.75984815627\n",
      "Epoch 132101, Training Loss: 31007, Validation Loss: 55930, 168983.07094196865\n",
      "Epoch 132201, Training Loss: 30762, Validation Loss: 51643, 141723.84391177818\n",
      "Epoch 132301, Training Loss: 30110, Validation Loss: 52892, 158723.22573224592\n",
      "Epoch 132401, Training Loss: 30268, Validation Loss: 53281, 130952.52172426286\n",
      "Epoch 132501, Training Loss: 30971, Validation Loss: 53597, 162852.484418453\n",
      "Epoch 132601, Training Loss: 30202, Validation Loss: 52103, 138202.50422490574\n",
      "Epoch 132701, Training Loss: 33044, Validation Loss: 54401, 169005.4539448099\n",
      "Epoch 132801, Training Loss: 30102, Validation Loss: 53186, 125549.36167269468\n",
      "Epoch 132901, Training Loss: 29987, Validation Loss: 55808, 130565.14591070711\n",
      "Epoch 133001, Training Loss: 29729, Validation Loss: 52715, 136848.99774829362\n",
      "Epoch 133101, Training Loss: 34280, Validation Loss: 54091, 136387.011671024\n",
      "Epoch 133201, Training Loss: 31031, Validation Loss: 55642, 141590.13259366283\n",
      "Epoch 133301, Training Loss: 31595, Validation Loss: 52096, 163452.76481024674\n",
      "Epoch 133401, Training Loss: 31529, Validation Loss: 54535, 147417.64425568812\n",
      "Epoch 133501, Training Loss: 30806, Validation Loss: 54820, 151088.77262484646\n",
      "Epoch 133601, Training Loss: 30295, Validation Loss: 56005, 163655.23261931867\n",
      "Epoch 133701, Training Loss: 30448, Validation Loss: 52292, 162132.36820013233\n",
      "Epoch 133801, Training Loss: 33232, Validation Loss: 53247, 133922.6181979674\n",
      "Epoch 133901, Training Loss: 31953, Validation Loss: 51558, 165198.73541217053\n",
      "Epoch 134001, Training Loss: 31074, Validation Loss: 53686, 150402.73314353172\n",
      "Epoch 134101, Training Loss: 30639, Validation Loss: 52963, 123886.06668100003\n",
      "Epoch 134201, Training Loss: 30290, Validation Loss: 52080, 162867.36270671012\n",
      "Epoch 134301, Training Loss: 30174, Validation Loss: 54791, 166648.57224493907\n",
      "Epoch 134401, Training Loss: 31333, Validation Loss: 52714, 150323.79155267763\n",
      "Epoch 134501, Training Loss: 30104, Validation Loss: 51820, 175934.08239808484\n",
      "Epoch 134601, Training Loss: 31738, Validation Loss: 52665, 155470.77826193647\n",
      "Epoch 134701, Training Loss: 31700, Validation Loss: 52476, 127958.67254684304\n",
      "Epoch 134801, Training Loss: 31574, Validation Loss: 54253, 164177.80104467442\n",
      "Epoch 134901, Training Loss: 30069, Validation Loss: 55742, 138160.0653372892\n",
      "Epoch 135001, Training Loss: 30270, Validation Loss: 52643, 153827.98713440198\n",
      "Epoch 135101, Training Loss: 32939, Validation Loss: 51648, 165946.08346215962\n",
      "Epoch 135201, Training Loss: 30366, Validation Loss: 53272, 121061.25131420398\n",
      "Epoch 135301, Training Loss: 31390, Validation Loss: 52275, 144761.80834066586\n",
      "Epoch 135401, Training Loss: 30235, Validation Loss: 53116, 131590.85966290755\n",
      "Epoch 135501, Training Loss: 31498, Validation Loss: 54679, 170619.27366950738\n",
      "Epoch 135601, Training Loss: 30876, Validation Loss: 53383, 166314.53464085615\n",
      "Epoch 135701, Training Loss: 29355, Validation Loss: 52247, 132717.30006196283\n",
      "Epoch 135801, Training Loss: 30255, Validation Loss: 52857, 138196.53238413628\n",
      "Epoch 135901, Training Loss: 30127, Validation Loss: 55423, 140656.26807427476\n",
      "Epoch 136001, Training Loss: 31895, Validation Loss: 55765, 171773.5763291498\n",
      "Epoch 136101, Training Loss: 31295, Validation Loss: 50970, 159587.35866568782\n",
      "Epoch 136201, Training Loss: 32291, Validation Loss: 54066, 121154.64730838893\n",
      "Epoch 136301, Training Loss: 31961, Validation Loss: 51318, 158295.5491912011\n",
      "Epoch 136401, Training Loss: 31500, Validation Loss: 53296, 183663.5868170538\n",
      "Epoch 136501, Training Loss: 30234, Validation Loss: 52122, 130494.02439633691\n",
      "Epoch 136601, Training Loss: 30949, Validation Loss: 52278, 155919.19705209747\n",
      "Epoch 136701, Training Loss: 31765, Validation Loss: 51981, 143350.14344275146\n",
      "Epoch 136801, Training Loss: 29673, Validation Loss: 53621, 164295.61919128033\n",
      "Epoch 136901, Training Loss: 30354, Validation Loss: 51799, 134261.9280925025\n",
      "Epoch 137001, Training Loss: 30143, Validation Loss: 53485, 142271.95488848683\n",
      "Epoch 137101, Training Loss: 31804, Validation Loss: 52982, 142967.04940444342\n",
      "Epoch 137201, Training Loss: 31296, Validation Loss: 52701, 130116.06240772607\n",
      "Epoch 137301, Training Loss: 30997, Validation Loss: 52958, 163391.10958981756\n",
      "Epoch 137401, Training Loss: 31764, Validation Loss: 55499, 136908.26218547687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137501, Training Loss: 28456, Validation Loss: 52586, 172745.52684672351\n",
      "Epoch 137601, Training Loss: 32572, Validation Loss: 51756, 161730.93862060033\n",
      "Epoch 137701, Training Loss: 31470, Validation Loss: 54796, 183354.80247238086\n",
      "Epoch 137801, Training Loss: 30423, Validation Loss: 53214, 141446.15690378263\n",
      "Epoch 137901, Training Loss: 30914, Validation Loss: 50566, 163473.11304273093\n",
      "Epoch 138001, Training Loss: 29296, Validation Loss: 54820, 152413.76854266395\n",
      "Epoch 138101, Training Loss: 30483, Validation Loss: 52579, 133681.25738903836\n",
      "Epoch 138201, Training Loss: 31608, Validation Loss: 52043, 166838.1791308301\n",
      "Epoch 138301, Training Loss: 30200, Validation Loss: 54763, 131100.05334093474\n",
      "Epoch 138401, Training Loss: 30621, Validation Loss: 52078, 168045.87996362973\n",
      "Epoch 138501, Training Loss: 32525, Validation Loss: 52019, 129251.9281902042\n",
      "Epoch 138601, Training Loss: 31335, Validation Loss: 53360, 167896.32843466586\n",
      "Epoch 138701, Training Loss: 31111, Validation Loss: 53994, 133713.4175974927\n",
      "Epoch 138801, Training Loss: 31991, Validation Loss: 54045, 158853.79702035186\n",
      "Epoch 138901, Training Loss: 31202, Validation Loss: 54851, 195390.06476324564\n",
      "Epoch 139001, Training Loss: 29044, Validation Loss: 53854, 129316.47538901921\n",
      "Epoch 139101, Training Loss: 31467, Validation Loss: 53721, 160336.71343629982\n",
      "Epoch 139201, Training Loss: 31779, Validation Loss: 54260, 168209.42571312602\n",
      "Epoch 139301, Training Loss: 32016, Validation Loss: 52634, 160175.20554326163\n",
      "Epoch 139401, Training Loss: 30471, Validation Loss: 54951, 135318.35220370517\n",
      "Epoch 139501, Training Loss: 29574, Validation Loss: 53162, 152099.47012146167\n",
      "Epoch 139601, Training Loss: 28380, Validation Loss: 54810, 128284.5509990257\n",
      "Epoch 139701, Training Loss: 30478, Validation Loss: 53901, 127924.2163268737\n",
      "Epoch 139801, Training Loss: 30417, Validation Loss: 55995, 133684.45812650683\n",
      "Epoch 139901, Training Loss: 28573, Validation Loss: 54045, 155109.5673112628\n",
      "Epoch 140001, Training Loss: 30458, Validation Loss: 53766, 128672.9617357573\n",
      "Epoch 140101, Training Loss: 29807, Validation Loss: 54035, 135805.69289416668\n",
      "Epoch 140201, Training Loss: 29929, Validation Loss: 53578, 148011.3799782238\n",
      "Epoch 140301, Training Loss: 29233, Validation Loss: 52125, 137544.54779333918\n",
      "Epoch 140401, Training Loss: 29961, Validation Loss: 54384, 127866.04171008061\n",
      "Epoch 140501, Training Loss: 32367, Validation Loss: 54293, 149301.42985343307\n",
      "Epoch 140601, Training Loss: 31564, Validation Loss: 53001, 145284.9770705855\n",
      "Epoch 140701, Training Loss: 30151, Validation Loss: 53959, 149175.28328540298\n",
      "Epoch 140801, Training Loss: 30182, Validation Loss: 51159, 149428.4325167526\n",
      "Epoch 140901, Training Loss: 31101, Validation Loss: 53740, 185256.89414714588\n",
      "Epoch 141001, Training Loss: 29574, Validation Loss: 52791, 129020.70239456971\n",
      "Epoch 141101, Training Loss: 31999, Validation Loss: 51345, 192728.88662139676\n",
      "Epoch 141201, Training Loss: 31507, Validation Loss: 51977, 144711.6678823877\n",
      "Epoch 141301, Training Loss: 28596, Validation Loss: 54910, 148245.35946124623\n",
      "Epoch 141401, Training Loss: 30168, Validation Loss: 52760, 176564.1689212119\n",
      "Epoch 141501, Training Loss: 32054, Validation Loss: 52412, 145355.00857431168\n",
      "Epoch 141601, Training Loss: 31777, Validation Loss: 54152, 151061.98131110673\n",
      "Epoch 141701, Training Loss: 31861, Validation Loss: 53290, 150216.88233916997\n",
      "Epoch 141801, Training Loss: 30137, Validation Loss: 54995, 143541.38371536386\n",
      "Epoch 141901, Training Loss: 29846, Validation Loss: 51555, 153831.1064457372\n",
      "Epoch 142001, Training Loss: 29616, Validation Loss: 52726, 143818.0131048576\n",
      "Epoch 142101, Training Loss: 30622, Validation Loss: 52497, 143889.51157306222\n",
      "Epoch 142201, Training Loss: 32661, Validation Loss: 53821, 170703.71820976288\n",
      "Epoch 142301, Training Loss: 30123, Validation Loss: 53410, 162883.04345967987\n",
      "Epoch 142401, Training Loss: 28426, Validation Loss: 53785, 137397.02421218698\n",
      "Epoch 142501, Training Loss: 30977, Validation Loss: 54604, 129974.12683262858\n",
      "Epoch 142601, Training Loss: 29437, Validation Loss: 51936, 137099.13969836646\n",
      "Epoch 142701, Training Loss: 29214, Validation Loss: 55044, 140600.71607418748\n",
      "Epoch 142801, Training Loss: 30758, Validation Loss: 54917, 135915.68320287514\n",
      "Epoch 142901, Training Loss: 30992, Validation Loss: 54751, 123657.98279685495\n",
      "Epoch 143001, Training Loss: 29686, Validation Loss: 53507, 128128.17290492728\n",
      "Epoch 143101, Training Loss: 30202, Validation Loss: 54822, 141373.95834674055\n",
      "Epoch 143201, Training Loss: 28874, Validation Loss: 54685, 124788.62968734736\n",
      "Epoch 143301, Training Loss: 30379, Validation Loss: 53330, 145736.75971619692\n",
      "Epoch 143401, Training Loss: 28848, Validation Loss: 54395, 119958.51230493965\n",
      "Epoch 143501, Training Loss: 31075, Validation Loss: 52755, 158853.72419891105\n",
      "Epoch 143601, Training Loss: 29645, Validation Loss: 52520, 151538.79097906404\n",
      "Epoch 143701, Training Loss: 30197, Validation Loss: 54060, 122271.86196620083\n",
      "Epoch 143801, Training Loss: 33293, Validation Loss: 52356, 148666.66077627754\n",
      "Epoch 143901, Training Loss: 29458, Validation Loss: 53492, 135623.79144828866\n",
      "Epoch 144001, Training Loss: 30280, Validation Loss: 57613, 169534.5936732732\n",
      "Epoch 144101, Training Loss: 31468, Validation Loss: 52072, 147443.4510289391\n",
      "Epoch 144201, Training Loss: 30543, Validation Loss: 53634, 121795.82011036524\n",
      "Epoch 144301, Training Loss: 30664, Validation Loss: 52485, 148217.20113407174\n",
      "Epoch 144401, Training Loss: 31402, Validation Loss: 55348, 147471.04587645576\n",
      "Epoch 144501, Training Loss: 33547, Validation Loss: 53557, 169731.61662740103\n",
      "Epoch 144601, Training Loss: 32524, Validation Loss: 52570, 142633.60559490084\n",
      "Epoch 144701, Training Loss: 30025, Validation Loss: 52943, 112996.72306467593\n",
      "Epoch 144801, Training Loss: 31201, Validation Loss: 54842, 158256.4400580002\n",
      "Epoch 144901, Training Loss: 30897, Validation Loss: 54834, 163865.81580587468\n",
      "Epoch 145001, Training Loss: 30902, Validation Loss: 54158, 147715.41902396147\n",
      "Epoch 145101, Training Loss: 31548, Validation Loss: 53162, 134383.0314981909\n",
      "Epoch 145201, Training Loss: 28249, Validation Loss: 53580, 144803.68834144154\n",
      "Epoch 145301, Training Loss: 29179, Validation Loss: 53884, 143387.92215286748\n",
      "Epoch 145401, Training Loss: 30413, Validation Loss: 54132, 164350.85602809174\n",
      "Epoch 145501, Training Loss: 28983, Validation Loss: 52707, 143705.26707531128\n",
      "Epoch 145601, Training Loss: 31148, Validation Loss: 54771, 155279.40936281352\n",
      "Epoch 145701, Training Loss: 32850, Validation Loss: 54596, 161739.17380098536\n",
      "Epoch 145801, Training Loss: 29428, Validation Loss: 53638, 131161.92775989324\n",
      "Epoch 145901, Training Loss: 30795, Validation Loss: 52580, 141469.87042613185\n",
      "Epoch 146001, Training Loss: 29691, Validation Loss: 52989, 139938.3917174377\n",
      "Epoch 146101, Training Loss: 30518, Validation Loss: 52448, 154064.3074370566\n",
      "Epoch 146201, Training Loss: 33303, Validation Loss: 52988, 164073.0478596604\n",
      "Epoch 146301, Training Loss: 30749, Validation Loss: 52310, 147922.20353163235\n",
      "Epoch 146401, Training Loss: 29963, Validation Loss: 53749, 156438.0492037938\n",
      "Epoch 146501, Training Loss: 32020, Validation Loss: 52828, 128379.75306553175\n",
      "Epoch 146601, Training Loss: 29831, Validation Loss: 52159, 136410.87588889216\n",
      "Epoch 146701, Training Loss: 29422, Validation Loss: 53012, 163162.1719578779\n",
      "Epoch 146801, Training Loss: 29040, Validation Loss: 52449, 129644.61485496341\n",
      "Epoch 146901, Training Loss: 30941, Validation Loss: 53968, 144668.69551213266\n",
      "Epoch 147001, Training Loss: 31676, Validation Loss: 55301, 147863.25799528937\n",
      "Epoch 147101, Training Loss: 31251, Validation Loss: 54690, 160214.4767786647\n",
      "Epoch 147201, Training Loss: 29181, Validation Loss: 54660, 135524.31543305947\n",
      "Epoch 147301, Training Loss: 31447, Validation Loss: 51618, 165049.78882991296\n",
      "Epoch 147401, Training Loss: 29931, Validation Loss: 52607, 169220.3746095417\n",
      "Epoch 147501, Training Loss: 31176, Validation Loss: 52847, 155395.53890229913\n",
      "Epoch 147601, Training Loss: 30290, Validation Loss: 51442, 175550.21345385571\n",
      "Epoch 147701, Training Loss: 29088, Validation Loss: 55064, 134660.04761718033\n",
      "Epoch 147801, Training Loss: 30247, Validation Loss: 54216, 150106.0929174678\n",
      "Epoch 147901, Training Loss: 31336, Validation Loss: 55554, 144812.2146357783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148001, Training Loss: 31724, Validation Loss: 51883, 156153.73282274202\n",
      "Epoch 148101, Training Loss: 30217, Validation Loss: 53824, 125229.11634648297\n",
      "Epoch 148201, Training Loss: 30964, Validation Loss: 53160, 139342.33955059687\n",
      "Epoch 148301, Training Loss: 30451, Validation Loss: 55670, 143695.56400071312\n",
      "Epoch 148401, Training Loss: 30172, Validation Loss: 54017, 169935.33892327923\n",
      "Epoch 148501, Training Loss: 29409, Validation Loss: 55419, 165600.6641655962\n",
      "Epoch 148601, Training Loss: 29557, Validation Loss: 54882, 154369.0675647423\n",
      "Epoch 148701, Training Loss: 31457, Validation Loss: 55821, 148344.06592329472\n",
      "Epoch 148801, Training Loss: 31433, Validation Loss: 53500, 129196.25427791472\n",
      "Epoch 148901, Training Loss: 31546, Validation Loss: 52130, 141467.8073153848\n",
      "Epoch 149001, Training Loss: 30012, Validation Loss: 52952, 131438.58142714654\n",
      "Epoch 149101, Training Loss: 29804, Validation Loss: 53617, 120243.22303335876\n",
      "Epoch 149201, Training Loss: 30525, Validation Loss: 53007, 166126.55200766592\n",
      "Epoch 149301, Training Loss: 29922, Validation Loss: 51015, 171338.84910619858\n",
      "Epoch 149401, Training Loss: 29831, Validation Loss: 51970, 159843.69758151783\n",
      "Epoch 149501, Training Loss: 29875, Validation Loss: 51754, 171149.1333320502\n",
      "Epoch 149601, Training Loss: 29850, Validation Loss: 55349, 163235.09730992987\n",
      "Epoch 149701, Training Loss: 31696, Validation Loss: 52832, 136875.63779959368\n",
      "Epoch 149801, Training Loss: 29823, Validation Loss: 52948, 133164.57772056013\n",
      "Epoch 149901, Training Loss: 29702, Validation Loss: 52783, 148048.1054265843\n",
      "Epoch 150001, Training Loss: 30408, Validation Loss: 54396, 126390.89505428445\n",
      "Epoch 150101, Training Loss: 30273, Validation Loss: 52928, 138843.6915806656\n",
      "Epoch 150201, Training Loss: 30684, Validation Loss: 53364, 168906.00210727338\n",
      "Epoch 150301, Training Loss: 32588, Validation Loss: 52727, 148842.42029790618\n",
      "Epoch 150401, Training Loss: 31775, Validation Loss: 52150, 166462.97777045463\n",
      "Epoch 150501, Training Loss: 27147, Validation Loss: 54251, 168270.51875300804\n",
      "Epoch 150601, Training Loss: 30824, Validation Loss: 50725, 179259.56907063993\n",
      "Epoch 150701, Training Loss: 32044, Validation Loss: 53901, 158544.83181386662\n",
      "Epoch 150801, Training Loss: 30077, Validation Loss: 52418, 155692.64560109717\n",
      "Epoch 150901, Training Loss: 29828, Validation Loss: 54772, 132294.08123445878\n",
      "Epoch 151001, Training Loss: 30731, Validation Loss: 52115, 127193.47783543965\n",
      "Epoch 151101, Training Loss: 32139, Validation Loss: 54152, 146191.46430637388\n",
      "Epoch 151201, Training Loss: 29076, Validation Loss: 54900, 148562.19911175407\n",
      "Epoch 151301, Training Loss: 30515, Validation Loss: 56113, 139004.3512859032\n",
      "Epoch 151401, Training Loss: 30898, Validation Loss: 52611, 143073.41280522873\n",
      "Epoch 151501, Training Loss: 29386, Validation Loss: 52440, 143129.70329505327\n",
      "Epoch 151601, Training Loss: 31201, Validation Loss: 55217, 162147.99942697218\n",
      "Epoch 151701, Training Loss: 31052, Validation Loss: 52564, 142242.59711403315\n",
      "Epoch 151801, Training Loss: 33164, Validation Loss: 53004, 160969.05462373685\n",
      "Epoch 151901, Training Loss: 30810, Validation Loss: 52879, 154774.60988429913\n",
      "Epoch 152001, Training Loss: 30972, Validation Loss: 54024, 175942.4429209319\n",
      "Epoch 152101, Training Loss: 30059, Validation Loss: 53107, 136289.14963111954\n",
      "Epoch 152201, Training Loss: 29937, Validation Loss: 53962, 141442.79053312266\n",
      "Epoch 152301, Training Loss: 30346, Validation Loss: 53519, 156496.21648070513\n",
      "Epoch 152401, Training Loss: 30903, Validation Loss: 54704, 168479.59075377847\n",
      "Epoch 152501, Training Loss: 32551, Validation Loss: 52682, 157250.26850041235\n",
      "Epoch 152601, Training Loss: 32022, Validation Loss: 55044, 159317.0972940907\n",
      "Epoch 152701, Training Loss: 29615, Validation Loss: 52822, 133387.21850052\n",
      "Epoch 152801, Training Loss: 29999, Validation Loss: 55539, 126751.04490363284\n",
      "Epoch 152901, Training Loss: 30033, Validation Loss: 54352, 139185.16733256498\n",
      "Epoch 153001, Training Loss: 28959, Validation Loss: 53594, 154914.06659973538\n",
      "Epoch 153101, Training Loss: 31079, Validation Loss: 55666, 180536.13905547664\n",
      "Epoch 153201, Training Loss: 29859, Validation Loss: 53551, 129526.53947900818\n",
      "Epoch 153301, Training Loss: 29368, Validation Loss: 53881, 134405.4827810215\n",
      "Epoch 153401, Training Loss: 29796, Validation Loss: 53798, 136464.18728252567\n",
      "Epoch 153501, Training Loss: 28639, Validation Loss: 54286, 163447.64002118516\n",
      "Epoch 153601, Training Loss: 30510, Validation Loss: 54946, 165630.2535335603\n",
      "Epoch 153701, Training Loss: 30670, Validation Loss: 53824, 133668.46826343198\n",
      "Epoch 153801, Training Loss: 30008, Validation Loss: 55066, 161702.62856334582\n",
      "Epoch 153901, Training Loss: 30547, Validation Loss: 54677, 143083.28538755045\n",
      "Epoch 154001, Training Loss: 31244, Validation Loss: 53775, 166328.2071052393\n",
      "Epoch 154101, Training Loss: 29503, Validation Loss: 54252, 135514.22747077694\n",
      "Epoch 154201, Training Loss: 30432, Validation Loss: 51135, 162508.040574106\n",
      "Epoch 154301, Training Loss: 30787, Validation Loss: 55229, 141201.79762584314\n",
      "Epoch 154401, Training Loss: 31101, Validation Loss: 52725, 140544.06228604147\n",
      "Epoch 154501, Training Loss: 31442, Validation Loss: 52093, 150331.73277599015\n",
      "Epoch 154601, Training Loss: 33045, Validation Loss: 52029, 171445.98078000583\n",
      "Epoch 154701, Training Loss: 30274, Validation Loss: 52161, 132288.66482923206\n",
      "Epoch 154801, Training Loss: 29736, Validation Loss: 52954, 139585.58025983447\n",
      "Epoch 154901, Training Loss: 29519, Validation Loss: 53207, 137978.5021349421\n",
      "Epoch 155001, Training Loss: 30381, Validation Loss: 51838, 138972.4281792034\n",
      "Epoch 155101, Training Loss: 28768, Validation Loss: 52233, 133251.4781145793\n",
      "Epoch 155201, Training Loss: 31159, Validation Loss: 52482, 136230.76112827365\n",
      "Epoch 155301, Training Loss: 31550, Validation Loss: 52651, 148468.89826016367\n",
      "Epoch 155401, Training Loss: 29164, Validation Loss: 52023, 142598.79061218642\n",
      "Epoch 155501, Training Loss: 30031, Validation Loss: 53125, 157504.72021888467\n",
      "Epoch 155601, Training Loss: 30570, Validation Loss: 53878, 151487.0018367102\n",
      "Epoch 155701, Training Loss: 32996, Validation Loss: 54760, 178484.15450948168\n",
      "Epoch 155801, Training Loss: 29663, Validation Loss: 53795, 147550.85367387245\n",
      "Epoch 155901, Training Loss: 29095, Validation Loss: 55274, 151677.1030507283\n",
      "Epoch 156001, Training Loss: 31287, Validation Loss: 57276, 170265.43405688758\n",
      "Epoch 156101, Training Loss: 31053, Validation Loss: 54271, 124723.1208939992\n",
      "Epoch 156201, Training Loss: 28854, Validation Loss: 52475, 164627.40369269825\n",
      "Epoch 156301, Training Loss: 30402, Validation Loss: 55219, 148170.38127656348\n",
      "Epoch 156401, Training Loss: 30905, Validation Loss: 53619, 132660.11118687232\n",
      "Epoch 156501, Training Loss: 29846, Validation Loss: 53913, 141224.62481176425\n",
      "Epoch 156601, Training Loss: 31058, Validation Loss: 53564, 129229.10856690242\n",
      "Epoch 156701, Training Loss: 29785, Validation Loss: 54145, 171330.08407981205\n",
      "Epoch 156801, Training Loss: 28864, Validation Loss: 52709, 138402.68881898906\n",
      "Epoch 156901, Training Loss: 30115, Validation Loss: 54616, 141856.53404251725\n",
      "Epoch 157001, Training Loss: 32081, Validation Loss: 53200, 142400.1951063702\n",
      "Epoch 157101, Training Loss: 29369, Validation Loss: 54110, 167976.00994186345\n",
      "Epoch 157201, Training Loss: 29574, Validation Loss: 52587, 132356.0005457651\n",
      "Epoch 157301, Training Loss: 29648, Validation Loss: 54174, 139552.3586382527\n",
      "Epoch 157401, Training Loss: 30062, Validation Loss: 52693, 154700.6779374732\n",
      "Epoch 157501, Training Loss: 30436, Validation Loss: 54991, 139806.5980549\n",
      "Epoch 157601, Training Loss: 29161, Validation Loss: 56324, 150029.55156643005\n",
      "Epoch 157701, Training Loss: 32547, Validation Loss: 53265, 161908.55346658247\n",
      "Epoch 157801, Training Loss: 30081, Validation Loss: 53372, 152633.06024615155\n",
      "Epoch 157901, Training Loss: 29991, Validation Loss: 54204, 146122.06107941826\n",
      "Epoch 158001, Training Loss: 30267, Validation Loss: 55757, 207838.59908115055\n",
      "Epoch 158101, Training Loss: 29846, Validation Loss: 54493, 130382.23080652885\n",
      "Epoch 158201, Training Loss: 29495, Validation Loss: 51976, 133572.61140214826\n",
      "Epoch 158301, Training Loss: 31384, Validation Loss: 52990, 184641.5324343504\n",
      "Epoch 158401, Training Loss: 30094, Validation Loss: 53720, 129965.23632932117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158501, Training Loss: 31329, Validation Loss: 51872, 157619.9010729148\n",
      "Epoch 158601, Training Loss: 31585, Validation Loss: 51223, 170830.6219371984\n",
      "Epoch 158701, Training Loss: 31543, Validation Loss: 54471, 159932.97344864477\n",
      "Epoch 158801, Training Loss: 30993, Validation Loss: 54047, 136972.66819308797\n",
      "Epoch 158901, Training Loss: 29034, Validation Loss: 53147, 143377.10323352378\n",
      "Epoch 159001, Training Loss: 30624, Validation Loss: 53547, 173301.4582936042\n",
      "Epoch 159101, Training Loss: 30742, Validation Loss: 51347, 157634.4950815243\n",
      "Epoch 159201, Training Loss: 28638, Validation Loss: 53833, 137669.76078009943\n",
      "Epoch 159301, Training Loss: 30837, Validation Loss: 54863, 152287.52324509376\n",
      "Epoch 159401, Training Loss: 31667, Validation Loss: 52954, 159156.5583611047\n",
      "Epoch 159501, Training Loss: 30246, Validation Loss: 52179, 157629.6156954974\n",
      "Epoch 159601, Training Loss: 31204, Validation Loss: 52566, 147473.26262215612\n",
      "Epoch 159701, Training Loss: 30857, Validation Loss: 52290, 129193.10223409448\n",
      "Epoch 159801, Training Loss: 30010, Validation Loss: 54880, 126516.62343244944\n",
      "Epoch 159901, Training Loss: 30039, Validation Loss: 56681, 160131.34343840202\n",
      "Epoch 160001, Training Loss: 31947, Validation Loss: 54299, 142386.9810836694\n",
      "Epoch 160101, Training Loss: 31489, Validation Loss: 54058, 149643.1663652164\n",
      "Epoch 160201, Training Loss: 30715, Validation Loss: 54342, 150438.14601742645\n",
      "Epoch 160301, Training Loss: 30127, Validation Loss: 53426, 177313.29967465953\n",
      "Epoch 160401, Training Loss: 29235, Validation Loss: 53594, 121377.09270263815\n",
      "Epoch 160501, Training Loss: 28410, Validation Loss: 55572, 141366.16840516095\n",
      "Epoch 160601, Training Loss: 29214, Validation Loss: 53755, 143835.5761084849\n",
      "Epoch 160701, Training Loss: 29987, Validation Loss: 54163, 148797.8698046024\n",
      "Epoch 160801, Training Loss: 30118, Validation Loss: 54247, 135379.94161401672\n",
      "Epoch 160901, Training Loss: 29503, Validation Loss: 54535, 163740.43345832586\n",
      "Epoch 161001, Training Loss: 30881, Validation Loss: 54251, 150583.7448828394\n",
      "Epoch 161101, Training Loss: 29222, Validation Loss: 52747, 148437.61212081407\n",
      "Epoch 161201, Training Loss: 31232, Validation Loss: 53512, 161851.67933534365\n",
      "Epoch 161301, Training Loss: 29300, Validation Loss: 52260, 142882.59442663513\n",
      "Epoch 161401, Training Loss: 32344, Validation Loss: 54982, 158383.86744442466\n",
      "Epoch 161501, Training Loss: 30148, Validation Loss: 53273, 159232.653991751\n",
      "Epoch 161601, Training Loss: 30136, Validation Loss: 55630, 178962.03593640152\n",
      "Epoch 161701, Training Loss: 29146, Validation Loss: 52948, 150187.25638003414\n",
      "Epoch 161801, Training Loss: 29413, Validation Loss: 56098, 140472.0951859764\n",
      "Epoch 161901, Training Loss: 29903, Validation Loss: 54769, 183172.72923056208\n",
      "Epoch 162001, Training Loss: 29999, Validation Loss: 53594, 154484.49162841387\n",
      "Epoch 162101, Training Loss: 29660, Validation Loss: 54483, 129020.03045806864\n",
      "Epoch 162201, Training Loss: 30491, Validation Loss: 55162, 144750.42755266832\n",
      "Epoch 162301, Training Loss: 30109, Validation Loss: 52244, 141467.27619515362\n",
      "Epoch 162401, Training Loss: 31027, Validation Loss: 54450, 155084.01339083706\n",
      "Epoch 162501, Training Loss: 28763, Validation Loss: 52821, 152649.08406641305\n",
      "Epoch 162601, Training Loss: 30181, Validation Loss: 54810, 135163.92683482962\n",
      "Epoch 162701, Training Loss: 30010, Validation Loss: 54064, 129331.96503914676\n",
      "Epoch 162801, Training Loss: 30349, Validation Loss: 53987, 140198.08004325282\n",
      "Epoch 162901, Training Loss: 28511, Validation Loss: 54114, 137796.108713037\n",
      "Epoch 163001, Training Loss: 29026, Validation Loss: 53684, 143818.16650036996\n",
      "Epoch 163101, Training Loss: 31140, Validation Loss: 54085, 159022.7556145497\n",
      "Epoch 163201, Training Loss: 29602, Validation Loss: 55029, 150837.34278183905\n",
      "Epoch 163301, Training Loss: 29246, Validation Loss: 54567, 151565.64682911872\n",
      "Epoch 163401, Training Loss: 28753, Validation Loss: 55404, 185854.74027836407\n",
      "Epoch 163501, Training Loss: 29266, Validation Loss: 53098, 159140.51773941523\n",
      "Epoch 163601, Training Loss: 30089, Validation Loss: 52679, 131316.3606777526\n",
      "Epoch 163701, Training Loss: 29132, Validation Loss: 54100, 135471.15252994397\n",
      "Epoch 163801, Training Loss: 29545, Validation Loss: 53269, 140526.34981243772\n",
      "Epoch 163901, Training Loss: 30203, Validation Loss: 53450, 130048.85227420565\n",
      "Epoch 164001, Training Loss: 29997, Validation Loss: 55353, 154573.22853729659\n",
      "Epoch 164101, Training Loss: 29283, Validation Loss: 53666, 148719.81350531697\n",
      "Epoch 164201, Training Loss: 29974, Validation Loss: 53813, 131707.89386715234\n",
      "Epoch 164301, Training Loss: 30021, Validation Loss: 54689, 149851.25106479187\n",
      "Epoch 164401, Training Loss: 29474, Validation Loss: 54663, 152174.42022565223\n",
      "Epoch 164501, Training Loss: 30845, Validation Loss: 55280, 178432.23550177994\n",
      "Epoch 164601, Training Loss: 30665, Validation Loss: 56282, 143060.18164275656\n",
      "Epoch 164701, Training Loss: 29388, Validation Loss: 52572, 131499.24483080866\n",
      "Epoch 164801, Training Loss: 29660, Validation Loss: 53535, 135120.5920533603\n",
      "Epoch 164901, Training Loss: 28176, Validation Loss: 52766, 158921.76796628468\n",
      "Epoch 165001, Training Loss: 30496, Validation Loss: 55983, 152361.07604999805\n",
      "Epoch 165101, Training Loss: 31341, Validation Loss: 54780, 157682.6203005379\n",
      "Epoch 165201, Training Loss: 29575, Validation Loss: 53613, 138013.2391863885\n",
      "Epoch 165301, Training Loss: 32241, Validation Loss: 53058, 133202.3486306507\n",
      "Epoch 165401, Training Loss: 30224, Validation Loss: 53760, 132068.62368633185\n",
      "Epoch 165501, Training Loss: 29959, Validation Loss: 54181, 151096.6100993439\n",
      "Epoch 165601, Training Loss: 30128, Validation Loss: 53785, 120453.3457695465\n",
      "Epoch 165701, Training Loss: 28570, Validation Loss: 53476, 124489.55486152512\n",
      "Epoch 165801, Training Loss: 29758, Validation Loss: 53912, 134236.9452612046\n",
      "Epoch 165901, Training Loss: 30010, Validation Loss: 52582, 149965.95450971142\n",
      "Epoch 166001, Training Loss: 31072, Validation Loss: 52968, 122118.72065339136\n",
      "Epoch 166101, Training Loss: 29746, Validation Loss: 55318, 151963.49280838287\n",
      "Epoch 166201, Training Loss: 30158, Validation Loss: 54305, 123944.59561829004\n",
      "Epoch 166301, Training Loss: 30913, Validation Loss: 53285, 134907.7644914657\n",
      "Epoch 166401, Training Loss: 30137, Validation Loss: 54424, 143332.12289589757\n",
      "Epoch 166501, Training Loss: 31182, Validation Loss: 52229, 161631.6500905285\n",
      "Epoch 166601, Training Loss: 33347, Validation Loss: 54126, 161266.6725769358\n",
      "Epoch 166701, Training Loss: 32197, Validation Loss: 57610, 129839.8905615103\n",
      "Epoch 166801, Training Loss: 31216, Validation Loss: 54677, 142718.45275114768\n",
      "Epoch 166901, Training Loss: 30271, Validation Loss: 52303, 149548.3484989331\n",
      "Epoch 167001, Training Loss: 29841, Validation Loss: 54146, 138924.63406636438\n",
      "Epoch 167101, Training Loss: 31079, Validation Loss: 52792, 169335.49412888178\n",
      "Epoch 167201, Training Loss: 31609, Validation Loss: 55013, 132323.30356470516\n",
      "Epoch 167301, Training Loss: 32773, Validation Loss: 55940, 148693.74452443383\n",
      "Epoch 167401, Training Loss: 31799, Validation Loss: 55394, 149564.8538743274\n",
      "Epoch 167501, Training Loss: 31065, Validation Loss: 54472, 156288.10632620534\n",
      "Epoch 167601, Training Loss: 32095, Validation Loss: 55052, 151975.30612414947\n",
      "Epoch 167701, Training Loss: 31853, Validation Loss: 55229, 143040.86179809194\n",
      "Epoch 167801, Training Loss: 29958, Validation Loss: 53474, 124969.97441475299\n",
      "Epoch 167901, Training Loss: 30931, Validation Loss: 55512, 120436.89820129234\n",
      "Epoch 168001, Training Loss: 30339, Validation Loss: 56137, 133451.03472644632\n",
      "Epoch 168101, Training Loss: 30746, Validation Loss: 52735, 134533.18994826337\n",
      "Epoch 168201, Training Loss: 30763, Validation Loss: 53221, 166804.97958404137\n",
      "Epoch 168301, Training Loss: 32417, Validation Loss: 54548, 147403.53688397672\n",
      "Epoch 168401, Training Loss: 31182, Validation Loss: 53959, 147639.08307957347\n",
      "Epoch 168501, Training Loss: 32178, Validation Loss: 52914, 149326.91424887878\n",
      "Epoch 168601, Training Loss: 30008, Validation Loss: 56608, 153382.43576169267\n",
      "Epoch 168701, Training Loss: 32404, Validation Loss: 54266, 169982.06363586962\n",
      "Epoch 168801, Training Loss: 31760, Validation Loss: 54888, 122493.65539856166\n",
      "Epoch 168901, Training Loss: 31674, Validation Loss: 56931, 166137.39270956983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169001, Training Loss: 31196, Validation Loss: 55638, 122484.36055369212\n",
      "Epoch 169101, Training Loss: 31966, Validation Loss: 52631, 142575.77326900093\n",
      "Epoch 169201, Training Loss: 31921, Validation Loss: 53879, 168120.67751433916\n",
      "Epoch 169301, Training Loss: 31400, Validation Loss: 54466, 173351.78009375217\n",
      "Epoch 169401, Training Loss: 30778, Validation Loss: 55081, 110848.53754983936\n",
      "Epoch 169501, Training Loss: 31454, Validation Loss: 54730, 166128.3339137155\n",
      "Epoch 169601, Training Loss: 31758, Validation Loss: 53420, 148868.4475099261\n",
      "Epoch 169701, Training Loss: 31342, Validation Loss: 56040, 182526.18900381745\n",
      "Epoch 169801, Training Loss: 31949, Validation Loss: 54168, 157763.9069991865\n",
      "Epoch 169901, Training Loss: 30471, Validation Loss: 54671, 131874.12208276583\n",
      "Epoch 170001, Training Loss: 29792, Validation Loss: 54837, 127383.10259350669\n",
      "Epoch 170101, Training Loss: 30118, Validation Loss: 53429, 158409.39324327558\n",
      "Epoch 170201, Training Loss: 30568, Validation Loss: 55376, 146677.374122773\n",
      "Epoch 170301, Training Loss: 31770, Validation Loss: 55369, 151059.21218301146\n",
      "Epoch 170401, Training Loss: 29819, Validation Loss: 54968, 175931.32451219438\n",
      "Epoch 170501, Training Loss: 30699, Validation Loss: 55730, 139984.59832172847\n",
      "Epoch 170601, Training Loss: 31312, Validation Loss: 53749, 184102.78179803866\n",
      "Epoch 170701, Training Loss: 33641, Validation Loss: 54425, 141684.94720120082\n",
      "Epoch 170801, Training Loss: 30719, Validation Loss: 53131, 121358.62320614952\n",
      "Epoch 170901, Training Loss: 31129, Validation Loss: 53091, 144260.86204973626\n",
      "Epoch 171001, Training Loss: 32322, Validation Loss: 55724, 139382.04656757886\n",
      "Epoch 171101, Training Loss: 30042, Validation Loss: 54158, 137325.12758155222\n",
      "Epoch 171201, Training Loss: 31879, Validation Loss: 55200, 138103.17695314393\n",
      "Epoch 171301, Training Loss: 31967, Validation Loss: 54094, 165050.13729378607\n",
      "Epoch 171401, Training Loss: 28740, Validation Loss: 55531, 152508.54805538928\n",
      "Epoch 171501, Training Loss: 30482, Validation Loss: 56583, 155774.5161450676\n",
      "Epoch 171601, Training Loss: 32399, Validation Loss: 58898, 184803.45156722993\n",
      "Epoch 171701, Training Loss: 30810, Validation Loss: 54018, 126883.3310176745\n",
      "Epoch 171801, Training Loss: 31078, Validation Loss: 53053, 144438.9156034643\n",
      "Epoch 171901, Training Loss: 30723, Validation Loss: 53592, 154435.35617344282\n",
      "Epoch 172001, Training Loss: 30639, Validation Loss: 56310, 132457.92001974856\n",
      "Epoch 172101, Training Loss: 31519, Validation Loss: 53529, 136487.0308861312\n",
      "Epoch 172201, Training Loss: 31615, Validation Loss: 55441, 152583.87029960242\n",
      "Epoch 172301, Training Loss: 31062, Validation Loss: 54813, 161213.93350920448\n",
      "Epoch 172401, Training Loss: 32502, Validation Loss: 58018, 210459.40634803753\n",
      "Epoch 172501, Training Loss: 32578, Validation Loss: 54737, 133236.2160920496\n",
      "Epoch 172601, Training Loss: 29788, Validation Loss: 56360, 153640.11013023445\n",
      "Epoch 172701, Training Loss: 30367, Validation Loss: 52783, 163076.51418340058\n",
      "Epoch 172801, Training Loss: 32586, Validation Loss: 52134, 151199.6888960966\n",
      "Epoch 172901, Training Loss: 32253, Validation Loss: 52502, 151984.16589022527\n",
      "Epoch 173001, Training Loss: 32401, Validation Loss: 53284, 160175.23925897502\n",
      "Epoch 173101, Training Loss: 30679, Validation Loss: 54170, 125824.56642692769\n",
      "Epoch 173201, Training Loss: 31612, Validation Loss: 54842, 157546.8193556564\n",
      "Epoch 173301, Training Loss: 31022, Validation Loss: 57743, 129751.49547916996\n",
      "Epoch 173401, Training Loss: 30910, Validation Loss: 53490, 151025.26695679894\n",
      "Epoch 173501, Training Loss: 32001, Validation Loss: 55499, 172043.82999007215\n",
      "Epoch 173601, Training Loss: 30238, Validation Loss: 53410, 146589.059698539\n",
      "Epoch 173701, Training Loss: 32027, Validation Loss: 53515, 161285.87845345662\n",
      "Epoch 173801, Training Loss: 32067, Validation Loss: 53752, 142571.72806490632\n",
      "Epoch 173901, Training Loss: 30088, Validation Loss: 53340, 136919.2607144196\n",
      "Epoch 174001, Training Loss: 29793, Validation Loss: 53720, 184154.7005724805\n",
      "Epoch 174101, Training Loss: 31446, Validation Loss: 54407, 167836.4889386366\n",
      "Epoch 174201, Training Loss: 30478, Validation Loss: 53090, 155091.11277534437\n",
      "Epoch 174301, Training Loss: 31844, Validation Loss: 54290, 154294.51495581842\n",
      "Epoch 174401, Training Loss: 32277, Validation Loss: 54137, 142105.35400643124\n",
      "Epoch 174501, Training Loss: 31815, Validation Loss: 52669, 144912.22093602395\n",
      "Epoch 174601, Training Loss: 30941, Validation Loss: 53934, 130074.78792951682\n",
      "Epoch 174701, Training Loss: 30931, Validation Loss: 55198, 134019.59334354592\n",
      "Epoch 174801, Training Loss: 32156, Validation Loss: 57290, 123540.92217292199\n",
      "Epoch 174901, Training Loss: 30264, Validation Loss: 54944, 138248.17970727634\n",
      "Epoch 175001, Training Loss: 32343, Validation Loss: 54797, 155444.28169458406\n",
      "Epoch 175101, Training Loss: 32191, Validation Loss: 52985, 130260.89980107387\n",
      "Epoch 175201, Training Loss: 30859, Validation Loss: 54307, 158889.2450623726\n",
      "Epoch 175301, Training Loss: 32922, Validation Loss: 55403, 167666.17820385666\n",
      "Epoch 175401, Training Loss: 29685, Validation Loss: 55699, 156894.21469926278\n",
      "Epoch 175501, Training Loss: 31921, Validation Loss: 54945, 178382.02273018472\n",
      "Epoch 175601, Training Loss: 32174, Validation Loss: 55047, 165199.37706879474\n",
      "Epoch 175701, Training Loss: 30473, Validation Loss: 54061, 127123.42277378507\n",
      "Epoch 175801, Training Loss: 32106, Validation Loss: 56157, 174255.21197726286\n",
      "Epoch 175901, Training Loss: 29834, Validation Loss: 56225, 162603.07119252475\n",
      "Epoch 176001, Training Loss: 33019, Validation Loss: 54127, 186551.05064154376\n",
      "Epoch 176101, Training Loss: 30372, Validation Loss: 54521, 153184.50103802475\n",
      "Epoch 176201, Training Loss: 31387, Validation Loss: 55668, 171684.05658174213\n",
      "Epoch 176301, Training Loss: 31367, Validation Loss: 54224, 158783.08212113785\n",
      "Epoch 176401, Training Loss: 30312, Validation Loss: 54792, 130044.98705738553\n",
      "Epoch 176501, Training Loss: 32555, Validation Loss: 54124, 158570.77845253437\n",
      "Epoch 176601, Training Loss: 29822, Validation Loss: 54317, 146884.30215590805\n",
      "Epoch 176701, Training Loss: 31220, Validation Loss: 55831, 155540.36162536114\n",
      "Epoch 176801, Training Loss: 29402, Validation Loss: 54295, 164402.47702350892\n",
      "Epoch 176901, Training Loss: 30906, Validation Loss: 56669, 142263.08461212847\n",
      "Epoch 177001, Training Loss: 29160, Validation Loss: 54553, 129811.08587835352\n",
      "Epoch 177101, Training Loss: 29959, Validation Loss: 55767, 149844.36488969016\n",
      "Epoch 177201, Training Loss: 32994, Validation Loss: 55251, 143204.41027032456\n",
      "Epoch 177301, Training Loss: 31188, Validation Loss: 55212, 160259.72574960248\n",
      "Epoch 177401, Training Loss: 29778, Validation Loss: 55254, 141115.49283811165\n",
      "Epoch 177501, Training Loss: 31344, Validation Loss: 53212, 165673.37216418269\n",
      "Epoch 177601, Training Loss: 29806, Validation Loss: 54509, 155569.63584378897\n",
      "Epoch 177701, Training Loss: 31133, Validation Loss: 53728, 120379.84040237474\n",
      "Epoch 177801, Training Loss: 29872, Validation Loss: 55255, 129755.60699664537\n",
      "Epoch 177901, Training Loss: 31347, Validation Loss: 53537, 148330.6626536855\n",
      "Epoch 178001, Training Loss: 30772, Validation Loss: 55354, 147497.51406226013\n",
      "Epoch 178101, Training Loss: 30811, Validation Loss: 55062, 132416.25570686918\n",
      "Epoch 178201, Training Loss: 31458, Validation Loss: 56574, 160575.01942097704\n",
      "Epoch 178301, Training Loss: 32653, Validation Loss: 52200, 186795.3621934171\n",
      "Epoch 178401, Training Loss: 31300, Validation Loss: 53644, 153042.1189469873\n",
      "Epoch 178501, Training Loss: 31724, Validation Loss: 55139, 157804.14718968255\n",
      "Epoch 178601, Training Loss: 31222, Validation Loss: 55883, 153217.64260684335\n",
      "Epoch 178701, Training Loss: 30739, Validation Loss: 54292, 150718.41528237393\n",
      "Epoch 178801, Training Loss: 33286, Validation Loss: 57780, 172355.96618363156\n",
      "Epoch 178901, Training Loss: 31887, Validation Loss: 59038, 156426.32351970967\n",
      "Epoch 179001, Training Loss: 30486, Validation Loss: 53523, 171122.94582335989\n",
      "Epoch 179101, Training Loss: 31820, Validation Loss: 53783, 184215.48834255562\n",
      "Epoch 179201, Training Loss: 31814, Validation Loss: 53838, 158046.22853397898\n",
      "Epoch 179301, Training Loss: 31150, Validation Loss: 54214, 154544.57900962682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179401, Training Loss: 30521, Validation Loss: 54075, 145860.0517247324\n",
      "Epoch 179501, Training Loss: 30120, Validation Loss: 52870, 143337.2327691925\n",
      "Epoch 179601, Training Loss: 31332, Validation Loss: 54453, 165935.01792224814\n",
      "Epoch 179701, Training Loss: 30889, Validation Loss: 56769, 115906.63482915107\n",
      "Epoch 179801, Training Loss: 29005, Validation Loss: 54439, 143850.46766624664\n",
      "Epoch 179901, Training Loss: 30862, Validation Loss: 55231, 133030.40491416096\n",
      "Epoch 180001, Training Loss: 31856, Validation Loss: 52873, 132642.53211019124\n",
      "Epoch 180101, Training Loss: 34127, Validation Loss: 54754, 160594.60305185313\n",
      "Epoch 180201, Training Loss: 30906, Validation Loss: 53535, 147948.9394905527\n",
      "Epoch 180301, Training Loss: 32389, Validation Loss: 54811, 143012.42097292253\n",
      "Epoch 180401, Training Loss: 30741, Validation Loss: 53132, 144803.4622073383\n",
      "Epoch 180501, Training Loss: 30972, Validation Loss: 55096, 147727.91827341792\n",
      "Epoch 180601, Training Loss: 31397, Validation Loss: 53541, 148924.054095713\n",
      "Epoch 180701, Training Loss: 30299, Validation Loss: 54048, 165978.17024759273\n",
      "Epoch 180801, Training Loss: 30183, Validation Loss: 55907, 141462.46260729647\n",
      "Epoch 180901, Training Loss: 29971, Validation Loss: 52807, 151394.2271823763\n",
      "Epoch 181001, Training Loss: 31383, Validation Loss: 56073, 139699.95677497928\n",
      "Epoch 181101, Training Loss: 30310, Validation Loss: 54942, 158327.2386188862\n",
      "Epoch 181201, Training Loss: 29655, Validation Loss: 53447, 160604.73009799464\n",
      "Epoch 181301, Training Loss: 32069, Validation Loss: 55555, 162532.88994787686\n",
      "Epoch 181401, Training Loss: 30811, Validation Loss: 54889, 155302.2384099109\n",
      "Epoch 181501, Training Loss: 31147, Validation Loss: 53178, 160549.44164139617\n",
      "Epoch 181601, Training Loss: 29819, Validation Loss: 53990, 144094.04426792817\n",
      "Epoch 181701, Training Loss: 30324, Validation Loss: 54809, 172691.17095617982\n",
      "Epoch 181801, Training Loss: 29844, Validation Loss: 54671, 122519.53526759791\n",
      "Epoch 181901, Training Loss: 31053, Validation Loss: 56066, 155790.9103766788\n",
      "Epoch 182001, Training Loss: 29909, Validation Loss: 54464, 123460.0319143649\n",
      "Epoch 182101, Training Loss: 30500, Validation Loss: 56547, 134969.25094376254\n",
      "Epoch 182201, Training Loss: 30628, Validation Loss: 54529, 140849.4466068793\n",
      "Epoch 182301, Training Loss: 30107, Validation Loss: 52914, 143152.90737987933\n",
      "Epoch 182401, Training Loss: 30385, Validation Loss: 54165, 153227.9457134134\n",
      "Epoch 182501, Training Loss: 30936, Validation Loss: 53226, 149088.10203147316\n",
      "Epoch 182601, Training Loss: 30497, Validation Loss: 55901, 135943.31009101405\n",
      "Epoch 182701, Training Loss: 32016, Validation Loss: 55948, 175029.39071993824\n",
      "Epoch 182801, Training Loss: 32326, Validation Loss: 54107, 152522.2980520759\n",
      "Epoch 182901, Training Loss: 33551, Validation Loss: 53453, 151602.09337042182\n",
      "Epoch 183001, Training Loss: 29369, Validation Loss: 55509, 139462.42441942246\n",
      "Epoch 183101, Training Loss: 32281, Validation Loss: 54295, 150526.308525327\n",
      "Epoch 183201, Training Loss: 31617, Validation Loss: 55492, 126342.0425866481\n",
      "Epoch 183301, Training Loss: 30524, Validation Loss: 56528, 124914.61952879892\n",
      "Epoch 183401, Training Loss: 30104, Validation Loss: 53863, 162114.95301099273\n",
      "Epoch 183501, Training Loss: 31359, Validation Loss: 53734, 143698.54642329225\n",
      "Epoch 183601, Training Loss: 32653, Validation Loss: 55678, 147264.36987540702\n",
      "Epoch 183701, Training Loss: 32829, Validation Loss: 53921, 135073.84984000397\n",
      "Epoch 183801, Training Loss: 30707, Validation Loss: 54839, 145232.7646415169\n",
      "Epoch 183901, Training Loss: 31779, Validation Loss: 54178, 153179.48092663023\n",
      "Epoch 184001, Training Loss: 32184, Validation Loss: 55349, 141521.3542015988\n",
      "Epoch 184101, Training Loss: 30890, Validation Loss: 55481, 141610.7364537479\n",
      "Epoch 184201, Training Loss: 31064, Validation Loss: 55186, 161785.82595140248\n",
      "Epoch 184301, Training Loss: 30838, Validation Loss: 53065, 150869.9792160339\n",
      "Epoch 184401, Training Loss: 30901, Validation Loss: 55196, 136938.1068958143\n",
      "Epoch 184501, Training Loss: 31111, Validation Loss: 55392, 162587.85677016593\n",
      "Epoch 184601, Training Loss: 29741, Validation Loss: 52567, 136665.43535267003\n",
      "Epoch 184701, Training Loss: 30583, Validation Loss: 55947, 151357.49754592366\n",
      "Epoch 184801, Training Loss: 32245, Validation Loss: 51531, 153225.83653956963\n",
      "Epoch 184901, Training Loss: 32387, Validation Loss: 56271, 189222.53380882577\n",
      "Epoch 185001, Training Loss: 30641, Validation Loss: 55417, 166140.13179403375\n",
      "Epoch 185101, Training Loss: 30840, Validation Loss: 53984, 180083.7237221423\n",
      "Epoch 185201, Training Loss: 30381, Validation Loss: 55739, 133039.23862590568\n",
      "Epoch 185301, Training Loss: 30359, Validation Loss: 56520, 180295.37847044563\n",
      "Epoch 185401, Training Loss: 31318, Validation Loss: 54343, 154462.24380360966\n",
      "Epoch 185501, Training Loss: 30840, Validation Loss: 53724, 157758.73555677623\n",
      "Epoch 185601, Training Loss: 31030, Validation Loss: 54549, 144366.51082170472\n",
      "Epoch 185701, Training Loss: 30697, Validation Loss: 54936, 151956.44024523874\n",
      "Epoch 185801, Training Loss: 30575, Validation Loss: 54436, 152610.46022161812\n",
      "Epoch 185901, Training Loss: 31600, Validation Loss: 56758, 144634.7306509639\n",
      "Epoch 186001, Training Loss: 32731, Validation Loss: 57243, 177495.4542337628\n",
      "Epoch 186101, Training Loss: 30155, Validation Loss: 53466, 126948.29571508458\n",
      "Epoch 186201, Training Loss: 30914, Validation Loss: 56005, 162243.24482499817\n",
      "Epoch 186301, Training Loss: 29285, Validation Loss: 54750, 138433.7966878293\n",
      "Epoch 186401, Training Loss: 29846, Validation Loss: 54193, 128181.48853578512\n",
      "Epoch 186501, Training Loss: 31060, Validation Loss: 55149, 172666.9930834238\n",
      "Epoch 186601, Training Loss: 31812, Validation Loss: 55193, 165936.95715339162\n",
      "Epoch 186701, Training Loss: 31791, Validation Loss: 53401, 166102.69053265458\n",
      "Epoch 186801, Training Loss: 31828, Validation Loss: 55267, 129476.81845376776\n",
      "Epoch 186901, Training Loss: 32138, Validation Loss: 55590, 165362.3371525545\n",
      "Epoch 187001, Training Loss: 31071, Validation Loss: 54931, 168458.81119333464\n",
      "Epoch 187101, Training Loss: 29143, Validation Loss: 56131, 165950.40200690733\n",
      "Epoch 187201, Training Loss: 32573, Validation Loss: 52648, 180716.45319991215\n",
      "Epoch 187301, Training Loss: 30413, Validation Loss: 55612, 159043.64711354667\n",
      "Epoch 187401, Training Loss: 32049, Validation Loss: 54880, 149716.42418515877\n",
      "Epoch 187501, Training Loss: 29816, Validation Loss: 56100, 126777.42562053769\n",
      "Epoch 187601, Training Loss: 30709, Validation Loss: 52973, 136268.99814547962\n",
      "Epoch 187701, Training Loss: 30715, Validation Loss: 52834, 133570.00706461404\n",
      "Epoch 187801, Training Loss: 30237, Validation Loss: 53042, 157646.12224874154\n",
      "Epoch 187901, Training Loss: 31481, Validation Loss: 53680, 155553.71626641403\n",
      "Epoch 188001, Training Loss: 31445, Validation Loss: 56878, 187807.6256078586\n",
      "Epoch 188101, Training Loss: 31211, Validation Loss: 52447, 131652.57579549178\n",
      "Epoch 188201, Training Loss: 30675, Validation Loss: 55332, 168294.69477457536\n",
      "Epoch 188301, Training Loss: 31982, Validation Loss: 54014, 128613.57175616184\n",
      "Epoch 188401, Training Loss: 32789, Validation Loss: 54535, 167965.18382096937\n",
      "Epoch 188501, Training Loss: 31528, Validation Loss: 55181, 148769.00477821473\n",
      "Epoch 188601, Training Loss: 30343, Validation Loss: 54149, 147017.9670524579\n",
      "Epoch 188701, Training Loss: 29579, Validation Loss: 53550, 138866.68953879215\n",
      "Epoch 188801, Training Loss: 31616, Validation Loss: 52777, 136101.22921661526\n",
      "Epoch 188901, Training Loss: 30491, Validation Loss: 54503, 128687.10678727801\n",
      "Epoch 189001, Training Loss: 28448, Validation Loss: 54541, 129803.09054996388\n",
      "Epoch 189101, Training Loss: 29632, Validation Loss: 56032, 167552.1405321487\n",
      "Epoch 189201, Training Loss: 30276, Validation Loss: 53567, 158219.35785592164\n",
      "Epoch 189301, Training Loss: 29030, Validation Loss: 54387, 151922.7748307917\n",
      "Epoch 189401, Training Loss: 31666, Validation Loss: 55207, 154225.72016151165\n",
      "Epoch 189501, Training Loss: 31779, Validation Loss: 56514, 150025.56139654628\n",
      "Epoch 189601, Training Loss: 31051, Validation Loss: 52906, 137970.68908295472\n",
      "Epoch 189701, Training Loss: 29195, Validation Loss: 55110, 125044.50444931639\n",
      "Epoch 189801, Training Loss: 29672, Validation Loss: 53434, 148002.66567365642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189901, Training Loss: 28404, Validation Loss: 55383, 148092.23215367866\n",
      "Epoch 190001, Training Loss: 29979, Validation Loss: 56708, 158518.64240647206\n",
      "Epoch 190101, Training Loss: 30427, Validation Loss: 55538, 182075.3425825575\n",
      "Epoch 190201, Training Loss: 32101, Validation Loss: 53140, 178156.85512937064\n",
      "Epoch 190301, Training Loss: 30081, Validation Loss: 55518, 125674.52554965124\n",
      "Epoch 190401, Training Loss: 31496, Validation Loss: 55924, 164996.22337374635\n",
      "Epoch 190501, Training Loss: 28510, Validation Loss: 54384, 144754.0825872438\n",
      "Epoch 190601, Training Loss: 30292, Validation Loss: 53473, 150307.19194852564\n",
      "Epoch 190701, Training Loss: 31060, Validation Loss: 55902, 144724.27925503434\n",
      "Epoch 190801, Training Loss: 31209, Validation Loss: 54085, 172155.61773520155\n",
      "Epoch 190901, Training Loss: 29762, Validation Loss: 52889, 157192.83063566522\n",
      "Epoch 191001, Training Loss: 30089, Validation Loss: 54100, 170587.86977187064\n",
      "Epoch 191101, Training Loss: 31675, Validation Loss: 55670, 146793.02681462181\n",
      "Epoch 191201, Training Loss: 30097, Validation Loss: 53859, 153905.86258447348\n",
      "Epoch 191301, Training Loss: 30507, Validation Loss: 55802, 157328.03638829905\n",
      "Epoch 191401, Training Loss: 32070, Validation Loss: 55316, 139580.36030904538\n",
      "Epoch 191501, Training Loss: 30660, Validation Loss: 53824, 165931.98194110682\n",
      "Epoch 191601, Training Loss: 31877, Validation Loss: 56079, 155000.68828085865\n",
      "Epoch 191701, Training Loss: 29734, Validation Loss: 54553, 174858.13213818555\n",
      "Epoch 191801, Training Loss: 31157, Validation Loss: 53980, 152879.81118769414\n",
      "Epoch 191901, Training Loss: 31327, Validation Loss: 53842, 149820.88950467276\n",
      "Epoch 192001, Training Loss: 31219, Validation Loss: 54637, 181413.1369137183\n",
      "Epoch 192101, Training Loss: 31963, Validation Loss: 53486, 132253.3727928333\n",
      "Epoch 192201, Training Loss: 28577, Validation Loss: 55933, 107888.32948083687\n",
      "Epoch 192301, Training Loss: 31091, Validation Loss: 55468, 174412.32464196283\n",
      "Epoch 192401, Training Loss: 29871, Validation Loss: 54942, 141812.29829176847\n",
      "Epoch 192501, Training Loss: 31140, Validation Loss: 54980, 151447.6456641758\n",
      "Epoch 192601, Training Loss: 29402, Validation Loss: 54645, 132241.82551115804\n",
      "Epoch 192701, Training Loss: 32370, Validation Loss: 55061, 144047.92533994032\n",
      "Epoch 192801, Training Loss: 30344, Validation Loss: 55105, 148758.18923001195\n",
      "Epoch 192901, Training Loss: 31696, Validation Loss: 54549, 191732.8103001942\n",
      "Epoch 193001, Training Loss: 30907, Validation Loss: 54101, 164227.23470513377\n",
      "Epoch 193101, Training Loss: 28705, Validation Loss: 54457, 170875.07153469344\n",
      "Epoch 193201, Training Loss: 30613, Validation Loss: 52161, 153223.18163993608\n",
      "Epoch 193301, Training Loss: 30574, Validation Loss: 55476, 150270.47472447986\n",
      "Epoch 193401, Training Loss: 29301, Validation Loss: 54669, 151509.75012088474\n",
      "Epoch 193501, Training Loss: 29836, Validation Loss: 54425, 135292.89779028343\n",
      "Epoch 193601, Training Loss: 31265, Validation Loss: 52508, 180000.01564388335\n",
      "Epoch 193701, Training Loss: 30236, Validation Loss: 57229, 153904.56551285618\n",
      "Epoch 193801, Training Loss: 30813, Validation Loss: 56471, 161964.31701278634\n",
      "Epoch 193901, Training Loss: 29387, Validation Loss: 52212, 136747.5057544448\n",
      "Epoch 194001, Training Loss: 30337, Validation Loss: 53564, 140388.06626958938\n",
      "Epoch 194101, Training Loss: 31501, Validation Loss: 52986, 174725.6306366291\n",
      "Epoch 194201, Training Loss: 29211, Validation Loss: 55350, 170357.96363870677\n",
      "Epoch 194301, Training Loss: 29521, Validation Loss: 55320, 123164.45089533638\n",
      "Epoch 194401, Training Loss: 30637, Validation Loss: 53977, 120412.148704475\n",
      "Epoch 194501, Training Loss: 30220, Validation Loss: 53797, 139479.441846747\n",
      "Epoch 194601, Training Loss: 31607, Validation Loss: 54781, 149576.44005681234\n",
      "Epoch 194701, Training Loss: 31433, Validation Loss: 53406, 158247.992227228\n",
      "Epoch 194801, Training Loss: 30072, Validation Loss: 56011, 152694.57297279156\n",
      "Epoch 194901, Training Loss: 29625, Validation Loss: 53592, 171028.81635475592\n",
      "Epoch 195001, Training Loss: 30907, Validation Loss: 55828, 153297.28385143107\n",
      "Epoch 195101, Training Loss: 31434, Validation Loss: 52221, 154606.30084978545\n",
      "Epoch 195201, Training Loss: 30850, Validation Loss: 54002, 156948.58567053534\n",
      "Epoch 195301, Training Loss: 31373, Validation Loss: 54167, 128763.07563893618\n",
      "Epoch 195401, Training Loss: 31557, Validation Loss: 54260, 170881.18532239058\n",
      "Epoch 195501, Training Loss: 29076, Validation Loss: 55031, 150135.46873104398\n",
      "Epoch 195601, Training Loss: 30677, Validation Loss: 53126, 158483.68097498087\n",
      "Epoch 195701, Training Loss: 29395, Validation Loss: 54094, 164699.3282064908\n",
      "Epoch 195801, Training Loss: 29824, Validation Loss: 55737, 150384.0559318297\n",
      "Epoch 195901, Training Loss: 30479, Validation Loss: 53864, 141623.66238001428\n",
      "Epoch 196001, Training Loss: 29310, Validation Loss: 55387, 153161.85358337132\n",
      "Epoch 196101, Training Loss: 30738, Validation Loss: 53638, 157366.33638030756\n",
      "Epoch 196201, Training Loss: 30653, Validation Loss: 54264, 123709.55116668445\n",
      "Epoch 196301, Training Loss: 31593, Validation Loss: 53115, 149641.9609149612\n",
      "Epoch 196401, Training Loss: 31917, Validation Loss: 55322, 154991.91036162173\n",
      "Epoch 196501, Training Loss: 31263, Validation Loss: 55740, 147398.4887483983\n",
      "Epoch 196601, Training Loss: 30070, Validation Loss: 54573, 163516.94455143443\n",
      "Epoch 196701, Training Loss: 33234, Validation Loss: 53971, 151638.58468726175\n",
      "Epoch 196801, Training Loss: 32007, Validation Loss: 55740, 147725.85795114702\n",
      "Epoch 196901, Training Loss: 29012, Validation Loss: 55443, 129675.19737668273\n",
      "Epoch 197001, Training Loss: 29729, Validation Loss: 55924, 161290.90881521459\n",
      "Epoch 197101, Training Loss: 33186, Validation Loss: 54697, 139505.83734076735\n",
      "Epoch 197201, Training Loss: 31769, Validation Loss: 56089, 168780.50026278914\n",
      "Epoch 197301, Training Loss: 31401, Validation Loss: 53394, 166754.24250420247\n",
      "Epoch 197401, Training Loss: 28495, Validation Loss: 53394, 105960.16071901037\n",
      "Epoch 197501, Training Loss: 31134, Validation Loss: 56378, 144803.28046454274\n",
      "Epoch 197601, Training Loss: 30381, Validation Loss: 55238, 159016.76358995977\n",
      "Epoch 197701, Training Loss: 29720, Validation Loss: 57959, 150307.21261565696\n",
      "Epoch 197801, Training Loss: 31226, Validation Loss: 55548, 157586.71958542138\n",
      "Epoch 197901, Training Loss: 29960, Validation Loss: 55762, 150696.99309918235\n",
      "Epoch 198001, Training Loss: 30761, Validation Loss: 54645, 179603.568530238\n",
      "Epoch 198101, Training Loss: 28627, Validation Loss: 53698, 145101.1227791066\n",
      "Epoch 198201, Training Loss: 30162, Validation Loss: 55009, 133571.40019300487\n",
      "Epoch 198301, Training Loss: 30188, Validation Loss: 54907, 156082.69124270853\n",
      "Epoch 198401, Training Loss: 31524, Validation Loss: 55037, 136100.2414894724\n",
      "Epoch 198501, Training Loss: 29391, Validation Loss: 58524, 172605.68635681906\n",
      "Epoch 198601, Training Loss: 30639, Validation Loss: 54546, 133305.1841819534\n",
      "Epoch 198701, Training Loss: 30061, Validation Loss: 56634, 150105.17652244656\n",
      "Epoch 198801, Training Loss: 31096, Validation Loss: 55999, 168300.32369632306\n",
      "Epoch 198901, Training Loss: 30608, Validation Loss: 56670, 185959.44643953574\n",
      "Epoch 199001, Training Loss: 31194, Validation Loss: 54381, 152048.767038749\n",
      "Epoch 199101, Training Loss: 30872, Validation Loss: 55058, 150789.7682908899\n",
      "Epoch 199201, Training Loss: 29453, Validation Loss: 54848, 153534.7139420683\n",
      "Epoch 199301, Training Loss: 29767, Validation Loss: 54314, 138424.22469489666\n",
      "Epoch 199401, Training Loss: 30445, Validation Loss: 55307, 157748.8714240386\n",
      "Epoch 199501, Training Loss: 30775, Validation Loss: 56545, 163833.7119909734\n",
      "Epoch 199601, Training Loss: 28736, Validation Loss: 54306, 149915.39234052293\n",
      "Epoch 199701, Training Loss: 30054, Validation Loss: 55838, 144656.77318662134\n",
      "Epoch 199801, Training Loss: 31057, Validation Loss: 56376, 152719.7727989056\n",
      "Epoch 199901, Training Loss: 30020, Validation Loss: 56197, 167758.69188159116\n",
      "Epoch 200001, Training Loss: 31107, Validation Loss: 58074, 193842.30979472114\n",
      "Epoch 200101, Training Loss: 30247, Validation Loss: 56563, 148933.9824030278\n",
      "Epoch 200201, Training Loss: 30195, Validation Loss: 52690, 127327.37161521634\n",
      "Epoch 200301, Training Loss: 31452, Validation Loss: 55567, 159731.7794779215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200401, Training Loss: 31036, Validation Loss: 55221, 131042.87809697053\n",
      "Epoch 200501, Training Loss: 30163, Validation Loss: 54270, 130771.16414908537\n",
      "Epoch 200601, Training Loss: 30306, Validation Loss: 54251, 154622.06002142705\n",
      "Epoch 200701, Training Loss: 30975, Validation Loss: 53776, 152282.3141660283\n",
      "Epoch 200801, Training Loss: 32548, Validation Loss: 57553, 142783.51561705777\n",
      "Epoch 200901, Training Loss: 31130, Validation Loss: 54296, 124315.1165399022\n",
      "Epoch 201001, Training Loss: 31454, Validation Loss: 56228, 183932.86279281325\n",
      "Epoch 201101, Training Loss: 29704, Validation Loss: 54855, 152716.03303173\n",
      "Epoch 201201, Training Loss: 29720, Validation Loss: 56525, 132414.4387802317\n",
      "Epoch 201301, Training Loss: 29479, Validation Loss: 52946, 152374.94759110818\n",
      "Epoch 201401, Training Loss: 30368, Validation Loss: 51613, 156137.49027224077\n",
      "Epoch 201501, Training Loss: 29693, Validation Loss: 53489, 125534.69725044779\n",
      "Epoch 201601, Training Loss: 28888, Validation Loss: 53455, 142735.5850187891\n",
      "Epoch 201701, Training Loss: 29393, Validation Loss: 56902, 171141.99933946235\n",
      "Epoch 201801, Training Loss: 31663, Validation Loss: 54637, 161255.00172183194\n",
      "Epoch 201901, Training Loss: 30470, Validation Loss: 53190, 127022.29297696517\n",
      "Epoch 202001, Training Loss: 31961, Validation Loss: 54031, 150315.38695969657\n",
      "Epoch 202101, Training Loss: 30441, Validation Loss: 53733, 147930.80939575762\n",
      "Epoch 202201, Training Loss: 30621, Validation Loss: 55237, 165193.63955269693\n",
      "Epoch 202301, Training Loss: 32409, Validation Loss: 55272, 204924.1524521423\n",
      "Epoch 202401, Training Loss: 31225, Validation Loss: 53728, 135497.30528530243\n",
      "Epoch 202501, Training Loss: 29654, Validation Loss: 52990, 163495.768807075\n",
      "Epoch 202601, Training Loss: 30773, Validation Loss: 53281, 160805.15865802224\n",
      "Epoch 202701, Training Loss: 31989, Validation Loss: 54747, 154267.3818689281\n",
      "Epoch 202801, Training Loss: 30495, Validation Loss: 55861, 149847.18698854797\n",
      "Epoch 202901, Training Loss: 30114, Validation Loss: 54792, 158123.6679209844\n",
      "Epoch 203001, Training Loss: 30315, Validation Loss: 56153, 113932.20180493516\n",
      "Epoch 203101, Training Loss: 30518, Validation Loss: 54569, 163883.3619001591\n",
      "Epoch 203201, Training Loss: 30446, Validation Loss: 54052, 160708.87965048148\n",
      "Epoch 203301, Training Loss: 30222, Validation Loss: 55036, 161543.08729846933\n",
      "Epoch 203401, Training Loss: 32145, Validation Loss: 53066, 157520.78402503385\n",
      "Epoch 203501, Training Loss: 29230, Validation Loss: 54296, 134761.20240843663\n",
      "Epoch 203601, Training Loss: 30070, Validation Loss: 55013, 149732.81440081843\n",
      "Epoch 203701, Training Loss: 31774, Validation Loss: 55414, 174171.82796551313\n",
      "Epoch 203801, Training Loss: 31709, Validation Loss: 56099, 144651.3750569141\n",
      "Epoch 203901, Training Loss: 29790, Validation Loss: 54253, 145790.26169619258\n",
      "Epoch 204001, Training Loss: 30661, Validation Loss: 54108, 145289.04337627644\n",
      "Epoch 204101, Training Loss: 29123, Validation Loss: 56288, 143897.49398777768\n",
      "Epoch 204201, Training Loss: 32068, Validation Loss: 54796, 152028.33404862593\n",
      "Epoch 204301, Training Loss: 30951, Validation Loss: 54323, 137661.62816285284\n",
      "Epoch 204401, Training Loss: 29406, Validation Loss: 54416, 144177.0494504449\n",
      "Epoch 204501, Training Loss: 32201, Validation Loss: 55329, 167014.8504516246\n",
      "Epoch 204601, Training Loss: 29790, Validation Loss: 53544, 127620.32937911505\n",
      "Epoch 204701, Training Loss: 29490, Validation Loss: 53975, 136102.24019535974\n",
      "Epoch 204801, Training Loss: 29321, Validation Loss: 53823, 153126.96289225423\n",
      "Epoch 204901, Training Loss: 31808, Validation Loss: 56802, 156097.10748054457\n",
      "Epoch 205001, Training Loss: 27622, Validation Loss: 55737, 155345.27673973795\n",
      "Epoch 205101, Training Loss: 30685, Validation Loss: 54763, 150715.12051280052\n",
      "Epoch 205201, Training Loss: 31429, Validation Loss: 52793, 151397.32774584854\n",
      "Epoch 205301, Training Loss: 32017, Validation Loss: 55579, 169049.1106613312\n",
      "Epoch 205401, Training Loss: 30634, Validation Loss: 55263, 129809.40196723162\n",
      "Epoch 205501, Training Loss: 30273, Validation Loss: 54664, 130761.5525927258\n",
      "Epoch 205601, Training Loss: 30822, Validation Loss: 52456, 151128.4246590055\n",
      "Epoch 205701, Training Loss: 30874, Validation Loss: 53648, 164862.39642623442\n",
      "Epoch 205801, Training Loss: 30440, Validation Loss: 53481, 139132.0146425077\n",
      "Epoch 205901, Training Loss: 31410, Validation Loss: 55117, 134720.57324514686\n",
      "Epoch 206001, Training Loss: 31512, Validation Loss: 56603, 151875.365962713\n",
      "Epoch 206101, Training Loss: 30050, Validation Loss: 53833, 140918.39911023862\n",
      "Epoch 206201, Training Loss: 31011, Validation Loss: 53160, 145623.48564943782\n",
      "Epoch 206301, Training Loss: 30652, Validation Loss: 54101, 168199.8487396075\n",
      "Epoch 206401, Training Loss: 29453, Validation Loss: 54155, 138074.8096534027\n",
      "Epoch 206501, Training Loss: 31428, Validation Loss: 54714, 133975.7750990985\n",
      "Epoch 206601, Training Loss: 30075, Validation Loss: 53546, 124168.76842585085\n",
      "Epoch 206701, Training Loss: 29557, Validation Loss: 53913, 155004.88233048216\n",
      "Epoch 206801, Training Loss: 30401, Validation Loss: 56050, 160729.33633351536\n",
      "Epoch 206901, Training Loss: 30545, Validation Loss: 56020, 143501.76759145476\n",
      "Epoch 207001, Training Loss: 31627, Validation Loss: 55193, 162735.35196776173\n",
      "Epoch 207101, Training Loss: 28872, Validation Loss: 54701, 155774.52080739668\n",
      "Epoch 207201, Training Loss: 30317, Validation Loss: 54307, 171518.5429477755\n",
      "Epoch 207301, Training Loss: 29355, Validation Loss: 54602, 154244.0378263098\n",
      "Epoch 207401, Training Loss: 30198, Validation Loss: 53023, 132872.66469806453\n",
      "Epoch 207501, Training Loss: 30389, Validation Loss: 54749, 176389.8182419517\n",
      "Epoch 207601, Training Loss: 31610, Validation Loss: 51916, 169695.60550027704\n",
      "Epoch 207701, Training Loss: 31643, Validation Loss: 55693, 149932.68003057447\n",
      "Epoch 207801, Training Loss: 29602, Validation Loss: 56046, 130925.27174145314\n",
      "Epoch 207901, Training Loss: 29117, Validation Loss: 55649, 125578.69846943583\n",
      "Epoch 208001, Training Loss: 27924, Validation Loss: 57019, 170511.48880545242\n",
      "Epoch 208101, Training Loss: 31788, Validation Loss: 54490, 140743.16045620528\n",
      "Epoch 208201, Training Loss: 30218, Validation Loss: 54997, 136609.69290284923\n",
      "Epoch 208301, Training Loss: 31045, Validation Loss: 55384, 183549.64571528364\n",
      "Epoch 208401, Training Loss: 30700, Validation Loss: 56424, 142466.20648902314\n",
      "Epoch 208501, Training Loss: 28609, Validation Loss: 54278, 121091.3747257425\n",
      "Epoch 208601, Training Loss: 32118, Validation Loss: 55247, 138116.2923649768\n",
      "Epoch 208701, Training Loss: 29634, Validation Loss: 53880, 145319.19367382498\n",
      "Epoch 208801, Training Loss: 30398, Validation Loss: 54342, 120705.02579011835\n",
      "Epoch 208901, Training Loss: 31049, Validation Loss: 55103, 144273.03232103173\n",
      "Epoch 209001, Training Loss: 29220, Validation Loss: 55543, 159179.50436470128\n",
      "Epoch 209101, Training Loss: 33639, Validation Loss: 54390, 159299.30331882095\n",
      "Epoch 209201, Training Loss: 29260, Validation Loss: 52373, 136732.77768431374\n",
      "Epoch 209301, Training Loss: 31039, Validation Loss: 55305, 145596.11125854656\n",
      "Epoch 209401, Training Loss: 30616, Validation Loss: 54335, 136395.15262993862\n",
      "Epoch 209501, Training Loss: 30346, Validation Loss: 53966, 151075.654839495\n",
      "Epoch 209601, Training Loss: 29530, Validation Loss: 54282, 143687.87074600067\n",
      "Epoch 209701, Training Loss: 29512, Validation Loss: 53968, 138202.5648073798\n",
      "Epoch 209801, Training Loss: 32374, Validation Loss: 54980, 173366.3376062683\n",
      "Epoch 209901, Training Loss: 29881, Validation Loss: 54468, 140542.26538560409\n",
      "Epoch 210001, Training Loss: 29280, Validation Loss: 53350, 138718.10785413274\n",
      "Epoch 210101, Training Loss: 31725, Validation Loss: 56422, 125132.54954669368\n",
      "Epoch 210201, Training Loss: 31865, Validation Loss: 56041, 186860.67829514496\n",
      "Epoch 210301, Training Loss: 30545, Validation Loss: 52927, 143725.30481498005\n",
      "Epoch 210401, Training Loss: 30374, Validation Loss: 54992, 158264.2166725357\n",
      "Epoch 210501, Training Loss: 30166, Validation Loss: 53549, 136515.8561591772\n",
      "Epoch 210601, Training Loss: 31647, Validation Loss: 56136, 151254.32418880743\n",
      "Epoch 210701, Training Loss: 30736, Validation Loss: 54897, 170042.99863434728\n",
      "Epoch 210801, Training Loss: 31231, Validation Loss: 54242, 143379.1663881746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210901, Training Loss: 29918, Validation Loss: 55394, 124472.43068122848\n",
      "Epoch 211001, Training Loss: 30182, Validation Loss: 55904, 165675.49123301194\n",
      "Epoch 211101, Training Loss: 31867, Validation Loss: 54479, 150856.8179866039\n",
      "Epoch 211201, Training Loss: 31156, Validation Loss: 55028, 131276.22735339636\n",
      "Epoch 211301, Training Loss: 32090, Validation Loss: 55081, 115778.57313900266\n",
      "Epoch 211401, Training Loss: 32008, Validation Loss: 51851, 144664.80105031782\n",
      "Epoch 211501, Training Loss: 30021, Validation Loss: 53071, 129071.54417349333\n",
      "Epoch 211601, Training Loss: 32396, Validation Loss: 54484, 165942.83708784982\n",
      "Epoch 211701, Training Loss: 32332, Validation Loss: 56635, 147359.86571288473\n",
      "Epoch 211801, Training Loss: 31963, Validation Loss: 53619, 152681.70328936045\n",
      "Epoch 211901, Training Loss: 30034, Validation Loss: 53177, 153848.0815590368\n",
      "Epoch 212001, Training Loss: 30828, Validation Loss: 54971, 153931.43553674052\n",
      "Epoch 212101, Training Loss: 30231, Validation Loss: 53966, 125512.1617279334\n",
      "Epoch 212201, Training Loss: 30358, Validation Loss: 53929, 179876.7600410931\n",
      "Epoch 212301, Training Loss: 29397, Validation Loss: 54294, 137404.67768905667\n",
      "Epoch 212401, Training Loss: 31093, Validation Loss: 55260, 133053.330845266\n",
      "Epoch 212501, Training Loss: 30398, Validation Loss: 56283, 154851.54487818462\n",
      "Epoch 212601, Training Loss: 27465, Validation Loss: 57623, 147267.15717764985\n",
      "Epoch 212701, Training Loss: 30377, Validation Loss: 54294, 155247.73099005784\n",
      "Epoch 212801, Training Loss: 30998, Validation Loss: 55618, 184097.3009455377\n",
      "Epoch 212901, Training Loss: 29757, Validation Loss: 55348, 145834.79226915407\n",
      "Epoch 213001, Training Loss: 31315, Validation Loss: 54490, 144802.82341835616\n",
      "Epoch 213101, Training Loss: 31846, Validation Loss: 55770, 151763.94597139338\n",
      "Epoch 213201, Training Loss: 30745, Validation Loss: 54794, 133290.44242903925\n",
      "Epoch 213301, Training Loss: 28476, Validation Loss: 54881, 132590.36187289716\n",
      "Epoch 213401, Training Loss: 29266, Validation Loss: 57244, 136411.1158095615\n",
      "Epoch 213501, Training Loss: 31030, Validation Loss: 54902, 147531.09780154208\n",
      "Epoch 213601, Training Loss: 29752, Validation Loss: 52161, 146281.8920434631\n",
      "Epoch 213701, Training Loss: 29562, Validation Loss: 55634, 136637.46334658007\n",
      "Epoch 213801, Training Loss: 30062, Validation Loss: 53539, 157031.45038674097\n",
      "Epoch 213901, Training Loss: 31291, Validation Loss: 55356, 153675.30562593983\n",
      "Epoch 214001, Training Loss: 29888, Validation Loss: 55041, 162441.09321514092\n",
      "Epoch 214101, Training Loss: 32544, Validation Loss: 54208, 150518.12048846146\n",
      "Epoch 214201, Training Loss: 29424, Validation Loss: 55638, 127399.4553394486\n",
      "Epoch 214301, Training Loss: 29464, Validation Loss: 53228, 154306.34798487643\n",
      "Epoch 214401, Training Loss: 30241, Validation Loss: 55062, 132578.53510578454\n",
      "Epoch 214501, Training Loss: 28186, Validation Loss: 55907, 171408.0724739773\n",
      "Epoch 214601, Training Loss: 29302, Validation Loss: 55870, 175298.67018439737\n",
      "Epoch 214701, Training Loss: 29570, Validation Loss: 54914, 160604.88657821363\n",
      "Epoch 214801, Training Loss: 28541, Validation Loss: 55661, 171147.61149846058\n",
      "Epoch 214901, Training Loss: 31171, Validation Loss: 54960, 144123.04862885067\n",
      "Epoch 215001, Training Loss: 30047, Validation Loss: 56022, 121198.18109740972\n",
      "Epoch 215101, Training Loss: 30709, Validation Loss: 55116, 163418.2758400633\n",
      "Epoch 215201, Training Loss: 30537, Validation Loss: 53837, 150591.4552044874\n",
      "Epoch 215301, Training Loss: 30375, Validation Loss: 55056, 144708.39990677053\n",
      "Epoch 215401, Training Loss: 30995, Validation Loss: 56445, 161099.94324062386\n",
      "Epoch 215501, Training Loss: 31216, Validation Loss: 54317, 149619.0081386639\n",
      "Epoch 215601, Training Loss: 31111, Validation Loss: 55971, 157150.90412844304\n",
      "Epoch 215701, Training Loss: 30853, Validation Loss: 54776, 146056.3111132251\n",
      "Epoch 215801, Training Loss: 31672, Validation Loss: 55287, 150748.7284204937\n",
      "Epoch 215901, Training Loss: 31278, Validation Loss: 55927, 189678.94876471773\n",
      "Epoch 216001, Training Loss: 29753, Validation Loss: 53101, 143495.85523284326\n",
      "Epoch 216101, Training Loss: 29893, Validation Loss: 53478, 157220.7489214371\n",
      "Epoch 216201, Training Loss: 30909, Validation Loss: 55076, 167983.21646067358\n",
      "Epoch 216301, Training Loss: 31109, Validation Loss: 53334, 151077.90267501265\n",
      "Epoch 216401, Training Loss: 32743, Validation Loss: 54297, 151763.22532946034\n",
      "Epoch 216501, Training Loss: 31056, Validation Loss: 54936, 172159.30037434847\n",
      "Epoch 216601, Training Loss: 29397, Validation Loss: 54748, 139697.4335676126\n",
      "Epoch 216701, Training Loss: 29765, Validation Loss: 55949, 174495.64584800313\n",
      "Epoch 216801, Training Loss: 29597, Validation Loss: 54160, 130743.35864487989\n",
      "Epoch 216901, Training Loss: 29483, Validation Loss: 52765, 136771.51062636767\n",
      "Epoch 217001, Training Loss: 30262, Validation Loss: 52817, 134156.81247369415\n",
      "Epoch 217101, Training Loss: 29913, Validation Loss: 54121, 169728.32826617095\n",
      "Epoch 217201, Training Loss: 29754, Validation Loss: 57283, 154887.4957188794\n",
      "Epoch 217301, Training Loss: 29492, Validation Loss: 55802, 150125.73604885762\n",
      "Epoch 217401, Training Loss: 30112, Validation Loss: 55665, 147604.78113273098\n",
      "Epoch 217501, Training Loss: 30860, Validation Loss: 56220, 132921.73771315353\n",
      "Epoch 217601, Training Loss: 30391, Validation Loss: 56115, 187019.42443311997\n",
      "Epoch 217701, Training Loss: 28556, Validation Loss: 55627, 139782.1924164702\n",
      "Epoch 217801, Training Loss: 30359, Validation Loss: 54348, 162177.218992286\n",
      "Epoch 217901, Training Loss: 31234, Validation Loss: 56980, 167672.3960028896\n",
      "Epoch 218001, Training Loss: 29157, Validation Loss: 54132, 140584.45474754315\n",
      "Epoch 218101, Training Loss: 29649, Validation Loss: 58243, 153808.4318689559\n",
      "Epoch 218201, Training Loss: 29250, Validation Loss: 53503, 126762.59291558011\n",
      "Epoch 218301, Training Loss: 31261, Validation Loss: 54663, 141749.68978335135\n",
      "Epoch 218401, Training Loss: 30746, Validation Loss: 53577, 135451.71080542626\n",
      "Epoch 218501, Training Loss: 30357, Validation Loss: 53623, 137343.53040349917\n",
      "Epoch 218601, Training Loss: 30615, Validation Loss: 52442, 133991.15706976474\n",
      "Epoch 218701, Training Loss: 30150, Validation Loss: 57106, 173519.53779839445\n",
      "Epoch 218801, Training Loss: 30749, Validation Loss: 55927, 160264.6518993507\n",
      "Epoch 218901, Training Loss: 30778, Validation Loss: 56147, 137492.33001099565\n",
      "Epoch 219001, Training Loss: 30698, Validation Loss: 55263, 177983.35191313567\n",
      "Epoch 219101, Training Loss: 30175, Validation Loss: 52954, 147733.87679340207\n",
      "Epoch 219201, Training Loss: 32696, Validation Loss: 52766, 165852.1764268749\n",
      "Epoch 219301, Training Loss: 31279, Validation Loss: 53789, 167436.34072715015\n",
      "Epoch 219401, Training Loss: 29294, Validation Loss: 54628, 128167.18284710907\n",
      "Epoch 219501, Training Loss: 31405, Validation Loss: 55177, 161779.77029093113\n",
      "Epoch 219601, Training Loss: 28679, Validation Loss: 54193, 133962.67131538672\n",
      "Epoch 219701, Training Loss: 29579, Validation Loss: 55503, 134747.1182405515\n",
      "Epoch 219801, Training Loss: 29292, Validation Loss: 55591, 129300.6583900683\n",
      "Epoch 219901, Training Loss: 30819, Validation Loss: 53304, 163463.2012072728\n",
      "Epoch 220001, Training Loss: 29827, Validation Loss: 56106, 135076.67744116473\n",
      "Epoch 220101, Training Loss: 31108, Validation Loss: 53880, 140723.35858447448\n",
      "Epoch 220201, Training Loss: 28360, Validation Loss: 55531, 158804.01438602462\n",
      "Epoch 220301, Training Loss: 31222, Validation Loss: 53580, 137447.18110905978\n",
      "Epoch 220401, Training Loss: 31846, Validation Loss: 53166, 166034.6163148806\n",
      "Epoch 220501, Training Loss: 30767, Validation Loss: 52906, 164878.60014359272\n",
      "Epoch 220601, Training Loss: 30087, Validation Loss: 55071, 137852.57036344736\n",
      "Epoch 220701, Training Loss: 30258, Validation Loss: 55809, 152262.53313161628\n",
      "Epoch 220801, Training Loss: 33501, Validation Loss: 54493, 161481.29709655527\n",
      "Epoch 220901, Training Loss: 29760, Validation Loss: 53422, 158533.69392090192\n",
      "Epoch 221001, Training Loss: 32054, Validation Loss: 54719, 158049.06668241348\n",
      "Epoch 221101, Training Loss: 29296, Validation Loss: 57008, 145907.38590824482\n",
      "Epoch 221201, Training Loss: 30142, Validation Loss: 54442, 130227.05952585438\n",
      "Epoch 221301, Training Loss: 30662, Validation Loss: 54094, 150088.8067716348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221401, Training Loss: 29505, Validation Loss: 54774, 145166.99339448762\n",
      "Epoch 221501, Training Loss: 30184, Validation Loss: 53259, 137480.78342453396\n",
      "Epoch 221601, Training Loss: 30332, Validation Loss: 57200, 154667.52631622215\n",
      "Epoch 221701, Training Loss: 29387, Validation Loss: 55274, 153099.28944952073\n",
      "Epoch 221801, Training Loss: 29068, Validation Loss: 53413, 143469.11278415233\n",
      "Epoch 221901, Training Loss: 30743, Validation Loss: 57888, 170905.48098987932\n",
      "Epoch 222001, Training Loss: 31779, Validation Loss: 54219, 175033.2009250818\n",
      "Epoch 222101, Training Loss: 29746, Validation Loss: 54288, 133525.63942955434\n",
      "Epoch 222201, Training Loss: 29528, Validation Loss: 55453, 132995.528416651\n",
      "Epoch 222301, Training Loss: 30193, Validation Loss: 56370, 140491.69999800948\n",
      "Epoch 222401, Training Loss: 31507, Validation Loss: 53566, 149855.2831232348\n",
      "Epoch 222501, Training Loss: 30532, Validation Loss: 53098, 126140.60309487963\n",
      "Epoch 222601, Training Loss: 31186, Validation Loss: 53607, 125061.7777763342\n",
      "Epoch 222701, Training Loss: 29326, Validation Loss: 55609, 167600.57799044056\n",
      "Epoch 222801, Training Loss: 29032, Validation Loss: 54201, 139276.66932332996\n",
      "Epoch 222901, Training Loss: 29386, Validation Loss: 56322, 164920.31446856304\n",
      "Epoch 223001, Training Loss: 29606, Validation Loss: 55387, 142723.09652016248\n",
      "Epoch 223101, Training Loss: 30014, Validation Loss: 53755, 135281.2597393372\n",
      "Epoch 223201, Training Loss: 29106, Validation Loss: 55323, 144393.3710835033\n",
      "Epoch 223301, Training Loss: 28978, Validation Loss: 55570, 135673.3336196537\n",
      "Epoch 223401, Training Loss: 30511, Validation Loss: 55676, 175787.473540388\n",
      "Epoch 223501, Training Loss: 31241, Validation Loss: 53197, 143792.10142782942\n",
      "Epoch 223601, Training Loss: 29827, Validation Loss: 54898, 149056.88425964382\n",
      "Epoch 223701, Training Loss: 30238, Validation Loss: 57030, 154189.46400488797\n",
      "Epoch 223801, Training Loss: 32507, Validation Loss: 57248, 209071.4810972794\n",
      "Epoch 223901, Training Loss: 29362, Validation Loss: 57067, 178623.31302922405\n",
      "Epoch 224001, Training Loss: 29341, Validation Loss: 54408, 145822.5208184034\n",
      "Epoch 224101, Training Loss: 29518, Validation Loss: 53490, 146242.8063868657\n",
      "Epoch 224201, Training Loss: 29848, Validation Loss: 54928, 141191.9835280717\n",
      "Epoch 224301, Training Loss: 29447, Validation Loss: 53544, 144962.46072793906\n",
      "Epoch 224401, Training Loss: 29566, Validation Loss: 55403, 132960.18946389988\n",
      "Epoch 224501, Training Loss: 28882, Validation Loss: 55917, 148343.61200548746\n",
      "Epoch 224601, Training Loss: 30882, Validation Loss: 54741, 156268.99499667788\n",
      "Epoch 224701, Training Loss: 30328, Validation Loss: 53655, 151745.9752110981\n",
      "Epoch 224801, Training Loss: 29939, Validation Loss: 55421, 130472.00502484912\n",
      "Epoch 224901, Training Loss: 30512, Validation Loss: 53851, 143644.63127004576\n",
      "Epoch 225001, Training Loss: 28902, Validation Loss: 57693, 144757.87598783284\n",
      "Epoch 225101, Training Loss: 32283, Validation Loss: 52364, 167090.75583988987\n",
      "Epoch 225201, Training Loss: 30368, Validation Loss: 55037, 161890.25802661982\n",
      "Epoch 225301, Training Loss: 29173, Validation Loss: 54869, 124611.2685940805\n",
      "Epoch 225401, Training Loss: 30604, Validation Loss: 58164, 157907.34005219696\n",
      "Epoch 225501, Training Loss: 29066, Validation Loss: 56784, 165760.12818638972\n",
      "Epoch 225601, Training Loss: 31758, Validation Loss: 55450, 159369.23851604742\n",
      "Epoch 225701, Training Loss: 30427, Validation Loss: 54201, 149584.68638574574\n",
      "Epoch 225801, Training Loss: 30634, Validation Loss: 54073, 137460.51140565364\n",
      "Epoch 225901, Training Loss: 28794, Validation Loss: 55969, 188123.2740181627\n",
      "Epoch 226001, Training Loss: 31121, Validation Loss: 55170, 119021.25195076535\n",
      "Epoch 226101, Training Loss: 31337, Validation Loss: 55977, 153470.41109968422\n",
      "Epoch 226201, Training Loss: 29706, Validation Loss: 56409, 152014.56563939535\n",
      "Epoch 226301, Training Loss: 29557, Validation Loss: 55608, 148542.4812169126\n",
      "Epoch 226401, Training Loss: 30831, Validation Loss: 55026, 152742.1847256633\n",
      "Epoch 226501, Training Loss: 29357, Validation Loss: 54939, 135183.54083529764\n",
      "Epoch 226601, Training Loss: 30499, Validation Loss: 55513, 120208.18343305642\n",
      "Epoch 226701, Training Loss: 30703, Validation Loss: 54469, 188253.1630882576\n",
      "Epoch 226801, Training Loss: 32681, Validation Loss: 56369, 140153.7480716806\n",
      "Epoch 226901, Training Loss: 29053, Validation Loss: 54524, 153673.6094202165\n",
      "Epoch 227001, Training Loss: 31626, Validation Loss: 53048, 164249.6567599099\n",
      "Epoch 227101, Training Loss: 31304, Validation Loss: 56205, 151987.3452940948\n",
      "Epoch 227201, Training Loss: 31687, Validation Loss: 54133, 169663.7627318876\n",
      "Epoch 227301, Training Loss: 30878, Validation Loss: 55323, 169305.07015194322\n",
      "Epoch 227401, Training Loss: 29692, Validation Loss: 54371, 117208.10702487717\n",
      "Epoch 227501, Training Loss: 29533, Validation Loss: 54508, 173295.3980072161\n",
      "Epoch 227601, Training Loss: 29793, Validation Loss: 56491, 143685.75985520746\n",
      "Epoch 227701, Training Loss: 31856, Validation Loss: 55064, 160106.82790738196\n",
      "Epoch 227801, Training Loss: 30423, Validation Loss: 55221, 140959.1630412009\n",
      "Epoch 227901, Training Loss: 30736, Validation Loss: 58453, 175634.82111554587\n",
      "Epoch 228001, Training Loss: 29519, Validation Loss: 56151, 141681.54731392753\n",
      "Epoch 228101, Training Loss: 28251, Validation Loss: 56523, 120043.46874046404\n",
      "Epoch 228201, Training Loss: 30565, Validation Loss: 53580, 178087.5031500813\n",
      "Epoch 228301, Training Loss: 31553, Validation Loss: 53089, 148702.65753167646\n",
      "Epoch 228401, Training Loss: 29034, Validation Loss: 55504, 149803.98525988404\n",
      "Epoch 228501, Training Loss: 30637, Validation Loss: 56874, 143403.84564596074\n",
      "Epoch 228601, Training Loss: 28714, Validation Loss: 54974, 131042.70679425212\n",
      "Epoch 228701, Training Loss: 29684, Validation Loss: 55949, 158670.33711977146\n",
      "Epoch 228801, Training Loss: 27987, Validation Loss: 53443, 144149.60366326294\n",
      "Epoch 228901, Training Loss: 31624, Validation Loss: 55675, 134478.03028142487\n",
      "Epoch 229001, Training Loss: 29262, Validation Loss: 53540, 136667.64272967793\n",
      "Epoch 229101, Training Loss: 29060, Validation Loss: 57251, 154702.04055851293\n",
      "Epoch 229201, Training Loss: 28515, Validation Loss: 54983, 153970.7421901392\n",
      "Epoch 229301, Training Loss: 30651, Validation Loss: 54369, 132906.2601090563\n",
      "Epoch 229401, Training Loss: 29720, Validation Loss: 54624, 162037.8835591971\n",
      "Epoch 229501, Training Loss: 31030, Validation Loss: 53324, 140659.24886099008\n",
      "Epoch 229601, Training Loss: 29838, Validation Loss: 54383, 146496.3080636354\n",
      "Epoch 229701, Training Loss: 31465, Validation Loss: 56627, 198890.09371038218\n",
      "Epoch 229801, Training Loss: 33030, Validation Loss: 55116, 180123.85655430515\n",
      "Epoch 229901, Training Loss: 30424, Validation Loss: 55092, 153167.2462547776\n",
      "Epoch 230001, Training Loss: 30330, Validation Loss: 54656, 141675.80962299265\n",
      "Epoch 230101, Training Loss: 29611, Validation Loss: 54079, 174631.95153982734\n",
      "Epoch 230201, Training Loss: 29070, Validation Loss: 55007, 153906.89448129083\n",
      "Epoch 230301, Training Loss: 29862, Validation Loss: 55348, 155881.26795575014\n",
      "Epoch 230401, Training Loss: 30264, Validation Loss: 56882, 141159.98947713574\n",
      "Epoch 230501, Training Loss: 31602, Validation Loss: 55950, 134640.21699482645\n",
      "Epoch 230601, Training Loss: 30587, Validation Loss: 54154, 143914.30022209397\n",
      "Epoch 230701, Training Loss: 27897, Validation Loss: 54835, 116223.93412296366\n",
      "Epoch 230801, Training Loss: 30182, Validation Loss: 56815, 171281.36463719737\n",
      "Epoch 230901, Training Loss: 31151, Validation Loss: 54896, 141848.09783415878\n",
      "Epoch 231001, Training Loss: 30902, Validation Loss: 53080, 150224.67523423795\n",
      "Epoch 231101, Training Loss: 29610, Validation Loss: 53573, 146607.69938910476\n",
      "Epoch 231201, Training Loss: 29468, Validation Loss: 52471, 137107.74307527128\n",
      "Epoch 231301, Training Loss: 28700, Validation Loss: 54167, 145874.37654384418\n",
      "Epoch 231401, Training Loss: 29598, Validation Loss: 54096, 140661.8804508142\n",
      "Epoch 231501, Training Loss: 29298, Validation Loss: 56311, 161046.2283823592\n",
      "Epoch 231601, Training Loss: 29298, Validation Loss: 54386, 141009.91069166438\n",
      "Epoch 231701, Training Loss: 32900, Validation Loss: 54151, 168266.2861079763\n",
      "Epoch 231801, Training Loss: 30073, Validation Loss: 55596, 151184.55904768148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231901, Training Loss: 28900, Validation Loss: 55967, 142501.36763019938\n",
      "Epoch 232001, Training Loss: 29970, Validation Loss: 55392, 159769.69228964893\n",
      "Epoch 232101, Training Loss: 29386, Validation Loss: 55541, 112229.13915167484\n",
      "Epoch 232201, Training Loss: 29387, Validation Loss: 55795, 141037.69173231133\n",
      "Epoch 232301, Training Loss: 30023, Validation Loss: 54944, 122786.41015201197\n",
      "Epoch 232401, Training Loss: 29906, Validation Loss: 54648, 146168.90896394104\n",
      "Epoch 232501, Training Loss: 29994, Validation Loss: 55975, 155010.27821219378\n",
      "Epoch 232601, Training Loss: 32133, Validation Loss: 56846, 177970.9483420881\n",
      "Epoch 232701, Training Loss: 30004, Validation Loss: 55656, 185870.80425194907\n",
      "Epoch 232801, Training Loss: 29317, Validation Loss: 55141, 138533.63217760128\n",
      "Epoch 232901, Training Loss: 30272, Validation Loss: 53019, 147463.05059995083\n",
      "Epoch 233001, Training Loss: 30624, Validation Loss: 57889, 163389.2585643484\n",
      "Epoch 233101, Training Loss: 30387, Validation Loss: 56780, 153906.55127154585\n",
      "Epoch 233201, Training Loss: 29147, Validation Loss: 53425, 145595.7720480399\n",
      "Epoch 233301, Training Loss: 30357, Validation Loss: 54856, 141363.19596083267\n",
      "Epoch 233401, Training Loss: 29400, Validation Loss: 54354, 123220.81461302415\n",
      "Epoch 233501, Training Loss: 31853, Validation Loss: 53162, 173886.89150918438\n",
      "Epoch 233601, Training Loss: 31608, Validation Loss: 56214, 158721.402178647\n",
      "Epoch 233701, Training Loss: 29741, Validation Loss: 55410, 134044.16725242665\n",
      "Epoch 233801, Training Loss: 30062, Validation Loss: 56700, 136577.56757354038\n",
      "Epoch 233901, Training Loss: 29265, Validation Loss: 54898, 141839.03789377934\n",
      "Epoch 234001, Training Loss: 29284, Validation Loss: 57174, 156083.17768431414\n",
      "Epoch 234101, Training Loss: 29088, Validation Loss: 56160, 126746.53484897604\n",
      "Epoch 234201, Training Loss: 29257, Validation Loss: 53778, 125512.63959437693\n",
      "Epoch 234301, Training Loss: 28869, Validation Loss: 54893, 134020.2356896613\n",
      "Epoch 234401, Training Loss: 29339, Validation Loss: 54453, 125558.61487381894\n",
      "Epoch 234501, Training Loss: 30416, Validation Loss: 54400, 138492.42927018282\n",
      "Epoch 234601, Training Loss: 30416, Validation Loss: 53491, 133639.00721094836\n",
      "Epoch 234701, Training Loss: 29802, Validation Loss: 55206, 150656.64278029712\n",
      "Epoch 234801, Training Loss: 30282, Validation Loss: 54600, 147385.0072130508\n",
      "Epoch 234901, Training Loss: 29701, Validation Loss: 56018, 163054.72297119355\n",
      "Epoch 235001, Training Loss: 30658, Validation Loss: 55828, 145478.42439833275\n",
      "Epoch 235101, Training Loss: 29876, Validation Loss: 54975, 162919.53204658258\n",
      "Epoch 235201, Training Loss: 29327, Validation Loss: 54532, 132348.71164974308\n",
      "Epoch 235301, Training Loss: 32378, Validation Loss: 54804, 131762.88487694686\n",
      "Epoch 235401, Training Loss: 29050, Validation Loss: 54067, 136217.42858614083\n",
      "Epoch 235501, Training Loss: 30855, Validation Loss: 53425, 149678.95201869062\n",
      "Epoch 235601, Training Loss: 28822, Validation Loss: 56626, 139993.2710173979\n",
      "Epoch 235701, Training Loss: 29600, Validation Loss: 55038, 148203.5286158514\n",
      "Epoch 235801, Training Loss: 29601, Validation Loss: 55657, 131286.49685503877\n",
      "Epoch 235901, Training Loss: 29851, Validation Loss: 56609, 142123.176751094\n",
      "Epoch 236001, Training Loss: 31349, Validation Loss: 53904, 137083.97093749203\n",
      "Epoch 236101, Training Loss: 32052, Validation Loss: 56167, 173766.82948839627\n",
      "Epoch 236201, Training Loss: 30870, Validation Loss: 55999, 120846.3481938135\n",
      "Epoch 236301, Training Loss: 31371, Validation Loss: 53804, 166632.6457136061\n",
      "Epoch 236401, Training Loss: 29687, Validation Loss: 55232, 134499.53715857927\n",
      "Epoch 236501, Training Loss: 27966, Validation Loss: 54872, 130985.94350335479\n",
      "Epoch 236601, Training Loss: 29865, Validation Loss: 54833, 148988.9807048981\n",
      "Epoch 236701, Training Loss: 28895, Validation Loss: 54781, 160263.16902862917\n",
      "Epoch 236801, Training Loss: 30853, Validation Loss: 55100, 121615.39216075643\n",
      "Epoch 236901, Training Loss: 27978, Validation Loss: 54608, 127282.86145061934\n",
      "Epoch 237001, Training Loss: 30102, Validation Loss: 55233, 153262.55811986205\n",
      "Epoch 237101, Training Loss: 30608, Validation Loss: 53094, 157261.13767089034\n",
      "Epoch 237201, Training Loss: 29691, Validation Loss: 54561, 133101.34306710775\n",
      "Epoch 237301, Training Loss: 30842, Validation Loss: 56737, 171539.4047196561\n",
      "Epoch 237401, Training Loss: 32241, Validation Loss: 57302, 156463.80652340624\n",
      "Epoch 237501, Training Loss: 30926, Validation Loss: 53317, 135763.96105758357\n",
      "Epoch 237601, Training Loss: 31199, Validation Loss: 54399, 182529.41707180082\n",
      "Epoch 237701, Training Loss: 30144, Validation Loss: 53046, 137737.52424549556\n",
      "Epoch 237801, Training Loss: 32435, Validation Loss: 56178, 173744.48798222924\n",
      "Epoch 237901, Training Loss: 32019, Validation Loss: 57073, 143529.92980501242\n",
      "Epoch 238001, Training Loss: 30727, Validation Loss: 54596, 161795.8300530156\n",
      "Epoch 238101, Training Loss: 29019, Validation Loss: 53875, 135509.56071198554\n",
      "Epoch 238201, Training Loss: 30222, Validation Loss: 55465, 137011.10430411523\n",
      "Epoch 238301, Training Loss: 29921, Validation Loss: 54891, 132185.1152029662\n",
      "Epoch 238401, Training Loss: 31116, Validation Loss: 57253, 164354.00924134068\n",
      "Epoch 238501, Training Loss: 30576, Validation Loss: 53933, 148867.29393213167\n",
      "Epoch 238601, Training Loss: 31215, Validation Loss: 55737, 150994.15345637096\n",
      "Epoch 238701, Training Loss: 28315, Validation Loss: 55069, 118611.7947486374\n",
      "Epoch 238801, Training Loss: 30410, Validation Loss: 57053, 142029.95784691142\n",
      "Epoch 238901, Training Loss: 29366, Validation Loss: 55253, 159649.05844289213\n",
      "Epoch 239001, Training Loss: 29704, Validation Loss: 55268, 167811.6434641539\n",
      "Epoch 239101, Training Loss: 32061, Validation Loss: 55938, 146860.76397094695\n",
      "Epoch 239201, Training Loss: 30513, Validation Loss: 54934, 142024.31965785433\n",
      "Epoch 239301, Training Loss: 30122, Validation Loss: 54316, 148391.49217703738\n",
      "Epoch 239401, Training Loss: 29965, Validation Loss: 54812, 112173.01198858093\n",
      "Epoch 239501, Training Loss: 30423, Validation Loss: 55256, 136903.23634019762\n",
      "Epoch 239601, Training Loss: 32650, Validation Loss: 56244, 153925.87961989568\n",
      "Epoch 239701, Training Loss: 29800, Validation Loss: 54698, 145430.65551268967\n",
      "Epoch 239801, Training Loss: 28957, Validation Loss: 54232, 134118.28444856507\n",
      "Epoch 239901, Training Loss: 28956, Validation Loss: 56019, 148685.89388517445\n",
      "Epoch 240001, Training Loss: 31304, Validation Loss: 55128, 147480.19110390684\n",
      "Epoch 240101, Training Loss: 29853, Validation Loss: 53997, 154928.0992528477\n",
      "Epoch 240201, Training Loss: 30131, Validation Loss: 56562, 163120.2916246845\n",
      "Epoch 240301, Training Loss: 30897, Validation Loss: 55499, 134687.9225942888\n",
      "Epoch 240401, Training Loss: 29988, Validation Loss: 55948, 125844.18761726217\n",
      "Epoch 240501, Training Loss: 29404, Validation Loss: 53604, 133405.9553289412\n",
      "Epoch 240601, Training Loss: 29221, Validation Loss: 55459, 137092.50289195817\n",
      "Epoch 240701, Training Loss: 29230, Validation Loss: 55537, 132349.88324283238\n",
      "Epoch 240801, Training Loss: 29778, Validation Loss: 53927, 143645.24934780737\n",
      "Epoch 240901, Training Loss: 29119, Validation Loss: 55912, 134123.81620898578\n",
      "Epoch 241001, Training Loss: 31076, Validation Loss: 53057, 164189.879099299\n",
      "Epoch 241101, Training Loss: 29524, Validation Loss: 54968, 149665.04628254328\n",
      "Epoch 241201, Training Loss: 30477, Validation Loss: 54232, 149799.97184903183\n",
      "Epoch 241301, Training Loss: 29955, Validation Loss: 55513, 143358.61226633916\n",
      "Epoch 241401, Training Loss: 30640, Validation Loss: 52319, 148141.67362735915\n",
      "Epoch 241501, Training Loss: 30178, Validation Loss: 55670, 149207.97695342082\n",
      "Epoch 241601, Training Loss: 28391, Validation Loss: 54003, 135868.81054371706\n",
      "Epoch 241701, Training Loss: 30073, Validation Loss: 54034, 151969.06854886265\n",
      "Epoch 241801, Training Loss: 30185, Validation Loss: 53885, 138392.52784878697\n",
      "Epoch 241901, Training Loss: 30577, Validation Loss: 53932, 163692.85933416846\n",
      "Epoch 242001, Training Loss: 29172, Validation Loss: 55581, 173034.10327473027\n",
      "Epoch 242101, Training Loss: 29573, Validation Loss: 55514, 127592.39179317138\n",
      "Epoch 242201, Training Loss: 29464, Validation Loss: 53664, 153145.97893468218\n",
      "Epoch 242301, Training Loss: 28978, Validation Loss: 56691, 154406.5900588659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242401, Training Loss: 30753, Validation Loss: 53997, 147515.96125573703\n",
      "Epoch 242501, Training Loss: 32078, Validation Loss: 53822, 149036.88598167343\n",
      "Epoch 242601, Training Loss: 30897, Validation Loss: 54201, 153977.6695117619\n",
      "Epoch 242701, Training Loss: 30887, Validation Loss: 55455, 125398.8128250388\n",
      "Epoch 242801, Training Loss: 30159, Validation Loss: 56067, 149114.62194190075\n",
      "Epoch 242901, Training Loss: 29954, Validation Loss: 56078, 138605.19056329384\n",
      "Epoch 243001, Training Loss: 30604, Validation Loss: 56424, 161387.6511073055\n",
      "Epoch 243101, Training Loss: 30081, Validation Loss: 53447, 138568.2143068332\n",
      "Epoch 243201, Training Loss: 30627, Validation Loss: 53221, 164857.2683593538\n",
      "Epoch 243301, Training Loss: 30459, Validation Loss: 54213, 128216.5847318165\n",
      "Epoch 243401, Training Loss: 30584, Validation Loss: 54359, 145424.87461571404\n",
      "Epoch 243501, Training Loss: 30787, Validation Loss: 55923, 156086.6462174809\n",
      "Epoch 243601, Training Loss: 29948, Validation Loss: 54007, 143237.51518217108\n",
      "Epoch 243701, Training Loss: 30342, Validation Loss: 55307, 151057.51121830026\n",
      "Epoch 243801, Training Loss: 30981, Validation Loss: 54961, 138394.66149142495\n",
      "Epoch 243901, Training Loss: 28339, Validation Loss: 55909, 139055.57119079283\n",
      "Epoch 244001, Training Loss: 28715, Validation Loss: 56947, 153718.57072991278\n",
      "Epoch 244101, Training Loss: 30966, Validation Loss: 55084, 150110.72089037887\n",
      "Epoch 244201, Training Loss: 31985, Validation Loss: 54767, 139691.15147083817\n",
      "Epoch 244301, Training Loss: 30762, Validation Loss: 56417, 148084.0859288195\n",
      "Epoch 244401, Training Loss: 29772, Validation Loss: 54940, 114227.77151167626\n",
      "Epoch 244501, Training Loss: 29340, Validation Loss: 55428, 142845.12182107882\n",
      "Epoch 244601, Training Loss: 30956, Validation Loss: 55035, 138394.4258960069\n",
      "Epoch 244701, Training Loss: 29909, Validation Loss: 54396, 157880.33982267784\n",
      "Epoch 244801, Training Loss: 30848, Validation Loss: 54186, 120028.12557291987\n",
      "Epoch 244901, Training Loss: 29967, Validation Loss: 53646, 125605.73447361111\n",
      "Epoch 245001, Training Loss: 30221, Validation Loss: 54717, 129372.31049213764\n",
      "Epoch 245101, Training Loss: 31295, Validation Loss: 55021, 141432.3328664024\n",
      "Epoch 245201, Training Loss: 29208, Validation Loss: 55425, 133155.796585957\n",
      "Epoch 245301, Training Loss: 28968, Validation Loss: 54862, 163992.58012749685\n",
      "Epoch 245401, Training Loss: 30373, Validation Loss: 54942, 131548.73813317335\n",
      "Epoch 245501, Training Loss: 30576, Validation Loss: 53898, 158464.48065476827\n",
      "Epoch 245601, Training Loss: 28552, Validation Loss: 55766, 141783.01532955022\n",
      "Epoch 245701, Training Loss: 30190, Validation Loss: 56193, 136994.03280582355\n",
      "Epoch 245801, Training Loss: 30453, Validation Loss: 56526, 150509.7499851759\n",
      "Epoch 245901, Training Loss: 29235, Validation Loss: 53874, 135466.10694183625\n",
      "Epoch 246001, Training Loss: 30589, Validation Loss: 56200, 135522.76149952319\n",
      "Epoch 246101, Training Loss: 30090, Validation Loss: 53635, 166134.46756977035\n",
      "Epoch 246201, Training Loss: 31012, Validation Loss: 55546, 159623.91401875656\n",
      "Epoch 246301, Training Loss: 28334, Validation Loss: 54452, 144980.7617700962\n",
      "Epoch 246401, Training Loss: 28310, Validation Loss: 55422, 156139.2167610596\n",
      "Epoch 246501, Training Loss: 30896, Validation Loss: 55015, 138117.64123921035\n",
      "Epoch 246601, Training Loss: 29179, Validation Loss: 56280, 124427.92357410988\n",
      "Epoch 246701, Training Loss: 31236, Validation Loss: 54612, 156796.18900607413\n",
      "Epoch 246801, Training Loss: 28055, Validation Loss: 58852, 136176.92427850037\n",
      "Epoch 246901, Training Loss: 29439, Validation Loss: 56138, 119288.89878004321\n",
      "Epoch 247001, Training Loss: 29364, Validation Loss: 55924, 154673.56248341416\n",
      "Epoch 247101, Training Loss: 29789, Validation Loss: 54668, 143158.05347602544\n",
      "Epoch 247201, Training Loss: 28360, Validation Loss: 56111, 119212.37489887464\n",
      "Epoch 247301, Training Loss: 30338, Validation Loss: 57802, 152474.5352309894\n",
      "Epoch 247401, Training Loss: 30489, Validation Loss: 57312, 168565.46336885943\n",
      "Epoch 247501, Training Loss: 28853, Validation Loss: 54412, 128097.54859773492\n",
      "Epoch 247601, Training Loss: 29721, Validation Loss: 55194, 136454.14791812486\n",
      "Epoch 247701, Training Loss: 30279, Validation Loss: 54634, 118273.81773266639\n",
      "Epoch 247801, Training Loss: 29899, Validation Loss: 56337, 137537.96562229347\n",
      "Epoch 247901, Training Loss: 29414, Validation Loss: 55799, 145123.6007386875\n",
      "Epoch 248001, Training Loss: 29682, Validation Loss: 55782, 147433.0083376975\n",
      "Epoch 248101, Training Loss: 31096, Validation Loss: 54302, 166047.3277646454\n",
      "Epoch 248201, Training Loss: 29764, Validation Loss: 53518, 112056.4872225265\n",
      "Epoch 248301, Training Loss: 30448, Validation Loss: 58842, 166390.40859219292\n",
      "Epoch 248401, Training Loss: 29583, Validation Loss: 55348, 156395.58036677615\n",
      "Epoch 248501, Training Loss: 28528, Validation Loss: 55522, 130278.00697587205\n",
      "Epoch 248601, Training Loss: 29415, Validation Loss: 55753, 150932.78236090965\n",
      "Epoch 248701, Training Loss: 26859, Validation Loss: 55352, 139111.9500245012\n",
      "Epoch 248801, Training Loss: 29434, Validation Loss: 55001, 138280.588929081\n",
      "Epoch 248901, Training Loss: 30882, Validation Loss: 53142, 169620.98487607844\n",
      "Epoch 249001, Training Loss: 29885, Validation Loss: 54354, 177381.14556279656\n",
      "Epoch 249101, Training Loss: 29876, Validation Loss: 56476, 158193.37641466153\n",
      "Epoch 249201, Training Loss: 28868, Validation Loss: 55563, 125183.52943819614\n",
      "Epoch 249301, Training Loss: 29095, Validation Loss: 55001, 154909.8662922353\n",
      "Epoch 249401, Training Loss: 30391, Validation Loss: 52422, 160505.41977187846\n",
      "Epoch 249501, Training Loss: 29946, Validation Loss: 54030, 139640.87298816632\n",
      "Epoch 249601, Training Loss: 31874, Validation Loss: 55863, 176330.68414170778\n",
      "Epoch 249701, Training Loss: 31978, Validation Loss: 56103, 154980.19087786085\n",
      "Epoch 249801, Training Loss: 31046, Validation Loss: 53543, 155047.90165334352\n",
      "Epoch 249901, Training Loss: 29846, Validation Loss: 55859, 150796.15205537505\n",
      "Epoch 250001, Training Loss: 28928, Validation Loss: 58163, 179539.19879632923\n",
      "Epoch 250101, Training Loss: 30032, Validation Loss: 56127, 160538.6583443811\n",
      "Epoch 250201, Training Loss: 32470, Validation Loss: 54703, 174308.25540511176\n",
      "Epoch 250301, Training Loss: 31785, Validation Loss: 54249, 175978.17462528305\n",
      "Epoch 250401, Training Loss: 30531, Validation Loss: 56697, 145756.99389338828\n",
      "Epoch 250501, Training Loss: 27935, Validation Loss: 54320, 144006.0437029088\n",
      "Epoch 250601, Training Loss: 28855, Validation Loss: 57951, 174642.8406594131\n",
      "Epoch 250701, Training Loss: 29019, Validation Loss: 55600, 131858.5146823726\n",
      "Epoch 250801, Training Loss: 29396, Validation Loss: 55315, 156886.44683692174\n",
      "Epoch 250901, Training Loss: 30351, Validation Loss: 53946, 116109.33650202103\n",
      "Epoch 251001, Training Loss: 31882, Validation Loss: 54674, 160028.7207135566\n",
      "Epoch 251101, Training Loss: 29216, Validation Loss: 54517, 155885.9765469305\n",
      "Epoch 251201, Training Loss: 29707, Validation Loss: 58690, 182693.53665964544\n",
      "Epoch 251301, Training Loss: 30565, Validation Loss: 55019, 135232.45825339467\n",
      "Epoch 251401, Training Loss: 30739, Validation Loss: 57787, 165931.71464650714\n",
      "Epoch 251501, Training Loss: 31002, Validation Loss: 57216, 135666.23234534476\n",
      "Epoch 251601, Training Loss: 31550, Validation Loss: 54709, 143345.3618356428\n",
      "Epoch 251701, Training Loss: 31199, Validation Loss: 53968, 129700.83981944397\n",
      "Epoch 251801, Training Loss: 30563, Validation Loss: 56322, 138152.53766069515\n",
      "Epoch 251901, Training Loss: 32115, Validation Loss: 56868, 171394.2852088487\n",
      "Epoch 252001, Training Loss: 30379, Validation Loss: 54327, 140764.66203803435\n",
      "Epoch 252101, Training Loss: 31608, Validation Loss: 53931, 145309.4339578798\n",
      "Epoch 252201, Training Loss: 31348, Validation Loss: 54343, 143017.96184661682\n",
      "Epoch 252301, Training Loss: 29485, Validation Loss: 56457, 138209.44450065566\n",
      "Epoch 252401, Training Loss: 30077, Validation Loss: 56746, 119208.3161814987\n",
      "Epoch 252501, Training Loss: 31809, Validation Loss: 55579, 140543.50919841326\n",
      "Epoch 252601, Training Loss: 30280, Validation Loss: 56242, 133313.54496189524\n",
      "Epoch 252701, Training Loss: 28184, Validation Loss: 55898, 147818.55425759664\n",
      "Epoch 252801, Training Loss: 29218, Validation Loss: 56588, 142011.73427171406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252901, Training Loss: 28933, Validation Loss: 54670, 127230.27012040773\n",
      "Epoch 253001, Training Loss: 30328, Validation Loss: 55846, 159039.53704480434\n",
      "Epoch 253101, Training Loss: 31758, Validation Loss: 56718, 143142.48455961497\n",
      "Epoch 253201, Training Loss: 31375, Validation Loss: 56400, 175525.8674203835\n",
      "Epoch 253301, Training Loss: 28799, Validation Loss: 54149, 140392.9445726807\n",
      "Epoch 253401, Training Loss: 28874, Validation Loss: 55440, 144805.38658522948\n",
      "Epoch 253501, Training Loss: 29494, Validation Loss: 55041, 133157.59088325995\n",
      "Epoch 253601, Training Loss: 28997, Validation Loss: 57853, 149867.22449459057\n",
      "Epoch 253701, Training Loss: 29153, Validation Loss: 54806, 152836.3829504698\n",
      "Epoch 253801, Training Loss: 29452, Validation Loss: 51757, 134296.60802009428\n",
      "Epoch 253901, Training Loss: 30383, Validation Loss: 56984, 157707.2574208356\n",
      "Epoch 254001, Training Loss: 30506, Validation Loss: 54431, 145634.7606839587\n",
      "Epoch 254101, Training Loss: 29419, Validation Loss: 55610, 134237.40659747238\n",
      "Epoch 254201, Training Loss: 30107, Validation Loss: 53482, 118263.3052707873\n",
      "Epoch 254301, Training Loss: 28708, Validation Loss: 54841, 152821.6411334946\n",
      "Epoch 254401, Training Loss: 29978, Validation Loss: 53745, 143445.72875682582\n",
      "Epoch 254501, Training Loss: 30027, Validation Loss: 54679, 132557.07518033154\n",
      "Epoch 254601, Training Loss: 29574, Validation Loss: 54183, 168094.2637969636\n",
      "Epoch 254701, Training Loss: 30228, Validation Loss: 57092, 151443.01760280886\n",
      "Epoch 254801, Training Loss: 30754, Validation Loss: 54878, 127463.77330013532\n",
      "Epoch 254901, Training Loss: 31303, Validation Loss: 56523, 150723.53456017905\n",
      "Epoch 255001, Training Loss: 28570, Validation Loss: 54246, 153185.1445065981\n",
      "Epoch 255101, Training Loss: 31298, Validation Loss: 55063, 116978.73257992002\n",
      "Epoch 255201, Training Loss: 29419, Validation Loss: 55348, 170936.02853172206\n",
      "Epoch 255301, Training Loss: 29076, Validation Loss: 56429, 131328.64239443908\n",
      "Epoch 255401, Training Loss: 29064, Validation Loss: 54294, 140078.92251348784\n",
      "Epoch 255501, Training Loss: 30703, Validation Loss: 56192, 169904.74792140146\n",
      "Epoch 255601, Training Loss: 29820, Validation Loss: 57082, 142622.91712764022\n",
      "Epoch 255701, Training Loss: 30490, Validation Loss: 55194, 152913.19232689822\n",
      "Epoch 255801, Training Loss: 30291, Validation Loss: 53799, 151548.5016481451\n",
      "Epoch 255901, Training Loss: 29995, Validation Loss: 54584, 116739.9144079996\n",
      "Epoch 256001, Training Loss: 29669, Validation Loss: 57069, 136005.09855787136\n",
      "Epoch 256101, Training Loss: 29378, Validation Loss: 54867, 138578.2863116754\n",
      "Epoch 256201, Training Loss: 27586, Validation Loss: 55949, 155227.8941054632\n",
      "Epoch 256301, Training Loss: 31347, Validation Loss: 56693, 154768.03080700332\n",
      "Epoch 256401, Training Loss: 30259, Validation Loss: 53902, 121344.98281343801\n",
      "Epoch 256501, Training Loss: 30957, Validation Loss: 56345, 120572.2754885356\n",
      "Epoch 256601, Training Loss: 29531, Validation Loss: 52724, 167140.67390118376\n",
      "Epoch 256701, Training Loss: 29730, Validation Loss: 54068, 122380.768483425\n",
      "Epoch 256801, Training Loss: 30305, Validation Loss: 54828, 143263.71142214007\n",
      "Epoch 256901, Training Loss: 28853, Validation Loss: 56224, 133105.5486022363\n",
      "Epoch 257001, Training Loss: 29822, Validation Loss: 54487, 161865.43946513953\n",
      "Epoch 257101, Training Loss: 29983, Validation Loss: 54994, 136573.27770841\n",
      "Epoch 257201, Training Loss: 27581, Validation Loss: 56710, 143455.50510633888\n",
      "Epoch 257301, Training Loss: 29416, Validation Loss: 56664, 123114.73618180914\n",
      "Epoch 257401, Training Loss: 30707, Validation Loss: 53889, 161777.79746071814\n",
      "Epoch 257501, Training Loss: 31577, Validation Loss: 57434, 148252.18524697243\n",
      "Epoch 257601, Training Loss: 29099, Validation Loss: 53561, 131191.1141601941\n",
      "Epoch 257701, Training Loss: 31184, Validation Loss: 55750, 141409.25530656573\n",
      "Epoch 257801, Training Loss: 30463, Validation Loss: 53225, 156792.33357902872\n",
      "Epoch 257901, Training Loss: 28252, Validation Loss: 55498, 130080.53699517853\n",
      "Epoch 258001, Training Loss: 28557, Validation Loss: 54393, 131912.15301598923\n",
      "Epoch 258101, Training Loss: 31760, Validation Loss: 58507, 174955.43489881634\n",
      "Epoch 258201, Training Loss: 28443, Validation Loss: 54811, 159589.15419013606\n",
      "Epoch 258301, Training Loss: 30342, Validation Loss: 57227, 128720.54934543926\n",
      "Epoch 258401, Training Loss: 30064, Validation Loss: 55068, 132646.83915776561\n",
      "Epoch 258501, Training Loss: 30140, Validation Loss: 55008, 130172.44186932202\n",
      "Epoch 258601, Training Loss: 30526, Validation Loss: 54243, 145683.33872436042\n",
      "Epoch 258701, Training Loss: 29651, Validation Loss: 54287, 119139.8262791684\n",
      "Epoch 258801, Training Loss: 28397, Validation Loss: 55433, 128920.17212157953\n",
      "Epoch 258901, Training Loss: 31565, Validation Loss: 54830, 141034.94651750464\n",
      "Epoch 259001, Training Loss: 30676, Validation Loss: 55900, 146063.29219200046\n",
      "Epoch 259101, Training Loss: 30671, Validation Loss: 54680, 136430.26586904642\n",
      "Epoch 259201, Training Loss: 32128, Validation Loss: 53020, 178797.0065401093\n",
      "Epoch 259301, Training Loss: 28896, Validation Loss: 57093, 152962.58490808698\n",
      "Epoch 259401, Training Loss: 29522, Validation Loss: 55638, 153576.84413558632\n",
      "Epoch 259501, Training Loss: 30876, Validation Loss: 56321, 186086.7869318799\n",
      "Epoch 259601, Training Loss: 31432, Validation Loss: 57233, 184162.0312776367\n",
      "Epoch 259701, Training Loss: 28287, Validation Loss: 55659, 141017.7741251717\n",
      "Epoch 259801, Training Loss: 28943, Validation Loss: 54029, 151762.11308290221\n",
      "Epoch 259901, Training Loss: 30950, Validation Loss: 54223, 145361.32195312198\n",
      "Epoch 260001, Training Loss: 31395, Validation Loss: 55673, 159440.9633109671\n",
      "Epoch 260101, Training Loss: 30426, Validation Loss: 54268, 135062.67861004276\n",
      "Epoch 260201, Training Loss: 29627, Validation Loss: 54104, 152471.25757582107\n",
      "Epoch 260301, Training Loss: 31379, Validation Loss: 53231, 141733.9538809501\n",
      "Epoch 260401, Training Loss: 30203, Validation Loss: 53844, 144097.60617427077\n",
      "Epoch 260501, Training Loss: 30637, Validation Loss: 54466, 121748.37224042245\n",
      "Epoch 260601, Training Loss: 29933, Validation Loss: 55781, 138882.14442383876\n",
      "Epoch 260701, Training Loss: 28917, Validation Loss: 53296, 123171.68944416223\n",
      "Epoch 260801, Training Loss: 28143, Validation Loss: 53734, 138226.95996555904\n",
      "Epoch 260901, Training Loss: 29004, Validation Loss: 56562, 131805.74805078498\n",
      "Epoch 261001, Training Loss: 28542, Validation Loss: 56122, 151601.72035533094\n",
      "Epoch 261101, Training Loss: 29102, Validation Loss: 53193, 140533.02201455343\n",
      "Epoch 261201, Training Loss: 30848, Validation Loss: 55857, 159501.16065935307\n",
      "Epoch 261301, Training Loss: 28762, Validation Loss: 56793, 134152.23992160574\n",
      "Epoch 261401, Training Loss: 30928, Validation Loss: 55966, 150218.72251306914\n",
      "Epoch 261501, Training Loss: 30220, Validation Loss: 53600, 152410.76530720215\n",
      "Epoch 261601, Training Loss: 29753, Validation Loss: 54486, 145229.80041287144\n",
      "Epoch 261701, Training Loss: 30717, Validation Loss: 56395, 136135.88652900356\n",
      "Epoch 261801, Training Loss: 31650, Validation Loss: 54410, 166896.2495552735\n",
      "Epoch 261901, Training Loss: 28959, Validation Loss: 55668, 131967.02222230085\n",
      "Epoch 262001, Training Loss: 29168, Validation Loss: 54574, 140072.09394575952\n",
      "Epoch 262101, Training Loss: 32405, Validation Loss: 54719, 157964.44445731927\n",
      "Epoch 262201, Training Loss: 27390, Validation Loss: 55330, 100423.05472537488\n",
      "Epoch 262301, Training Loss: 29529, Validation Loss: 54559, 142159.42995644195\n",
      "Epoch 262401, Training Loss: 29672, Validation Loss: 54218, 175803.01654284092\n",
      "Epoch 262501, Training Loss: 30145, Validation Loss: 54622, 139911.90578403627\n",
      "Epoch 262601, Training Loss: 31651, Validation Loss: 56425, 166163.4356159451\n",
      "Epoch 262701, Training Loss: 30124, Validation Loss: 57634, 128276.9039584172\n",
      "Epoch 262801, Training Loss: 30365, Validation Loss: 54687, 135955.3904311888\n",
      "Epoch 262901, Training Loss: 30571, Validation Loss: 56654, 159706.15946644393\n",
      "Epoch 263001, Training Loss: 29813, Validation Loss: 54183, 135442.88740869967\n",
      "Epoch 263101, Training Loss: 31176, Validation Loss: 56711, 142190.6249238017\n",
      "Epoch 263201, Training Loss: 30203, Validation Loss: 53573, 169789.80592775266\n",
      "Epoch 263301, Training Loss: 28345, Validation Loss: 53374, 129069.24188529691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263401, Training Loss: 29692, Validation Loss: 56435, 156816.23722802772\n",
      "Epoch 263501, Training Loss: 32306, Validation Loss: 54253, 159277.20115153491\n",
      "Epoch 263601, Training Loss: 29001, Validation Loss: 54156, 149202.86997888298\n",
      "Epoch 263701, Training Loss: 30140, Validation Loss: 53413, 148657.26800971848\n",
      "Epoch 263801, Training Loss: 31242, Validation Loss: 55929, 153200.75924480107\n",
      "Epoch 263901, Training Loss: 32748, Validation Loss: 55090, 162272.91323202214\n",
      "Epoch 264001, Training Loss: 28739, Validation Loss: 54858, 165369.7769294587\n",
      "Epoch 264101, Training Loss: 30172, Validation Loss: 54488, 146471.8358094142\n",
      "Epoch 264201, Training Loss: 29758, Validation Loss: 54426, 167196.0353799208\n",
      "Epoch 264301, Training Loss: 29039, Validation Loss: 54809, 110598.32695760795\n",
      "Epoch 264401, Training Loss: 28417, Validation Loss: 56197, 149073.99787693127\n",
      "Epoch 264501, Training Loss: 29820, Validation Loss: 56549, 128417.92986455574\n",
      "Epoch 264601, Training Loss: 32563, Validation Loss: 57466, 147078.2356058539\n",
      "Epoch 264701, Training Loss: 29421, Validation Loss: 55593, 136772.87059608658\n",
      "Epoch 264801, Training Loss: 30226, Validation Loss: 54720, 142982.83705604542\n",
      "Epoch 264901, Training Loss: 29613, Validation Loss: 56288, 140068.0872409905\n",
      "Epoch 265001, Training Loss: 29707, Validation Loss: 54612, 143527.47113261392\n",
      "Epoch 265101, Training Loss: 27597, Validation Loss: 56261, 141523.37936556645\n",
      "Epoch 265201, Training Loss: 28704, Validation Loss: 54234, 133744.91651809774\n",
      "Epoch 265301, Training Loss: 29630, Validation Loss: 54807, 127744.44306271011\n",
      "Epoch 265401, Training Loss: 29703, Validation Loss: 55184, 161104.9855789499\n",
      "Epoch 265501, Training Loss: 29573, Validation Loss: 54765, 132123.5638739057\n",
      "Epoch 265601, Training Loss: 28985, Validation Loss: 55019, 122216.33586210995\n",
      "Epoch 265701, Training Loss: 27997, Validation Loss: 55414, 118056.72351290399\n",
      "Epoch 265801, Training Loss: 29210, Validation Loss: 52919, 142794.92824522007\n",
      "Epoch 265901, Training Loss: 29643, Validation Loss: 55139, 170870.81576781036\n",
      "Epoch 266001, Training Loss: 28993, Validation Loss: 52812, 141188.17277870484\n",
      "Epoch 266101, Training Loss: 31354, Validation Loss: 54820, 138542.7856358273\n",
      "Epoch 266201, Training Loss: 29363, Validation Loss: 55368, 138397.53566676687\n",
      "Epoch 266301, Training Loss: 30933, Validation Loss: 53368, 156057.9773999496\n",
      "Epoch 266401, Training Loss: 31771, Validation Loss: 53609, 177088.4771228348\n",
      "Epoch 266501, Training Loss: 29798, Validation Loss: 53749, 172781.30852203758\n",
      "Epoch 266601, Training Loss: 29180, Validation Loss: 56690, 152112.16001275758\n",
      "Epoch 266701, Training Loss: 29656, Validation Loss: 56206, 124631.55652921888\n",
      "Epoch 266801, Training Loss: 29524, Validation Loss: 54835, 129777.28888044282\n",
      "Epoch 266901, Training Loss: 29343, Validation Loss: 57174, 155568.19673256722\n",
      "Epoch 267001, Training Loss: 30044, Validation Loss: 59754, 156389.28642529267\n",
      "Epoch 267101, Training Loss: 31372, Validation Loss: 55122, 140758.34588189272\n",
      "Epoch 267201, Training Loss: 29445, Validation Loss: 54693, 153871.9327998082\n",
      "Epoch 267301, Training Loss: 29747, Validation Loss: 54767, 130468.77646963976\n",
      "Epoch 267401, Training Loss: 29029, Validation Loss: 54652, 126971.60143824038\n",
      "Epoch 267501, Training Loss: 28384, Validation Loss: 55463, 138481.7039439185\n",
      "Epoch 267601, Training Loss: 29297, Validation Loss: 55204, 121766.08378946772\n",
      "Epoch 267701, Training Loss: 29054, Validation Loss: 55666, 131119.6956036005\n",
      "Epoch 267801, Training Loss: 30412, Validation Loss: 55531, 157510.68245422383\n",
      "Epoch 267901, Training Loss: 30278, Validation Loss: 57583, 144604.29053235176\n",
      "Epoch 268001, Training Loss: 29209, Validation Loss: 55869, 137019.60158693945\n",
      "Epoch 268101, Training Loss: 30329, Validation Loss: 53329, 163768.72617622113\n",
      "Epoch 268201, Training Loss: 28666, Validation Loss: 55299, 114862.64301141817\n",
      "Epoch 268301, Training Loss: 32307, Validation Loss: 56515, 132928.87819738215\n",
      "Epoch 268401, Training Loss: 30277, Validation Loss: 56615, 137428.3735552697\n",
      "Epoch 268501, Training Loss: 31487, Validation Loss: 58857, 166496.87919764355\n",
      "Epoch 268601, Training Loss: 29691, Validation Loss: 53487, 172140.0047964131\n",
      "Epoch 268701, Training Loss: 30570, Validation Loss: 56340, 123844.31263793411\n",
      "Epoch 268801, Training Loss: 28407, Validation Loss: 57083, 158766.84449461818\n",
      "Epoch 268901, Training Loss: 30534, Validation Loss: 54844, 130041.52881903201\n",
      "Epoch 269001, Training Loss: 28669, Validation Loss: 54695, 116938.79396989364\n",
      "Epoch 269101, Training Loss: 31388, Validation Loss: 57522, 145141.165856818\n",
      "Epoch 269201, Training Loss: 29835, Validation Loss: 56586, 140676.06550844546\n",
      "Epoch 269301, Training Loss: 31004, Validation Loss: 57227, 134217.6391266055\n",
      "Epoch 269401, Training Loss: 30819, Validation Loss: 54436, 178043.27343816552\n",
      "Epoch 269501, Training Loss: 28776, Validation Loss: 54143, 124870.03873038807\n",
      "Epoch 269601, Training Loss: 31119, Validation Loss: 55482, 147252.76224090115\n",
      "Epoch 269701, Training Loss: 29980, Validation Loss: 54015, 128649.82094818114\n",
      "Epoch 269801, Training Loss: 30655, Validation Loss: 55268, 155290.2324810155\n",
      "Epoch 269901, Training Loss: 28589, Validation Loss: 54317, 104527.40265889747\n",
      "Epoch 270001, Training Loss: 32468, Validation Loss: 55831, 133836.57915808633\n",
      "Epoch 270101, Training Loss: 30097, Validation Loss: 55502, 147010.69946564757\n",
      "Epoch 270201, Training Loss: 29340, Validation Loss: 56351, 122044.78025144614\n",
      "Epoch 270301, Training Loss: 29237, Validation Loss: 55608, 164356.5677155346\n",
      "Epoch 270401, Training Loss: 28053, Validation Loss: 55384, 115867.25749399894\n",
      "Epoch 270501, Training Loss: 29198, Validation Loss: 53421, 134014.43319628132\n",
      "Epoch 270601, Training Loss: 28601, Validation Loss: 54920, 136600.13912177848\n",
      "Epoch 270701, Training Loss: 29544, Validation Loss: 54520, 139269.84027847945\n",
      "Epoch 270801, Training Loss: 28046, Validation Loss: 54249, 139074.9853811261\n",
      "Epoch 270901, Training Loss: 29748, Validation Loss: 54367, 140248.94105755418\n",
      "Epoch 271001, Training Loss: 28978, Validation Loss: 55799, 113113.77811260635\n",
      "Epoch 271101, Training Loss: 27509, Validation Loss: 53881, 123901.17909278639\n",
      "Epoch 271201, Training Loss: 29834, Validation Loss: 56888, 117283.99563188865\n",
      "Epoch 271301, Training Loss: 30109, Validation Loss: 52562, 155439.70173404133\n",
      "Epoch 271401, Training Loss: 29892, Validation Loss: 58015, 148600.80146671177\n",
      "Epoch 271501, Training Loss: 30850, Validation Loss: 58166, 137349.37834202888\n",
      "Epoch 271601, Training Loss: 29134, Validation Loss: 55421, 129147.13308678199\n",
      "Epoch 271701, Training Loss: 30531, Validation Loss: 55084, 150522.8928731544\n",
      "Epoch 271801, Training Loss: 30271, Validation Loss: 53678, 163703.77124112146\n",
      "Epoch 271901, Training Loss: 27631, Validation Loss: 55695, 109531.19405785654\n",
      "Epoch 272001, Training Loss: 29154, Validation Loss: 57058, 152295.53670956395\n",
      "Epoch 272101, Training Loss: 30224, Validation Loss: 55029, 164740.8715272279\n",
      "Epoch 272201, Training Loss: 31321, Validation Loss: 56012, 151248.75736168973\n",
      "Epoch 272301, Training Loss: 28570, Validation Loss: 54977, 145109.52827782242\n",
      "Epoch 272401, Training Loss: 28802, Validation Loss: 56191, 157033.10144698826\n",
      "Epoch 272501, Training Loss: 28857, Validation Loss: 57556, 188697.3464365261\n",
      "Epoch 272601, Training Loss: 32093, Validation Loss: 54158, 149785.84412207114\n",
      "Epoch 272701, Training Loss: 29246, Validation Loss: 53541, 127789.71651304953\n",
      "Epoch 272801, Training Loss: 28487, Validation Loss: 56480, 145024.77708978317\n",
      "Epoch 272901, Training Loss: 29369, Validation Loss: 53694, 168549.86054330753\n",
      "Epoch 273001, Training Loss: 29446, Validation Loss: 53687, 116654.12544283205\n",
      "Epoch 273101, Training Loss: 29523, Validation Loss: 55446, 122743.59416001076\n",
      "Epoch 273201, Training Loss: 27744, Validation Loss: 55168, 145174.3626515605\n",
      "Epoch 273301, Training Loss: 30461, Validation Loss: 54834, 132092.69176189107\n",
      "Epoch 273401, Training Loss: 29272, Validation Loss: 53002, 147145.79281561117\n",
      "Epoch 273501, Training Loss: 29054, Validation Loss: 54717, 114360.07597046373\n",
      "Epoch 273601, Training Loss: 28165, Validation Loss: 56631, 159645.54192748014\n",
      "Epoch 273701, Training Loss: 29643, Validation Loss: 56310, 156186.47830939075\n",
      "Epoch 273801, Training Loss: 27373, Validation Loss: 54164, 129589.88033317107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273901, Training Loss: 28762, Validation Loss: 57517, 122510.988845518\n",
      "Epoch 274001, Training Loss: 29730, Validation Loss: 54812, 146506.7798362114\n",
      "Epoch 274101, Training Loss: 29555, Validation Loss: 55449, 114202.97471631403\n",
      "Epoch 274201, Training Loss: 29635, Validation Loss: 57125, 131552.85590776545\n",
      "Epoch 274301, Training Loss: 31497, Validation Loss: 54836, 153845.72738466033\n",
      "Epoch 274401, Training Loss: 29101, Validation Loss: 55651, 131730.13698611682\n",
      "Epoch 274501, Training Loss: 29421, Validation Loss: 55891, 151557.5816562025\n",
      "Epoch 274601, Training Loss: 30087, Validation Loss: 56405, 163670.2309341337\n",
      "Epoch 274701, Training Loss: 28899, Validation Loss: 55982, 112435.33069421523\n",
      "Epoch 274801, Training Loss: 29636, Validation Loss: 56307, 121309.20158372427\n",
      "Epoch 274901, Training Loss: 31398, Validation Loss: 55420, 132847.64596253744\n",
      "Epoch 275001, Training Loss: 30637, Validation Loss: 55238, 146073.28787813903\n",
      "Epoch 275101, Training Loss: 28663, Validation Loss: 54201, 132220.53913764577\n",
      "Epoch 275201, Training Loss: 28397, Validation Loss: 55780, 145722.0981280674\n",
      "Epoch 275301, Training Loss: 30549, Validation Loss: 56141, 138644.65049221026\n",
      "Epoch 275401, Training Loss: 29607, Validation Loss: 54896, 122829.57037743875\n",
      "Epoch 275501, Training Loss: 28763, Validation Loss: 53604, 135090.00699554058\n",
      "Epoch 275601, Training Loss: 28990, Validation Loss: 54684, 126703.28511933428\n",
      "Epoch 275701, Training Loss: 29288, Validation Loss: 54135, 137842.11337658454\n",
      "Epoch 275801, Training Loss: 27736, Validation Loss: 53909, 119270.40609978163\n",
      "Epoch 275901, Training Loss: 28729, Validation Loss: 55395, 145370.62911610887\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m     grad_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_gradient_norm(model)\n\u001b[0;32m---> 32\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     36\u001b[0m grad_norms\u001b[38;5;241m.\u001b[39mappend(grad_norm \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:434\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=9e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999999, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
