{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1850 entries, 0 to 1849\n",
      "Data columns (total 31 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Unnamed: 0                         1850 non-null   int64  \n",
      " 1   price                              1850 non-null   float64\n",
      " 2   area                               1850 non-null   float64\n",
      " 3   rooms                              1850 non-null   float64\n",
      " 4   floor                              1850 non-null   float64\n",
      " 5   storeys                            1850 non-null   float64\n",
      " 6   bathroom_count                     1850 non-null   float64\n",
      " 7   new_building                       1850 non-null   int64  \n",
      " 8   F_Balcony                          1850 non-null   int64  \n",
      " 9   F_Internet                         1850 non-null   int64  \n",
      " 10  F_Security                         1850 non-null   int64  \n",
      " 11  F_Elevator                         1850 non-null   int64  \n",
      " 12  F_Air Conditioner                  1850 non-null   int64  \n",
      " 13  F_Heating System                   1850 non-null   int64  \n",
      " 14  F_Furniture                        1850 non-null   int64  \n",
      " 15  SL_opera                           1850 non-null   int64  \n",
      " 16  SL_zvartnoc                        1850 non-null   int64  \n",
      " 17  SL_central_cemetery                1850 non-null   int64  \n",
      " 18  SL_shengavit_subway                1850 non-null   int64  \n",
      " 19  L_pharmacy                         1850 non-null   int64  \n",
      " 20  L_atm                              1850 non-null   int64  \n",
      " 21  L_parking                          1850 non-null   int64  \n",
      " 22  L_university                       1850 non-null   int64  \n",
      " 23  building_type_Monolit              1850 non-null   int64  \n",
      " 24  building_type_Other                1850 non-null   int64  \n",
      " 25  building_type_Panel                1850 non-null   int64  \n",
      " 26  building_type_Stone                1850 non-null   int64  \n",
      " 27  condition_Good Condition           1850 non-null   int64  \n",
      " 28  condition_Needs Renovation/Repair  1850 non-null   int64  \n",
      " 29  condition_New/Excellent Condition  1850 non-null   int64  \n",
      " 30  condition_Other                    1850 non-null   int64  \n",
      "dtypes: float64(6), int64(25)\n",
      "memory usage: 448.2 KB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\", \n",
    "                                                           \"L_clinic\", \"L_place_of_worship\",\n",
    "                                                           \"L_hospital\", \"L_kindergarten\",\n",
    "                                                           \"SL_baghramyan_subway\"])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 64\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.18),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 240723, Validation Loss: 244170, 5.282248829189155\n",
      "Epoch 101, Training Loss: 62587, Validation Loss: 61138, 66305.02547601871\n",
      "Epoch 201, Training Loss: 62410, Validation Loss: 63473, 149536.84273430044\n",
      "Epoch 301, Training Loss: 62510, Validation Loss: 75158, 163751.0563065162\n",
      "Epoch 00345: reducing learning rate of group 0 to 9.9900e-03.\n",
      "Epoch 401, Training Loss: 59250, Validation Loss: 57868, 183575.72304144167\n",
      "Epoch 00488: reducing learning rate of group 0 to 9.9800e-03.\n",
      "Epoch 501, Training Loss: 59835, Validation Loss: 54852, 194913.03632342978\n",
      "Epoch 00589: reducing learning rate of group 0 to 9.9700e-03.\n",
      "Epoch 601, Training Loss: 59798, Validation Loss: 61881, 131192.32429570457\n",
      "Epoch 00690: reducing learning rate of group 0 to 9.9601e-03.\n",
      "Epoch 701, Training Loss: 62775, Validation Loss: 87344, 243632.14424374118\n",
      "Epoch 00791: reducing learning rate of group 0 to 9.9501e-03.\n",
      "Epoch 801, Training Loss: 56008, Validation Loss: 61206, 188560.8657432499\n",
      "Epoch 00892: reducing learning rate of group 0 to 9.9401e-03.\n",
      "Epoch 901, Training Loss: 60409, Validation Loss: 57863, 130640.90660649033\n",
      "Epoch 01000: reducing learning rate of group 0 to 9.9302e-03.\n",
      "Epoch 1001, Training Loss: 56811, Validation Loss: 57584, 178249.94959954193\n",
      "Epoch 1101, Training Loss: 57856, Validation Loss: 57792, 166768.56320593823\n",
      "Epoch 01144: reducing learning rate of group 0 to 9.9203e-03.\n",
      "Epoch 1201, Training Loss: 59175, Validation Loss: 82632, 159693.69571405975\n",
      "Epoch 01245: reducing learning rate of group 0 to 9.9104e-03.\n",
      "Epoch 1301, Training Loss: 59541, Validation Loss: 56733, 188071.39729672988\n",
      "Epoch 1401, Training Loss: 58268, Validation Loss: 53782, 142911.75824457107\n",
      "Epoch 01435: reducing learning rate of group 0 to 9.9004e-03.\n",
      "Epoch 1501, Training Loss: 58744, Validation Loss: 60488, 138409.47609225824\n",
      "Epoch 01536: reducing learning rate of group 0 to 9.8905e-03.\n",
      "Epoch 1601, Training Loss: 57379, Validation Loss: 76207, 133518.81525711514\n",
      "Epoch 01637: reducing learning rate of group 0 to 9.8807e-03.\n",
      "Epoch 1701, Training Loss: 60213, Validation Loss: 60285, 195644.1337663027\n",
      "Epoch 01738: reducing learning rate of group 0 to 9.8708e-03.\n",
      "Epoch 1801, Training Loss: 55677, Validation Loss: 70064, 157977.7284314186\n",
      "Epoch 01839: reducing learning rate of group 0 to 9.8609e-03.\n",
      "Epoch 1901, Training Loss: 56636, Validation Loss: 54558, 135026.1644757931\n",
      "Epoch 01940: reducing learning rate of group 0 to 9.8510e-03.\n",
      "Epoch 2001, Training Loss: 56655, Validation Loss: 55082, 199988.67817466348\n",
      "Epoch 02041: reducing learning rate of group 0 to 9.8412e-03.\n",
      "Epoch 2101, Training Loss: 54399, Validation Loss: 74262, 225256.2153038619\n",
      "Epoch 02142: reducing learning rate of group 0 to 9.8314e-03.\n",
      "Epoch 2201, Training Loss: 54649, Validation Loss: 56096, 158523.2221029315\n",
      "Epoch 02243: reducing learning rate of group 0 to 9.8215e-03.\n",
      "Epoch 2301, Training Loss: 57464, Validation Loss: 55857, 194678.27283829672\n",
      "Epoch 02344: reducing learning rate of group 0 to 9.8117e-03.\n",
      "Epoch 2401, Training Loss: 55180, Validation Loss: 54113, 181113.55489213215\n",
      "Epoch 02445: reducing learning rate of group 0 to 9.8019e-03.\n",
      "Epoch 2501, Training Loss: 56769, Validation Loss: 60044, 150084.9467169328\n",
      "Epoch 02546: reducing learning rate of group 0 to 9.7921e-03.\n",
      "Epoch 2601, Training Loss: 52033, Validation Loss: 57017, 106982.94596463314\n",
      "Epoch 02647: reducing learning rate of group 0 to 9.7823e-03.\n",
      "Epoch 2701, Training Loss: 54416, Validation Loss: 57295, 172734.36035159774\n",
      "Epoch 02748: reducing learning rate of group 0 to 9.7725e-03.\n",
      "Epoch 2801, Training Loss: 56910, Validation Loss: 58270, 116057.53355482122\n",
      "Epoch 02849: reducing learning rate of group 0 to 9.7627e-03.\n",
      "Epoch 2901, Training Loss: 51564, Validation Loss: 56263, 200928.8498971434\n",
      "Epoch 02950: reducing learning rate of group 0 to 9.7530e-03.\n",
      "Epoch 3001, Training Loss: 52630, Validation Loss: 66950, 175170.77438690574\n",
      "Epoch 03051: reducing learning rate of group 0 to 9.7432e-03.\n",
      "Epoch 3101, Training Loss: 50968, Validation Loss: 80808, 106918.99221135165\n",
      "Epoch 03152: reducing learning rate of group 0 to 9.7335e-03.\n",
      "Epoch 3201, Training Loss: 52405, Validation Loss: 58504, 201495.42408965653\n",
      "Epoch 03253: reducing learning rate of group 0 to 9.7237e-03.\n",
      "Epoch 3301, Training Loss: 55173, Validation Loss: 58112, 163643.8800897283\n",
      "Epoch 03354: reducing learning rate of group 0 to 9.7140e-03.\n",
      "Epoch 3401, Training Loss: 58848, Validation Loss: 55608, 191774.1286962129\n",
      "Epoch 03455: reducing learning rate of group 0 to 9.7043e-03.\n",
      "Epoch 3501, Training Loss: 53989, Validation Loss: 60661, 172700.3245860387\n",
      "Epoch 03556: reducing learning rate of group 0 to 9.6946e-03.\n",
      "Epoch 3601, Training Loss: 55188, Validation Loss: 56573, 154160.4200414145\n",
      "Epoch 03657: reducing learning rate of group 0 to 9.6849e-03.\n",
      "Epoch 3701, Training Loss: 54271, Validation Loss: 54650, 126766.81815919887\n",
      "Epoch 03758: reducing learning rate of group 0 to 9.6752e-03.\n",
      "Epoch 3801, Training Loss: 57703, Validation Loss: 60788, 106375.5268755537\n",
      "Epoch 03859: reducing learning rate of group 0 to 9.6656e-03.\n",
      "Epoch 3901, Training Loss: 54213, Validation Loss: 65534, 78336.6859277694\n",
      "Epoch 03960: reducing learning rate of group 0 to 9.6559e-03.\n",
      "Epoch 4001, Training Loss: 54270, Validation Loss: 57389, 138945.42109650638\n",
      "Epoch 04061: reducing learning rate of group 0 to 9.6462e-03.\n",
      "Epoch 4101, Training Loss: 52371, Validation Loss: 77713, 140790.9746867092\n",
      "Epoch 04162: reducing learning rate of group 0 to 9.6366e-03.\n",
      "Epoch 4201, Training Loss: 51830, Validation Loss: 53519, 153080.74024101844\n",
      "Epoch 04263: reducing learning rate of group 0 to 9.6269e-03.\n",
      "Epoch 4301, Training Loss: 53989, Validation Loss: 77345, 167561.00222094607\n",
      "Epoch 04364: reducing learning rate of group 0 to 9.6173e-03.\n",
      "Epoch 4401, Training Loss: 53963, Validation Loss: 54557, 153486.32764507097\n",
      "Epoch 04465: reducing learning rate of group 0 to 9.6077e-03.\n",
      "Epoch 4501, Training Loss: 50638, Validation Loss: 66667, 121617.74665658035\n",
      "Epoch 04566: reducing learning rate of group 0 to 9.5981e-03.\n",
      "Epoch 4601, Training Loss: 51166, Validation Loss: 54803, 106938.96177751222\n",
      "Epoch 04667: reducing learning rate of group 0 to 9.5885e-03.\n",
      "Epoch 4701, Training Loss: 59065, Validation Loss: 65362, 127040.28187266407\n",
      "Epoch 04768: reducing learning rate of group 0 to 9.5789e-03.\n",
      "Epoch 4801, Training Loss: 53172, Validation Loss: 66683, 110277.1847839832\n",
      "Epoch 04869: reducing learning rate of group 0 to 9.5693e-03.\n",
      "Epoch 4901, Training Loss: 53298, Validation Loss: 59081, 136219.41162035766\n",
      "Epoch 04970: reducing learning rate of group 0 to 9.5598e-03.\n",
      "Epoch 5001, Training Loss: 53429, Validation Loss: 62669, 111385.02867604706\n",
      "Epoch 05071: reducing learning rate of group 0 to 9.5502e-03.\n",
      "Epoch 5101, Training Loss: 53997, Validation Loss: 60345, 112437.91818267363\n",
      "Epoch 05172: reducing learning rate of group 0 to 9.5406e-03.\n",
      "Epoch 5201, Training Loss: 51643, Validation Loss: 61377, 162528.97973294917\n",
      "Epoch 05273: reducing learning rate of group 0 to 9.5311e-03.\n",
      "Epoch 5301, Training Loss: 51082, Validation Loss: 64021, 106906.7633929507\n",
      "Epoch 05374: reducing learning rate of group 0 to 9.5216e-03.\n",
      "Epoch 5401, Training Loss: 52855, Validation Loss: 77615, 81166.89888318593\n",
      "Epoch 05475: reducing learning rate of group 0 to 9.5121e-03.\n",
      "Epoch 5501, Training Loss: 50642, Validation Loss: 59696, 76862.78919752456\n",
      "Epoch 05576: reducing learning rate of group 0 to 9.5025e-03.\n",
      "Epoch 5601, Training Loss: 50585, Validation Loss: 58467, 114291.28861945019\n",
      "Epoch 05677: reducing learning rate of group 0 to 9.4930e-03.\n",
      "Epoch 5701, Training Loss: 52579, Validation Loss: 73492, 209564.61215498243\n",
      "Epoch 05778: reducing learning rate of group 0 to 9.4835e-03.\n",
      "Epoch 5801, Training Loss: 53548, Validation Loss: 63459, 135201.16724197264\n",
      "Epoch 05879: reducing learning rate of group 0 to 9.4741e-03.\n",
      "Epoch 5901, Training Loss: 53612, Validation Loss: 56474, 128119.97159717917\n",
      "Epoch 05980: reducing learning rate of group 0 to 9.4646e-03.\n",
      "Epoch 6001, Training Loss: 54232, Validation Loss: 62828, 172681.42975341063\n",
      "Epoch 06081: reducing learning rate of group 0 to 9.4551e-03.\n",
      "Epoch 6101, Training Loss: 51474, Validation Loss: 53497, 96609.80041548483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06182: reducing learning rate of group 0 to 9.4457e-03.\n",
      "Epoch 6201, Training Loss: 50905, Validation Loss: 58843, 131365.2797179419\n",
      "Epoch 06283: reducing learning rate of group 0 to 9.4362e-03.\n",
      "Epoch 6301, Training Loss: 49894, Validation Loss: 61585, 93888.239554798\n",
      "Epoch 06384: reducing learning rate of group 0 to 9.4268e-03.\n",
      "Epoch 6401, Training Loss: 53053, Validation Loss: 61761, 139678.17225663038\n",
      "Epoch 06485: reducing learning rate of group 0 to 9.4174e-03.\n",
      "Epoch 6501, Training Loss: 51919, Validation Loss: 56042, 100060.59379270564\n",
      "Epoch 06586: reducing learning rate of group 0 to 9.4079e-03.\n",
      "Epoch 6601, Training Loss: 51047, Validation Loss: 69333, 143968.7712864515\n",
      "Epoch 06687: reducing learning rate of group 0 to 9.3985e-03.\n",
      "Epoch 6701, Training Loss: 50943, Validation Loss: 59543, 94307.71941668977\n",
      "Epoch 06788: reducing learning rate of group 0 to 9.3891e-03.\n",
      "Epoch 6801, Training Loss: 52932, Validation Loss: 58114, 134501.94969033054\n",
      "Epoch 06889: reducing learning rate of group 0 to 9.3797e-03.\n",
      "Epoch 6901, Training Loss: 53144, Validation Loss: 55476, 148745.7895444258\n",
      "Epoch 06990: reducing learning rate of group 0 to 9.3704e-03.\n",
      "Epoch 7001, Training Loss: 53096, Validation Loss: 57097, 134808.03728330214\n",
      "Epoch 07091: reducing learning rate of group 0 to 9.3610e-03.\n",
      "Epoch 7101, Training Loss: 48289, Validation Loss: 62608, 100538.13406008983\n",
      "Epoch 07192: reducing learning rate of group 0 to 9.3516e-03.\n",
      "Epoch 7201, Training Loss: 51568, Validation Loss: 56897, 95199.83964416821\n",
      "Epoch 07293: reducing learning rate of group 0 to 9.3423e-03.\n",
      "Epoch 7301, Training Loss: 54276, Validation Loss: 55585, 103057.59441737493\n",
      "Epoch 07394: reducing learning rate of group 0 to 9.3329e-03.\n",
      "Epoch 7401, Training Loss: 52690, Validation Loss: 54754, 115059.26725147224\n",
      "Epoch 07495: reducing learning rate of group 0 to 9.3236e-03.\n",
      "Epoch 7501, Training Loss: 49388, Validation Loss: 56745, 87898.24383514332\n",
      "Epoch 07596: reducing learning rate of group 0 to 9.3143e-03.\n",
      "Epoch 7601, Training Loss: 49830, Validation Loss: 60510, 88879.40806169478\n",
      "Epoch 07697: reducing learning rate of group 0 to 9.3050e-03.\n",
      "Epoch 7701, Training Loss: 51424, Validation Loss: 66957, 84371.81157292897\n",
      "Epoch 07798: reducing learning rate of group 0 to 9.2957e-03.\n",
      "Epoch 7801, Training Loss: 52514, Validation Loss: 54013, 104049.94120828714\n",
      "Epoch 07899: reducing learning rate of group 0 to 9.2864e-03.\n",
      "Epoch 7901, Training Loss: 49019, Validation Loss: 56254, 83851.0123966617\n",
      "Epoch 08000: reducing learning rate of group 0 to 9.2771e-03.\n",
      "Epoch 8001, Training Loss: 49113, Validation Loss: 59965, 85127.52849532568\n",
      "Epoch 08101: reducing learning rate of group 0 to 9.2678e-03.\n",
      "Epoch 8101, Training Loss: 54296, Validation Loss: 55953, 117282.1399838195\n",
      "Epoch 8201, Training Loss: 52251, Validation Loss: 62939, 111611.79533020745\n",
      "Epoch 08202: reducing learning rate of group 0 to 9.2585e-03.\n",
      "Epoch 8301, Training Loss: 51457, Validation Loss: 70171, 87959.99156501303\n",
      "Epoch 08303: reducing learning rate of group 0 to 9.2493e-03.\n",
      "Epoch 8401, Training Loss: 51039, Validation Loss: 68384, 141753.2425100765\n",
      "Epoch 08404: reducing learning rate of group 0 to 9.2400e-03.\n",
      "Epoch 8501, Training Loss: 53658, Validation Loss: 56606, 95826.97200914314\n",
      "Epoch 08505: reducing learning rate of group 0 to 9.2308e-03.\n",
      "Epoch 8601, Training Loss: 51364, Validation Loss: 57372, 90629.16909058986\n",
      "Epoch 08606: reducing learning rate of group 0 to 9.2216e-03.\n",
      "Epoch 8701, Training Loss: 58643, Validation Loss: 57259, 95891.02903775054\n",
      "Epoch 08707: reducing learning rate of group 0 to 9.2123e-03.\n",
      "Epoch 8801, Training Loss: 51348, Validation Loss: 57066, 63701.455250439845\n",
      "Epoch 08808: reducing learning rate of group 0 to 9.2031e-03.\n",
      "Epoch 8901, Training Loss: 48957, Validation Loss: 63870, 74400.15607913632\n",
      "Epoch 08909: reducing learning rate of group 0 to 9.1939e-03.\n",
      "Epoch 9001, Training Loss: 50948, Validation Loss: 57508, 164524.29987144162\n",
      "Epoch 09010: reducing learning rate of group 0 to 9.1847e-03.\n",
      "Epoch 9101, Training Loss: 51878, Validation Loss: 56926, 107557.0288267604\n",
      "Epoch 09111: reducing learning rate of group 0 to 9.1755e-03.\n",
      "Epoch 9201, Training Loss: 50777, Validation Loss: 60741, 72190.33759887793\n",
      "Epoch 09212: reducing learning rate of group 0 to 9.1664e-03.\n",
      "Epoch 9301, Training Loss: 50510, Validation Loss: 60170, 117390.29877127796\n",
      "Epoch 09313: reducing learning rate of group 0 to 9.1572e-03.\n",
      "Epoch 9401, Training Loss: 53974, Validation Loss: 56209, 85484.34963454369\n",
      "Epoch 09414: reducing learning rate of group 0 to 9.1480e-03.\n",
      "Epoch 9501, Training Loss: 50727, Validation Loss: 55924, 70337.34139405245\n",
      "Epoch 09515: reducing learning rate of group 0 to 9.1389e-03.\n",
      "Epoch 9601, Training Loss: 52319, Validation Loss: 55583, 67656.13146694493\n",
      "Epoch 09616: reducing learning rate of group 0 to 9.1298e-03.\n",
      "Epoch 9701, Training Loss: 50365, Validation Loss: 56690, 84812.20964365073\n",
      "Epoch 09717: reducing learning rate of group 0 to 9.1206e-03.\n",
      "Epoch 9801, Training Loss: 54550, Validation Loss: 66853, 107057.10205846808\n",
      "Epoch 09818: reducing learning rate of group 0 to 9.1115e-03.\n",
      "Epoch 9901, Training Loss: 48616, Validation Loss: 60152, 71318.50195512785\n",
      "Epoch 09919: reducing learning rate of group 0 to 9.1024e-03.\n",
      "Epoch 10001, Training Loss: 51830, Validation Loss: 58045, 107528.10388643376\n",
      "Epoch 10020: reducing learning rate of group 0 to 9.0933e-03.\n",
      "Epoch 10101, Training Loss: 53974, Validation Loss: 60979, 81650.3046781282\n",
      "Epoch 10121: reducing learning rate of group 0 to 9.0842e-03.\n",
      "Epoch 10201, Training Loss: 53304, Validation Loss: 65662, 88609.05610124301\n",
      "Epoch 10222: reducing learning rate of group 0 to 9.0751e-03.\n",
      "Epoch 10301, Training Loss: 51163, Validation Loss: 56243, 63752.68504396436\n",
      "Epoch 10323: reducing learning rate of group 0 to 9.0660e-03.\n",
      "Epoch 10401, Training Loss: 49373, Validation Loss: 69269, 108285.55218097055\n",
      "Epoch 10424: reducing learning rate of group 0 to 9.0570e-03.\n",
      "Epoch 10501, Training Loss: 50169, Validation Loss: 56689, 115145.63178660565\n",
      "Epoch 10525: reducing learning rate of group 0 to 9.0479e-03.\n",
      "Epoch 10601, Training Loss: 53795, Validation Loss: 56801, 90174.58998462127\n",
      "Epoch 10626: reducing learning rate of group 0 to 9.0389e-03.\n",
      "Epoch 10701, Training Loss: 55071, Validation Loss: 55717, 130748.59060958242\n",
      "Epoch 10727: reducing learning rate of group 0 to 9.0298e-03.\n",
      "Epoch 10801, Training Loss: 49949, Validation Loss: 54209, 85724.81042781235\n",
      "Epoch 10828: reducing learning rate of group 0 to 9.0208e-03.\n",
      "Epoch 10901, Training Loss: 53366, Validation Loss: 57534, 96837.73100046226\n",
      "Epoch 10929: reducing learning rate of group 0 to 9.0118e-03.\n",
      "Epoch 11001, Training Loss: 52882, Validation Loss: 56946, 105312.90478736337\n",
      "Epoch 11030: reducing learning rate of group 0 to 9.0028e-03.\n",
      "Epoch 11101, Training Loss: 52304, Validation Loss: 54261, 91172.75668737058\n",
      "Epoch 11131: reducing learning rate of group 0 to 8.9938e-03.\n",
      "Epoch 11201, Training Loss: 54079, Validation Loss: 65883, 93027.66534689949\n",
      "Epoch 11232: reducing learning rate of group 0 to 8.9848e-03.\n",
      "Epoch 11301, Training Loss: 48666, Validation Loss: 56123, 87368.72027498182\n",
      "Epoch 11333: reducing learning rate of group 0 to 8.9758e-03.\n",
      "Epoch 11401, Training Loss: 53465, Validation Loss: 61694, 67849.26197428619\n",
      "Epoch 11434: reducing learning rate of group 0 to 8.9668e-03.\n",
      "Epoch 11501, Training Loss: 53445, Validation Loss: 58752, 90223.65956153696\n",
      "Epoch 11535: reducing learning rate of group 0 to 8.9578e-03.\n",
      "Epoch 11601, Training Loss: 49828, Validation Loss: 55667, 73168.78670181887\n",
      "Epoch 11636: reducing learning rate of group 0 to 8.9489e-03.\n",
      "Epoch 11701, Training Loss: 50789, Validation Loss: 56815, 93240.34397825318\n",
      "Epoch 11737: reducing learning rate of group 0 to 8.9399e-03.\n",
      "Epoch 11801, Training Loss: 49753, Validation Loss: 54559, 76725.84461326714\n",
      "Epoch 11838: reducing learning rate of group 0 to 8.9310e-03.\n",
      "Epoch 11901, Training Loss: 50841, Validation Loss: 56822, 88061.97224977339\n",
      "Epoch 11939: reducing learning rate of group 0 to 8.9221e-03.\n",
      "Epoch 12001, Training Loss: 50618, Validation Loss: 59605, 56066.3465486216\n",
      "Epoch 12040: reducing learning rate of group 0 to 8.9131e-03.\n",
      "Epoch 12101, Training Loss: 51212, Validation Loss: 67310, 125545.75798865898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12141: reducing learning rate of group 0 to 8.9042e-03.\n",
      "Epoch 12201, Training Loss: 51089, Validation Loss: 57150, 114910.0025381559\n",
      "Epoch 12242: reducing learning rate of group 0 to 8.8953e-03.\n",
      "Epoch 12301, Training Loss: 53388, Validation Loss: 62761, 87158.64843925029\n",
      "Epoch 12343: reducing learning rate of group 0 to 8.8864e-03.\n",
      "Epoch 12401, Training Loss: 50802, Validation Loss: 64639, 76768.99716838874\n",
      "Epoch 12444: reducing learning rate of group 0 to 8.8775e-03.\n",
      "Epoch 12501, Training Loss: 50268, Validation Loss: 57653, 127230.39288752315\n",
      "Epoch 12545: reducing learning rate of group 0 to 8.8687e-03.\n",
      "Epoch 12601, Training Loss: 49900, Validation Loss: 56472, 124449.08308937783\n",
      "Epoch 12646: reducing learning rate of group 0 to 8.8598e-03.\n",
      "Epoch 12701, Training Loss: 48911, Validation Loss: 58599, 83766.61602467044\n",
      "Epoch 12747: reducing learning rate of group 0 to 8.8509e-03.\n",
      "Epoch 12801, Training Loss: 50966, Validation Loss: 66062, 83344.36688111977\n",
      "Epoch 12848: reducing learning rate of group 0 to 8.8421e-03.\n",
      "Epoch 12901, Training Loss: 49479, Validation Loss: 56319, 103181.93882076487\n",
      "Epoch 12949: reducing learning rate of group 0 to 8.8333e-03.\n",
      "Epoch 13001, Training Loss: 50220, Validation Loss: 56175, 135847.79287941792\n",
      "Epoch 13050: reducing learning rate of group 0 to 8.8244e-03.\n",
      "Epoch 13101, Training Loss: 51861, Validation Loss: 53695, 77449.51475834585\n",
      "Epoch 13151: reducing learning rate of group 0 to 8.8156e-03.\n",
      "Epoch 13201, Training Loss: 50042, Validation Loss: 54521, 91254.76388215693\n",
      "Epoch 13260: reducing learning rate of group 0 to 8.8068e-03.\n",
      "Epoch 13301, Training Loss: 47680, Validation Loss: 64692, 93310.5003649027\n",
      "Epoch 13361: reducing learning rate of group 0 to 8.7980e-03.\n",
      "Epoch 13401, Training Loss: 49411, Validation Loss: 58534, 107436.08226102109\n",
      "Epoch 13462: reducing learning rate of group 0 to 8.7892e-03.\n",
      "Epoch 13501, Training Loss: 48485, Validation Loss: 56643, 87258.08011915804\n",
      "Epoch 13563: reducing learning rate of group 0 to 8.7804e-03.\n",
      "Epoch 13601, Training Loss: 48455, Validation Loss: 58827, 68991.55888031422\n",
      "Epoch 13664: reducing learning rate of group 0 to 8.7716e-03.\n",
      "Epoch 13701, Training Loss: 50985, Validation Loss: 59754, 96337.57421859291\n",
      "Epoch 13765: reducing learning rate of group 0 to 8.7628e-03.\n",
      "Epoch 13801, Training Loss: 49731, Validation Loss: 56420, 85600.5110565746\n",
      "Epoch 13866: reducing learning rate of group 0 to 8.7541e-03.\n",
      "Epoch 13901, Training Loss: 52533, Validation Loss: 58217, 63977.39630350425\n",
      "Epoch 13967: reducing learning rate of group 0 to 8.7453e-03.\n",
      "Epoch 14001, Training Loss: 49578, Validation Loss: 52691, 92535.95887105256\n",
      "Epoch 14068: reducing learning rate of group 0 to 8.7366e-03.\n",
      "Epoch 14101, Training Loss: 50992, Validation Loss: 58937, 90093.6718762\n",
      "Epoch 14169: reducing learning rate of group 0 to 8.7278e-03.\n",
      "Epoch 14201, Training Loss: 50037, Validation Loss: 59330, 111225.57155369104\n",
      "Epoch 14270: reducing learning rate of group 0 to 8.7191e-03.\n",
      "Epoch 14301, Training Loss: 51394, Validation Loss: 59373, 85880.77402826991\n",
      "Epoch 14401, Training Loss: 51327, Validation Loss: 62860, 78235.28245476073\n",
      "Epoch 14453: reducing learning rate of group 0 to 8.7104e-03.\n",
      "Epoch 14501, Training Loss: 50769, Validation Loss: 56464, 89285.80972560734\n",
      "Epoch 14554: reducing learning rate of group 0 to 8.7017e-03.\n",
      "Epoch 14601, Training Loss: 49787, Validation Loss: 53480, 95783.03542872363\n",
      "Epoch 14655: reducing learning rate of group 0 to 8.6930e-03.\n",
      "Epoch 14701, Training Loss: 48464, Validation Loss: 60397, 77947.54866688298\n",
      "Epoch 14756: reducing learning rate of group 0 to 8.6843e-03.\n",
      "Epoch 14801, Training Loss: 50172, Validation Loss: 54941, 109967.55902768637\n",
      "Epoch 14901: reducing learning rate of group 0 to 8.6756e-03.\n",
      "Epoch 14901, Training Loss: 48461, Validation Loss: 57960, 99415.90824245918\n",
      "Epoch 15001, Training Loss: 48007, Validation Loss: 62789, 89549.78568287042\n",
      "Epoch 15002: reducing learning rate of group 0 to 8.6669e-03.\n",
      "Epoch 15101, Training Loss: 45516, Validation Loss: 55207, 66213.88545162119\n",
      "Epoch 15103: reducing learning rate of group 0 to 8.6583e-03.\n",
      "Epoch 15201, Training Loss: 49320, Validation Loss: 60368, 60464.44522044505\n",
      "Epoch 15204: reducing learning rate of group 0 to 8.6496e-03.\n",
      "Epoch 15301, Training Loss: 48067, Validation Loss: 53683, 87006.97136630611\n",
      "Epoch 15305: reducing learning rate of group 0 to 8.6409e-03.\n",
      "Epoch 15401, Training Loss: 46670, Validation Loss: 59028, 86829.20707796577\n",
      "Epoch 15406: reducing learning rate of group 0 to 8.6323e-03.\n",
      "Epoch 15501, Training Loss: 48800, Validation Loss: 53544, 70462.24542206952\n",
      "Epoch 15507: reducing learning rate of group 0 to 8.6237e-03.\n",
      "Epoch 15601, Training Loss: 48843, Validation Loss: 54043, 77270.2577166226\n",
      "Epoch 15608: reducing learning rate of group 0 to 8.6150e-03.\n",
      "Epoch 15701, Training Loss: 47091, Validation Loss: 54886, 70537.82433825474\n",
      "Epoch 15709: reducing learning rate of group 0 to 8.6064e-03.\n",
      "Epoch 15801, Training Loss: 46822, Validation Loss: 58524, 101945.35315356951\n",
      "Epoch 15810: reducing learning rate of group 0 to 8.5978e-03.\n",
      "Epoch 15901, Training Loss: 47425, Validation Loss: 58436, 79459.67048959399\n",
      "Epoch 15911: reducing learning rate of group 0 to 8.5892e-03.\n",
      "Epoch 16001, Training Loss: 47727, Validation Loss: 54644, 89843.10893696465\n",
      "Epoch 16012: reducing learning rate of group 0 to 8.5806e-03.\n",
      "Epoch 16101, Training Loss: 50903, Validation Loss: 64392, 81781.94549887833\n",
      "Epoch 16113: reducing learning rate of group 0 to 8.5721e-03.\n",
      "Epoch 16201, Training Loss: 50993, Validation Loss: 55803, 79735.5369966872\n",
      "Epoch 16214: reducing learning rate of group 0 to 8.5635e-03.\n",
      "Epoch 16301, Training Loss: 49155, Validation Loss: 57364, 82959.3067909308\n",
      "Epoch 16315: reducing learning rate of group 0 to 8.5549e-03.\n",
      "Epoch 16401, Training Loss: 48584, Validation Loss: 60212, 67355.26259552436\n",
      "Epoch 16416: reducing learning rate of group 0 to 8.5464e-03.\n",
      "Epoch 16501, Training Loss: 50569, Validation Loss: 55668, 82587.28815146809\n",
      "Epoch 16517: reducing learning rate of group 0 to 8.5378e-03.\n",
      "Epoch 16601, Training Loss: 46851, Validation Loss: 55453, 107476.48756940874\n",
      "Epoch 16618: reducing learning rate of group 0 to 8.5293e-03.\n",
      "Epoch 16701, Training Loss: 50877, Validation Loss: 53827, 71468.89853265013\n",
      "Epoch 16719: reducing learning rate of group 0 to 8.5208e-03.\n",
      "Epoch 16801, Training Loss: 48994, Validation Loss: 56991, 93403.41334588286\n",
      "Epoch 16820: reducing learning rate of group 0 to 8.5122e-03.\n",
      "Epoch 16901, Training Loss: 45841, Validation Loss: 53925, 68558.69879620154\n",
      "Epoch 16921: reducing learning rate of group 0 to 8.5037e-03.\n",
      "Epoch 17001, Training Loss: 49439, Validation Loss: 54223, 62377.64501724943\n",
      "Epoch 17022: reducing learning rate of group 0 to 8.4952e-03.\n",
      "Epoch 17101, Training Loss: 47611, Validation Loss: 55763, 59455.795069040636\n",
      "Epoch 17123: reducing learning rate of group 0 to 8.4867e-03.\n",
      "Epoch 17201, Training Loss: 46159, Validation Loss: 73723, 61411.315621018955\n",
      "Epoch 17224: reducing learning rate of group 0 to 8.4782e-03.\n",
      "Epoch 17301, Training Loss: 47273, Validation Loss: 68437, 87891.50769573054\n",
      "Epoch 17325: reducing learning rate of group 0 to 8.4698e-03.\n",
      "Epoch 17401, Training Loss: 50340, Validation Loss: 54042, 90901.29850774728\n",
      "Epoch 17426: reducing learning rate of group 0 to 8.4613e-03.\n",
      "Epoch 17501, Training Loss: 44609, Validation Loss: 167189, 67369.73869594127\n",
      "Epoch 17527: reducing learning rate of group 0 to 8.4528e-03.\n",
      "Epoch 17601, Training Loss: 43386, Validation Loss: 56333, 69119.07414020576\n",
      "Epoch 17628: reducing learning rate of group 0 to 8.4444e-03.\n",
      "Epoch 17701, Training Loss: 48709, Validation Loss: 50347, 81806.30296610137\n",
      "Epoch 17801, Training Loss: 47886, Validation Loss: 53535, 86533.67302147493\n",
      "Epoch 17802: reducing learning rate of group 0 to 8.4359e-03.\n",
      "Epoch 17901, Training Loss: 48060, Validation Loss: 55223, 148055.38994379292\n",
      "Epoch 17903: reducing learning rate of group 0 to 8.4275e-03.\n",
      "Epoch 18001, Training Loss: 46274, Validation Loss: 58092, 95053.78813145966\n",
      "Epoch 18004: reducing learning rate of group 0 to 8.4191e-03.\n",
      "Epoch 18101, Training Loss: 45568, Validation Loss: 57822, 57999.65089028907\n",
      "Epoch 18105: reducing learning rate of group 0 to 8.4106e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18201, Training Loss: 47172, Validation Loss: 53667, 68146.387195258\n",
      "Epoch 18206: reducing learning rate of group 0 to 8.4022e-03.\n",
      "Epoch 18301, Training Loss: 48713, Validation Loss: 54129, 68497.89515597101\n",
      "Epoch 18307: reducing learning rate of group 0 to 8.3938e-03.\n",
      "Epoch 18401, Training Loss: 45058, Validation Loss: 53269, 83384.23653381236\n",
      "Epoch 18408: reducing learning rate of group 0 to 8.3854e-03.\n",
      "Epoch 18501, Training Loss: 44869, Validation Loss: 55606, 69918.95118578558\n",
      "Epoch 18509: reducing learning rate of group 0 to 8.3771e-03.\n",
      "Epoch 18601, Training Loss: 46786, Validation Loss: 58549, 83189.55451867374\n",
      "Epoch 18610: reducing learning rate of group 0 to 8.3687e-03.\n",
      "Epoch 18701, Training Loss: 47096, Validation Loss: 68881, 87735.34928951973\n",
      "Epoch 18711: reducing learning rate of group 0 to 8.3603e-03.\n",
      "Epoch 18801, Training Loss: 47386, Validation Loss: 55306, 77117.66002872684\n",
      "Epoch 18829: reducing learning rate of group 0 to 8.3519e-03.\n",
      "Epoch 18901, Training Loss: 45772, Validation Loss: 54109, 75474.73639616778\n",
      "Epoch 18930: reducing learning rate of group 0 to 8.3436e-03.\n",
      "Epoch 19001, Training Loss: 45184, Validation Loss: 55363, 75474.15662070492\n",
      "Epoch 19031: reducing learning rate of group 0 to 8.3353e-03.\n",
      "Epoch 19101, Training Loss: 46016, Validation Loss: 51606, 51874.18718191116\n",
      "Epoch 19132: reducing learning rate of group 0 to 8.3269e-03.\n",
      "Epoch 19201, Training Loss: 42153, Validation Loss: 56890, 74039.38155723433\n",
      "Epoch 19233: reducing learning rate of group 0 to 8.3186e-03.\n",
      "Epoch 19301, Training Loss: 46961, Validation Loss: 56063, 93615.5171992788\n",
      "Epoch 19334: reducing learning rate of group 0 to 8.3103e-03.\n",
      "Epoch 19401, Training Loss: 44108, Validation Loss: 54211, 81494.35427990965\n",
      "Epoch 19435: reducing learning rate of group 0 to 8.3020e-03.\n",
      "Epoch 19501, Training Loss: 48365, Validation Loss: 76826, 79031.34439522051\n",
      "Epoch 19601, Training Loss: 49053, Validation Loss: 52746, 61297.65832523286\n",
      "Epoch 19613: reducing learning rate of group 0 to 8.2937e-03.\n",
      "Epoch 19701, Training Loss: 44349, Validation Loss: 55115, 58714.24952905692\n",
      "Epoch 19714: reducing learning rate of group 0 to 8.2854e-03.\n",
      "Epoch 19801, Training Loss: 48235, Validation Loss: 58377, 84792.26554456943\n",
      "Epoch 19815: reducing learning rate of group 0 to 8.2771e-03.\n",
      "Epoch 19901, Training Loss: 47269, Validation Loss: 52691, 87606.18077893545\n",
      "Epoch 19916: reducing learning rate of group 0 to 8.2688e-03.\n",
      "Epoch 20001, Training Loss: 53487, Validation Loss: 66903, 90971.83010428469\n",
      "Epoch 20017: reducing learning rate of group 0 to 8.2605e-03.\n",
      "Epoch 20101, Training Loss: 46663, Validation Loss: 61117, 64112.89903711338\n",
      "Epoch 20118: reducing learning rate of group 0 to 8.2523e-03.\n",
      "Epoch 20201, Training Loss: 45297, Validation Loss: 60102, 78945.95396993245\n",
      "Epoch 20267: reducing learning rate of group 0 to 8.2440e-03.\n",
      "Epoch 20301, Training Loss: 43642, Validation Loss: 53737, 69968.50683776326\n",
      "Epoch 20368: reducing learning rate of group 0 to 8.2358e-03.\n",
      "Epoch 20401, Training Loss: 45745, Validation Loss: 59255, 62122.820246016454\n",
      "Epoch 20472: reducing learning rate of group 0 to 8.2275e-03.\n",
      "Epoch 20501, Training Loss: 48918, Validation Loss: 57277, 59396.765129265434\n",
      "Epoch 20573: reducing learning rate of group 0 to 8.2193e-03.\n",
      "Epoch 20601, Training Loss: 47165, Validation Loss: 56231, 60205.70022802051\n",
      "Epoch 20674: reducing learning rate of group 0 to 8.2111e-03.\n",
      "Epoch 20701, Training Loss: 44419, Validation Loss: 54888, 60917.587404468446\n",
      "Epoch 20775: reducing learning rate of group 0 to 8.2029e-03.\n",
      "Epoch 20801, Training Loss: 46714, Validation Loss: 53950, 85781.22580737981\n",
      "Epoch 20876: reducing learning rate of group 0 to 8.1947e-03.\n",
      "Epoch 20901, Training Loss: 48634, Validation Loss: 55665, 60670.53968245373\n",
      "Epoch 20977: reducing learning rate of group 0 to 8.1865e-03.\n",
      "Epoch 21001, Training Loss: 45452, Validation Loss: 56428, 67434.34168727354\n",
      "Epoch 21078: reducing learning rate of group 0 to 8.1783e-03.\n",
      "Epoch 21101, Training Loss: 50104, Validation Loss: 50531, 62035.60357609593\n",
      "Epoch 21179: reducing learning rate of group 0 to 8.1701e-03.\n",
      "Epoch 21201, Training Loss: 46308, Validation Loss: 61251, 79856.57402049264\n",
      "Epoch 21301, Training Loss: 47404, Validation Loss: 52276, 70275.10373093096\n",
      "Epoch 21366: reducing learning rate of group 0 to 8.1620e-03.\n",
      "Epoch 21401, Training Loss: 44943, Validation Loss: 53209, 51411.45369300476\n",
      "Epoch 21467: reducing learning rate of group 0 to 8.1538e-03.\n",
      "Epoch 21501, Training Loss: 46589, Validation Loss: 54541, 91560.0843318451\n",
      "Epoch 21568: reducing learning rate of group 0 to 8.1456e-03.\n",
      "Epoch 21601, Training Loss: 47806, Validation Loss: 49707, 77748.97824867199\n",
      "Epoch 21669: reducing learning rate of group 0 to 8.1375e-03.\n",
      "Epoch 21701, Training Loss: 44879, Validation Loss: 61551, 65489.1444127276\n",
      "Epoch 21770: reducing learning rate of group 0 to 8.1294e-03.\n",
      "Epoch 21801, Training Loss: 44589, Validation Loss: 53363, 92432.1483456551\n",
      "Epoch 21871: reducing learning rate of group 0 to 8.1212e-03.\n",
      "Epoch 21901, Training Loss: 45125, Validation Loss: 58285, 66511.2511665363\n",
      "Epoch 21972: reducing learning rate of group 0 to 8.1131e-03.\n",
      "Epoch 22001, Training Loss: 44217, Validation Loss: 56502, 109464.28142537776\n",
      "Epoch 22073: reducing learning rate of group 0 to 8.1050e-03.\n",
      "Epoch 22101, Training Loss: 44302, Validation Loss: 52335, 69101.84398844006\n",
      "Epoch 22174: reducing learning rate of group 0 to 8.0969e-03.\n",
      "Epoch 22201, Training Loss: 44348, Validation Loss: 52773, 57390.92155898894\n",
      "Epoch 22275: reducing learning rate of group 0 to 8.0888e-03.\n",
      "Epoch 22301, Training Loss: 43650, Validation Loss: 55168, 76161.60452367285\n",
      "Epoch 22376: reducing learning rate of group 0 to 8.0807e-03.\n",
      "Epoch 22401, Training Loss: 45554, Validation Loss: 57093, 72211.86803591433\n",
      "Epoch 22477: reducing learning rate of group 0 to 8.0726e-03.\n",
      "Epoch 22501, Training Loss: 43938, Validation Loss: 54145, 80196.27274498453\n",
      "Epoch 22578: reducing learning rate of group 0 to 8.0645e-03.\n",
      "Epoch 22601, Training Loss: 46000, Validation Loss: 54879, 80405.73372333877\n",
      "Epoch 22679: reducing learning rate of group 0 to 8.0565e-03.\n",
      "Epoch 22701, Training Loss: 40925, Validation Loss: 57821, 73076.21700281576\n",
      "Epoch 22780: reducing learning rate of group 0 to 8.0484e-03.\n",
      "Epoch 22801, Training Loss: 43149, Validation Loss: 51652, 64799.29032560587\n",
      "Epoch 22881: reducing learning rate of group 0 to 8.0404e-03.\n",
      "Epoch 22901, Training Loss: 43122, Validation Loss: 53609, 59292.608042651846\n",
      "Epoch 22982: reducing learning rate of group 0 to 8.0323e-03.\n",
      "Epoch 23001, Training Loss: 44087, Validation Loss: 54599, 55049.15540756817\n",
      "Epoch 23100: reducing learning rate of group 0 to 8.0243e-03.\n",
      "Epoch 23101, Training Loss: 44385, Validation Loss: 58139, 70888.6014334141\n",
      "Epoch 23201: reducing learning rate of group 0 to 8.0163e-03.\n",
      "Epoch 23201, Training Loss: 43786, Validation Loss: 57699, 64906.68947862568\n",
      "Epoch 23301, Training Loss: 42699, Validation Loss: 49634, 56024.686036350555\n",
      "Epoch 23302: reducing learning rate of group 0 to 8.0083e-03.\n",
      "Epoch 23401, Training Loss: 42735, Validation Loss: 51436, 55687.662166375725\n",
      "Epoch 23403: reducing learning rate of group 0 to 8.0003e-03.\n",
      "Epoch 23501, Training Loss: 44137, Validation Loss: 52614, 93640.21222582391\n",
      "Epoch 23504: reducing learning rate of group 0 to 7.9923e-03.\n",
      "Epoch 23601, Training Loss: 44737, Validation Loss: 52020, 43521.36107697147\n",
      "Epoch 23605: reducing learning rate of group 0 to 7.9843e-03.\n",
      "Epoch 23701, Training Loss: 43772, Validation Loss: 54188, 56598.82364340173\n",
      "Epoch 23706: reducing learning rate of group 0 to 7.9763e-03.\n",
      "Epoch 23801, Training Loss: 44578, Validation Loss: 51267, 78103.6102823008\n",
      "Epoch 23807: reducing learning rate of group 0 to 7.9683e-03.\n",
      "Epoch 23901, Training Loss: 44120, Validation Loss: 52042, 65238.38845366571\n",
      "Epoch 23908: reducing learning rate of group 0 to 7.9603e-03.\n",
      "Epoch 24001, Training Loss: 44174, Validation Loss: 52831, 56011.37488626\n",
      "Epoch 24009: reducing learning rate of group 0 to 7.9524e-03.\n",
      "Epoch 24101, Training Loss: 43517, Validation Loss: 49287, 61000.34599772213\n",
      "Epoch 24110: reducing learning rate of group 0 to 7.9444e-03.\n",
      "Epoch 24201, Training Loss: 44165, Validation Loss: 53945, 69485.98992784992\n",
      "Epoch 24211: reducing learning rate of group 0 to 7.9365e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24301, Training Loss: 40470, Validation Loss: 52125, 90676.88579248777\n",
      "Epoch 24312: reducing learning rate of group 0 to 7.9285e-03.\n",
      "Epoch 24401, Training Loss: 43471, Validation Loss: 51384, 74150.34453216875\n",
      "Epoch 24413: reducing learning rate of group 0 to 7.9206e-03.\n",
      "Epoch 24501, Training Loss: 41790, Validation Loss: 60529, 65486.42073662996\n",
      "Epoch 24514: reducing learning rate of group 0 to 7.9127e-03.\n",
      "Epoch 24601, Training Loss: 43853, Validation Loss: 54604, 61221.72209837028\n",
      "Epoch 24615: reducing learning rate of group 0 to 7.9048e-03.\n",
      "Epoch 24701, Training Loss: 42003, Validation Loss: 58211, 62923.97498912074\n",
      "Epoch 24716: reducing learning rate of group 0 to 7.8969e-03.\n",
      "Epoch 24801, Training Loss: 43500, Validation Loss: 57811, 67068.21805279015\n",
      "Epoch 24817: reducing learning rate of group 0 to 7.8890e-03.\n",
      "Epoch 24901, Training Loss: 42109, Validation Loss: 50950, 63357.22572416529\n",
      "Epoch 24918: reducing learning rate of group 0 to 7.8811e-03.\n",
      "Epoch 25001, Training Loss: 42184, Validation Loss: 56313, 72481.25358530392\n",
      "Epoch 25019: reducing learning rate of group 0 to 7.8732e-03.\n",
      "Epoch 25101, Training Loss: 46711, Validation Loss: 50116, 78818.29293495994\n",
      "Epoch 25120: reducing learning rate of group 0 to 7.8653e-03.\n",
      "Epoch 25201, Training Loss: 45987, Validation Loss: 55454, 61153.12047124621\n",
      "Epoch 25221: reducing learning rate of group 0 to 7.8575e-03.\n",
      "Epoch 25301, Training Loss: 45552, Validation Loss: 62141, 73865.98608411776\n",
      "Epoch 25322: reducing learning rate of group 0 to 7.8496e-03.\n",
      "Epoch 25401, Training Loss: 44307, Validation Loss: 60456, 59678.05751588844\n",
      "Epoch 25423: reducing learning rate of group 0 to 7.8418e-03.\n",
      "Epoch 25501, Training Loss: 43371, Validation Loss: 56410, 72700.90485828981\n",
      "Epoch 25524: reducing learning rate of group 0 to 7.8339e-03.\n",
      "Epoch 25601, Training Loss: 41088, Validation Loss: 52053, 67104.17956949577\n",
      "Epoch 25651: reducing learning rate of group 0 to 7.8261e-03.\n",
      "Epoch 25701, Training Loss: 42079, Validation Loss: 57570, 64414.52783510939\n",
      "Epoch 25752: reducing learning rate of group 0 to 7.8183e-03.\n",
      "Epoch 25801, Training Loss: 42763, Validation Loss: 60823, 71573.22629527714\n",
      "Epoch 25853: reducing learning rate of group 0 to 7.8104e-03.\n",
      "Epoch 25901, Training Loss: 42086, Validation Loss: 49696, 56523.252408657434\n",
      "Epoch 25954: reducing learning rate of group 0 to 7.8026e-03.\n",
      "Epoch 26001, Training Loss: 43188, Validation Loss: 52141, 77281.28825074721\n",
      "Epoch 26065: reducing learning rate of group 0 to 7.7948e-03.\n",
      "Epoch 26101, Training Loss: 43674, Validation Loss: 49113, 56394.445862737855\n",
      "Epoch 26166: reducing learning rate of group 0 to 7.7870e-03.\n",
      "Epoch 26201, Training Loss: 41045, Validation Loss: 52213, 78002.96970126826\n",
      "Epoch 26267: reducing learning rate of group 0 to 7.7792e-03.\n",
      "Epoch 26301, Training Loss: 44774, Validation Loss: 53474, 77061.77639529177\n",
      "Epoch 26368: reducing learning rate of group 0 to 7.7715e-03.\n",
      "Epoch 26401, Training Loss: 44157, Validation Loss: 51274, 64209.35223117876\n",
      "Epoch 26469: reducing learning rate of group 0 to 7.7637e-03.\n",
      "Epoch 26501, Training Loss: 41244, Validation Loss: 52040, 57215.63789569735\n",
      "Epoch 26570: reducing learning rate of group 0 to 7.7559e-03.\n",
      "Epoch 26601, Training Loss: 44736, Validation Loss: 54914, 53246.95640897877\n",
      "Epoch 26671: reducing learning rate of group 0 to 7.7482e-03.\n",
      "Epoch 26701, Training Loss: 46365, Validation Loss: 51921, 58202.50225409691\n",
      "Epoch 26772: reducing learning rate of group 0 to 7.7404e-03.\n",
      "Epoch 26801, Training Loss: 42705, Validation Loss: 50961, 63725.22435412816\n",
      "Epoch 26873: reducing learning rate of group 0 to 7.7327e-03.\n",
      "Epoch 26901, Training Loss: 41034, Validation Loss: 54732, 71175.01815134291\n",
      "Epoch 26974: reducing learning rate of group 0 to 7.7250e-03.\n",
      "Epoch 27001, Training Loss: 45092, Validation Loss: 60243, 66207.35080793516\n",
      "Epoch 27075: reducing learning rate of group 0 to 7.7172e-03.\n",
      "Epoch 27101, Training Loss: 42251, Validation Loss: 57542, 72821.52295770738\n",
      "Epoch 27176: reducing learning rate of group 0 to 7.7095e-03.\n",
      "Epoch 27201, Training Loss: 41891, Validation Loss: 50097, 60798.7765184982\n",
      "Epoch 27277: reducing learning rate of group 0 to 7.7018e-03.\n",
      "Epoch 27301, Training Loss: 41956, Validation Loss: 49900, 66053.62205574197\n",
      "Epoch 27378: reducing learning rate of group 0 to 7.6941e-03.\n",
      "Epoch 27401, Training Loss: 42366, Validation Loss: 50895, 49503.18798769085\n",
      "Epoch 27479: reducing learning rate of group 0 to 7.6864e-03.\n",
      "Epoch 27501, Training Loss: 42982, Validation Loss: 51951, 61919.617393132496\n",
      "Epoch 27580: reducing learning rate of group 0 to 7.6787e-03.\n",
      "Epoch 27601, Training Loss: 39715, Validation Loss: 51172, 70822.27643163742\n",
      "Epoch 27681: reducing learning rate of group 0 to 7.6710e-03.\n",
      "Epoch 27701, Training Loss: 43933, Validation Loss: 48929, 93414.18159158586\n",
      "Epoch 27782: reducing learning rate of group 0 to 7.6634e-03.\n",
      "Epoch 27801, Training Loss: 39497, Validation Loss: 50653, 74213.18403247076\n",
      "Epoch 27883: reducing learning rate of group 0 to 7.6557e-03.\n",
      "Epoch 27901, Training Loss: 41945, Validation Loss: 53689, 68356.6389200501\n",
      "Epoch 27984: reducing learning rate of group 0 to 7.6481e-03.\n",
      "Epoch 28001, Training Loss: 41545, Validation Loss: 53454, 72633.8489511579\n",
      "Epoch 28085: reducing learning rate of group 0 to 7.6404e-03.\n",
      "Epoch 28101, Training Loss: 42924, Validation Loss: 51954, 82496.44759542226\n",
      "Epoch 28186: reducing learning rate of group 0 to 7.6328e-03.\n",
      "Epoch 28201, Training Loss: 39319, Validation Loss: 53920, 77952.47709384027\n",
      "Epoch 28287: reducing learning rate of group 0 to 7.6251e-03.\n",
      "Epoch 28301, Training Loss: 38895, Validation Loss: 51732, 53705.41619846265\n",
      "Epoch 28388: reducing learning rate of group 0 to 7.6175e-03.\n",
      "Epoch 28401, Training Loss: 42279, Validation Loss: 52602, 66563.56233760399\n",
      "Epoch 28489: reducing learning rate of group 0 to 7.6099e-03.\n",
      "Epoch 28501, Training Loss: 45886, Validation Loss: 60059, 54914.83764059883\n",
      "Epoch 28590: reducing learning rate of group 0 to 7.6023e-03.\n",
      "Epoch 28601, Training Loss: 43680, Validation Loss: 51716, 55647.35897566413\n",
      "Epoch 28691: reducing learning rate of group 0 to 7.5947e-03.\n",
      "Epoch 28701, Training Loss: 42087, Validation Loss: 48608, 61783.98700212565\n",
      "Epoch 28792: reducing learning rate of group 0 to 7.5871e-03.\n",
      "Epoch 28801, Training Loss: 42420, Validation Loss: 62189, 57319.0546248594\n",
      "Epoch 28893: reducing learning rate of group 0 to 7.5795e-03.\n",
      "Epoch 28901, Training Loss: 46682, Validation Loss: 54279, 86799.03470265806\n",
      "Epoch 28994: reducing learning rate of group 0 to 7.5719e-03.\n",
      "Epoch 29001, Training Loss: 39025, Validation Loss: 49909, 60108.01239387406\n",
      "Epoch 29095: reducing learning rate of group 0 to 7.5643e-03.\n",
      "Epoch 29101, Training Loss: 41049, Validation Loss: 51416, 55218.18141240879\n",
      "Epoch 29196: reducing learning rate of group 0 to 7.5568e-03.\n",
      "Epoch 29201, Training Loss: 43583, Validation Loss: 50621, 77817.79467644577\n",
      "Epoch 29297: reducing learning rate of group 0 to 7.5492e-03.\n",
      "Epoch 29301, Training Loss: 41938, Validation Loss: 50524, 71070.74797873669\n",
      "Epoch 29398: reducing learning rate of group 0 to 7.5417e-03.\n",
      "Epoch 29401, Training Loss: 41777, Validation Loss: 56057, 86922.91787425314\n",
      "Epoch 29499: reducing learning rate of group 0 to 7.5341e-03.\n",
      "Epoch 29501, Training Loss: 41178, Validation Loss: 57337, 71172.89437975826\n",
      "Epoch 29600: reducing learning rate of group 0 to 7.5266e-03.\n",
      "Epoch 29601, Training Loss: 42010, Validation Loss: 61951, 61133.280921553116\n",
      "Epoch 29701: reducing learning rate of group 0 to 7.5191e-03.\n",
      "Epoch 29701, Training Loss: 43100, Validation Loss: 48951, 68007.61909079461\n",
      "Epoch 29801, Training Loss: 40117, Validation Loss: 54798, 65881.14219453865\n",
      "Epoch 29802: reducing learning rate of group 0 to 7.5116e-03.\n",
      "Epoch 29901, Training Loss: 40727, Validation Loss: 51003, 59318.20821792475\n",
      "Epoch 29903: reducing learning rate of group 0 to 7.5040e-03.\n",
      "Epoch 30001, Training Loss: 38815, Validation Loss: 56951, 50828.157279377796\n",
      "Epoch 30004: reducing learning rate of group 0 to 7.4965e-03.\n",
      "Epoch 30101, Training Loss: 38566, Validation Loss: 55828, 79705.51937722928\n",
      "Epoch 30105: reducing learning rate of group 0 to 7.4890e-03.\n",
      "Epoch 30201, Training Loss: 42403, Validation Loss: 48800, 77232.8667340677\n",
      "Epoch 30206: reducing learning rate of group 0 to 7.4816e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30301, Training Loss: 44880, Validation Loss: 48343, 59074.52452782627\n",
      "Epoch 30307: reducing learning rate of group 0 to 7.4741e-03.\n",
      "Epoch 30401, Training Loss: 43003, Validation Loss: 59550, 56801.00054208445\n",
      "Epoch 30408: reducing learning rate of group 0 to 7.4666e-03.\n",
      "Epoch 30501, Training Loss: 40165, Validation Loss: 54040, 61296.776127930956\n",
      "Epoch 30509: reducing learning rate of group 0 to 7.4591e-03.\n",
      "Epoch 30601, Training Loss: 39773, Validation Loss: 52137, 76774.6716438546\n",
      "Epoch 30610: reducing learning rate of group 0 to 7.4517e-03.\n",
      "Epoch 30701, Training Loss: 38696, Validation Loss: 59485, 62127.242468233024\n",
      "Epoch 30711: reducing learning rate of group 0 to 7.4442e-03.\n",
      "Epoch 30801, Training Loss: 41291, Validation Loss: 50983, 48870.00164494194\n",
      "Epoch 30812: reducing learning rate of group 0 to 7.4368e-03.\n",
      "Epoch 30901, Training Loss: 40796, Validation Loss: 52091, 72006.31751231088\n",
      "Epoch 30913: reducing learning rate of group 0 to 7.4293e-03.\n",
      "Epoch 31001, Training Loss: 40979, Validation Loss: 51725, 63066.92156567211\n",
      "Epoch 31014: reducing learning rate of group 0 to 7.4219e-03.\n",
      "Epoch 31101, Training Loss: 40871, Validation Loss: 56964, 76764.42871923586\n",
      "Epoch 31115: reducing learning rate of group 0 to 7.4145e-03.\n",
      "Epoch 31201, Training Loss: 39802, Validation Loss: 58948, 78229.42247976958\n",
      "Epoch 31216: reducing learning rate of group 0 to 7.4071e-03.\n",
      "Epoch 31301, Training Loss: 38908, Validation Loss: 53903, 65236.62040401396\n",
      "Epoch 31317: reducing learning rate of group 0 to 7.3997e-03.\n",
      "Epoch 31401, Training Loss: 40033, Validation Loss: 52179, 66897.13645257837\n",
      "Epoch 31418: reducing learning rate of group 0 to 7.3923e-03.\n",
      "Epoch 31501, Training Loss: 41519, Validation Loss: 48666, 72795.55807439776\n",
      "Epoch 31519: reducing learning rate of group 0 to 7.3849e-03.\n",
      "Epoch 31601, Training Loss: 39961, Validation Loss: 53985, 68064.18955179343\n",
      "Epoch 31620: reducing learning rate of group 0 to 7.3775e-03.\n",
      "Epoch 31701, Training Loss: 40735, Validation Loss: 50242, 60215.97637890559\n",
      "Epoch 31721: reducing learning rate of group 0 to 7.3701e-03.\n",
      "Epoch 31801, Training Loss: 40305, Validation Loss: 62561, 59545.90447799285\n",
      "Epoch 31822: reducing learning rate of group 0 to 7.3627e-03.\n",
      "Epoch 31901, Training Loss: 41840, Validation Loss: 51739, 56623.00037717907\n",
      "Epoch 31923: reducing learning rate of group 0 to 7.3554e-03.\n",
      "Epoch 32001, Training Loss: 40123, Validation Loss: 53702, 73025.42267670261\n",
      "Epoch 32026: reducing learning rate of group 0 to 7.3480e-03.\n",
      "Epoch 32101, Training Loss: 44044, Validation Loss: 51445, 57606.95430172681\n",
      "Epoch 32127: reducing learning rate of group 0 to 7.3407e-03.\n",
      "Epoch 32201, Training Loss: 40972, Validation Loss: 57573, 61043.8090450817\n",
      "Epoch 32228: reducing learning rate of group 0 to 7.3333e-03.\n",
      "Epoch 32301, Training Loss: 42410, Validation Loss: 53059, 64213.51811139844\n",
      "Epoch 32329: reducing learning rate of group 0 to 7.3260e-03.\n",
      "Epoch 32401, Training Loss: 40733, Validation Loss: 52858, 61214.19858579539\n",
      "Epoch 32430: reducing learning rate of group 0 to 7.3187e-03.\n",
      "Epoch 32501, Training Loss: 39776, Validation Loss: 54460, 64570.20332188237\n",
      "Epoch 32567: reducing learning rate of group 0 to 7.3114e-03.\n",
      "Epoch 32601, Training Loss: 40357, Validation Loss: 51935, 77773.42004623705\n",
      "Epoch 32668: reducing learning rate of group 0 to 7.3040e-03.\n",
      "Epoch 32701, Training Loss: 38928, Validation Loss: 52662, 72333.83706135134\n",
      "Epoch 32769: reducing learning rate of group 0 to 7.2967e-03.\n",
      "Epoch 32801, Training Loss: 40254, Validation Loss: 49527, 86620.59509499966\n",
      "Epoch 32870: reducing learning rate of group 0 to 7.2894e-03.\n",
      "Epoch 32901, Training Loss: 39498, Validation Loss: 56359, 56005.381213252236\n",
      "Epoch 32971: reducing learning rate of group 0 to 7.2822e-03.\n",
      "Epoch 33001, Training Loss: 38822, Validation Loss: 56806, 68550.0624890295\n",
      "Epoch 33072: reducing learning rate of group 0 to 7.2749e-03.\n",
      "Epoch 33101, Training Loss: 40871, Validation Loss: 53345, 55869.2818000986\n",
      "Epoch 33173: reducing learning rate of group 0 to 7.2676e-03.\n",
      "Epoch 33201, Training Loss: 38421, Validation Loss: 51782, 83221.34438831163\n",
      "Epoch 33274: reducing learning rate of group 0 to 7.2603e-03.\n",
      "Epoch 33301, Training Loss: 41124, Validation Loss: 50361, 63721.471827299254\n",
      "Epoch 33375: reducing learning rate of group 0 to 7.2531e-03.\n",
      "Epoch 33401, Training Loss: 40745, Validation Loss: 50199, 71379.22913187934\n",
      "Epoch 33476: reducing learning rate of group 0 to 7.2458e-03.\n",
      "Epoch 33501, Training Loss: 41446, Validation Loss: 53785, 52006.46536719337\n",
      "Epoch 33577: reducing learning rate of group 0 to 7.2386e-03.\n",
      "Epoch 33601, Training Loss: 39744, Validation Loss: 54960, 68801.04688520556\n",
      "Epoch 33678: reducing learning rate of group 0 to 7.2313e-03.\n",
      "Epoch 33701, Training Loss: 41638, Validation Loss: 56358, 61377.51195351694\n",
      "Epoch 33779: reducing learning rate of group 0 to 7.2241e-03.\n",
      "Epoch 33801, Training Loss: 41649, Validation Loss: 52117, 92041.66558356135\n",
      "Epoch 33880: reducing learning rate of group 0 to 7.2169e-03.\n",
      "Epoch 33901, Training Loss: 41895, Validation Loss: 56881, 61530.85145094836\n",
      "Epoch 33981: reducing learning rate of group 0 to 7.2097e-03.\n",
      "Epoch 34001, Training Loss: 37918, Validation Loss: 52353, 52212.13155811508\n",
      "Epoch 34082: reducing learning rate of group 0 to 7.2024e-03.\n",
      "Epoch 34101, Training Loss: 39371, Validation Loss: 56463, 66240.35254739656\n",
      "Epoch 34183: reducing learning rate of group 0 to 7.1952e-03.\n",
      "Epoch 34201, Training Loss: 41554, Validation Loss: 59325, 57483.58380799366\n",
      "Epoch 34284: reducing learning rate of group 0 to 7.1881e-03.\n",
      "Epoch 34301, Training Loss: 42171, Validation Loss: 51855, 77890.03835049087\n",
      "Epoch 34385: reducing learning rate of group 0 to 7.1809e-03.\n",
      "Epoch 34401, Training Loss: 39495, Validation Loss: 58715, 54008.986700996385\n",
      "Epoch 34486: reducing learning rate of group 0 to 7.1737e-03.\n",
      "Epoch 34501, Training Loss: 38144, Validation Loss: 55965, 80182.57633790998\n",
      "Epoch 34587: reducing learning rate of group 0 to 7.1665e-03.\n",
      "Epoch 34601, Training Loss: 39327, Validation Loss: 50709, 48867.64107147441\n",
      "Epoch 34688: reducing learning rate of group 0 to 7.1593e-03.\n",
      "Epoch 34701, Training Loss: 40038, Validation Loss: 62264, 51622.73987369464\n",
      "Epoch 34789: reducing learning rate of group 0 to 7.1522e-03.\n",
      "Epoch 34801, Training Loss: 41487, Validation Loss: 54665, 68665.9195846357\n",
      "Epoch 34890: reducing learning rate of group 0 to 7.1450e-03.\n",
      "Epoch 34901, Training Loss: 39666, Validation Loss: 50936, 63210.55682784572\n",
      "Epoch 34991: reducing learning rate of group 0 to 7.1379e-03.\n",
      "Epoch 35001, Training Loss: 43353, Validation Loss: 56219, 76435.78007389889\n",
      "Epoch 35092: reducing learning rate of group 0 to 7.1307e-03.\n",
      "Epoch 35101, Training Loss: 39464, Validation Loss: 59334, 72804.04279086656\n",
      "Epoch 35193: reducing learning rate of group 0 to 7.1236e-03.\n",
      "Epoch 35201, Training Loss: 40007, Validation Loss: 61133, 64325.13550573959\n",
      "Epoch 35294: reducing learning rate of group 0 to 7.1165e-03.\n",
      "Epoch 35301, Training Loss: 39073, Validation Loss: 50511, 63220.778876430064\n",
      "Epoch 35395: reducing learning rate of group 0 to 7.1094e-03.\n",
      "Epoch 35401, Training Loss: 43185, Validation Loss: 53419, 76942.77912480762\n",
      "Epoch 35496: reducing learning rate of group 0 to 7.1023e-03.\n",
      "Epoch 35501, Training Loss: 38973, Validation Loss: 52867, 70149.8791297297\n",
      "Epoch 35597: reducing learning rate of group 0 to 7.0952e-03.\n",
      "Epoch 35601, Training Loss: 44595, Validation Loss: 59149, 67691.2813051821\n",
      "Epoch 35698: reducing learning rate of group 0 to 7.0881e-03.\n",
      "Epoch 35701, Training Loss: 41678, Validation Loss: 49385, 71575.43245157968\n",
      "Epoch 35799: reducing learning rate of group 0 to 7.0810e-03.\n",
      "Epoch 35801, Training Loss: 38730, Validation Loss: 50573, 61224.20732400866\n",
      "Epoch 35900: reducing learning rate of group 0 to 7.0739e-03.\n",
      "Epoch 35901, Training Loss: 40795, Validation Loss: 57379, 52296.78431394908\n",
      "Epoch 36001: reducing learning rate of group 0 to 7.0668e-03.\n",
      "Epoch 36001, Training Loss: 41892, Validation Loss: 57925, 70935.90139217931\n",
      "Epoch 36101, Training Loss: 41543, Validation Loss: 50150, 59617.74088862936\n",
      "Epoch 36102: reducing learning rate of group 0 to 7.0598e-03.\n",
      "Epoch 36201, Training Loss: 39856, Validation Loss: 54141, 57344.145377666304\n",
      "Epoch 36203: reducing learning rate of group 0 to 7.0527e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36301, Training Loss: 38993, Validation Loss: 52286, 84278.2498711257\n",
      "Epoch 36304: reducing learning rate of group 0 to 7.0456e-03.\n",
      "Epoch 36401, Training Loss: 38998, Validation Loss: 50856, 50855.67149341248\n",
      "Epoch 36405: reducing learning rate of group 0 to 7.0386e-03.\n",
      "Epoch 36501, Training Loss: 41682, Validation Loss: 56715, 57326.6342232477\n",
      "Epoch 36506: reducing learning rate of group 0 to 7.0316e-03.\n",
      "Epoch 36601, Training Loss: 39377, Validation Loss: 50624, 71763.1944738031\n",
      "Epoch 36607: reducing learning rate of group 0 to 7.0245e-03.\n",
      "Epoch 36701, Training Loss: 39259, Validation Loss: 52190, 58882.30051599076\n",
      "Epoch 36708: reducing learning rate of group 0 to 7.0175e-03.\n",
      "Epoch 36801, Training Loss: 38783, Validation Loss: 50130, 62422.12746146304\n",
      "Epoch 36809: reducing learning rate of group 0 to 7.0105e-03.\n",
      "Epoch 36901, Training Loss: 40031, Validation Loss: 54864, 79754.90182259923\n",
      "Epoch 36910: reducing learning rate of group 0 to 7.0035e-03.\n",
      "Epoch 37001, Training Loss: 39419, Validation Loss: 49559, 73900.06109100535\n",
      "Epoch 37011: reducing learning rate of group 0 to 6.9965e-03.\n",
      "Epoch 37101, Training Loss: 42447, Validation Loss: 52675, 61972.2560232889\n",
      "Epoch 37112: reducing learning rate of group 0 to 6.9895e-03.\n",
      "Epoch 37201, Training Loss: 39700, Validation Loss: 58710, 75111.7625216574\n",
      "Epoch 37213: reducing learning rate of group 0 to 6.9825e-03.\n",
      "Epoch 37301, Training Loss: 40545, Validation Loss: 54788, 59471.579491988465\n",
      "Epoch 37314: reducing learning rate of group 0 to 6.9755e-03.\n",
      "Epoch 37401, Training Loss: 40539, Validation Loss: 50702, 72441.35708155124\n",
      "Epoch 37415: reducing learning rate of group 0 to 6.9685e-03.\n",
      "Epoch 37501, Training Loss: 41121, Validation Loss: 53506, 61461.5794215804\n",
      "Epoch 37516: reducing learning rate of group 0 to 6.9616e-03.\n",
      "Epoch 37601, Training Loss: 41302, Validation Loss: 56517, 99320.09107358776\n",
      "Epoch 37617: reducing learning rate of group 0 to 6.9546e-03.\n",
      "Epoch 37701, Training Loss: 38605, Validation Loss: 53773, 62437.751024021796\n",
      "Epoch 37718: reducing learning rate of group 0 to 6.9476e-03.\n",
      "Epoch 37801, Training Loss: 39065, Validation Loss: 53984, 64362.747379252665\n",
      "Epoch 37819: reducing learning rate of group 0 to 6.9407e-03.\n",
      "Epoch 37901, Training Loss: 39629, Validation Loss: 53483, 58230.81170836332\n",
      "Epoch 37920: reducing learning rate of group 0 to 6.9338e-03.\n",
      "Epoch 38001, Training Loss: 39631, Validation Loss: 65201, 61554.388715780595\n",
      "Epoch 38021: reducing learning rate of group 0 to 6.9268e-03.\n",
      "Epoch 38101, Training Loss: 38378, Validation Loss: 50738, 63897.303379626486\n",
      "Epoch 38122: reducing learning rate of group 0 to 6.9199e-03.\n",
      "Epoch 38201, Training Loss: 38503, Validation Loss: 58412, 59974.16276122853\n",
      "Epoch 38223: reducing learning rate of group 0 to 6.9130e-03.\n",
      "Epoch 38301, Training Loss: 37221, Validation Loss: 50921, 80085.31587684502\n",
      "Epoch 38324: reducing learning rate of group 0 to 6.9061e-03.\n",
      "Epoch 38401, Training Loss: 37143, Validation Loss: 57724, 80749.2318112667\n",
      "Epoch 38425: reducing learning rate of group 0 to 6.8992e-03.\n",
      "Epoch 38501, Training Loss: 37798, Validation Loss: 52067, 60056.732512519906\n",
      "Epoch 38526: reducing learning rate of group 0 to 6.8923e-03.\n",
      "Epoch 38601, Training Loss: 38265, Validation Loss: 53210, 58119.214357391\n",
      "Epoch 38627: reducing learning rate of group 0 to 6.8854e-03.\n",
      "Epoch 38701, Training Loss: 38909, Validation Loss: 54561, 80907.00517564638\n",
      "Epoch 38728: reducing learning rate of group 0 to 6.8785e-03.\n",
      "Epoch 38801, Training Loss: 41460, Validation Loss: 58477, 71398.7723090793\n",
      "Epoch 38829: reducing learning rate of group 0 to 6.8716e-03.\n",
      "Epoch 38901, Training Loss: 38146, Validation Loss: 56847, 70573.22373448477\n",
      "Epoch 38930: reducing learning rate of group 0 to 6.8647e-03.\n",
      "Epoch 39001, Training Loss: 39449, Validation Loss: 56267, 87791.10544888234\n",
      "Epoch 39031: reducing learning rate of group 0 to 6.8579e-03.\n",
      "Epoch 39101, Training Loss: 38301, Validation Loss: 60788, 84224.81641596709\n",
      "Epoch 39132: reducing learning rate of group 0 to 6.8510e-03.\n",
      "Epoch 39201, Training Loss: 42750, Validation Loss: 54418, 70543.39073143451\n",
      "Epoch 39233: reducing learning rate of group 0 to 6.8442e-03.\n",
      "Epoch 39301, Training Loss: 36254, Validation Loss: 54854, 58188.61878717869\n",
      "Epoch 39334: reducing learning rate of group 0 to 6.8373e-03.\n",
      "Epoch 39401, Training Loss: 41614, Validation Loss: 56332, 95243.63432689337\n",
      "Epoch 39435: reducing learning rate of group 0 to 6.8305e-03.\n",
      "Epoch 39501, Training Loss: 37425, Validation Loss: 53265, 56261.31406899862\n",
      "Epoch 39536: reducing learning rate of group 0 to 6.8236e-03.\n",
      "Epoch 39601, Training Loss: 40467, Validation Loss: 54367, 70399.98485422913\n",
      "Epoch 39637: reducing learning rate of group 0 to 6.8168e-03.\n",
      "Epoch 39701, Training Loss: 41531, Validation Loss: 55468, 60135.84695461181\n",
      "Epoch 39738: reducing learning rate of group 0 to 6.8100e-03.\n",
      "Epoch 39801, Training Loss: 42945, Validation Loss: 53406, 64990.673502099155\n",
      "Epoch 39839: reducing learning rate of group 0 to 6.8032e-03.\n",
      "Epoch 39901, Training Loss: 38658, Validation Loss: 54212, 67358.77799084607\n",
      "Epoch 39940: reducing learning rate of group 0 to 6.7964e-03.\n",
      "Epoch 40001, Training Loss: 36691, Validation Loss: 51748, 72360.11252629518\n",
      "Epoch 40041: reducing learning rate of group 0 to 6.7896e-03.\n",
      "Epoch 40101, Training Loss: 39683, Validation Loss: 52539, 70092.52813643117\n",
      "Epoch 40142: reducing learning rate of group 0 to 6.7828e-03.\n",
      "Epoch 40201, Training Loss: 36361, Validation Loss: 51532, 67703.28066539917\n",
      "Epoch 40243: reducing learning rate of group 0 to 6.7760e-03.\n",
      "Epoch 40301, Training Loss: 40215, Validation Loss: 51332, 56356.86409376061\n",
      "Epoch 40344: reducing learning rate of group 0 to 6.7692e-03.\n",
      "Epoch 40401, Training Loss: 40112, Validation Loss: 52586, 59247.659010429576\n",
      "Epoch 40445: reducing learning rate of group 0 to 6.7625e-03.\n",
      "Epoch 40501, Training Loss: 38135, Validation Loss: 53895, 79257.95119242829\n",
      "Epoch 40546: reducing learning rate of group 0 to 6.7557e-03.\n",
      "Epoch 40601, Training Loss: 37050, Validation Loss: 51410, 75973.29673722823\n",
      "Epoch 40647: reducing learning rate of group 0 to 6.7490e-03.\n",
      "Epoch 40701, Training Loss: 39902, Validation Loss: 51194, 71729.27058347572\n",
      "Epoch 40748: reducing learning rate of group 0 to 6.7422e-03.\n",
      "Epoch 40801, Training Loss: 37432, Validation Loss: 55907, 1039455.5645892139\n",
      "Epoch 40849: reducing learning rate of group 0 to 6.7355e-03.\n",
      "Epoch 40901, Training Loss: 40162, Validation Loss: 51491, 67448.80988708523\n",
      "Epoch 40950: reducing learning rate of group 0 to 6.7287e-03.\n",
      "Epoch 41001, Training Loss: 40961, Validation Loss: 54960, 70335.23551446454\n",
      "Epoch 41051: reducing learning rate of group 0 to 6.7220e-03.\n",
      "Epoch 41101, Training Loss: 38422, Validation Loss: 51993, 65447.167334697406\n",
      "Epoch 41152: reducing learning rate of group 0 to 6.7153e-03.\n",
      "Epoch 41201, Training Loss: 38585, Validation Loss: 51391, 61299.92337551323\n",
      "Epoch 41253: reducing learning rate of group 0 to 6.7086e-03.\n",
      "Epoch 41301, Training Loss: 37525, Validation Loss: 52962, 71122.7443016822\n",
      "Epoch 41354: reducing learning rate of group 0 to 6.7019e-03.\n",
      "Epoch 41401, Training Loss: 37585, Validation Loss: 52288, 67346.41046866744\n",
      "Epoch 41455: reducing learning rate of group 0 to 6.6952e-03.\n",
      "Epoch 41501, Training Loss: 37972, Validation Loss: 54693, 72449.60458762808\n",
      "Epoch 41556: reducing learning rate of group 0 to 6.6885e-03.\n",
      "Epoch 41601, Training Loss: 34582, Validation Loss: 52547, 56094.848354894755\n",
      "Epoch 41657: reducing learning rate of group 0 to 6.6818e-03.\n",
      "Epoch 41701, Training Loss: 38707, Validation Loss: 55964, 72411.12689097051\n",
      "Epoch 41758: reducing learning rate of group 0 to 6.6751e-03.\n",
      "Epoch 41801, Training Loss: 39231, Validation Loss: 53720, 58114.1149519367\n",
      "Epoch 41859: reducing learning rate of group 0 to 6.6684e-03.\n",
      "Epoch 41901, Training Loss: 38305, Validation Loss: 55938, 61876.7945257404\n",
      "Epoch 41960: reducing learning rate of group 0 to 6.6617e-03.\n",
      "Epoch 42001, Training Loss: 37481, Validation Loss: 50283, 64836.127773859545\n",
      "Epoch 42061: reducing learning rate of group 0 to 6.6551e-03.\n",
      "Epoch 42101, Training Loss: 37940, Validation Loss: 48939, 60328.20711816027\n",
      "Epoch 42162: reducing learning rate of group 0 to 6.6484e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42201, Training Loss: 39981, Validation Loss: 51214, 92722.79223123241\n",
      "Epoch 42263: reducing learning rate of group 0 to 6.6418e-03.\n",
      "Epoch 42301, Training Loss: 39249, Validation Loss: 54859, 79574.71571581582\n",
      "Epoch 42364: reducing learning rate of group 0 to 6.6351e-03.\n",
      "Epoch 42401, Training Loss: 37615, Validation Loss: 50955, 76428.53197524308\n",
      "Epoch 42465: reducing learning rate of group 0 to 6.6285e-03.\n",
      "Epoch 42501, Training Loss: 40070, Validation Loss: 52486, 73400.67457873016\n",
      "Epoch 42566: reducing learning rate of group 0 to 6.6219e-03.\n",
      "Epoch 42601, Training Loss: 39381, Validation Loss: 51118, 85289.3084712585\n",
      "Epoch 42667: reducing learning rate of group 0 to 6.6153e-03.\n",
      "Epoch 42701, Training Loss: 38220, Validation Loss: 58607, 63008.502536454005\n",
      "Epoch 42768: reducing learning rate of group 0 to 6.6086e-03.\n",
      "Epoch 42801, Training Loss: 35026, Validation Loss: 56507, 57250.15358084472\n",
      "Epoch 42869: reducing learning rate of group 0 to 6.6020e-03.\n",
      "Epoch 42901, Training Loss: 35805, Validation Loss: 63918, 63321.35814476229\n",
      "Epoch 42970: reducing learning rate of group 0 to 6.5954e-03.\n",
      "Epoch 43001, Training Loss: 37637, Validation Loss: 53994, 60835.53289297963\n",
      "Epoch 43071: reducing learning rate of group 0 to 6.5888e-03.\n",
      "Epoch 43101, Training Loss: 39615, Validation Loss: 53314, 57439.54896767452\n",
      "Epoch 43172: reducing learning rate of group 0 to 6.5822e-03.\n",
      "Epoch 43201, Training Loss: 39501, Validation Loss: 56277, 65550.88703395078\n",
      "Epoch 43273: reducing learning rate of group 0 to 6.5757e-03.\n",
      "Epoch 43301, Training Loss: 39990, Validation Loss: 55791, 78211.22402484332\n",
      "Epoch 43374: reducing learning rate of group 0 to 6.5691e-03.\n",
      "Epoch 43401, Training Loss: 37930, Validation Loss: 54684, 65265.62243379259\n",
      "Epoch 43475: reducing learning rate of group 0 to 6.5625e-03.\n",
      "Epoch 43501, Training Loss: 39400, Validation Loss: 50715, 72560.74548003206\n",
      "Epoch 43576: reducing learning rate of group 0 to 6.5560e-03.\n",
      "Epoch 43601, Training Loss: 38251, Validation Loss: 49514, 66770.05410754234\n",
      "Epoch 43677: reducing learning rate of group 0 to 6.5494e-03.\n",
      "Epoch 43701, Training Loss: 36655, Validation Loss: 56630, 76925.31921187094\n",
      "Epoch 43778: reducing learning rate of group 0 to 6.5429e-03.\n",
      "Epoch 43801, Training Loss: 37383, Validation Loss: 52150, 75621.71314840364\n",
      "Epoch 43879: reducing learning rate of group 0 to 6.5363e-03.\n",
      "Epoch 43901, Training Loss: 37408, Validation Loss: 50323, 74163.48795391813\n",
      "Epoch 43980: reducing learning rate of group 0 to 6.5298e-03.\n",
      "Epoch 44001, Training Loss: 38004, Validation Loss: 58691, 59900.814757596214\n",
      "Epoch 44081: reducing learning rate of group 0 to 6.5232e-03.\n",
      "Epoch 44101, Training Loss: 35198, Validation Loss: 50949, 76703.83434854192\n",
      "Epoch 44182: reducing learning rate of group 0 to 6.5167e-03.\n",
      "Epoch 44201, Training Loss: 38741, Validation Loss: 48219, 74168.64005143\n",
      "Epoch 44283: reducing learning rate of group 0 to 6.5102e-03.\n",
      "Epoch 44301, Training Loss: 39806, Validation Loss: 52474, 61087.26283058672\n",
      "Epoch 44384: reducing learning rate of group 0 to 6.5037e-03.\n",
      "Epoch 44401, Training Loss: 36826, Validation Loss: 52609, 73140.02698211432\n",
      "Epoch 44485: reducing learning rate of group 0 to 6.4972e-03.\n",
      "Epoch 44501, Training Loss: 36560, Validation Loss: 50402, 76187.72603281822\n",
      "Epoch 44586: reducing learning rate of group 0 to 6.4907e-03.\n",
      "Epoch 44601, Training Loss: 36857, Validation Loss: 59374, 80239.89629589058\n",
      "Epoch 44687: reducing learning rate of group 0 to 6.4842e-03.\n",
      "Epoch 44701, Training Loss: 39030, Validation Loss: 53777, 81912.94697917778\n",
      "Epoch 44788: reducing learning rate of group 0 to 6.4777e-03.\n",
      "Epoch 44801, Training Loss: 38710, Validation Loss: 53343, 69398.63683669812\n",
      "Epoch 44889: reducing learning rate of group 0 to 6.4712e-03.\n",
      "Epoch 44901, Training Loss: 39292, Validation Loss: 54177, 65692.02345546237\n",
      "Epoch 44990: reducing learning rate of group 0 to 6.4648e-03.\n",
      "Epoch 45001, Training Loss: 35886, Validation Loss: 55538, 58419.75765932287\n",
      "Epoch 45091: reducing learning rate of group 0 to 6.4583e-03.\n",
      "Epoch 45101, Training Loss: 38374, Validation Loss: 56038, 62870.96008240414\n",
      "Epoch 45192: reducing learning rate of group 0 to 6.4518e-03.\n",
      "Epoch 45201, Training Loss: 39394, Validation Loss: 57116, 57280.82315734698\n",
      "Epoch 45293: reducing learning rate of group 0 to 6.4454e-03.\n",
      "Epoch 45301, Training Loss: 36779, Validation Loss: 58330, 67903.2832953763\n",
      "Epoch 45394: reducing learning rate of group 0 to 6.4389e-03.\n",
      "Epoch 45401, Training Loss: 36694, Validation Loss: 51537, 80362.77896423596\n",
      "Epoch 45495: reducing learning rate of group 0 to 6.4325e-03.\n",
      "Epoch 45501, Training Loss: 37807, Validation Loss: 54297, 63598.175143818014\n",
      "Epoch 45596: reducing learning rate of group 0 to 6.4261e-03.\n",
      "Epoch 45601, Training Loss: 37313, Validation Loss: 51439, 65216.086709579395\n",
      "Epoch 45697: reducing learning rate of group 0 to 6.4196e-03.\n",
      "Epoch 45701, Training Loss: 38261, Validation Loss: 60555, 88860.86609733915\n",
      "Epoch 45798: reducing learning rate of group 0 to 6.4132e-03.\n",
      "Epoch 45801, Training Loss: 35526, Validation Loss: 51297, 69330.22183691822\n",
      "Epoch 45899: reducing learning rate of group 0 to 6.4068e-03.\n",
      "Epoch 45901, Training Loss: 39918, Validation Loss: 61590, 77764.28670554615\n",
      "Epoch 46000: reducing learning rate of group 0 to 6.4004e-03.\n",
      "Epoch 46001, Training Loss: 39133, Validation Loss: 56046, 65411.49810028775\n",
      "Epoch 46101: reducing learning rate of group 0 to 6.3940e-03.\n",
      "Epoch 46101, Training Loss: 35813, Validation Loss: 54880, 74959.91962002525\n",
      "Epoch 46201, Training Loss: 37485, Validation Loss: 51885, 56971.53472455495\n",
      "Epoch 46202: reducing learning rate of group 0 to 6.3876e-03.\n",
      "Epoch 46301, Training Loss: 34919, Validation Loss: 54767, 57812.9272814623\n",
      "Epoch 46303: reducing learning rate of group 0 to 6.3812e-03.\n",
      "Epoch 46401, Training Loss: 38919, Validation Loss: 51911, 63789.328716020646\n",
      "Epoch 46404: reducing learning rate of group 0 to 6.3748e-03.\n",
      "Epoch 46501, Training Loss: 36133, Validation Loss: 53269, 79471.77352025187\n",
      "Epoch 46505: reducing learning rate of group 0 to 6.3685e-03.\n",
      "Epoch 46601, Training Loss: 34659, Validation Loss: 54312, 58896.925247910374\n",
      "Epoch 46606: reducing learning rate of group 0 to 6.3621e-03.\n",
      "Epoch 46701, Training Loss: 35724, Validation Loss: 54013, 66908.24976661571\n",
      "Epoch 46707: reducing learning rate of group 0 to 6.3557e-03.\n",
      "Epoch 46801, Training Loss: 38435, Validation Loss: 54553, 62008.16788478286\n",
      "Epoch 46808: reducing learning rate of group 0 to 6.3494e-03.\n",
      "Epoch 46901, Training Loss: 36900, Validation Loss: 54423, 82083.96354334756\n",
      "Epoch 46909: reducing learning rate of group 0 to 6.3430e-03.\n",
      "Epoch 47001, Training Loss: 35968, Validation Loss: 55109, 65269.534435160014\n",
      "Epoch 47010: reducing learning rate of group 0 to 6.3367e-03.\n",
      "Epoch 47101, Training Loss: 39577, Validation Loss: 57165, 64947.825633016975\n",
      "Epoch 47111: reducing learning rate of group 0 to 6.3304e-03.\n",
      "Epoch 47201, Training Loss: 37176, Validation Loss: 62369, 73053.17817853589\n",
      "Epoch 47212: reducing learning rate of group 0 to 6.3240e-03.\n",
      "Epoch 47301, Training Loss: 39443, Validation Loss: 52691, 82868.15602219605\n",
      "Epoch 47313: reducing learning rate of group 0 to 6.3177e-03.\n",
      "Epoch 47401, Training Loss: 38342, Validation Loss: 54356, 69676.59409617574\n",
      "Epoch 47414: reducing learning rate of group 0 to 6.3114e-03.\n",
      "Epoch 47501, Training Loss: 36114, Validation Loss: 61112, 67193.14687247177\n",
      "Epoch 47515: reducing learning rate of group 0 to 6.3051e-03.\n",
      "Epoch 47601, Training Loss: 36870, Validation Loss: 63760, 76785.08999043236\n",
      "Epoch 47616: reducing learning rate of group 0 to 6.2988e-03.\n",
      "Epoch 47701, Training Loss: 37019, Validation Loss: 58305, 70668.41733431286\n",
      "Epoch 47717: reducing learning rate of group 0 to 6.2925e-03.\n",
      "Epoch 47801, Training Loss: 35289, Validation Loss: 56084, 70299.15568381757\n",
      "Epoch 47818: reducing learning rate of group 0 to 6.2862e-03.\n",
      "Epoch 47901, Training Loss: 39306, Validation Loss: 56972, 58539.585384364495\n",
      "Epoch 47919: reducing learning rate of group 0 to 6.2799e-03.\n",
      "Epoch 48001, Training Loss: 36854, Validation Loss: 51092, 63045.37458747497\n",
      "Epoch 48020: reducing learning rate of group 0 to 6.2736e-03.\n",
      "Epoch 48101, Training Loss: 35552, Validation Loss: 58006, 80268.98814289954\n",
      "Epoch 48121: reducing learning rate of group 0 to 6.2673e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48201, Training Loss: 40628, Validation Loss: 56557, 68643.28576766989\n",
      "Epoch 48222: reducing learning rate of group 0 to 6.2611e-03.\n",
      "Epoch 48301, Training Loss: 38806, Validation Loss: 54478, 71094.30177212345\n",
      "Epoch 48323: reducing learning rate of group 0 to 6.2548e-03.\n",
      "Epoch 48401, Training Loss: 35066, Validation Loss: 49688, 58302.52283497533\n",
      "Epoch 48424: reducing learning rate of group 0 to 6.2486e-03.\n",
      "Epoch 48501, Training Loss: 37552, Validation Loss: 60360, 72880.20805357\n",
      "Epoch 48525: reducing learning rate of group 0 to 6.2423e-03.\n",
      "Epoch 48601, Training Loss: 39260, Validation Loss: 58390, 81838.89140278219\n",
      "Epoch 48626: reducing learning rate of group 0 to 6.2361e-03.\n",
      "Epoch 48701, Training Loss: 39738, Validation Loss: 50810, 65933.92058548862\n",
      "Epoch 48727: reducing learning rate of group 0 to 6.2298e-03.\n",
      "Epoch 48801, Training Loss: 37128, Validation Loss: 51035, 67979.4671154582\n",
      "Epoch 48828: reducing learning rate of group 0 to 6.2236e-03.\n",
      "Epoch 48901, Training Loss: 36070, Validation Loss: 53145, 61172.70630583524\n",
      "Epoch 48929: reducing learning rate of group 0 to 6.2174e-03.\n",
      "Epoch 49001, Training Loss: 36872, Validation Loss: 51087, 61797.13843598446\n",
      "Epoch 49030: reducing learning rate of group 0 to 6.2112e-03.\n",
      "Epoch 49101, Training Loss: 36602, Validation Loss: 55704, 65504.8315704189\n",
      "Epoch 49131: reducing learning rate of group 0 to 6.2049e-03.\n",
      "Epoch 49201, Training Loss: 37529, Validation Loss: 51896, 85683.50882397879\n",
      "Epoch 49232: reducing learning rate of group 0 to 6.1987e-03.\n",
      "Epoch 49301, Training Loss: 36405, Validation Loss: 52180, 48690.70329458039\n",
      "Epoch 49333: reducing learning rate of group 0 to 6.1925e-03.\n",
      "Epoch 49401, Training Loss: 35089, Validation Loss: 51287, 66479.01787057136\n",
      "Epoch 49434: reducing learning rate of group 0 to 6.1863e-03.\n",
      "Epoch 49501, Training Loss: 34851, Validation Loss: 55731, 70044.46778089293\n",
      "Epoch 49535: reducing learning rate of group 0 to 6.1802e-03.\n",
      "Epoch 49601, Training Loss: 36456, Validation Loss: 55396, 75129.36538174098\n",
      "Epoch 49636: reducing learning rate of group 0 to 6.1740e-03.\n",
      "Epoch 49701, Training Loss: 34020, Validation Loss: 48637, 64700.99882294083\n",
      "Epoch 49737: reducing learning rate of group 0 to 6.1678e-03.\n",
      "Epoch 49801, Training Loss: 37281, Validation Loss: 56652, 73411.80094814401\n",
      "Epoch 49838: reducing learning rate of group 0 to 6.1616e-03.\n",
      "Epoch 49901, Training Loss: 36791, Validation Loss: 51588, 64644.87738286962\n",
      "Epoch 49939: reducing learning rate of group 0 to 6.1555e-03.\n",
      "Epoch 50001, Training Loss: 34976, Validation Loss: 55972, 68422.44749212197\n",
      "Epoch 50040: reducing learning rate of group 0 to 6.1493e-03.\n",
      "Epoch 50101, Training Loss: 34198, Validation Loss: 53200, 61530.26398594049\n",
      "Epoch 50141: reducing learning rate of group 0 to 6.1432e-03.\n",
      "Epoch 50201, Training Loss: 33418, Validation Loss: 50972, 63328.784276953455\n",
      "Epoch 50242: reducing learning rate of group 0 to 6.1370e-03.\n",
      "Epoch 50301, Training Loss: 39212, Validation Loss: 52690, 71020.12536146026\n",
      "Epoch 50343: reducing learning rate of group 0 to 6.1309e-03.\n",
      "Epoch 50401, Training Loss: 36825, Validation Loss: 53753, 71043.74128063867\n",
      "Epoch 50444: reducing learning rate of group 0 to 6.1248e-03.\n",
      "Epoch 50501, Training Loss: 36718, Validation Loss: 50989, 55111.92780027007\n",
      "Epoch 50545: reducing learning rate of group 0 to 6.1186e-03.\n",
      "Epoch 50601, Training Loss: 34674, Validation Loss: 52806, 62066.74515149964\n",
      "Epoch 50646: reducing learning rate of group 0 to 6.1125e-03.\n",
      "Epoch 50701, Training Loss: 37493, Validation Loss: 53023, 77406.87135611493\n",
      "Epoch 50747: reducing learning rate of group 0 to 6.1064e-03.\n",
      "Epoch 50801, Training Loss: 34960, Validation Loss: 53537, 66869.21163401667\n",
      "Epoch 50848: reducing learning rate of group 0 to 6.1003e-03.\n",
      "Epoch 50901, Training Loss: 39151, Validation Loss: 49938, 67759.30020690832\n",
      "Epoch 50949: reducing learning rate of group 0 to 6.0942e-03.\n",
      "Epoch 51001, Training Loss: 39606, Validation Loss: 50397, 77166.8069805842\n",
      "Epoch 51050: reducing learning rate of group 0 to 6.0881e-03.\n",
      "Epoch 51101, Training Loss: 37087, Validation Loss: 55527, 72226.65475652153\n",
      "Epoch 51151: reducing learning rate of group 0 to 6.0820e-03.\n",
      "Epoch 51201, Training Loss: 35280, Validation Loss: 50555, 71724.68008197869\n",
      "Epoch 51252: reducing learning rate of group 0 to 6.0759e-03.\n",
      "Epoch 51301, Training Loss: 34737, Validation Loss: 53412, 72794.27304902952\n",
      "Epoch 51353: reducing learning rate of group 0 to 6.0699e-03.\n",
      "Epoch 51401, Training Loss: 34989, Validation Loss: 49704, 57260.10774001407\n",
      "Epoch 51454: reducing learning rate of group 0 to 6.0638e-03.\n",
      "Epoch 51501, Training Loss: 36085, Validation Loss: 54509, 84771.67077487038\n",
      "Epoch 51555: reducing learning rate of group 0 to 6.0577e-03.\n",
      "Epoch 51601, Training Loss: 35060, Validation Loss: 50406, 65084.67736535807\n",
      "Epoch 51656: reducing learning rate of group 0 to 6.0517e-03.\n",
      "Epoch 51701, Training Loss: 33927, Validation Loss: 53754, 62986.92055319565\n",
      "Epoch 51757: reducing learning rate of group 0 to 6.0456e-03.\n",
      "Epoch 51801, Training Loss: 36014, Validation Loss: 54281, 65607.51197315122\n",
      "Epoch 51858: reducing learning rate of group 0 to 6.0396e-03.\n",
      "Epoch 51901, Training Loss: 37230, Validation Loss: 64163, 83602.07177671867\n",
      "Epoch 51959: reducing learning rate of group 0 to 6.0335e-03.\n",
      "Epoch 52001, Training Loss: 35317, Validation Loss: 52836, 50926.02312434671\n",
      "Epoch 52060: reducing learning rate of group 0 to 6.0275e-03.\n",
      "Epoch 52101, Training Loss: 38645, Validation Loss: 54731, 56905.235362629755\n",
      "Epoch 52161: reducing learning rate of group 0 to 6.0215e-03.\n",
      "Epoch 52201, Training Loss: 35580, Validation Loss: 54596, 61077.44459948908\n",
      "Epoch 52262: reducing learning rate of group 0 to 6.0154e-03.\n",
      "Epoch 52301, Training Loss: 36557, Validation Loss: 54667, 68552.00970912773\n",
      "Epoch 52363: reducing learning rate of group 0 to 6.0094e-03.\n",
      "Epoch 52401, Training Loss: 36567, Validation Loss: 56188, 75625.99321941145\n",
      "Epoch 52464: reducing learning rate of group 0 to 6.0034e-03.\n",
      "Epoch 52501, Training Loss: 36877, Validation Loss: 60048, 63370.69084340604\n",
      "Epoch 52565: reducing learning rate of group 0 to 5.9974e-03.\n",
      "Epoch 52601, Training Loss: 37052, Validation Loss: 51560, 65344.50038132549\n",
      "Epoch 52666: reducing learning rate of group 0 to 5.9914e-03.\n",
      "Epoch 52701, Training Loss: 34601, Validation Loss: 51637, 72455.98906847258\n",
      "Epoch 52767: reducing learning rate of group 0 to 5.9854e-03.\n",
      "Epoch 52801, Training Loss: 35142, Validation Loss: 52518, 62265.702392357234\n",
      "Epoch 52868: reducing learning rate of group 0 to 5.9794e-03.\n",
      "Epoch 52901, Training Loss: 35173, Validation Loss: 52013, 77759.19111443305\n",
      "Epoch 52969: reducing learning rate of group 0 to 5.9735e-03.\n",
      "Epoch 53001, Training Loss: 40598, Validation Loss: 51203, 74325.53824072346\n",
      "Epoch 53070: reducing learning rate of group 0 to 5.9675e-03.\n",
      "Epoch 53101, Training Loss: 37013, Validation Loss: 54501, 62583.33680730633\n",
      "Epoch 53171: reducing learning rate of group 0 to 5.9615e-03.\n",
      "Epoch 53201, Training Loss: 34896, Validation Loss: 55935, 69343.2915950629\n",
      "Epoch 53272: reducing learning rate of group 0 to 5.9556e-03.\n",
      "Epoch 53301, Training Loss: 37585, Validation Loss: 53732, 68047.95796886853\n",
      "Epoch 53373: reducing learning rate of group 0 to 5.9496e-03.\n",
      "Epoch 53401, Training Loss: 35387, Validation Loss: 51633, 69527.19753380721\n",
      "Epoch 53474: reducing learning rate of group 0 to 5.9437e-03.\n",
      "Epoch 53501, Training Loss: 35549, Validation Loss: 53012, 71035.40267842535\n",
      "Epoch 53575: reducing learning rate of group 0 to 5.9377e-03.\n",
      "Epoch 53601, Training Loss: 34239, Validation Loss: 56638, 63563.187391221196\n",
      "Epoch 53676: reducing learning rate of group 0 to 5.9318e-03.\n",
      "Epoch 53701, Training Loss: 34300, Validation Loss: 56178, 63693.09267504396\n",
      "Epoch 53777: reducing learning rate of group 0 to 5.9258e-03.\n",
      "Epoch 53801, Training Loss: 35495, Validation Loss: 54878, 63083.75583863139\n",
      "Epoch 53878: reducing learning rate of group 0 to 5.9199e-03.\n",
      "Epoch 53901, Training Loss: 36631, Validation Loss: 52555, 75007.51663097586\n",
      "Epoch 53979: reducing learning rate of group 0 to 5.9140e-03.\n",
      "Epoch 54001, Training Loss: 34673, Validation Loss: 55017, 57978.424898478144\n",
      "Epoch 54080: reducing learning rate of group 0 to 5.9081e-03.\n",
      "Epoch 54101, Training Loss: 36624, Validation Loss: 53750, 75457.7105368928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54181: reducing learning rate of group 0 to 5.9022e-03.\n",
      "Epoch 54201, Training Loss: 37284, Validation Loss: 52431, 68393.36867514825\n",
      "Epoch 54282: reducing learning rate of group 0 to 5.8963e-03.\n",
      "Epoch 54301, Training Loss: 36251, Validation Loss: 58011, 59164.293666189245\n",
      "Epoch 54383: reducing learning rate of group 0 to 5.8904e-03.\n",
      "Epoch 54401, Training Loss: 39513, Validation Loss: 50683, 75585.19238488509\n",
      "Epoch 54484: reducing learning rate of group 0 to 5.8845e-03.\n",
      "Epoch 54501, Training Loss: 36755, Validation Loss: 59015, 59798.15783923527\n",
      "Epoch 54585: reducing learning rate of group 0 to 5.8786e-03.\n",
      "Epoch 54601, Training Loss: 34637, Validation Loss: 50060, 68064.78578546659\n",
      "Epoch 54686: reducing learning rate of group 0 to 5.8727e-03.\n",
      "Epoch 54701, Training Loss: 34806, Validation Loss: 60294, 77913.14038284168\n",
      "Epoch 54787: reducing learning rate of group 0 to 5.8669e-03.\n",
      "Epoch 54801, Training Loss: 34748, Validation Loss: 49543, 51767.77309934401\n",
      "Epoch 54888: reducing learning rate of group 0 to 5.8610e-03.\n",
      "Epoch 54901, Training Loss: 38437, Validation Loss: 59750, 51813.61672103262\n",
      "Epoch 54989: reducing learning rate of group 0 to 5.8551e-03.\n",
      "Epoch 55001, Training Loss: 33254, Validation Loss: 53760, 62393.28679855847\n",
      "Epoch 55090: reducing learning rate of group 0 to 5.8493e-03.\n",
      "Epoch 55101, Training Loss: 36115, Validation Loss: 53946, 57866.34181363283\n",
      "Epoch 55191: reducing learning rate of group 0 to 5.8434e-03.\n",
      "Epoch 55201, Training Loss: 35859, Validation Loss: 63164, 59771.677593413384\n",
      "Epoch 55292: reducing learning rate of group 0 to 5.8376e-03.\n",
      "Epoch 55301, Training Loss: 36040, Validation Loss: 52834, 63107.41621774602\n",
      "Epoch 55393: reducing learning rate of group 0 to 5.8317e-03.\n",
      "Epoch 55401, Training Loss: 37585, Validation Loss: 56973, 74987.25413289898\n",
      "Epoch 55494: reducing learning rate of group 0 to 5.8259e-03.\n",
      "Epoch 55501, Training Loss: 35512, Validation Loss: 53571, 78772.3486126559\n",
      "Epoch 55595: reducing learning rate of group 0 to 5.8201e-03.\n",
      "Epoch 55601, Training Loss: 38336, Validation Loss: 59957, 51566.775869264675\n",
      "Epoch 55696: reducing learning rate of group 0 to 5.8143e-03.\n",
      "Epoch 55701, Training Loss: 35619, Validation Loss: 58178, 68981.98773647845\n",
      "Epoch 55797: reducing learning rate of group 0 to 5.8084e-03.\n",
      "Epoch 55801, Training Loss: 38934, Validation Loss: 52616, 63424.013253698875\n",
      "Epoch 55898: reducing learning rate of group 0 to 5.8026e-03.\n",
      "Epoch 55901, Training Loss: 37312, Validation Loss: 51229, 60417.993834328714\n",
      "Epoch 55999: reducing learning rate of group 0 to 5.7968e-03.\n",
      "Epoch 56001, Training Loss: 36603, Validation Loss: 57575, 59594.081739820926\n",
      "Epoch 56100: reducing learning rate of group 0 to 5.7910e-03.\n",
      "Epoch 56101, Training Loss: 35296, Validation Loss: 51198, 70427.62930978592\n",
      "Epoch 56201: reducing learning rate of group 0 to 5.7852e-03.\n",
      "Epoch 56201, Training Loss: 39584, Validation Loss: 55487, 80620.50528911273\n",
      "Epoch 56301, Training Loss: 33673, Validation Loss: 54909, 95235.95187766488\n",
      "Epoch 56302: reducing learning rate of group 0 to 5.7795e-03.\n",
      "Epoch 56401, Training Loss: 34634, Validation Loss: 58241, 68363.85578904238\n",
      "Epoch 56403: reducing learning rate of group 0 to 5.7737e-03.\n",
      "Epoch 56501, Training Loss: 38191, Validation Loss: 54156, 54605.964162627926\n",
      "Epoch 56504: reducing learning rate of group 0 to 5.7679e-03.\n",
      "Epoch 56601, Training Loss: 34874, Validation Loss: 59920, 57485.35703637348\n",
      "Epoch 56605: reducing learning rate of group 0 to 5.7621e-03.\n",
      "Epoch 56701, Training Loss: 38722, Validation Loss: 57889, 63324.5188643201\n",
      "Epoch 56706: reducing learning rate of group 0 to 5.7564e-03.\n",
      "Epoch 56801, Training Loss: 34914, Validation Loss: 54934, 58252.0276584597\n",
      "Epoch 56807: reducing learning rate of group 0 to 5.7506e-03.\n",
      "Epoch 56901, Training Loss: 35439, Validation Loss: 57390, 61754.04950889747\n",
      "Epoch 56908: reducing learning rate of group 0 to 5.7449e-03.\n",
      "Epoch 57001, Training Loss: 36343, Validation Loss: 54484, 70499.69016302239\n",
      "Epoch 57009: reducing learning rate of group 0 to 5.7391e-03.\n",
      "Epoch 57101, Training Loss: 34352, Validation Loss: 51096, 70540.24804606505\n",
      "Epoch 57110: reducing learning rate of group 0 to 5.7334e-03.\n",
      "Epoch 57201, Training Loss: 34702, Validation Loss: 54701, 64926.670397417714\n",
      "Epoch 57211: reducing learning rate of group 0 to 5.7277e-03.\n",
      "Epoch 57301, Training Loss: 38669, Validation Loss: 53176, 83185.31980126843\n",
      "Epoch 57312: reducing learning rate of group 0 to 5.7219e-03.\n",
      "Epoch 57401, Training Loss: 35230, Validation Loss: 51625, 75776.38854627422\n",
      "Epoch 57413: reducing learning rate of group 0 to 5.7162e-03.\n",
      "Epoch 57501, Training Loss: 35497, Validation Loss: 59902, 72384.19909057124\n",
      "Epoch 57514: reducing learning rate of group 0 to 5.7105e-03.\n",
      "Epoch 57601, Training Loss: 37021, Validation Loss: 52677, 75978.68101704658\n",
      "Epoch 57615: reducing learning rate of group 0 to 5.7048e-03.\n",
      "Epoch 57701, Training Loss: 36583, Validation Loss: 56766, 68149.10820843419\n",
      "Epoch 57716: reducing learning rate of group 0 to 5.6991e-03.\n",
      "Epoch 57801, Training Loss: 35687, Validation Loss: 55542, 50621.145608987565\n",
      "Epoch 57817: reducing learning rate of group 0 to 5.6934e-03.\n",
      "Epoch 57901, Training Loss: 36294, Validation Loss: 56470, 64341.91135623553\n",
      "Epoch 57918: reducing learning rate of group 0 to 5.6877e-03.\n",
      "Epoch 58001, Training Loss: 37829, Validation Loss: 52589, 73888.9497195205\n",
      "Epoch 58019: reducing learning rate of group 0 to 5.6820e-03.\n",
      "Epoch 58101, Training Loss: 38014, Validation Loss: 50046, 56288.37391925806\n",
      "Epoch 58120: reducing learning rate of group 0 to 5.6763e-03.\n",
      "Epoch 58201, Training Loss: 35590, Validation Loss: 57672, 67880.00980877211\n",
      "Epoch 58221: reducing learning rate of group 0 to 5.6706e-03.\n",
      "Epoch 58301, Training Loss: 33386, Validation Loss: 59454, 71607.72181737954\n",
      "Epoch 58322: reducing learning rate of group 0 to 5.6650e-03.\n",
      "Epoch 58401, Training Loss: 35298, Validation Loss: 50004, 67398.73374607414\n",
      "Epoch 58423: reducing learning rate of group 0 to 5.6593e-03.\n",
      "Epoch 58501, Training Loss: 34148, Validation Loss: 58836, 57241.46942259109\n",
      "Epoch 58524: reducing learning rate of group 0 to 5.6536e-03.\n",
      "Epoch 58601, Training Loss: 36190, Validation Loss: 54206, 65701.06152542295\n",
      "Epoch 58625: reducing learning rate of group 0 to 5.6480e-03.\n",
      "Epoch 58701, Training Loss: 37851, Validation Loss: 54638, 72601.04452702588\n",
      "Epoch 58726: reducing learning rate of group 0 to 5.6423e-03.\n",
      "Epoch 58801, Training Loss: 35405, Validation Loss: 56860, 60385.82777782291\n",
      "Epoch 58827: reducing learning rate of group 0 to 5.6367e-03.\n",
      "Epoch 58901, Training Loss: 36361, Validation Loss: 58563, 61131.21743316121\n",
      "Epoch 58928: reducing learning rate of group 0 to 5.6311e-03.\n",
      "Epoch 59001, Training Loss: 33062, Validation Loss: 54506, 66687.71063235728\n",
      "Epoch 59029: reducing learning rate of group 0 to 5.6254e-03.\n",
      "Epoch 59101, Training Loss: 35375, Validation Loss: 52361, 58666.202334617126\n",
      "Epoch 59130: reducing learning rate of group 0 to 5.6198e-03.\n",
      "Epoch 59201, Training Loss: 35596, Validation Loss: 54849, 59662.78885733388\n",
      "Epoch 59231: reducing learning rate of group 0 to 5.6142e-03.\n",
      "Epoch 59301, Training Loss: 36625, Validation Loss: 54474, 67145.22298960965\n",
      "Epoch 59332: reducing learning rate of group 0 to 5.6086e-03.\n",
      "Epoch 59401, Training Loss: 36709, Validation Loss: 54750, 55556.40091368848\n",
      "Epoch 59433: reducing learning rate of group 0 to 5.6030e-03.\n",
      "Epoch 59501, Training Loss: 34607, Validation Loss: 58620, 79799.14335668199\n",
      "Epoch 59534: reducing learning rate of group 0 to 5.5974e-03.\n",
      "Epoch 59601, Training Loss: 34397, Validation Loss: 55216, 68991.23167455061\n",
      "Epoch 59635: reducing learning rate of group 0 to 5.5918e-03.\n",
      "Epoch 59701, Training Loss: 36931, Validation Loss: 52684, 69251.04782968141\n",
      "Epoch 59736: reducing learning rate of group 0 to 5.5862e-03.\n",
      "Epoch 59801, Training Loss: 34990, Validation Loss: 58612, 80375.42217724283\n",
      "Epoch 59837: reducing learning rate of group 0 to 5.5806e-03.\n",
      "Epoch 59901, Training Loss: 35188, Validation Loss: 50238, 59017.24713330823\n",
      "Epoch 59938: reducing learning rate of group 0 to 5.5750e-03.\n",
      "Epoch 60001, Training Loss: 38880, Validation Loss: 53036, 57825.38390295687\n",
      "Epoch 60039: reducing learning rate of group 0 to 5.5694e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60101, Training Loss: 31756, Validation Loss: 58606, 55639.507540283994\n",
      "Epoch 60140: reducing learning rate of group 0 to 5.5639e-03.\n",
      "Epoch 60201, Training Loss: 33170, Validation Loss: 52440, 68789.17145246074\n",
      "Epoch 60241: reducing learning rate of group 0 to 5.5583e-03.\n",
      "Epoch 60301, Training Loss: 33569, Validation Loss: 51659, 72493.61865444416\n",
      "Epoch 60342: reducing learning rate of group 0 to 5.5527e-03.\n",
      "Epoch 60401, Training Loss: 36319, Validation Loss: 50300, 75736.34046903595\n",
      "Epoch 60443: reducing learning rate of group 0 to 5.5472e-03.\n",
      "Epoch 60501, Training Loss: 33136, Validation Loss: 55544, 65199.65814712516\n",
      "Epoch 60544: reducing learning rate of group 0 to 5.5416e-03.\n",
      "Epoch 60601, Training Loss: 40557, Validation Loss: 53253, 69861.7586277912\n",
      "Epoch 60645: reducing learning rate of group 0 to 5.5361e-03.\n",
      "Epoch 60701, Training Loss: 37399, Validation Loss: 56853, 67425.21634340705\n",
      "Epoch 60746: reducing learning rate of group 0 to 5.5306e-03.\n",
      "Epoch 60801, Training Loss: 35291, Validation Loss: 56662, 59467.93713434661\n",
      "Epoch 60847: reducing learning rate of group 0 to 5.5250e-03.\n",
      "Epoch 60901, Training Loss: 36049, Validation Loss: 56758, 65341.4512094736\n",
      "Epoch 60948: reducing learning rate of group 0 to 5.5195e-03.\n",
      "Epoch 61001, Training Loss: 34422, Validation Loss: 59664, 54848.05150432526\n",
      "Epoch 61049: reducing learning rate of group 0 to 5.5140e-03.\n",
      "Epoch 61101, Training Loss: 36565, Validation Loss: 60908, 56765.549273637036\n",
      "Epoch 61150: reducing learning rate of group 0 to 5.5085e-03.\n",
      "Epoch 61201, Training Loss: 34272, Validation Loss: 55658, 51201.83664277722\n",
      "Epoch 61251: reducing learning rate of group 0 to 5.5030e-03.\n",
      "Epoch 61301, Training Loss: 35828, Validation Loss: 57977, 64435.14626040421\n",
      "Epoch 61352: reducing learning rate of group 0 to 5.4975e-03.\n",
      "Epoch 61401, Training Loss: 36120, Validation Loss: 53314, 67099.20943376735\n",
      "Epoch 61453: reducing learning rate of group 0 to 5.4920e-03.\n",
      "Epoch 61501, Training Loss: 36194, Validation Loss: 52281, 76029.3229466746\n",
      "Epoch 61554: reducing learning rate of group 0 to 5.4865e-03.\n",
      "Epoch 61601, Training Loss: 32236, Validation Loss: 56223, 58254.91415089966\n",
      "Epoch 61655: reducing learning rate of group 0 to 5.4810e-03.\n",
      "Epoch 61701, Training Loss: 34840, Validation Loss: 58644, 74709.57488585483\n",
      "Epoch 61756: reducing learning rate of group 0 to 5.4755e-03.\n",
      "Epoch 61801, Training Loss: 37115, Validation Loss: 51789, 69308.68591777953\n",
      "Epoch 61857: reducing learning rate of group 0 to 5.4700e-03.\n",
      "Epoch 61901, Training Loss: 33648, Validation Loss: 53402, 62499.641429741925\n",
      "Epoch 61958: reducing learning rate of group 0 to 5.4646e-03.\n",
      "Epoch 62001, Training Loss: 34706, Validation Loss: 56013, 71018.09741898986\n",
      "Epoch 62059: reducing learning rate of group 0 to 5.4591e-03.\n",
      "Epoch 62101, Training Loss: 34752, Validation Loss: 56378, 72740.28714704154\n",
      "Epoch 62160: reducing learning rate of group 0 to 5.4536e-03.\n",
      "Epoch 62201, Training Loss: 36249, Validation Loss: 53285, 68426.4586356037\n",
      "Epoch 62261: reducing learning rate of group 0 to 5.4482e-03.\n",
      "Epoch 62301, Training Loss: 38567, Validation Loss: 54341, 69419.10669353574\n",
      "Epoch 62362: reducing learning rate of group 0 to 5.4427e-03.\n",
      "Epoch 62401, Training Loss: 38101, Validation Loss: 58555, 65744.39186309413\n",
      "Epoch 62463: reducing learning rate of group 0 to 5.4373e-03.\n",
      "Epoch 62501, Training Loss: 33090, Validation Loss: 55677, 69708.83452205357\n",
      "Epoch 62564: reducing learning rate of group 0 to 5.4319e-03.\n",
      "Epoch 62601, Training Loss: 34249, Validation Loss: 55939, 60157.16660731349\n",
      "Epoch 62665: reducing learning rate of group 0 to 5.4264e-03.\n",
      "Epoch 62701, Training Loss: 37039, Validation Loss: 832871, 70224.31599260267\n",
      "Epoch 62766: reducing learning rate of group 0 to 5.4210e-03.\n",
      "Epoch 62801, Training Loss: 35277, Validation Loss: 3018802, 73893.00907838038\n",
      "Epoch 62867: reducing learning rate of group 0 to 5.4156e-03.\n",
      "Epoch 62901, Training Loss: 32982, Validation Loss: 58194, 48953.73871406529\n",
      "Epoch 62968: reducing learning rate of group 0 to 5.4102e-03.\n",
      "Epoch 63001, Training Loss: 37134, Validation Loss: 52647, 54944.87479674467\n",
      "Epoch 63069: reducing learning rate of group 0 to 5.4047e-03.\n",
      "Epoch 63101, Training Loss: 33601, Validation Loss: 53850, 56967.79587281893\n",
      "Epoch 63170: reducing learning rate of group 0 to 5.3993e-03.\n",
      "Epoch 63201, Training Loss: 33865, Validation Loss: 53020, 54352.878879453405\n",
      "Epoch 63271: reducing learning rate of group 0 to 5.3939e-03.\n",
      "Epoch 63301, Training Loss: 39073, Validation Loss: 52035, 100964.57107622242\n",
      "Epoch 63372: reducing learning rate of group 0 to 5.3885e-03.\n",
      "Epoch 63401, Training Loss: 36304, Validation Loss: 53038, 76011.26766388633\n",
      "Epoch 63473: reducing learning rate of group 0 to 5.3832e-03.\n",
      "Epoch 63501, Training Loss: 35243, Validation Loss: 52636, 64418.98911379627\n",
      "Epoch 63574: reducing learning rate of group 0 to 5.3778e-03.\n",
      "Epoch 63601, Training Loss: 34096, Validation Loss: 50648, 61194.91493350929\n",
      "Epoch 63675: reducing learning rate of group 0 to 5.3724e-03.\n",
      "Epoch 63701, Training Loss: 34493, Validation Loss: 56797, 63547.37847411833\n",
      "Epoch 63776: reducing learning rate of group 0 to 5.3670e-03.\n",
      "Epoch 63801, Training Loss: 32771, Validation Loss: 52563, 60808.39149822444\n",
      "Epoch 63877: reducing learning rate of group 0 to 5.3617e-03.\n",
      "Epoch 63901, Training Loss: 34317, Validation Loss: 56075, 64569.50520910267\n",
      "Epoch 63978: reducing learning rate of group 0 to 5.3563e-03.\n",
      "Epoch 64001, Training Loss: 34837, Validation Loss: 52495, 58672.3319389717\n",
      "Epoch 64079: reducing learning rate of group 0 to 5.3509e-03.\n",
      "Epoch 64101, Training Loss: 36863, Validation Loss: 54797, 72326.49426300426\n",
      "Epoch 64180: reducing learning rate of group 0 to 5.3456e-03.\n",
      "Epoch 64201, Training Loss: 33521, Validation Loss: 57955, 66707.50700319976\n",
      "Epoch 64281: reducing learning rate of group 0 to 5.3402e-03.\n",
      "Epoch 64301, Training Loss: 33604, Validation Loss: 52715, 82206.45047648996\n",
      "Epoch 64382: reducing learning rate of group 0 to 5.3349e-03.\n",
      "Epoch 64401, Training Loss: 36531, Validation Loss: 55959, 65561.28563894611\n",
      "Epoch 64483: reducing learning rate of group 0 to 5.3296e-03.\n",
      "Epoch 64501, Training Loss: 32525, Validation Loss: 53365, 72187.88910329527\n",
      "Epoch 64584: reducing learning rate of group 0 to 5.3242e-03.\n",
      "Epoch 64601, Training Loss: 32162, Validation Loss: 52685, 68596.23113394383\n",
      "Epoch 64685: reducing learning rate of group 0 to 5.3189e-03.\n",
      "Epoch 64701, Training Loss: 35721, Validation Loss: 51313, 79901.24226111396\n",
      "Epoch 64786: reducing learning rate of group 0 to 5.3136e-03.\n",
      "Epoch 64801, Training Loss: 33877, Validation Loss: 54631, 78822.62659231751\n",
      "Epoch 64887: reducing learning rate of group 0 to 5.3083e-03.\n",
      "Epoch 64901, Training Loss: 35680, Validation Loss: 52737, 59911.69994877764\n",
      "Epoch 64988: reducing learning rate of group 0 to 5.3030e-03.\n",
      "Epoch 65001, Training Loss: 34014, Validation Loss: 52444, 59510.299649395834\n",
      "Epoch 65089: reducing learning rate of group 0 to 5.2977e-03.\n",
      "Epoch 65101, Training Loss: 36088, Validation Loss: 52260, 68550.46889042277\n",
      "Epoch 65190: reducing learning rate of group 0 to 5.2924e-03.\n",
      "Epoch 65201, Training Loss: 35638, Validation Loss: 58549, 64973.95315196985\n",
      "Epoch 65291: reducing learning rate of group 0 to 5.2871e-03.\n",
      "Epoch 65301, Training Loss: 34521, Validation Loss: 51781, 76298.80386363527\n",
      "Epoch 65392: reducing learning rate of group 0 to 5.2818e-03.\n",
      "Epoch 65401, Training Loss: 34583, Validation Loss: 52176, 64556.03019483341\n",
      "Epoch 65493: reducing learning rate of group 0 to 5.2765e-03.\n",
      "Epoch 65501, Training Loss: 34497, Validation Loss: 52858, 61645.4805014077\n",
      "Epoch 65594: reducing learning rate of group 0 to 5.2712e-03.\n",
      "Epoch 65601, Training Loss: 33064, Validation Loss: 53378, 64224.02533655806\n",
      "Epoch 65695: reducing learning rate of group 0 to 5.2660e-03.\n",
      "Epoch 65701, Training Loss: 35100, Validation Loss: 50797, 70241.89268922037\n",
      "Epoch 65796: reducing learning rate of group 0 to 5.2607e-03.\n",
      "Epoch 65801, Training Loss: 38245, Validation Loss: 56418, 79475.34405938209\n",
      "Epoch 65897: reducing learning rate of group 0 to 5.2554e-03.\n",
      "Epoch 65901, Training Loss: 34219, Validation Loss: 52575, 64814.81162944888\n",
      "Epoch 65998: reducing learning rate of group 0 to 5.2502e-03.\n",
      "Epoch 66001, Training Loss: 37470, Validation Loss: 53879, 64403.46280193776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66099: reducing learning rate of group 0 to 5.2449e-03.\n",
      "Epoch 66101, Training Loss: 35656, Validation Loss: 52304, 65837.32293530447\n",
      "Epoch 66200: reducing learning rate of group 0 to 5.2397e-03.\n",
      "Epoch 66201, Training Loss: 35532, Validation Loss: 52390, 64088.67036188974\n",
      "Epoch 66301: reducing learning rate of group 0 to 5.2344e-03.\n",
      "Epoch 66301, Training Loss: 32859, Validation Loss: 55716, 67158.66983212491\n",
      "Epoch 66401, Training Loss: 34223, Validation Loss: 50609, 55411.45069831641\n",
      "Epoch 66402: reducing learning rate of group 0 to 5.2292e-03.\n",
      "Epoch 66501, Training Loss: 29464, Validation Loss: 57801, 62043.52636088737\n",
      "Epoch 66503: reducing learning rate of group 0 to 5.2240e-03.\n",
      "Epoch 66601, Training Loss: 37069, Validation Loss: 54399, 71468.36232801386\n",
      "Epoch 66604: reducing learning rate of group 0 to 5.2188e-03.\n",
      "Epoch 66701, Training Loss: 36372, Validation Loss: 60166, 65149.30177227241\n",
      "Epoch 66705: reducing learning rate of group 0 to 5.2135e-03.\n",
      "Epoch 66801, Training Loss: 34058, Validation Loss: 57792, 70566.10070736308\n",
      "Epoch 66806: reducing learning rate of group 0 to 5.2083e-03.\n",
      "Epoch 66901, Training Loss: 36892, Validation Loss: 55013, 59046.65264854609\n",
      "Epoch 66907: reducing learning rate of group 0 to 5.2031e-03.\n",
      "Epoch 67001, Training Loss: 32810, Validation Loss: 55034, 57899.545818662446\n",
      "Epoch 67008: reducing learning rate of group 0 to 5.1979e-03.\n",
      "Epoch 67101, Training Loss: 34475, Validation Loss: 54933, 58897.87739661414\n",
      "Epoch 67109: reducing learning rate of group 0 to 5.1927e-03.\n",
      "Epoch 67201, Training Loss: 32201, Validation Loss: 53972, 61794.204035623254\n",
      "Epoch 67210: reducing learning rate of group 0 to 5.1875e-03.\n",
      "Epoch 67301, Training Loss: 35269, Validation Loss: 55564, 55729.05752880262\n",
      "Epoch 67311: reducing learning rate of group 0 to 5.1823e-03.\n",
      "Epoch 67401, Training Loss: 35193, Validation Loss: 53027, 73709.36205831586\n",
      "Epoch 67412: reducing learning rate of group 0 to 5.1772e-03.\n",
      "Epoch 67501, Training Loss: 33366, Validation Loss: 53004, 61015.80893357096\n",
      "Epoch 67513: reducing learning rate of group 0 to 5.1720e-03.\n",
      "Epoch 67601, Training Loss: 31658, Validation Loss: 57945, 64191.69322154993\n",
      "Epoch 67614: reducing learning rate of group 0 to 5.1668e-03.\n",
      "Epoch 67701, Training Loss: 36629, Validation Loss: 55078, 79355.51593779221\n",
      "Epoch 67715: reducing learning rate of group 0 to 5.1616e-03.\n",
      "Epoch 67801, Training Loss: 35320, Validation Loss: 52802, 66165.12194825737\n",
      "Epoch 67816: reducing learning rate of group 0 to 5.1565e-03.\n",
      "Epoch 67901, Training Loss: 34883, Validation Loss: 55585, 71565.66654408429\n",
      "Epoch 67917: reducing learning rate of group 0 to 5.1513e-03.\n",
      "Epoch 68001, Training Loss: 34538, Validation Loss: 55008, 59288.17006422856\n",
      "Epoch 68018: reducing learning rate of group 0 to 5.1462e-03.\n",
      "Epoch 68101, Training Loss: 35370, Validation Loss: 51320, 67449.53212255023\n",
      "Epoch 68119: reducing learning rate of group 0 to 5.1410e-03.\n",
      "Epoch 68201, Training Loss: 31488, Validation Loss: 57354, 70986.60790174252\n",
      "Epoch 68220: reducing learning rate of group 0 to 5.1359e-03.\n",
      "Epoch 68301, Training Loss: 32949, Validation Loss: 54891, 71662.62634090269\n",
      "Epoch 68321: reducing learning rate of group 0 to 5.1307e-03.\n",
      "Epoch 68401, Training Loss: 32725, Validation Loss: 55143, 72450.49448120508\n",
      "Epoch 68422: reducing learning rate of group 0 to 5.1256e-03.\n",
      "Epoch 68501, Training Loss: 33925, Validation Loss: 61094, 80058.14132856527\n",
      "Epoch 68523: reducing learning rate of group 0 to 5.1205e-03.\n",
      "Epoch 68601, Training Loss: 33721, Validation Loss: 52490, 66546.83011797174\n",
      "Epoch 68624: reducing learning rate of group 0 to 5.1154e-03.\n",
      "Epoch 68701, Training Loss: 34228, Validation Loss: 62397, 67240.26598852022\n",
      "Epoch 68725: reducing learning rate of group 0 to 5.1103e-03.\n",
      "Epoch 68801, Training Loss: 31812, Validation Loss: 53776, 65353.77400171874\n",
      "Epoch 68826: reducing learning rate of group 0 to 5.1051e-03.\n",
      "Epoch 68901, Training Loss: 32580, Validation Loss: 56477, 51986.92584882108\n",
      "Epoch 68927: reducing learning rate of group 0 to 5.1000e-03.\n",
      "Epoch 69001, Training Loss: 35043, Validation Loss: 58477, 71102.23877031612\n",
      "Epoch 69028: reducing learning rate of group 0 to 5.0949e-03.\n",
      "Epoch 69101, Training Loss: 33132, Validation Loss: 54900, 55991.53285856937\n",
      "Epoch 69129: reducing learning rate of group 0 to 5.0898e-03.\n",
      "Epoch 69201, Training Loss: 33850, Validation Loss: 52347, 65113.5047327606\n",
      "Epoch 69230: reducing learning rate of group 0 to 5.0848e-03.\n",
      "Epoch 69301, Training Loss: 33525, Validation Loss: 54871, 64458.29451193439\n",
      "Epoch 69331: reducing learning rate of group 0 to 5.0797e-03.\n",
      "Epoch 69401, Training Loss: 33322, Validation Loss: 57069, 57935.51090658726\n",
      "Epoch 69432: reducing learning rate of group 0 to 5.0746e-03.\n",
      "Epoch 69501, Training Loss: 33210, Validation Loss: 55251, 67447.00134252012\n",
      "Epoch 69533: reducing learning rate of group 0 to 5.0695e-03.\n",
      "Epoch 69601, Training Loss: 35461, Validation Loss: 52339, 66178.37836791373\n",
      "Epoch 69634: reducing learning rate of group 0 to 5.0644e-03.\n",
      "Epoch 69701, Training Loss: 34304, Validation Loss: 59008, 60672.89933754244\n",
      "Epoch 69735: reducing learning rate of group 0 to 5.0594e-03.\n",
      "Epoch 69801, Training Loss: 33207, Validation Loss: 52797, 60344.386027174296\n",
      "Epoch 69836: reducing learning rate of group 0 to 5.0543e-03.\n",
      "Epoch 69901, Training Loss: 33034, Validation Loss: 53967, 74875.53106017357\n",
      "Epoch 69937: reducing learning rate of group 0 to 5.0493e-03.\n",
      "Epoch 70001, Training Loss: 32127, Validation Loss: 52464, 62765.38720538805\n",
      "Epoch 70038: reducing learning rate of group 0 to 5.0442e-03.\n",
      "Epoch 70101, Training Loss: 33178, Validation Loss: 53918, 78303.03972420933\n",
      "Epoch 70139: reducing learning rate of group 0 to 5.0392e-03.\n",
      "Epoch 70201, Training Loss: 35161, Validation Loss: 52666, 60664.931083870855\n",
      "Epoch 70240: reducing learning rate of group 0 to 5.0341e-03.\n",
      "Epoch 70301, Training Loss: 33680, Validation Loss: 55692, 75405.28623691651\n",
      "Epoch 70341: reducing learning rate of group 0 to 5.0291e-03.\n",
      "Epoch 70401, Training Loss: 34899, Validation Loss: 51691, 66420.61778054455\n",
      "Epoch 70442: reducing learning rate of group 0 to 5.0241e-03.\n",
      "Epoch 70501, Training Loss: 32525, Validation Loss: 56092, 77611.50743071972\n",
      "Epoch 70543: reducing learning rate of group 0 to 5.0190e-03.\n",
      "Epoch 70601, Training Loss: 36456, Validation Loss: 51417, 71953.88106345409\n",
      "Epoch 70644: reducing learning rate of group 0 to 5.0140e-03.\n",
      "Epoch 70701, Training Loss: 33662, Validation Loss: 54099, 62630.97470002951\n",
      "Epoch 70745: reducing learning rate of group 0 to 5.0090e-03.\n",
      "Epoch 70801, Training Loss: 34501, Validation Loss: 54511, 59350.49070183018\n",
      "Epoch 70846: reducing learning rate of group 0 to 5.0040e-03.\n",
      "Epoch 70901, Training Loss: 32731, Validation Loss: 54259, 58189.34036012346\n",
      "Epoch 70947: reducing learning rate of group 0 to 4.9990e-03.\n",
      "Epoch 71001, Training Loss: 33561, Validation Loss: 51041, 58827.6601266874\n",
      "Epoch 71048: reducing learning rate of group 0 to 4.9940e-03.\n",
      "Epoch 71101, Training Loss: 34608, Validation Loss: 55893, 60264.57700801897\n",
      "Epoch 71149: reducing learning rate of group 0 to 4.9890e-03.\n",
      "Epoch 71201, Training Loss: 33497, Validation Loss: 53212, 66172.36613604455\n",
      "Epoch 71250: reducing learning rate of group 0 to 4.9840e-03.\n",
      "Epoch 71301, Training Loss: 32568, Validation Loss: 51143, 50514.32556589885\n",
      "Epoch 71351: reducing learning rate of group 0 to 4.9790e-03.\n",
      "Epoch 71401, Training Loss: 34484, Validation Loss: 53365, 72691.41038436232\n",
      "Epoch 71452: reducing learning rate of group 0 to 4.9741e-03.\n",
      "Epoch 71501, Training Loss: 31198, Validation Loss: 51543, 71366.19936234572\n",
      "Epoch 71553: reducing learning rate of group 0 to 4.9691e-03.\n",
      "Epoch 71601, Training Loss: 34778, Validation Loss: 54613, 70960.6335482103\n",
      "Epoch 71654: reducing learning rate of group 0 to 4.9641e-03.\n",
      "Epoch 71701, Training Loss: 34709, Validation Loss: 55812, 54532.445574589736\n",
      "Epoch 71755: reducing learning rate of group 0 to 4.9592e-03.\n",
      "Epoch 71801, Training Loss: 36414, Validation Loss: 51599, 64119.76470443286\n",
      "Epoch 71856: reducing learning rate of group 0 to 4.9542e-03.\n",
      "Epoch 71901, Training Loss: 32519, Validation Loss: 53620, 83299.47440394336\n",
      "Epoch 71957: reducing learning rate of group 0 to 4.9492e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72001, Training Loss: 36984, Validation Loss: 52056, 64812.32205950075\n",
      "Epoch 72058: reducing learning rate of group 0 to 4.9443e-03.\n",
      "Epoch 72101, Training Loss: 34237, Validation Loss: 54335, 62782.129332487595\n",
      "Epoch 72159: reducing learning rate of group 0 to 4.9393e-03.\n",
      "Epoch 72201, Training Loss: 32561, Validation Loss: 54828, 50517.857892524065\n",
      "Epoch 72260: reducing learning rate of group 0 to 4.9344e-03.\n",
      "Epoch 72301, Training Loss: 32116, Validation Loss: 53916, 74506.50773759038\n",
      "Epoch 72361: reducing learning rate of group 0 to 4.9295e-03.\n",
      "Epoch 72401, Training Loss: 34776, Validation Loss: 53787, 63064.73009217125\n",
      "Epoch 72462: reducing learning rate of group 0 to 4.9245e-03.\n",
      "Epoch 72501, Training Loss: 31505, Validation Loss: 207688, 60715.40500686487\n",
      "Epoch 72563: reducing learning rate of group 0 to 4.9196e-03.\n",
      "Epoch 72601, Training Loss: 31411, Validation Loss: 794244, 61151.0629901392\n",
      "Epoch 72664: reducing learning rate of group 0 to 4.9147e-03.\n",
      "Epoch 72701, Training Loss: 33352, Validation Loss: 1415444, 85518.16657491117\n",
      "Epoch 72765: reducing learning rate of group 0 to 4.9098e-03.\n",
      "Epoch 72801, Training Loss: 32927, Validation Loss: 53172, 57085.15924740189\n",
      "Epoch 72866: reducing learning rate of group 0 to 4.9049e-03.\n",
      "Epoch 72901, Training Loss: 31347, Validation Loss: 52757, 63274.39221952033\n",
      "Epoch 72967: reducing learning rate of group 0 to 4.9000e-03.\n",
      "Epoch 73001, Training Loss: 34832, Validation Loss: 50925, 68389.62852549227\n",
      "Epoch 73068: reducing learning rate of group 0 to 4.8951e-03.\n",
      "Epoch 73101, Training Loss: 33265, Validation Loss: 54047, 52925.82515699884\n",
      "Epoch 73169: reducing learning rate of group 0 to 4.8902e-03.\n",
      "Epoch 73201, Training Loss: 32008, Validation Loss: 55058, 60216.55961880263\n",
      "Epoch 73270: reducing learning rate of group 0 to 4.8853e-03.\n",
      "Epoch 73301, Training Loss: 33049, Validation Loss: 58528, 76377.83314998509\n",
      "Epoch 73371: reducing learning rate of group 0 to 4.8804e-03.\n",
      "Epoch 73401, Training Loss: 37359, Validation Loss: 53082, 52331.44852731709\n",
      "Epoch 73472: reducing learning rate of group 0 to 4.8755e-03.\n",
      "Epoch 73501, Training Loss: 32739, Validation Loss: 54856, 66071.38688374538\n",
      "Epoch 73573: reducing learning rate of group 0 to 4.8706e-03.\n",
      "Epoch 73601, Training Loss: 31159, Validation Loss: 56318, 60233.15538284808\n",
      "Epoch 73674: reducing learning rate of group 0 to 4.8658e-03.\n",
      "Epoch 73701, Training Loss: 33393, Validation Loss: 53855, 75280.5572398044\n",
      "Epoch 73775: reducing learning rate of group 0 to 4.8609e-03.\n",
      "Epoch 73801, Training Loss: 31897, Validation Loss: 56197, 58698.00574821451\n",
      "Epoch 73876: reducing learning rate of group 0 to 4.8560e-03.\n",
      "Epoch 73901, Training Loss: 36421, Validation Loss: 59445, 58837.520974678926\n",
      "Epoch 73977: reducing learning rate of group 0 to 4.8512e-03.\n",
      "Epoch 74001, Training Loss: 36047, Validation Loss: 54731, 68233.26164116304\n",
      "Epoch 74078: reducing learning rate of group 0 to 4.8463e-03.\n",
      "Epoch 74101, Training Loss: 34115, Validation Loss: 55588, 61687.90607929568\n",
      "Epoch 74179: reducing learning rate of group 0 to 4.8415e-03.\n",
      "Epoch 74201, Training Loss: 34807, Validation Loss: 50939, 57365.05760672817\n",
      "Epoch 74280: reducing learning rate of group 0 to 4.8366e-03.\n",
      "Epoch 74301, Training Loss: 32435, Validation Loss: 54061, 67321.88156204186\n",
      "Epoch 74381: reducing learning rate of group 0 to 4.8318e-03.\n",
      "Epoch 74401, Training Loss: 32450, Validation Loss: 56316, 60385.19153309128\n",
      "Epoch 74482: reducing learning rate of group 0 to 4.8270e-03.\n",
      "Epoch 74501, Training Loss: 32924, Validation Loss: 55343, 70762.5996712303\n",
      "Epoch 74583: reducing learning rate of group 0 to 4.8222e-03.\n",
      "Epoch 74601, Training Loss: 32807, Validation Loss: 55312, 68938.53088798455\n",
      "Epoch 74684: reducing learning rate of group 0 to 4.8173e-03.\n",
      "Epoch 74701, Training Loss: 37689, Validation Loss: 53171, 66239.81477926247\n",
      "Epoch 74785: reducing learning rate of group 0 to 4.8125e-03.\n",
      "Epoch 74801, Training Loss: 30788, Validation Loss: 54399, 73573.21538069702\n",
      "Epoch 74886: reducing learning rate of group 0 to 4.8077e-03.\n",
      "Epoch 74901, Training Loss: 32927, Validation Loss: 53525, 66550.64129588865\n",
      "Epoch 74987: reducing learning rate of group 0 to 4.8029e-03.\n",
      "Epoch 75001, Training Loss: 36473, Validation Loss: 54049, 71835.7979933911\n",
      "Epoch 75088: reducing learning rate of group 0 to 4.7981e-03.\n",
      "Epoch 75101, Training Loss: 34145, Validation Loss: 55923, 63873.42293687747\n",
      "Epoch 75189: reducing learning rate of group 0 to 4.7933e-03.\n",
      "Epoch 75201, Training Loss: 33410, Validation Loss: 54954, 74319.17968632423\n",
      "Epoch 75290: reducing learning rate of group 0 to 4.7885e-03.\n",
      "Epoch 75301, Training Loss: 34558, Validation Loss: 57690, 61475.72957020593\n",
      "Epoch 75391: reducing learning rate of group 0 to 4.7837e-03.\n",
      "Epoch 75401, Training Loss: 35304, Validation Loss: 53937, 59722.27509954071\n",
      "Epoch 75492: reducing learning rate of group 0 to 4.7789e-03.\n",
      "Epoch 75501, Training Loss: 36987, Validation Loss: 56667, 60057.20165578896\n",
      "Epoch 75593: reducing learning rate of group 0 to 4.7741e-03.\n",
      "Epoch 75601, Training Loss: 31045, Validation Loss: 54317, 58734.689896903874\n",
      "Epoch 75694: reducing learning rate of group 0 to 4.7694e-03.\n",
      "Epoch 75701, Training Loss: 33953, Validation Loss: 55830, 67008.85940100062\n",
      "Epoch 75795: reducing learning rate of group 0 to 4.7646e-03.\n",
      "Epoch 75801, Training Loss: 34832, Validation Loss: 53628, 69434.4108515044\n",
      "Epoch 75896: reducing learning rate of group 0 to 4.7598e-03.\n",
      "Epoch 75901, Training Loss: 33150, Validation Loss: 52935, 59141.16333973061\n",
      "Epoch 75997: reducing learning rate of group 0 to 4.7551e-03.\n",
      "Epoch 76001, Training Loss: 31965, Validation Loss: 54367, 62519.29301574188\n",
      "Epoch 76098: reducing learning rate of group 0 to 4.7503e-03.\n",
      "Epoch 76101, Training Loss: 34650, Validation Loss: 55169, 55076.799415296984\n",
      "Epoch 76199: reducing learning rate of group 0 to 4.7456e-03.\n",
      "Epoch 76201, Training Loss: 32238, Validation Loss: 51465, 72024.99943709448\n",
      "Epoch 76300: reducing learning rate of group 0 to 4.7408e-03.\n",
      "Epoch 76301, Training Loss: 33986, Validation Loss: 52733, 53997.83556769358\n",
      "Epoch 76401: reducing learning rate of group 0 to 4.7361e-03.\n",
      "Epoch 76401, Training Loss: 33242, Validation Loss: 58178, 55421.81313203989\n",
      "Epoch 76501, Training Loss: 34747, Validation Loss: 55653, 67619.45827866204\n",
      "Epoch 76502: reducing learning rate of group 0 to 4.7314e-03.\n",
      "Epoch 76601, Training Loss: 38043, Validation Loss: 58244, 64927.86579538018\n",
      "Epoch 76603: reducing learning rate of group 0 to 4.7266e-03.\n",
      "Epoch 76701, Training Loss: 32356, Validation Loss: 54709, 71027.0960246169\n",
      "Epoch 76704: reducing learning rate of group 0 to 4.7219e-03.\n",
      "Epoch 76801, Training Loss: 35787, Validation Loss: 56103, 71763.6870024611\n",
      "Epoch 76805: reducing learning rate of group 0 to 4.7172e-03.\n",
      "Epoch 76901, Training Loss: 32028, Validation Loss: 50705, 69894.61959850202\n",
      "Epoch 76906: reducing learning rate of group 0 to 4.7125e-03.\n",
      "Epoch 77001, Training Loss: 33308, Validation Loss: 50488, 59810.57068786363\n",
      "Epoch 77007: reducing learning rate of group 0 to 4.7077e-03.\n",
      "Epoch 77101, Training Loss: 33046, Validation Loss: 54921, 52103.06559495718\n",
      "Epoch 77108: reducing learning rate of group 0 to 4.7030e-03.\n",
      "Epoch 77201, Training Loss: 32785, Validation Loss: 55518, 57318.145224936656\n",
      "Epoch 77209: reducing learning rate of group 0 to 4.6983e-03.\n",
      "Epoch 77301, Training Loss: 33294, Validation Loss: 52562, 56131.75733870429\n",
      "Epoch 77310: reducing learning rate of group 0 to 4.6936e-03.\n",
      "Epoch 77401, Training Loss: 32521, Validation Loss: 52198, 50023.45133996982\n",
      "Epoch 77411: reducing learning rate of group 0 to 4.6889e-03.\n",
      "Epoch 77501, Training Loss: 31321, Validation Loss: 57098, 62385.6502863492\n",
      "Epoch 77512: reducing learning rate of group 0 to 4.6843e-03.\n",
      "Epoch 77601, Training Loss: 32760, Validation Loss: 53811, 61379.45351431998\n",
      "Epoch 77613: reducing learning rate of group 0 to 4.6796e-03.\n",
      "Epoch 77701, Training Loss: 34734, Validation Loss: 53493, 59088.76530123632\n",
      "Epoch 77714: reducing learning rate of group 0 to 4.6749e-03.\n",
      "Epoch 77801, Training Loss: 34907, Validation Loss: 52126, 56536.68250992154\n",
      "Epoch 77815: reducing learning rate of group 0 to 4.6702e-03.\n",
      "Epoch 77901, Training Loss: 32916, Validation Loss: 55413, 67550.5992071777\n",
      "Epoch 77916: reducing learning rate of group 0 to 4.6655e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78001, Training Loss: 32822, Validation Loss: 56847, 58683.768501727995\n",
      "Epoch 78017: reducing learning rate of group 0 to 4.6609e-03.\n",
      "Epoch 78101, Training Loss: 34572, Validation Loss: 52776, 76518.2906080673\n",
      "Epoch 78118: reducing learning rate of group 0 to 4.6562e-03.\n",
      "Epoch 78201, Training Loss: 30519, Validation Loss: 54947, 69305.09549236\n",
      "Epoch 78219: reducing learning rate of group 0 to 4.6516e-03.\n",
      "Epoch 78301, Training Loss: 31141, Validation Loss: 52257, 63996.46509731697\n",
      "Epoch 78320: reducing learning rate of group 0 to 4.6469e-03.\n",
      "Epoch 78401, Training Loss: 33832, Validation Loss: 51338, 66255.52465805241\n",
      "Epoch 78421: reducing learning rate of group 0 to 4.6423e-03.\n",
      "Epoch 78501, Training Loss: 32765, Validation Loss: 50668, 66723.340684018\n",
      "Epoch 78522: reducing learning rate of group 0 to 4.6376e-03.\n",
      "Epoch 78601, Training Loss: 35595, Validation Loss: 50086, 70267.06267615968\n",
      "Epoch 78623: reducing learning rate of group 0 to 4.6330e-03.\n",
      "Epoch 78701, Training Loss: 34008, Validation Loss: 53394, 76439.48075867815\n",
      "Epoch 78724: reducing learning rate of group 0 to 4.6283e-03.\n",
      "Epoch 78801, Training Loss: 34270, Validation Loss: 55493, 82687.07266741431\n",
      "Epoch 78825: reducing learning rate of group 0 to 4.6237e-03.\n",
      "Epoch 78901, Training Loss: 32778, Validation Loss: 55494, 69529.48172238396\n",
      "Epoch 78926: reducing learning rate of group 0 to 4.6191e-03.\n",
      "Epoch 79001, Training Loss: 32546, Validation Loss: 56478, 57276.903365655075\n",
      "Epoch 79027: reducing learning rate of group 0 to 4.6145e-03.\n",
      "Epoch 79101, Training Loss: 31105, Validation Loss: 52837, 64306.696370473364\n",
      "Epoch 79128: reducing learning rate of group 0 to 4.6099e-03.\n",
      "Epoch 79201, Training Loss: 33438, Validation Loss: 53748, 75103.1691941567\n",
      "Epoch 79229: reducing learning rate of group 0 to 4.6053e-03.\n",
      "Epoch 79301, Training Loss: 31878, Validation Loss: 54878, 64981.34055526202\n",
      "Epoch 79330: reducing learning rate of group 0 to 4.6006e-03.\n",
      "Epoch 79401, Training Loss: 31491, Validation Loss: 58588, 82090.38455991138\n",
      "Epoch 79431: reducing learning rate of group 0 to 4.5960e-03.\n",
      "Epoch 79501, Training Loss: 31086, Validation Loss: 55995, 91531.85259406581\n",
      "Epoch 79532: reducing learning rate of group 0 to 4.5914e-03.\n",
      "Epoch 79601, Training Loss: 33047, Validation Loss: 54484, 62448.53640753446\n",
      "Epoch 79633: reducing learning rate of group 0 to 4.5869e-03.\n",
      "Epoch 79701, Training Loss: 31839, Validation Loss: 49086, 66198.66135690195\n",
      "Epoch 79734: reducing learning rate of group 0 to 4.5823e-03.\n",
      "Epoch 79801, Training Loss: 33388, Validation Loss: 52294, 65326.04766304124\n",
      "Epoch 79835: reducing learning rate of group 0 to 4.5777e-03.\n",
      "Epoch 79901, Training Loss: 32709, Validation Loss: 54629, 72437.33978816694\n",
      "Epoch 79936: reducing learning rate of group 0 to 4.5731e-03.\n",
      "Epoch 80001, Training Loss: 33676, Validation Loss: 53937, 65656.08684232969\n",
      "Epoch 80037: reducing learning rate of group 0 to 4.5685e-03.\n",
      "Epoch 80101, Training Loss: 33254, Validation Loss: 51658, 60096.3771760572\n",
      "Epoch 80138: reducing learning rate of group 0 to 4.5640e-03.\n",
      "Epoch 80201, Training Loss: 31312, Validation Loss: 53919, 65505.48903678373\n",
      "Epoch 80239: reducing learning rate of group 0 to 4.5594e-03.\n",
      "Epoch 80301, Training Loss: 35185, Validation Loss: 52367, 52805.25277978746\n",
      "Epoch 80340: reducing learning rate of group 0 to 4.5548e-03.\n",
      "Epoch 80401, Training Loss: 33171, Validation Loss: 53594, 79789.18977070313\n",
      "Epoch 80441: reducing learning rate of group 0 to 4.5503e-03.\n",
      "Epoch 80501, Training Loss: 32604, Validation Loss: 54608, 68164.46397106741\n",
      "Epoch 80542: reducing learning rate of group 0 to 4.5457e-03.\n",
      "Epoch 80601, Training Loss: 30316, Validation Loss: 51549, 71107.42751274476\n",
      "Epoch 80643: reducing learning rate of group 0 to 4.5412e-03.\n",
      "Epoch 80701, Training Loss: 32389, Validation Loss: 52741, 74400.93715641937\n",
      "Epoch 80744: reducing learning rate of group 0 to 4.5367e-03.\n",
      "Epoch 80801, Training Loss: 31962, Validation Loss: 52650, 49601.267261688656\n",
      "Epoch 80845: reducing learning rate of group 0 to 4.5321e-03.\n",
      "Epoch 80901, Training Loss: 33303, Validation Loss: 53128, 55480.316383309\n",
      "Epoch 80946: reducing learning rate of group 0 to 4.5276e-03.\n",
      "Epoch 81001, Training Loss: 35395, Validation Loss: 56365, 74544.36546649062\n",
      "Epoch 81047: reducing learning rate of group 0 to 4.5231e-03.\n",
      "Epoch 81101, Training Loss: 32275, Validation Loss: 52829, 68313.81016910057\n",
      "Epoch 81148: reducing learning rate of group 0 to 4.5185e-03.\n",
      "Epoch 81201, Training Loss: 33101, Validation Loss: 54146, 75552.95993546293\n",
      "Epoch 81249: reducing learning rate of group 0 to 4.5140e-03.\n",
      "Epoch 81301, Training Loss: 30121, Validation Loss: 55074, 76823.76961307376\n",
      "Epoch 81350: reducing learning rate of group 0 to 4.5095e-03.\n",
      "Epoch 81401, Training Loss: 33330, Validation Loss: 52379, 65980.89516270699\n",
      "Epoch 81451: reducing learning rate of group 0 to 4.5050e-03.\n",
      "Epoch 81501, Training Loss: 31411, Validation Loss: 52504, 60273.15757145362\n",
      "Epoch 81552: reducing learning rate of group 0 to 4.5005e-03.\n",
      "Epoch 81601, Training Loss: 33981, Validation Loss: 56361, 69206.04739452894\n",
      "Epoch 81653: reducing learning rate of group 0 to 4.4960e-03.\n",
      "Epoch 81701, Training Loss: 32463, Validation Loss: 52643, 65188.5778835781\n",
      "Epoch 81754: reducing learning rate of group 0 to 4.4915e-03.\n",
      "Epoch 81801, Training Loss: 33923, Validation Loss: 54436, 63161.79808145334\n",
      "Epoch 81855: reducing learning rate of group 0 to 4.4870e-03.\n",
      "Epoch 81901, Training Loss: 31357, Validation Loss: 54024, 61574.89331303816\n",
      "Epoch 81956: reducing learning rate of group 0 to 4.4825e-03.\n",
      "Epoch 82001, Training Loss: 34624, Validation Loss: 52732, 69367.61716855783\n",
      "Epoch 82057: reducing learning rate of group 0 to 4.4780e-03.\n",
      "Epoch 82101, Training Loss: 31141, Validation Loss: 58687, 75928.01070278797\n",
      "Epoch 82158: reducing learning rate of group 0 to 4.4736e-03.\n",
      "Epoch 82201, Training Loss: 33076, Validation Loss: 54527, 80333.51517916181\n",
      "Epoch 82259: reducing learning rate of group 0 to 4.4691e-03.\n",
      "Epoch 82301, Training Loss: 31993, Validation Loss: 54420, 64862.23168873757\n",
      "Epoch 82360: reducing learning rate of group 0 to 4.4646e-03.\n",
      "Epoch 82401, Training Loss: 32903, Validation Loss: 54813, 62920.6186878189\n",
      "Epoch 82461: reducing learning rate of group 0 to 4.4601e-03.\n",
      "Epoch 82501, Training Loss: 32341, Validation Loss: 50986, 75046.03046624754\n",
      "Epoch 82562: reducing learning rate of group 0 to 4.4557e-03.\n",
      "Epoch 82601, Training Loss: 32092, Validation Loss: 54941, 61119.52615412488\n",
      "Epoch 82663: reducing learning rate of group 0 to 4.4512e-03.\n",
      "Epoch 82701, Training Loss: 31810, Validation Loss: 57453, 64144.06572338214\n",
      "Epoch 82764: reducing learning rate of group 0 to 4.4468e-03.\n",
      "Epoch 82801, Training Loss: 33176, Validation Loss: 54922, 64876.993997017744\n",
      "Epoch 82865: reducing learning rate of group 0 to 4.4423e-03.\n",
      "Epoch 82901, Training Loss: 34252, Validation Loss: 51995, 63367.670762070826\n",
      "Epoch 82966: reducing learning rate of group 0 to 4.4379e-03.\n",
      "Epoch 83001, Training Loss: 32469, Validation Loss: 54489, 73072.5665820548\n",
      "Epoch 83067: reducing learning rate of group 0 to 4.4335e-03.\n",
      "Epoch 83101, Training Loss: 33084, Validation Loss: 52919, 55686.16452459898\n",
      "Epoch 83168: reducing learning rate of group 0 to 4.4290e-03.\n",
      "Epoch 83201, Training Loss: 33973, Validation Loss: 53572, 78658.32065360353\n",
      "Epoch 83269: reducing learning rate of group 0 to 4.4246e-03.\n",
      "Epoch 83301, Training Loss: 32554, Validation Loss: 53808, 56157.854502093505\n",
      "Epoch 83370: reducing learning rate of group 0 to 4.4202e-03.\n",
      "Epoch 83401, Training Loss: 34534, Validation Loss: 57618, 60514.12158090648\n",
      "Epoch 83471: reducing learning rate of group 0 to 4.4157e-03.\n",
      "Epoch 83501, Training Loss: 30473, Validation Loss: 52289, 59113.71812135957\n",
      "Epoch 83572: reducing learning rate of group 0 to 4.4113e-03.\n",
      "Epoch 83601, Training Loss: 32594, Validation Loss: 55841, 72704.968506086\n",
      "Epoch 83673: reducing learning rate of group 0 to 4.4069e-03.\n",
      "Epoch 83701, Training Loss: 33248, Validation Loss: 54514, 64869.510845650046\n",
      "Epoch 83774: reducing learning rate of group 0 to 4.4025e-03.\n",
      "Epoch 83801, Training Loss: 33078, Validation Loss: 53030, 63527.0019926403\n",
      "Epoch 83875: reducing learning rate of group 0 to 4.3981e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83901, Training Loss: 30626, Validation Loss: 55099, 66218.68833858606\n",
      "Epoch 83976: reducing learning rate of group 0 to 4.3937e-03.\n",
      "Epoch 84001, Training Loss: 33509, Validation Loss: 55807, 64603.65832709686\n",
      "Epoch 84077: reducing learning rate of group 0 to 4.3893e-03.\n",
      "Epoch 84101, Training Loss: 31202, Validation Loss: 51885, 60655.46380089105\n",
      "Epoch 84178: reducing learning rate of group 0 to 4.3849e-03.\n",
      "Epoch 84201, Training Loss: 33942, Validation Loss: 54299, 68741.30567622896\n",
      "Epoch 84279: reducing learning rate of group 0 to 4.3805e-03.\n",
      "Epoch 84301, Training Loss: 33046, Validation Loss: 56358, 62373.314102519544\n",
      "Epoch 84380: reducing learning rate of group 0 to 4.3762e-03.\n",
      "Epoch 84401, Training Loss: 33079, Validation Loss: 54754, 57578.48542533641\n",
      "Epoch 84481: reducing learning rate of group 0 to 4.3718e-03.\n",
      "Epoch 84501, Training Loss: 31932, Validation Loss: 52056, 59970.64782004291\n",
      "Epoch 84582: reducing learning rate of group 0 to 4.3674e-03.\n",
      "Epoch 84601, Training Loss: 32663, Validation Loss: 52807, 67279.5147623427\n",
      "Epoch 84683: reducing learning rate of group 0 to 4.3630e-03.\n",
      "Epoch 84701, Training Loss: 31060, Validation Loss: 53575, 57554.972440431186\n",
      "Epoch 84784: reducing learning rate of group 0 to 4.3587e-03.\n",
      "Epoch 84801, Training Loss: 30177, Validation Loss: 55694, 64378.529493146845\n",
      "Epoch 84885: reducing learning rate of group 0 to 4.3543e-03.\n",
      "Epoch 84901, Training Loss: 29452, Validation Loss: 60446, 67339.47186309045\n",
      "Epoch 84986: reducing learning rate of group 0 to 4.3500e-03.\n",
      "Epoch 85001, Training Loss: 32005, Validation Loss: 51333, 64289.34176417313\n",
      "Epoch 85087: reducing learning rate of group 0 to 4.3456e-03.\n",
      "Epoch 85101, Training Loss: 29948, Validation Loss: 54072, 71957.60595219326\n",
      "Epoch 85188: reducing learning rate of group 0 to 4.3413e-03.\n",
      "Epoch 85201, Training Loss: 33713, Validation Loss: 56800, 74543.47070308989\n",
      "Epoch 85289: reducing learning rate of group 0 to 4.3369e-03.\n",
      "Epoch 85301, Training Loss: 32160, Validation Loss: 51851, 79027.61586775894\n",
      "Epoch 85390: reducing learning rate of group 0 to 4.3326e-03.\n",
      "Epoch 85401, Training Loss: 31872, Validation Loss: 54662, 77400.30964587336\n",
      "Epoch 85491: reducing learning rate of group 0 to 4.3283e-03.\n",
      "Epoch 85501, Training Loss: 32195, Validation Loss: 55642, 82172.79399368927\n",
      "Epoch 85592: reducing learning rate of group 0 to 4.3239e-03.\n",
      "Epoch 85601, Training Loss: 35422, Validation Loss: 56340, 54511.07944952117\n",
      "Epoch 85693: reducing learning rate of group 0 to 4.3196e-03.\n",
      "Epoch 85701, Training Loss: 32809, Validation Loss: 53348, 70708.77801827935\n",
      "Epoch 85794: reducing learning rate of group 0 to 4.3153e-03.\n",
      "Epoch 85801, Training Loss: 33673, Validation Loss: 51634, 59868.23607478261\n",
      "Epoch 85895: reducing learning rate of group 0 to 4.3110e-03.\n",
      "Epoch 85901, Training Loss: 33821, Validation Loss: 58328, 76014.94818243844\n",
      "Epoch 85996: reducing learning rate of group 0 to 4.3067e-03.\n",
      "Epoch 86001, Training Loss: 31815, Validation Loss: 54194, 55821.408906319535\n",
      "Epoch 86097: reducing learning rate of group 0 to 4.3024e-03.\n",
      "Epoch 86101, Training Loss: 33475, Validation Loss: 54403, 59627.67998516796\n",
      "Epoch 86198: reducing learning rate of group 0 to 4.2981e-03.\n",
      "Epoch 86201, Training Loss: 32773, Validation Loss: 53809, 62724.34699648537\n",
      "Epoch 86299: reducing learning rate of group 0 to 4.2938e-03.\n",
      "Epoch 86301, Training Loss: 33865, Validation Loss: 51461, 72450.87019505964\n",
      "Epoch 86400: reducing learning rate of group 0 to 4.2895e-03.\n",
      "Epoch 86401, Training Loss: 32501, Validation Loss: 54913, 74072.19506142683\n",
      "Epoch 86501: reducing learning rate of group 0 to 4.2852e-03.\n",
      "Epoch 86501, Training Loss: 31427, Validation Loss: 53974, 54449.85318893287\n",
      "Epoch 86601, Training Loss: 32372, Validation Loss: 52230, 68030.86431392016\n",
      "Epoch 86602: reducing learning rate of group 0 to 4.2809e-03.\n",
      "Epoch 86701, Training Loss: 37142, Validation Loss: 58236, 75302.80865569755\n",
      "Epoch 86703: reducing learning rate of group 0 to 4.2766e-03.\n",
      "Epoch 86801, Training Loss: 31515, Validation Loss: 55147, 68879.26260127702\n",
      "Epoch 86804: reducing learning rate of group 0 to 4.2723e-03.\n",
      "Epoch 86901, Training Loss: 31678, Validation Loss: 57228, 62234.67979879994\n",
      "Epoch 86905: reducing learning rate of group 0 to 4.2681e-03.\n",
      "Epoch 87001, Training Loss: 34463, Validation Loss: 52728, 66574.4125659605\n",
      "Epoch 87006: reducing learning rate of group 0 to 4.2638e-03.\n",
      "Epoch 87101, Training Loss: 31324, Validation Loss: 56076, 71898.73758690563\n",
      "Epoch 87107: reducing learning rate of group 0 to 4.2595e-03.\n",
      "Epoch 87201, Training Loss: 33069, Validation Loss: 53053, 76430.1761247613\n",
      "Epoch 87208: reducing learning rate of group 0 to 4.2553e-03.\n",
      "Epoch 87301, Training Loss: 31617, Validation Loss: 55980, 72215.3724863139\n",
      "Epoch 87309: reducing learning rate of group 0 to 4.2510e-03.\n",
      "Epoch 87401, Training Loss: 32384, Validation Loss: 50660, 59570.94439794874\n",
      "Epoch 87410: reducing learning rate of group 0 to 4.2468e-03.\n",
      "Epoch 87501, Training Loss: 30706, Validation Loss: 52400, 68908.74680584072\n",
      "Epoch 87511: reducing learning rate of group 0 to 4.2425e-03.\n",
      "Epoch 87601, Training Loss: 33381, Validation Loss: 55774, 70268.62851786154\n",
      "Epoch 87612: reducing learning rate of group 0 to 4.2383e-03.\n",
      "Epoch 87701, Training Loss: 29821, Validation Loss: 54235, 58813.67150014874\n",
      "Epoch 87713: reducing learning rate of group 0 to 4.2340e-03.\n",
      "Epoch 87801, Training Loss: 31943, Validation Loss: 53667, 64683.85075433753\n",
      "Epoch 87814: reducing learning rate of group 0 to 4.2298e-03.\n",
      "Epoch 87901, Training Loss: 31330, Validation Loss: 59604, 69428.01472408361\n",
      "Epoch 87915: reducing learning rate of group 0 to 4.2256e-03.\n",
      "Epoch 88001, Training Loss: 33291, Validation Loss: 51900, 55240.73483583661\n",
      "Epoch 88016: reducing learning rate of group 0 to 4.2213e-03.\n",
      "Epoch 88101, Training Loss: 36814, Validation Loss: 55411, 60843.40372180855\n",
      "Epoch 88117: reducing learning rate of group 0 to 4.2171e-03.\n",
      "Epoch 88201, Training Loss: 30509, Validation Loss: 52613, 66110.99735796482\n",
      "Epoch 88218: reducing learning rate of group 0 to 4.2129e-03.\n",
      "Epoch 88301, Training Loss: 34428, Validation Loss: 52698, 75545.19084983236\n",
      "Epoch 88319: reducing learning rate of group 0 to 4.2087e-03.\n",
      "Epoch 88401, Training Loss: 34860, Validation Loss: 51878, 74810.62833502483\n",
      "Epoch 88420: reducing learning rate of group 0 to 4.2045e-03.\n",
      "Epoch 88501, Training Loss: 30753, Validation Loss: 52960, 76329.64014165818\n",
      "Epoch 88521: reducing learning rate of group 0 to 4.2003e-03.\n",
      "Epoch 88601, Training Loss: 32132, Validation Loss: 54346, 61726.88818250375\n",
      "Epoch 88622: reducing learning rate of group 0 to 4.1961e-03.\n",
      "Epoch 88701, Training Loss: 31919, Validation Loss: 52526, 54945.096957715985\n",
      "Epoch 88723: reducing learning rate of group 0 to 4.1919e-03.\n",
      "Epoch 88801, Training Loss: 30085, Validation Loss: 55473, 68317.85505172303\n",
      "Epoch 88824: reducing learning rate of group 0 to 4.1877e-03.\n",
      "Epoch 88901, Training Loss: 32374, Validation Loss: 52019, 53629.625802482806\n",
      "Epoch 88925: reducing learning rate of group 0 to 4.1835e-03.\n",
      "Epoch 89001, Training Loss: 32874, Validation Loss: 61560, 64952.08303396171\n",
      "Epoch 89026: reducing learning rate of group 0 to 4.1793e-03.\n",
      "Epoch 89101, Training Loss: 34211, Validation Loss: 54446, 61897.01365434646\n",
      "Epoch 89127: reducing learning rate of group 0 to 4.1751e-03.\n",
      "Epoch 89201, Training Loss: 31634, Validation Loss: 56845, 61947.4341145553\n",
      "Epoch 89228: reducing learning rate of group 0 to 4.1710e-03.\n",
      "Epoch 89301, Training Loss: 33347, Validation Loss: 56843, 68500.22113058194\n",
      "Epoch 89329: reducing learning rate of group 0 to 4.1668e-03.\n",
      "Epoch 89401, Training Loss: 34423, Validation Loss: 54103, 79156.05537565761\n",
      "Epoch 89430: reducing learning rate of group 0 to 4.1626e-03.\n",
      "Epoch 89501, Training Loss: 31804, Validation Loss: 57222, 63257.65891882739\n",
      "Epoch 89531: reducing learning rate of group 0 to 4.1585e-03.\n",
      "Epoch 89601, Training Loss: 34184, Validation Loss: 60360, 62401.65103576045\n",
      "Epoch 89632: reducing learning rate of group 0 to 4.1543e-03.\n",
      "Epoch 89701, Training Loss: 32829, Validation Loss: 53347, 60569.01685266864\n",
      "Epoch 89733: reducing learning rate of group 0 to 4.1502e-03.\n",
      "Epoch 89801, Training Loss: 31003, Validation Loss: 54764, 71909.36495954978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89834: reducing learning rate of group 0 to 4.1460e-03.\n",
      "Epoch 89901, Training Loss: 32631, Validation Loss: 58366, 61910.76591765928\n",
      "Epoch 89935: reducing learning rate of group 0 to 4.1419e-03.\n",
      "Epoch 90001, Training Loss: 35011, Validation Loss: 53706, 62983.56815033554\n",
      "Epoch 90036: reducing learning rate of group 0 to 4.1377e-03.\n",
      "Epoch 90101, Training Loss: 31204, Validation Loss: 63488, 69305.63499106595\n",
      "Epoch 90137: reducing learning rate of group 0 to 4.1336e-03.\n",
      "Epoch 90201, Training Loss: 33170, Validation Loss: 52879, 68712.46946540078\n",
      "Epoch 90238: reducing learning rate of group 0 to 4.1294e-03.\n",
      "Epoch 90301, Training Loss: 32199, Validation Loss: 54629, 72351.62787029971\n",
      "Epoch 90339: reducing learning rate of group 0 to 4.1253e-03.\n",
      "Epoch 90401, Training Loss: 30934, Validation Loss: 52230, 70952.76992050956\n",
      "Epoch 90440: reducing learning rate of group 0 to 4.1212e-03.\n",
      "Epoch 90501, Training Loss: 32933, Validation Loss: 56565, 67648.2540689535\n",
      "Epoch 90541: reducing learning rate of group 0 to 4.1171e-03.\n",
      "Epoch 90601, Training Loss: 33055, Validation Loss: 51250, 81896.2646042526\n",
      "Epoch 90642: reducing learning rate of group 0 to 4.1130e-03.\n",
      "Epoch 90701, Training Loss: 30826, Validation Loss: 56180, 72269.7369863621\n",
      "Epoch 90743: reducing learning rate of group 0 to 4.1088e-03.\n",
      "Epoch 90801, Training Loss: 33540, Validation Loss: 51865, 58899.75126520132\n",
      "Epoch 90844: reducing learning rate of group 0 to 4.1047e-03.\n",
      "Epoch 90901, Training Loss: 31427, Validation Loss: 52901, 65694.58852653371\n",
      "Epoch 90945: reducing learning rate of group 0 to 4.1006e-03.\n",
      "Epoch 91001, Training Loss: 33557, Validation Loss: 51814, 60356.18282483685\n",
      "Epoch 91046: reducing learning rate of group 0 to 4.0965e-03.\n",
      "Epoch 91101, Training Loss: 31045, Validation Loss: 56699, 64819.28991616998\n",
      "Epoch 91147: reducing learning rate of group 0 to 4.0924e-03.\n",
      "Epoch 91201, Training Loss: 30600, Validation Loss: 54739, 59668.44842563604\n",
      "Epoch 91248: reducing learning rate of group 0 to 4.0883e-03.\n",
      "Epoch 91301, Training Loss: 33351, Validation Loss: 51504, 64006.251008025756\n",
      "Epoch 91349: reducing learning rate of group 0 to 4.0842e-03.\n",
      "Epoch 91401, Training Loss: 33907, Validation Loss: 53970, 69375.95065466237\n",
      "Epoch 91450: reducing learning rate of group 0 to 4.0802e-03.\n",
      "Epoch 91501, Training Loss: 31040, Validation Loss: 54125, 65208.163074620184\n",
      "Epoch 91551: reducing learning rate of group 0 to 4.0761e-03.\n",
      "Epoch 91601, Training Loss: 31117, Validation Loss: 54072, 55143.415756559116\n",
      "Epoch 91652: reducing learning rate of group 0 to 4.0720e-03.\n",
      "Epoch 91701, Training Loss: 32492, Validation Loss: 53058, 60696.6568334315\n",
      "Epoch 91753: reducing learning rate of group 0 to 4.0679e-03.\n",
      "Epoch 91801, Training Loss: 32780, Validation Loss: 56554, 74114.27008866427\n",
      "Epoch 91854: reducing learning rate of group 0 to 4.0639e-03.\n",
      "Epoch 91901, Training Loss: 35262, Validation Loss: 51742, 63994.127512241925\n",
      "Epoch 91955: reducing learning rate of group 0 to 4.0598e-03.\n",
      "Epoch 92001, Training Loss: 32958, Validation Loss: 53587, 69840.62000526565\n",
      "Epoch 92056: reducing learning rate of group 0 to 4.0557e-03.\n",
      "Epoch 92101, Training Loss: 32126, Validation Loss: 55923, 53088.10740558297\n",
      "Epoch 92157: reducing learning rate of group 0 to 4.0517e-03.\n",
      "Epoch 92201, Training Loss: 32761, Validation Loss: 58018, 69951.99518321571\n",
      "Epoch 92258: reducing learning rate of group 0 to 4.0476e-03.\n",
      "Epoch 92301, Training Loss: 31813, Validation Loss: 56580, 56665.39653058061\n",
      "Epoch 92359: reducing learning rate of group 0 to 4.0436e-03.\n",
      "Epoch 92401, Training Loss: 33234, Validation Loss: 55108, 60525.06421880544\n",
      "Epoch 92460: reducing learning rate of group 0 to 4.0395e-03.\n",
      "Epoch 92501, Training Loss: 31484, Validation Loss: 55251, 61706.71185638849\n",
      "Epoch 92561: reducing learning rate of group 0 to 4.0355e-03.\n",
      "Epoch 92601, Training Loss: 32961, Validation Loss: 54170, 66536.15163959411\n",
      "Epoch 92662: reducing learning rate of group 0 to 4.0315e-03.\n",
      "Epoch 92701, Training Loss: 31839, Validation Loss: 49881, 58592.08225698257\n",
      "Epoch 92763: reducing learning rate of group 0 to 4.0274e-03.\n",
      "Epoch 92801, Training Loss: 34015, Validation Loss: 54891, 77254.76088802105\n",
      "Epoch 92864: reducing learning rate of group 0 to 4.0234e-03.\n",
      "Epoch 92901, Training Loss: 31537, Validation Loss: 54207, 66321.90460529116\n",
      "Epoch 92965: reducing learning rate of group 0 to 4.0194e-03.\n",
      "Epoch 93001, Training Loss: 30960, Validation Loss: 53233, 79917.37068552924\n",
      "Epoch 93066: reducing learning rate of group 0 to 4.0154e-03.\n",
      "Epoch 93101, Training Loss: 32092, Validation Loss: 52850, 75497.12773597288\n",
      "Epoch 93167: reducing learning rate of group 0 to 4.0114e-03.\n",
      "Epoch 93201, Training Loss: 30570, Validation Loss: 52448, 57691.959342308946\n",
      "Epoch 93268: reducing learning rate of group 0 to 4.0073e-03.\n",
      "Epoch 93301, Training Loss: 32409, Validation Loss: 57961, 63512.42244807032\n",
      "Epoch 93369: reducing learning rate of group 0 to 4.0033e-03.\n",
      "Epoch 93401, Training Loss: 30963, Validation Loss: 54355, 80982.92334287084\n",
      "Epoch 93470: reducing learning rate of group 0 to 3.9993e-03.\n",
      "Epoch 93501, Training Loss: 30960, Validation Loss: 55448, 63410.74563887648\n",
      "Epoch 93571: reducing learning rate of group 0 to 3.9953e-03.\n",
      "Epoch 93601, Training Loss: 32301, Validation Loss: 53247, 53413.906736269804\n",
      "Epoch 93672: reducing learning rate of group 0 to 3.9913e-03.\n",
      "Epoch 93701, Training Loss: 30055, Validation Loss: 53945, 70573.75497277697\n",
      "Epoch 93773: reducing learning rate of group 0 to 3.9873e-03.\n",
      "Epoch 93801, Training Loss: 31947, Validation Loss: 57261, 79528.8505352445\n",
      "Epoch 93874: reducing learning rate of group 0 to 3.9834e-03.\n",
      "Epoch 93901, Training Loss: 33214, Validation Loss: 56978, 66497.11052249673\n",
      "Epoch 93975: reducing learning rate of group 0 to 3.9794e-03.\n",
      "Epoch 94001, Training Loss: 30053, Validation Loss: 56783, 54877.28942719944\n",
      "Epoch 94076: reducing learning rate of group 0 to 3.9754e-03.\n",
      "Epoch 94101, Training Loss: 31744, Validation Loss: 55473, 55274.2793715075\n",
      "Epoch 94177: reducing learning rate of group 0 to 3.9714e-03.\n",
      "Epoch 94201, Training Loss: 29738, Validation Loss: 57811, 64220.844273838215\n",
      "Epoch 94278: reducing learning rate of group 0 to 3.9674e-03.\n",
      "Epoch 94301, Training Loss: 31412, Validation Loss: 51708, 63752.380481164415\n",
      "Epoch 94379: reducing learning rate of group 0 to 3.9635e-03.\n",
      "Epoch 94401, Training Loss: 34456, Validation Loss: 56030, 65666.25346107785\n",
      "Epoch 94480: reducing learning rate of group 0 to 3.9595e-03.\n",
      "Epoch 94501, Training Loss: 32529, Validation Loss: 54908, 68800.75035004137\n",
      "Epoch 94581: reducing learning rate of group 0 to 3.9556e-03.\n",
      "Epoch 94601, Training Loss: 31089, Validation Loss: 55138, 65144.948017882125\n",
      "Epoch 94682: reducing learning rate of group 0 to 3.9516e-03.\n",
      "Epoch 94701, Training Loss: 34255, Validation Loss: 54887, 66415.08942908775\n",
      "Epoch 94783: reducing learning rate of group 0 to 3.9476e-03.\n",
      "Epoch 94801, Training Loss: 32540, Validation Loss: 55712, 70295.53009476366\n",
      "Epoch 94884: reducing learning rate of group 0 to 3.9437e-03.\n",
      "Epoch 94901, Training Loss: 31601, Validation Loss: 55592, 62648.812243802626\n",
      "Epoch 94985: reducing learning rate of group 0 to 3.9398e-03.\n",
      "Epoch 95001, Training Loss: 31062, Validation Loss: 53799, 66299.31813406154\n",
      "Epoch 95086: reducing learning rate of group 0 to 3.9358e-03.\n",
      "Epoch 95101, Training Loss: 32405, Validation Loss: 55225, 62791.636086671766\n",
      "Epoch 95187: reducing learning rate of group 0 to 3.9319e-03.\n",
      "Epoch 95201, Training Loss: 31747, Validation Loss: 51388, 51508.55264246623\n",
      "Epoch 95288: reducing learning rate of group 0 to 3.9280e-03.\n",
      "Epoch 95301, Training Loss: 31947, Validation Loss: 53023, 66988.86163840724\n",
      "Epoch 95389: reducing learning rate of group 0 to 3.9240e-03.\n",
      "Epoch 95401, Training Loss: 29986, Validation Loss: 52511, 59777.89641041347\n",
      "Epoch 95490: reducing learning rate of group 0 to 3.9201e-03.\n",
      "Epoch 95501, Training Loss: 31574, Validation Loss: 54306, 56962.66124900124\n",
      "Epoch 95591: reducing learning rate of group 0 to 3.9162e-03.\n",
      "Epoch 95601, Training Loss: 30299, Validation Loss: 52977, 62208.68906888383\n",
      "Epoch 95692: reducing learning rate of group 0 to 3.9123e-03.\n",
      "Epoch 95701, Training Loss: 29962, Validation Loss: 53278, 63870.26868614999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95793: reducing learning rate of group 0 to 3.9083e-03.\n",
      "Epoch 95801, Training Loss: 31788, Validation Loss: 56141, 52730.6051606747\n",
      "Epoch 95894: reducing learning rate of group 0 to 3.9044e-03.\n",
      "Epoch 95901, Training Loss: 30523, Validation Loss: 52317, 63457.1979601148\n",
      "Epoch 95995: reducing learning rate of group 0 to 3.9005e-03.\n",
      "Epoch 96001, Training Loss: 31798, Validation Loss: 53194, 71585.43985561481\n",
      "Epoch 96096: reducing learning rate of group 0 to 3.8966e-03.\n",
      "Epoch 96101, Training Loss: 29817, Validation Loss: 54053, 87691.6013138094\n",
      "Epoch 96197: reducing learning rate of group 0 to 3.8927e-03.\n",
      "Epoch 96201, Training Loss: 31171, Validation Loss: 52559, 69567.19447598627\n",
      "Epoch 96298: reducing learning rate of group 0 to 3.8888e-03.\n",
      "Epoch 96301, Training Loss: 31550, Validation Loss: 53534, 57187.09874566421\n",
      "Epoch 96399: reducing learning rate of group 0 to 3.8850e-03.\n",
      "Epoch 96401, Training Loss: 33203, Validation Loss: 52844, 59373.92942390669\n",
      "Epoch 96500: reducing learning rate of group 0 to 3.8811e-03.\n",
      "Epoch 96501, Training Loss: 34099, Validation Loss: 51379, 61145.455946097674\n",
      "Epoch 96601: reducing learning rate of group 0 to 3.8772e-03.\n",
      "Epoch 96601, Training Loss: 29468, Validation Loss: 53875, 62142.59380407166\n",
      "Epoch 96701, Training Loss: 31093, Validation Loss: 53705, 66466.83275240335\n",
      "Epoch 96702: reducing learning rate of group 0 to 3.8733e-03.\n",
      "Epoch 96801, Training Loss: 31401, Validation Loss: 55447, 52180.13808891142\n",
      "Epoch 96803: reducing learning rate of group 0 to 3.8694e-03.\n",
      "Epoch 96901, Training Loss: 31393, Validation Loss: 56345, 65309.16915340023\n",
      "Epoch 96904: reducing learning rate of group 0 to 3.8656e-03.\n",
      "Epoch 97001, Training Loss: 32889, Validation Loss: 52365, 72304.69371294043\n",
      "Epoch 97005: reducing learning rate of group 0 to 3.8617e-03.\n",
      "Epoch 97101, Training Loss: 33420, Validation Loss: 53884, 60183.49912905646\n",
      "Epoch 97106: reducing learning rate of group 0 to 3.8578e-03.\n",
      "Epoch 97201, Training Loss: 32528, Validation Loss: 52709, 60070.15680481715\n",
      "Epoch 97207: reducing learning rate of group 0 to 3.8540e-03.\n",
      "Epoch 97301, Training Loss: 35973, Validation Loss: 51369, 61013.175006111815\n",
      "Epoch 97308: reducing learning rate of group 0 to 3.8501e-03.\n",
      "Epoch 97401, Training Loss: 30907, Validation Loss: 51856, 53069.70817301346\n",
      "Epoch 97409: reducing learning rate of group 0 to 3.8463e-03.\n",
      "Epoch 97501, Training Loss: 31871, Validation Loss: 53646, 66452.88742687531\n",
      "Epoch 97510: reducing learning rate of group 0 to 3.8424e-03.\n",
      "Epoch 97601, Training Loss: 31000, Validation Loss: 53880, 76154.30394632797\n",
      "Epoch 97611: reducing learning rate of group 0 to 3.8386e-03.\n",
      "Epoch 97701, Training Loss: 33781, Validation Loss: 52471, 65214.77090099497\n",
      "Epoch 97712: reducing learning rate of group 0 to 3.8348e-03.\n",
      "Epoch 97801, Training Loss: 31554, Validation Loss: 55640, 66444.54414362994\n",
      "Epoch 97813: reducing learning rate of group 0 to 3.8309e-03.\n",
      "Epoch 97901, Training Loss: 31633, Validation Loss: 52195, 71061.76461949896\n",
      "Epoch 97914: reducing learning rate of group 0 to 3.8271e-03.\n",
      "Epoch 98001, Training Loss: 37120, Validation Loss: 54445, 69779.38600576836\n",
      "Epoch 98015: reducing learning rate of group 0 to 3.8233e-03.\n",
      "Epoch 98101, Training Loss: 32916, Validation Loss: 51606, 53596.164832109\n",
      "Epoch 98116: reducing learning rate of group 0 to 3.8194e-03.\n",
      "Epoch 98201, Training Loss: 31417, Validation Loss: 53825, 85612.28416198753\n",
      "Epoch 98217: reducing learning rate of group 0 to 3.8156e-03.\n",
      "Epoch 98301, Training Loss: 29646, Validation Loss: 55326, 83740.38005802776\n",
      "Epoch 98318: reducing learning rate of group 0 to 3.8118e-03.\n",
      "Epoch 98401, Training Loss: 28417, Validation Loss: 56390, 65404.01675880867\n",
      "Epoch 98419: reducing learning rate of group 0 to 3.8080e-03.\n",
      "Epoch 98501, Training Loss: 28820, Validation Loss: 53409, 68022.35501393641\n",
      "Epoch 98520: reducing learning rate of group 0 to 3.8042e-03.\n",
      "Epoch 98601, Training Loss: 31745, Validation Loss: 54078, 102011.8661629411\n",
      "Epoch 98621: reducing learning rate of group 0 to 3.8004e-03.\n",
      "Epoch 98701, Training Loss: 37945, Validation Loss: 59654, 76602.85851167676\n",
      "Epoch 98722: reducing learning rate of group 0 to 3.7966e-03.\n",
      "Epoch 98801, Training Loss: 31657, Validation Loss: 53449, 66955.32286628125\n",
      "Epoch 98823: reducing learning rate of group 0 to 3.7928e-03.\n",
      "Epoch 98901, Training Loss: 30565, Validation Loss: 51897, 56730.59859243804\n",
      "Epoch 98924: reducing learning rate of group 0 to 3.7890e-03.\n",
      "Epoch 99001, Training Loss: 32032, Validation Loss: 51850, 70281.97894085787\n",
      "Epoch 99025: reducing learning rate of group 0 to 3.7852e-03.\n",
      "Epoch 99101, Training Loss: 32959, Validation Loss: 54624, 61009.21253766401\n",
      "Epoch 99126: reducing learning rate of group 0 to 3.7814e-03.\n",
      "Epoch 99201, Training Loss: 32369, Validation Loss: 53034, 79375.59250583575\n",
      "Epoch 99227: reducing learning rate of group 0 to 3.7776e-03.\n",
      "Epoch 99301, Training Loss: 34153, Validation Loss: 52428, 71158.36454138078\n",
      "Epoch 99328: reducing learning rate of group 0 to 3.7739e-03.\n",
      "Epoch 99401, Training Loss: 29379, Validation Loss: 57980, 65199.64901934962\n",
      "Epoch 99429: reducing learning rate of group 0 to 3.7701e-03.\n",
      "Epoch 99501, Training Loss: 29574, Validation Loss: 55434, 75938.48969654775\n",
      "Epoch 99530: reducing learning rate of group 0 to 3.7663e-03.\n",
      "Epoch 99601, Training Loss: 33693, Validation Loss: 55619, 82478.35192081898\n",
      "Epoch 99631: reducing learning rate of group 0 to 3.7625e-03.\n",
      "Epoch 99701, Training Loss: 31964, Validation Loss: 52725, 68201.79021501871\n",
      "Epoch 99732: reducing learning rate of group 0 to 3.7588e-03.\n",
      "Epoch 99801, Training Loss: 31265, Validation Loss: 56000, 61366.363431487\n",
      "Epoch 99833: reducing learning rate of group 0 to 3.7550e-03.\n",
      "Epoch 99901, Training Loss: 32221, Validation Loss: 52974, 68013.96318347439\n",
      "Epoch 99934: reducing learning rate of group 0 to 3.7513e-03.\n",
      "Epoch 100001, Training Loss: 31424, Validation Loss: 59660, 64986.35601799556\n",
      "Epoch 100035: reducing learning rate of group 0 to 3.7475e-03.\n",
      "Epoch 100101, Training Loss: 31228, Validation Loss: 54294, 68564.68257444692\n",
      "Epoch 100136: reducing learning rate of group 0 to 3.7438e-03.\n",
      "Epoch 100201, Training Loss: 31532, Validation Loss: 51822, 72669.09944222018\n",
      "Epoch 100237: reducing learning rate of group 0 to 3.7400e-03.\n",
      "Epoch 100301, Training Loss: 32123, Validation Loss: 57017, 78004.50775456302\n",
      "Epoch 100338: reducing learning rate of group 0 to 3.7363e-03.\n",
      "Epoch 100401, Training Loss: 33085, Validation Loss: 54644, 63809.48784824432\n",
      "Epoch 100439: reducing learning rate of group 0 to 3.7326e-03.\n",
      "Epoch 100501, Training Loss: 29598, Validation Loss: 56390, 68930.94659305127\n",
      "Epoch 100540: reducing learning rate of group 0 to 3.7288e-03.\n",
      "Epoch 100601, Training Loss: 31411, Validation Loss: 52914, 55176.98353277225\n",
      "Epoch 100641: reducing learning rate of group 0 to 3.7251e-03.\n",
      "Epoch 100701, Training Loss: 33853, Validation Loss: 54440, 85281.25433238853\n",
      "Epoch 100742: reducing learning rate of group 0 to 3.7214e-03.\n",
      "Epoch 100801, Training Loss: 32316, Validation Loss: 54859, 71524.50144371264\n",
      "Epoch 100843: reducing learning rate of group 0 to 3.7176e-03.\n",
      "Epoch 100901, Training Loss: 32529, Validation Loss: 54071, 67728.6289188987\n",
      "Epoch 100944: reducing learning rate of group 0 to 3.7139e-03.\n",
      "Epoch 101001, Training Loss: 31365, Validation Loss: 51429, 69073.96093567468\n",
      "Epoch 101045: reducing learning rate of group 0 to 3.7102e-03.\n",
      "Epoch 101101, Training Loss: 30749, Validation Loss: 55120, 64005.06314435962\n",
      "Epoch 101146: reducing learning rate of group 0 to 3.7065e-03.\n",
      "Epoch 101201, Training Loss: 30138, Validation Loss: 54549, 64600.76242059518\n",
      "Epoch 101247: reducing learning rate of group 0 to 3.7028e-03.\n",
      "Epoch 101301, Training Loss: 31783, Validation Loss: 52826, 67664.07898443192\n",
      "Epoch 101348: reducing learning rate of group 0 to 3.6991e-03.\n",
      "Epoch 101401, Training Loss: 30580, Validation Loss: 58552, 69711.47010808886\n",
      "Epoch 101449: reducing learning rate of group 0 to 3.6954e-03.\n",
      "Epoch 101501, Training Loss: 30820, Validation Loss: 54187, 64163.66945433771\n",
      "Epoch 101550: reducing learning rate of group 0 to 3.6917e-03.\n",
      "Epoch 101601, Training Loss: 35712, Validation Loss: 54853, 75933.43973429996\n",
      "Epoch 101651: reducing learning rate of group 0 to 3.6880e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101701, Training Loss: 31812, Validation Loss: 54678, 71729.71167660438\n",
      "Epoch 101752: reducing learning rate of group 0 to 3.6843e-03.\n",
      "Epoch 101801, Training Loss: 34189, Validation Loss: 51772, 74709.65386942758\n",
      "Epoch 101853: reducing learning rate of group 0 to 3.6806e-03.\n",
      "Epoch 101901, Training Loss: 30993, Validation Loss: 54618, 66949.2713114709\n",
      "Epoch 101954: reducing learning rate of group 0 to 3.6770e-03.\n",
      "Epoch 102001, Training Loss: 30255, Validation Loss: 54391, 81388.47580328066\n",
      "Epoch 102055: reducing learning rate of group 0 to 3.6733e-03.\n",
      "Epoch 102101, Training Loss: 30976, Validation Loss: 56977, 57756.698964445386\n",
      "Epoch 102156: reducing learning rate of group 0 to 3.6696e-03.\n",
      "Epoch 102201, Training Loss: 33844, Validation Loss: 54572, 55938.03768011602\n",
      "Epoch 102257: reducing learning rate of group 0 to 3.6659e-03.\n",
      "Epoch 102301, Training Loss: 31112, Validation Loss: 53584, 60439.64549111729\n",
      "Epoch 102358: reducing learning rate of group 0 to 3.6623e-03.\n",
      "Epoch 102401, Training Loss: 30821, Validation Loss: 54894, 63501.04802450662\n",
      "Epoch 102459: reducing learning rate of group 0 to 3.6586e-03.\n",
      "Epoch 102501, Training Loss: 31027, Validation Loss: 54404, 69070.4887964212\n",
      "Epoch 102560: reducing learning rate of group 0 to 3.6549e-03.\n",
      "Epoch 102601, Training Loss: 34026, Validation Loss: 51566, 83629.49206437908\n",
      "Epoch 102661: reducing learning rate of group 0 to 3.6513e-03.\n",
      "Epoch 102701, Training Loss: 30469, Validation Loss: 50645, 66416.51021249774\n",
      "Epoch 102762: reducing learning rate of group 0 to 3.6476e-03.\n",
      "Epoch 102801, Training Loss: 32426, Validation Loss: 51942, 75750.35226774622\n",
      "Epoch 102863: reducing learning rate of group 0 to 3.6440e-03.\n",
      "Epoch 102901, Training Loss: 33380, Validation Loss: 54023, 85979.93348886528\n",
      "Epoch 102964: reducing learning rate of group 0 to 3.6403e-03.\n",
      "Epoch 103001, Training Loss: 29028, Validation Loss: 55339, 71460.00815169886\n",
      "Epoch 103065: reducing learning rate of group 0 to 3.6367e-03.\n",
      "Epoch 103101, Training Loss: 29336, Validation Loss: 53503, 66474.45290234499\n",
      "Epoch 103166: reducing learning rate of group 0 to 3.6331e-03.\n",
      "Epoch 103201, Training Loss: 32746, Validation Loss: 54379, 69311.55613940458\n",
      "Epoch 103267: reducing learning rate of group 0 to 3.6294e-03.\n",
      "Epoch 103301, Training Loss: 30903, Validation Loss: 53560, 72126.60604857818\n",
      "Epoch 103368: reducing learning rate of group 0 to 3.6258e-03.\n",
      "Epoch 103401, Training Loss: 31840, Validation Loss: 54406, 75326.52191357646\n",
      "Epoch 103469: reducing learning rate of group 0 to 3.6222e-03.\n",
      "Epoch 103501, Training Loss: 33102, Validation Loss: 55581, 63497.96815446516\n",
      "Epoch 103570: reducing learning rate of group 0 to 3.6186e-03.\n",
      "Epoch 103601, Training Loss: 30537, Validation Loss: 52023, 63499.017555941835\n",
      "Epoch 103671: reducing learning rate of group 0 to 3.6149e-03.\n",
      "Epoch 103701, Training Loss: 29546, Validation Loss: 54529, 69164.90419889436\n",
      "Epoch 103772: reducing learning rate of group 0 to 3.6113e-03.\n",
      "Epoch 103801, Training Loss: 30306, Validation Loss: 57100, 64325.12159431671\n",
      "Epoch 103873: reducing learning rate of group 0 to 3.6077e-03.\n",
      "Epoch 103901, Training Loss: 31153, Validation Loss: 56288, 63141.97094401501\n",
      "Epoch 103974: reducing learning rate of group 0 to 3.6041e-03.\n",
      "Epoch 104001, Training Loss: 32353, Validation Loss: 55928, 76611.22307428982\n",
      "Epoch 104075: reducing learning rate of group 0 to 3.6005e-03.\n",
      "Epoch 104101, Training Loss: 31311, Validation Loss: 52861, 70774.10658427696\n",
      "Epoch 104176: reducing learning rate of group 0 to 3.5969e-03.\n",
      "Epoch 104201, Training Loss: 30685, Validation Loss: 52703, 73483.02781420598\n",
      "Epoch 104277: reducing learning rate of group 0 to 3.5933e-03.\n",
      "Epoch 104301, Training Loss: 31033, Validation Loss: 51217, 98754.38506293454\n",
      "Epoch 104378: reducing learning rate of group 0 to 3.5897e-03.\n",
      "Epoch 104401, Training Loss: 31234, Validation Loss: 53041, 75614.82225765761\n",
      "Epoch 104479: reducing learning rate of group 0 to 3.5861e-03.\n",
      "Epoch 104501, Training Loss: 29634, Validation Loss: 53496, 63011.23598655633\n",
      "Epoch 104580: reducing learning rate of group 0 to 3.5825e-03.\n",
      "Epoch 104601, Training Loss: 35277, Validation Loss: 54377, 75484.53951596764\n",
      "Epoch 104681: reducing learning rate of group 0 to 3.5790e-03.\n",
      "Epoch 104701, Training Loss: 31793, Validation Loss: 54413, 76297.33868091325\n",
      "Epoch 104782: reducing learning rate of group 0 to 3.5754e-03.\n",
      "Epoch 104801, Training Loss: 30624, Validation Loss: 57791, 71511.68867981371\n",
      "Epoch 104883: reducing learning rate of group 0 to 3.5718e-03.\n",
      "Epoch 104901, Training Loss: 32425, Validation Loss: 54866, 59674.175498374614\n",
      "Epoch 104984: reducing learning rate of group 0 to 3.5682e-03.\n",
      "Epoch 105001, Training Loss: 29777, Validation Loss: 50673, 87060.24653152516\n",
      "Epoch 105085: reducing learning rate of group 0 to 3.5647e-03.\n",
      "Epoch 105101, Training Loss: 34688, Validation Loss: 53900, 62249.471125808195\n",
      "Epoch 105186: reducing learning rate of group 0 to 3.5611e-03.\n",
      "Epoch 105201, Training Loss: 31738, Validation Loss: 53813, 73723.20400912927\n",
      "Epoch 105287: reducing learning rate of group 0 to 3.5575e-03.\n",
      "Epoch 105301, Training Loss: 32833, Validation Loss: 54997, 69025.64395251324\n",
      "Epoch 105388: reducing learning rate of group 0 to 3.5540e-03.\n",
      "Epoch 105401, Training Loss: 31713, Validation Loss: 53352, 65280.49904033289\n",
      "Epoch 105489: reducing learning rate of group 0 to 3.5504e-03.\n",
      "Epoch 105501, Training Loss: 30098, Validation Loss: 51664, 65795.13373783852\n",
      "Epoch 105590: reducing learning rate of group 0 to 3.5469e-03.\n",
      "Epoch 105601, Training Loss: 30441, Validation Loss: 53279, 69997.8696618515\n",
      "Epoch 105691: reducing learning rate of group 0 to 3.5433e-03.\n",
      "Epoch 105701, Training Loss: 31237, Validation Loss: 52807, 70463.23400629577\n",
      "Epoch 105792: reducing learning rate of group 0 to 3.5398e-03.\n",
      "Epoch 105801, Training Loss: 28996, Validation Loss: 53212, 67919.12934039558\n",
      "Epoch 105893: reducing learning rate of group 0 to 3.5362e-03.\n",
      "Epoch 105901, Training Loss: 30948, Validation Loss: 51072, 51199.97016285668\n",
      "Epoch 105994: reducing learning rate of group 0 to 3.5327e-03.\n",
      "Epoch 106001, Training Loss: 29864, Validation Loss: 53016, 71708.88088309353\n",
      "Epoch 106095: reducing learning rate of group 0 to 3.5292e-03.\n",
      "Epoch 106101, Training Loss: 31928, Validation Loss: 51563, 78562.9849900954\n",
      "Epoch 106196: reducing learning rate of group 0 to 3.5256e-03.\n",
      "Epoch 106201, Training Loss: 30740, Validation Loss: 52569, 61562.06059046914\n",
      "Epoch 106297: reducing learning rate of group 0 to 3.5221e-03.\n",
      "Epoch 106301, Training Loss: 31707, Validation Loss: 55683, 62966.708406288235\n",
      "Epoch 106398: reducing learning rate of group 0 to 3.5186e-03.\n",
      "Epoch 106401, Training Loss: 29396, Validation Loss: 56663, 63240.3677417881\n",
      "Epoch 106499: reducing learning rate of group 0 to 3.5151e-03.\n",
      "Epoch 106501, Training Loss: 31880, Validation Loss: 50518, 74736.58760066763\n",
      "Epoch 106600: reducing learning rate of group 0 to 3.5116e-03.\n",
      "Epoch 106601, Training Loss: 31622, Validation Loss: 51207, 70136.52097900817\n",
      "Epoch 106701: reducing learning rate of group 0 to 3.5081e-03.\n",
      "Epoch 106701, Training Loss: 33744, Validation Loss: 53004, 67550.8101669381\n",
      "Epoch 106801, Training Loss: 29717, Validation Loss: 51602, 70246.10142884612\n",
      "Epoch 106802: reducing learning rate of group 0 to 3.5045e-03.\n",
      "Epoch 106901, Training Loss: 32041, Validation Loss: 54010, 73513.94569997788\n",
      "Epoch 106903: reducing learning rate of group 0 to 3.5010e-03.\n",
      "Epoch 107001, Training Loss: 31651, Validation Loss: 53669, 63786.4785135884\n",
      "Epoch 107004: reducing learning rate of group 0 to 3.4975e-03.\n",
      "Epoch 107101, Training Loss: 29686, Validation Loss: 57297, 73144.01652104883\n",
      "Epoch 107105: reducing learning rate of group 0 to 3.4940e-03.\n",
      "Epoch 107201, Training Loss: 31357, Validation Loss: 55093, 66663.8822636718\n",
      "Epoch 107206: reducing learning rate of group 0 to 3.4905e-03.\n",
      "Epoch 107301, Training Loss: 31016, Validation Loss: 53347, 64222.03981848786\n",
      "Epoch 107307: reducing learning rate of group 0 to 3.4871e-03.\n",
      "Epoch 107401, Training Loss: 31502, Validation Loss: 55856, 61308.31286600968\n",
      "Epoch 107408: reducing learning rate of group 0 to 3.4836e-03.\n",
      "Epoch 107501, Training Loss: 31156, Validation Loss: 58388, 70593.09377215036\n",
      "Epoch 107509: reducing learning rate of group 0 to 3.4801e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107601, Training Loss: 33575, Validation Loss: 54271, 67707.22209778984\n",
      "Epoch 107610: reducing learning rate of group 0 to 3.4766e-03.\n",
      "Epoch 107701, Training Loss: 30913, Validation Loss: 52669, 70122.47092139442\n",
      "Epoch 107711: reducing learning rate of group 0 to 3.4731e-03.\n",
      "Epoch 107801, Training Loss: 32222, Validation Loss: 56996, 84929.5762634523\n",
      "Epoch 107812: reducing learning rate of group 0 to 3.4697e-03.\n",
      "Epoch 107901, Training Loss: 30961, Validation Loss: 53316, 59271.88128557483\n",
      "Epoch 107913: reducing learning rate of group 0 to 3.4662e-03.\n",
      "Epoch 108001, Training Loss: 30493, Validation Loss: 52294, 69607.19770262898\n",
      "Epoch 108014: reducing learning rate of group 0 to 3.4627e-03.\n",
      "Epoch 108101, Training Loss: 30460, Validation Loss: 52213, 63219.768455060395\n",
      "Epoch 108115: reducing learning rate of group 0 to 3.4593e-03.\n",
      "Epoch 108201, Training Loss: 29012, Validation Loss: 51669, 70745.37337270389\n",
      "Epoch 108216: reducing learning rate of group 0 to 3.4558e-03.\n",
      "Epoch 108301, Training Loss: 33851, Validation Loss: 57244, 91758.46655113126\n",
      "Epoch 108317: reducing learning rate of group 0 to 3.4523e-03.\n",
      "Epoch 108401, Training Loss: 31669, Validation Loss: 52392, 73787.61264114715\n",
      "Epoch 108418: reducing learning rate of group 0 to 3.4489e-03.\n",
      "Epoch 108501, Training Loss: 32076, Validation Loss: 52852, 87576.9469109077\n",
      "Epoch 108519: reducing learning rate of group 0 to 3.4454e-03.\n",
      "Epoch 108601, Training Loss: 30458, Validation Loss: 57365, 63523.377579038824\n",
      "Epoch 108620: reducing learning rate of group 0 to 3.4420e-03.\n",
      "Epoch 108701, Training Loss: 30450, Validation Loss: 52588, 60753.54899732717\n",
      "Epoch 108721: reducing learning rate of group 0 to 3.4386e-03.\n",
      "Epoch 108801, Training Loss: 30013, Validation Loss: 53785, 64912.09322143994\n",
      "Epoch 108822: reducing learning rate of group 0 to 3.4351e-03.\n",
      "Epoch 108901, Training Loss: 29720, Validation Loss: 54594, 67729.11904452852\n",
      "Epoch 108923: reducing learning rate of group 0 to 3.4317e-03.\n",
      "Epoch 109001, Training Loss: 31811, Validation Loss: 53422, 63211.61152424832\n",
      "Epoch 109024: reducing learning rate of group 0 to 3.4282e-03.\n",
      "Epoch 109101, Training Loss: 31113, Validation Loss: 54247, 62045.63987274476\n",
      "Epoch 109125: reducing learning rate of group 0 to 3.4248e-03.\n",
      "Epoch 109201, Training Loss: 32747, Validation Loss: 55398, 66025.6005054236\n",
      "Epoch 109226: reducing learning rate of group 0 to 3.4214e-03.\n",
      "Epoch 109301, Training Loss: 32118, Validation Loss: 53568, 62824.13514739241\n",
      "Epoch 109327: reducing learning rate of group 0 to 3.4180e-03.\n",
      "Epoch 109401, Training Loss: 30520, Validation Loss: 57964, 45422.762074841776\n",
      "Epoch 109428: reducing learning rate of group 0 to 3.4146e-03.\n",
      "Epoch 109501, Training Loss: 31486, Validation Loss: 51201, 76633.36187511151\n",
      "Epoch 109529: reducing learning rate of group 0 to 3.4111e-03.\n",
      "Epoch 109601, Training Loss: 30990, Validation Loss: 54842, 69478.29747350895\n",
      "Epoch 109630: reducing learning rate of group 0 to 3.4077e-03.\n",
      "Epoch 109701, Training Loss: 31199, Validation Loss: 52730, 72344.21254734264\n",
      "Epoch 109731: reducing learning rate of group 0 to 3.4043e-03.\n",
      "Epoch 109801, Training Loss: 33071, Validation Loss: 53461, 55759.103771828195\n",
      "Epoch 109832: reducing learning rate of group 0 to 3.4009e-03.\n",
      "Epoch 109901, Training Loss: 29968, Validation Loss: 56182, 67281.57934072563\n",
      "Epoch 109933: reducing learning rate of group 0 to 3.3975e-03.\n",
      "Epoch 110001, Training Loss: 28745, Validation Loss: 56167, 68227.77885215839\n",
      "Epoch 110034: reducing learning rate of group 0 to 3.3941e-03.\n",
      "Epoch 110101, Training Loss: 30804, Validation Loss: 56122, 58403.66027709831\n",
      "Epoch 110135: reducing learning rate of group 0 to 3.3907e-03.\n",
      "Epoch 110201, Training Loss: 34781, Validation Loss: 56810, 73518.5711867663\n",
      "Epoch 110236: reducing learning rate of group 0 to 3.3873e-03.\n",
      "Epoch 110301, Training Loss: 32524, Validation Loss: 53816, 70425.40208372599\n",
      "Epoch 110337: reducing learning rate of group 0 to 3.3839e-03.\n",
      "Epoch 110401, Training Loss: 30234, Validation Loss: 50844, 65559.98390438293\n",
      "Epoch 110438: reducing learning rate of group 0 to 3.3806e-03.\n",
      "Epoch 110501, Training Loss: 30543, Validation Loss: 52660, 82033.42292229\n",
      "Epoch 110539: reducing learning rate of group 0 to 3.3772e-03.\n",
      "Epoch 110601, Training Loss: 30616, Validation Loss: 51281, 57532.52003085718\n",
      "Epoch 110640: reducing learning rate of group 0 to 3.3738e-03.\n",
      "Epoch 110701, Training Loss: 31212, Validation Loss: 50963, 55703.7932199534\n",
      "Epoch 110741: reducing learning rate of group 0 to 3.3704e-03.\n",
      "Epoch 110801, Training Loss: 33071, Validation Loss: 55204, 78910.77527565979\n",
      "Epoch 110842: reducing learning rate of group 0 to 3.3671e-03.\n",
      "Epoch 110901, Training Loss: 30539, Validation Loss: 53859, 70917.03042944046\n",
      "Epoch 110943: reducing learning rate of group 0 to 3.3637e-03.\n",
      "Epoch 111001, Training Loss: 32837, Validation Loss: 53975, 73356.04331921409\n",
      "Epoch 111044: reducing learning rate of group 0 to 3.3603e-03.\n",
      "Epoch 111101, Training Loss: 31951, Validation Loss: 53805, 59188.48467843966\n",
      "Epoch 111145: reducing learning rate of group 0 to 3.3570e-03.\n",
      "Epoch 111201, Training Loss: 30575, Validation Loss: 52900, 70875.37414971067\n",
      "Epoch 111246: reducing learning rate of group 0 to 3.3536e-03.\n",
      "Epoch 111301, Training Loss: 30180, Validation Loss: 53897, 75470.19282838795\n",
      "Epoch 111347: reducing learning rate of group 0 to 3.3503e-03.\n",
      "Epoch 111401, Training Loss: 32317, Validation Loss: 55651, 60080.66724101006\n",
      "Epoch 111448: reducing learning rate of group 0 to 3.3469e-03.\n",
      "Epoch 111501, Training Loss: 28392, Validation Loss: 51832, 67187.30049851084\n",
      "Epoch 111549: reducing learning rate of group 0 to 3.3436e-03.\n",
      "Epoch 111601, Training Loss: 31536, Validation Loss: 55505, 69474.98651325425\n",
      "Epoch 111650: reducing learning rate of group 0 to 3.3402e-03.\n",
      "Epoch 111701, Training Loss: 29170, Validation Loss: 54680, 67269.60747735786\n",
      "Epoch 111751: reducing learning rate of group 0 to 3.3369e-03.\n",
      "Epoch 111801, Training Loss: 30200, Validation Loss: 56732, 79199.93324340378\n",
      "Epoch 111852: reducing learning rate of group 0 to 3.3335e-03.\n",
      "Epoch 111901, Training Loss: 31450, Validation Loss: 55138, 64144.06059666008\n",
      "Epoch 111953: reducing learning rate of group 0 to 3.3302e-03.\n",
      "Epoch 112001, Training Loss: 35560, Validation Loss: 54786, 73223.2466563338\n",
      "Epoch 112054: reducing learning rate of group 0 to 3.3269e-03.\n",
      "Epoch 112101, Training Loss: 33290, Validation Loss: 55445, 61861.824250817124\n",
      "Epoch 112155: reducing learning rate of group 0 to 3.3236e-03.\n",
      "Epoch 112201, Training Loss: 30669, Validation Loss: 54186, 69016.23491176008\n",
      "Epoch 112256: reducing learning rate of group 0 to 3.3202e-03.\n",
      "Epoch 112301, Training Loss: 31247, Validation Loss: 53192, 60471.65844706492\n",
      "Epoch 112357: reducing learning rate of group 0 to 3.3169e-03.\n",
      "Epoch 112401, Training Loss: 29479, Validation Loss: 53124, 61704.39117997353\n",
      "Epoch 112458: reducing learning rate of group 0 to 3.3136e-03.\n",
      "Epoch 112501, Training Loss: 33450, Validation Loss: 51693, 81436.1738145479\n",
      "Epoch 112559: reducing learning rate of group 0 to 3.3103e-03.\n",
      "Epoch 112601, Training Loss: 32414, Validation Loss: 54923, 63131.381494017696\n",
      "Epoch 112660: reducing learning rate of group 0 to 3.3070e-03.\n",
      "Epoch 112701, Training Loss: 29705, Validation Loss: 56116, 78518.31877676542\n",
      "Epoch 112761: reducing learning rate of group 0 to 3.3037e-03.\n",
      "Epoch 112801, Training Loss: 33301, Validation Loss: 51965, 69801.90419929434\n",
      "Epoch 112862: reducing learning rate of group 0 to 3.3004e-03.\n",
      "Epoch 112901, Training Loss: 31956, Validation Loss: 51366, 62248.95551829756\n",
      "Epoch 112963: reducing learning rate of group 0 to 3.2971e-03.\n",
      "Epoch 113001, Training Loss: 29182, Validation Loss: 54289, 67033.02402465661\n",
      "Epoch 113064: reducing learning rate of group 0 to 3.2938e-03.\n",
      "Epoch 113101, Training Loss: 29793, Validation Loss: 57111, 66517.47255930082\n",
      "Epoch 113165: reducing learning rate of group 0 to 3.2905e-03.\n",
      "Epoch 113201, Training Loss: 28540, Validation Loss: 52962, 66594.40957182705\n",
      "Epoch 113266: reducing learning rate of group 0 to 3.2872e-03.\n",
      "Epoch 113301, Training Loss: 31682, Validation Loss: 54279, 67526.25360432322\n",
      "Epoch 113367: reducing learning rate of group 0 to 3.2839e-03.\n",
      "Epoch 113401, Training Loss: 29225, Validation Loss: 54914, 72034.87682958385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113468: reducing learning rate of group 0 to 3.2806e-03.\n",
      "Epoch 113501, Training Loss: 31792, Validation Loss: 52185, 70810.83016674982\n",
      "Epoch 113569: reducing learning rate of group 0 to 3.2773e-03.\n",
      "Epoch 113601, Training Loss: 33241, Validation Loss: 51770, 72665.28251616281\n",
      "Epoch 113670: reducing learning rate of group 0 to 3.2740e-03.\n",
      "Epoch 113701, Training Loss: 36788, Validation Loss: 53221, 67144.71960650683\n",
      "Epoch 113771: reducing learning rate of group 0 to 3.2708e-03.\n",
      "Epoch 113801, Training Loss: 32755, Validation Loss: 52205, 72023.65174032185\n",
      "Epoch 113872: reducing learning rate of group 0 to 3.2675e-03.\n",
      "Epoch 113901, Training Loss: 31228, Validation Loss: 52619, 68313.48872129817\n",
      "Epoch 113973: reducing learning rate of group 0 to 3.2642e-03.\n",
      "Epoch 114001, Training Loss: 31573, Validation Loss: 50437, 51797.67750119156\n",
      "Epoch 114074: reducing learning rate of group 0 to 3.2610e-03.\n",
      "Epoch 114101, Training Loss: 32191, Validation Loss: 52728, 70929.03499372296\n",
      "Epoch 114175: reducing learning rate of group 0 to 3.2577e-03.\n",
      "Epoch 114201, Training Loss: 30483, Validation Loss: 53510, 76699.98456290858\n",
      "Epoch 114276: reducing learning rate of group 0 to 3.2545e-03.\n",
      "Epoch 114301, Training Loss: 31283, Validation Loss: 52475, 68924.02136281115\n",
      "Epoch 114377: reducing learning rate of group 0 to 3.2512e-03.\n",
      "Epoch 114401, Training Loss: 31203, Validation Loss: 53027, 73254.26810050922\n",
      "Epoch 114478: reducing learning rate of group 0 to 3.2479e-03.\n",
      "Epoch 114501, Training Loss: 30961, Validation Loss: 54472, 67625.62001159863\n",
      "Epoch 114579: reducing learning rate of group 0 to 3.2447e-03.\n",
      "Epoch 114601, Training Loss: 34046, Validation Loss: 51034, 89703.15615322998\n",
      "Epoch 114680: reducing learning rate of group 0 to 3.2415e-03.\n",
      "Epoch 114701, Training Loss: 30725, Validation Loss: 52032, 69641.83486160559\n",
      "Epoch 114781: reducing learning rate of group 0 to 3.2382e-03.\n",
      "Epoch 114801, Training Loss: 29864, Validation Loss: 53431, 62835.61402873639\n",
      "Epoch 114882: reducing learning rate of group 0 to 3.2350e-03.\n",
      "Epoch 114901, Training Loss: 31008, Validation Loss: 54571, 65033.73219539004\n",
      "Epoch 114983: reducing learning rate of group 0 to 3.2317e-03.\n",
      "Epoch 115001, Training Loss: 29912, Validation Loss: 54313, 59360.04913765725\n",
      "Epoch 115084: reducing learning rate of group 0 to 3.2285e-03.\n",
      "Epoch 115101, Training Loss: 31691, Validation Loss: 52802, 77978.93660166653\n",
      "Epoch 115185: reducing learning rate of group 0 to 3.2253e-03.\n",
      "Epoch 115201, Training Loss: 30581, Validation Loss: 54218, 63254.11636945332\n",
      "Epoch 115286: reducing learning rate of group 0 to 3.2221e-03.\n",
      "Epoch 115301, Training Loss: 30485, Validation Loss: 51685, 70952.70305999032\n",
      "Epoch 115387: reducing learning rate of group 0 to 3.2188e-03.\n",
      "Epoch 115401, Training Loss: 30786, Validation Loss: 54524, 63883.9982770702\n",
      "Epoch 115488: reducing learning rate of group 0 to 3.2156e-03.\n",
      "Epoch 115501, Training Loss: 30905, Validation Loss: 54834, 71855.45423019839\n",
      "Epoch 115589: reducing learning rate of group 0 to 3.2124e-03.\n",
      "Epoch 115601, Training Loss: 29098, Validation Loss: 53501, 69647.59193386794\n",
      "Epoch 115690: reducing learning rate of group 0 to 3.2092e-03.\n",
      "Epoch 115701, Training Loss: 32344, Validation Loss: 52767, 65309.48494602109\n",
      "Epoch 115791: reducing learning rate of group 0 to 3.2060e-03.\n",
      "Epoch 115801, Training Loss: 35604, Validation Loss: 56895, 57334.05380611374\n",
      "Epoch 115892: reducing learning rate of group 0 to 3.2028e-03.\n",
      "Epoch 115901, Training Loss: 28674, Validation Loss: 56143, 65764.77277283312\n",
      "Epoch 115993: reducing learning rate of group 0 to 3.1996e-03.\n",
      "Epoch 116001, Training Loss: 29530, Validation Loss: 54546, 85740.33425054516\n",
      "Epoch 116094: reducing learning rate of group 0 to 3.1964e-03.\n",
      "Epoch 116101, Training Loss: 31926, Validation Loss: 55437, 63333.2661856578\n",
      "Epoch 116195: reducing learning rate of group 0 to 3.1932e-03.\n",
      "Epoch 116201, Training Loss: 31939, Validation Loss: 54994, 76342.74029255683\n",
      "Epoch 116296: reducing learning rate of group 0 to 3.1900e-03.\n",
      "Epoch 116301, Training Loss: 32540, Validation Loss: 53431, 56342.52356056452\n",
      "Epoch 116397: reducing learning rate of group 0 to 3.1868e-03.\n",
      "Epoch 116401, Training Loss: 30901, Validation Loss: 52139, 74481.28138350377\n",
      "Epoch 116498: reducing learning rate of group 0 to 3.1836e-03.\n",
      "Epoch 116501, Training Loss: 29994, Validation Loss: 55920, 62335.143525553074\n",
      "Epoch 116599: reducing learning rate of group 0 to 3.1804e-03.\n",
      "Epoch 116601, Training Loss: 30881, Validation Loss: 54495, 67301.70583138394\n",
      "Epoch 116700: reducing learning rate of group 0 to 3.1772e-03.\n",
      "Epoch 116701, Training Loss: 29764, Validation Loss: 57212, 58815.04521194848\n",
      "Epoch 116801: reducing learning rate of group 0 to 3.1741e-03.\n",
      "Epoch 116801, Training Loss: 29303, Validation Loss: 52435, 74201.39385383944\n",
      "Epoch 116901, Training Loss: 30382, Validation Loss: 52739, 67521.82265473733\n",
      "Epoch 116902: reducing learning rate of group 0 to 3.1709e-03.\n",
      "Epoch 117001, Training Loss: 33105, Validation Loss: 51318, 61937.470380074694\n",
      "Epoch 117003: reducing learning rate of group 0 to 3.1677e-03.\n",
      "Epoch 117101, Training Loss: 29909, Validation Loss: 55393, 74382.66662069577\n",
      "Epoch 117104: reducing learning rate of group 0 to 3.1645e-03.\n",
      "Epoch 117201, Training Loss: 31087, Validation Loss: 55248, 64506.814572157884\n",
      "Epoch 117205: reducing learning rate of group 0 to 3.1614e-03.\n",
      "Epoch 117301, Training Loss: 32699, Validation Loss: 52929, 73707.96062552427\n",
      "Epoch 117306: reducing learning rate of group 0 to 3.1582e-03.\n",
      "Epoch 117401, Training Loss: 27985, Validation Loss: 53776, 67260.45976348416\n",
      "Epoch 117407: reducing learning rate of group 0 to 3.1551e-03.\n",
      "Epoch 117501, Training Loss: 30114, Validation Loss: 54521, 80242.95198033703\n",
      "Epoch 117508: reducing learning rate of group 0 to 3.1519e-03.\n",
      "Epoch 117601, Training Loss: 29854, Validation Loss: 53263, 61743.79966721267\n",
      "Epoch 117609: reducing learning rate of group 0 to 3.1488e-03.\n",
      "Epoch 117701, Training Loss: 30112, Validation Loss: 56601, 67495.54351421139\n",
      "Epoch 117710: reducing learning rate of group 0 to 3.1456e-03.\n",
      "Epoch 117801, Training Loss: 31313, Validation Loss: 56371, 66247.26910155824\n",
      "Epoch 117811: reducing learning rate of group 0 to 3.1425e-03.\n",
      "Epoch 117901, Training Loss: 29533, Validation Loss: 52350, 54642.28665188411\n",
      "Epoch 117912: reducing learning rate of group 0 to 3.1393e-03.\n",
      "Epoch 118001, Training Loss: 31590, Validation Loss: 54668, 70405.1296758186\n",
      "Epoch 118013: reducing learning rate of group 0 to 3.1362e-03.\n",
      "Epoch 118101, Training Loss: 30661, Validation Loss: 51201, 68953.22781801033\n",
      "Epoch 118114: reducing learning rate of group 0 to 3.1330e-03.\n",
      "Epoch 118201, Training Loss: 31439, Validation Loss: 51197, 61604.54389175846\n",
      "Epoch 118215: reducing learning rate of group 0 to 3.1299e-03.\n",
      "Epoch 118301, Training Loss: 29286, Validation Loss: 55488, 62763.45398653103\n",
      "Epoch 118316: reducing learning rate of group 0 to 3.1268e-03.\n",
      "Epoch 118401, Training Loss: 30365, Validation Loss: 52148, 64309.98957631915\n",
      "Epoch 118417: reducing learning rate of group 0 to 3.1237e-03.\n",
      "Epoch 118501, Training Loss: 29886, Validation Loss: 55972, 73077.61107801911\n",
      "Epoch 118518: reducing learning rate of group 0 to 3.1205e-03.\n",
      "Epoch 118601, Training Loss: 33351, Validation Loss: 53519, 68431.57931824938\n",
      "Epoch 118619: reducing learning rate of group 0 to 3.1174e-03.\n",
      "Epoch 118701, Training Loss: 30405, Validation Loss: 55024, 65432.9484005766\n",
      "Epoch 118720: reducing learning rate of group 0 to 3.1143e-03.\n",
      "Epoch 118801, Training Loss: 30842, Validation Loss: 54303, 64780.58677477667\n",
      "Epoch 118821: reducing learning rate of group 0 to 3.1112e-03.\n",
      "Epoch 118901, Training Loss: 30058, Validation Loss: 52654, 59856.70792753296\n",
      "Epoch 118922: reducing learning rate of group 0 to 3.1081e-03.\n",
      "Epoch 119001, Training Loss: 31327, Validation Loss: 55512, 66298.42495695334\n",
      "Epoch 119023: reducing learning rate of group 0 to 3.1050e-03.\n",
      "Epoch 119101, Training Loss: 33565, Validation Loss: 53537, 76927.3918278727\n",
      "Epoch 119124: reducing learning rate of group 0 to 3.1019e-03.\n",
      "Epoch 119201, Training Loss: 31646, Validation Loss: 51005, 68750.55568104098\n",
      "Epoch 119225: reducing learning rate of group 0 to 3.0988e-03.\n",
      "Epoch 119301, Training Loss: 29767, Validation Loss: 52502, 75676.5911762922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119326: reducing learning rate of group 0 to 3.0957e-03.\n",
      "Epoch 119401, Training Loss: 30502, Validation Loss: 59407, 61234.46367955977\n",
      "Epoch 119427: reducing learning rate of group 0 to 3.0926e-03.\n",
      "Epoch 119501, Training Loss: 32309, Validation Loss: 57761, 67605.92433375564\n",
      "Epoch 119528: reducing learning rate of group 0 to 3.0895e-03.\n",
      "Epoch 119601, Training Loss: 30378, Validation Loss: 53343, 70131.35900567596\n",
      "Epoch 119629: reducing learning rate of group 0 to 3.0864e-03.\n",
      "Epoch 119701, Training Loss: 30976, Validation Loss: 53727, 62717.87086427633\n",
      "Epoch 119730: reducing learning rate of group 0 to 3.0833e-03.\n",
      "Epoch 119801, Training Loss: 31911, Validation Loss: 56746, 57536.45751258102\n",
      "Epoch 119831: reducing learning rate of group 0 to 3.0802e-03.\n",
      "Epoch 119901, Training Loss: 31448, Validation Loss: 56509, 68961.35478069684\n",
      "Epoch 119932: reducing learning rate of group 0 to 3.0771e-03.\n",
      "Epoch 120001, Training Loss: 30137, Validation Loss: 53270, 65244.401334785645\n",
      "Epoch 120033: reducing learning rate of group 0 to 3.0740e-03.\n",
      "Epoch 120101, Training Loss: 30825, Validation Loss: 52534, 67505.5851876532\n",
      "Epoch 120134: reducing learning rate of group 0 to 3.0710e-03.\n",
      "Epoch 120201, Training Loss: 30475, Validation Loss: 56355, 56101.44559382476\n",
      "Epoch 120235: reducing learning rate of group 0 to 3.0679e-03.\n",
      "Epoch 120301, Training Loss: 29835, Validation Loss: 54369, 55322.35024829371\n",
      "Epoch 120336: reducing learning rate of group 0 to 3.0648e-03.\n",
      "Epoch 120401, Training Loss: 31714, Validation Loss: 56264, 60841.44436121619\n",
      "Epoch 120437: reducing learning rate of group 0 to 3.0618e-03.\n",
      "Epoch 120501, Training Loss: 29958, Validation Loss: 57252, 60415.437333188056\n",
      "Epoch 120538: reducing learning rate of group 0 to 3.0587e-03.\n",
      "Epoch 120601, Training Loss: 35966, Validation Loss: 52611, 66012.93543436332\n",
      "Epoch 120639: reducing learning rate of group 0 to 3.0556e-03.\n",
      "Epoch 120701, Training Loss: 30701, Validation Loss: 54458, 73770.28518455799\n",
      "Epoch 120740: reducing learning rate of group 0 to 3.0526e-03.\n",
      "Epoch 120801, Training Loss: 32924, Validation Loss: 51861, 68346.90012902881\n",
      "Epoch 120841: reducing learning rate of group 0 to 3.0495e-03.\n",
      "Epoch 120901, Training Loss: 34403, Validation Loss: 52473, 66841.83514798818\n",
      "Epoch 120942: reducing learning rate of group 0 to 3.0465e-03.\n",
      "Epoch 121001, Training Loss: 32185, Validation Loss: 52522, 73646.13183629839\n",
      "Epoch 121043: reducing learning rate of group 0 to 3.0434e-03.\n",
      "Epoch 121101, Training Loss: 30192, Validation Loss: 52735, 60845.68024965541\n",
      "Epoch 121144: reducing learning rate of group 0 to 3.0404e-03.\n",
      "Epoch 121201, Training Loss: 28380, Validation Loss: 56810, 64656.88513059161\n",
      "Epoch 121245: reducing learning rate of group 0 to 3.0374e-03.\n",
      "Epoch 121301, Training Loss: 31167, Validation Loss: 52126, 73813.73942180823\n",
      "Epoch 121346: reducing learning rate of group 0 to 3.0343e-03.\n",
      "Epoch 121401, Training Loss: 30366, Validation Loss: 54340, 58460.59615186924\n",
      "Epoch 121447: reducing learning rate of group 0 to 3.0313e-03.\n",
      "Epoch 121501, Training Loss: 31566, Validation Loss: 52742, 70967.63454651019\n",
      "Epoch 121548: reducing learning rate of group 0 to 3.0283e-03.\n",
      "Epoch 121601, Training Loss: 30075, Validation Loss: 54963, 54505.84040590541\n",
      "Epoch 121649: reducing learning rate of group 0 to 3.0252e-03.\n",
      "Epoch 121701, Training Loss: 27987, Validation Loss: 53341, 66159.68194082032\n",
      "Epoch 121750: reducing learning rate of group 0 to 3.0222e-03.\n",
      "Epoch 121801, Training Loss: 29932, Validation Loss: 55140, 60771.48263756972\n",
      "Epoch 121851: reducing learning rate of group 0 to 3.0192e-03.\n",
      "Epoch 121901, Training Loss: 29195, Validation Loss: 55610, 65309.0441201746\n",
      "Epoch 121952: reducing learning rate of group 0 to 3.0162e-03.\n",
      "Epoch 122001, Training Loss: 31077, Validation Loss: 52952, 68465.62762746128\n",
      "Epoch 122053: reducing learning rate of group 0 to 3.0131e-03.\n",
      "Epoch 122101, Training Loss: 31576, Validation Loss: 60923, 59385.19017490418\n",
      "Epoch 122154: reducing learning rate of group 0 to 3.0101e-03.\n",
      "Epoch 122201, Training Loss: 33498, Validation Loss: 52103, 56065.515013826815\n",
      "Epoch 122255: reducing learning rate of group 0 to 3.0071e-03.\n",
      "Epoch 122301, Training Loss: 28703, Validation Loss: 51308, 64977.68419753601\n",
      "Epoch 122356: reducing learning rate of group 0 to 3.0041e-03.\n",
      "Epoch 122401, Training Loss: 30820, Validation Loss: 53223, 78823.84355746485\n",
      "Epoch 122457: reducing learning rate of group 0 to 3.0011e-03.\n",
      "Epoch 122501, Training Loss: 29674, Validation Loss: 53924, 73884.90486105178\n",
      "Epoch 122558: reducing learning rate of group 0 to 2.9981e-03.\n",
      "Epoch 122601, Training Loss: 32413, Validation Loss: 56468, 76344.49336426506\n",
      "Epoch 122659: reducing learning rate of group 0 to 2.9951e-03.\n",
      "Epoch 122701, Training Loss: 30574, Validation Loss: 55445, 64674.81268911376\n",
      "Epoch 122760: reducing learning rate of group 0 to 2.9921e-03.\n",
      "Epoch 122801, Training Loss: 28664, Validation Loss: 54441, 60463.91231906156\n",
      "Epoch 122861: reducing learning rate of group 0 to 2.9891e-03.\n",
      "Epoch 122901, Training Loss: 30139, Validation Loss: 53643, 89360.54784966353\n",
      "Epoch 122962: reducing learning rate of group 0 to 2.9861e-03.\n",
      "Epoch 123001, Training Loss: 28144, Validation Loss: 51056, 62474.97871868699\n",
      "Epoch 123063: reducing learning rate of group 0 to 2.9832e-03.\n",
      "Epoch 123101, Training Loss: 28949, Validation Loss: 55095, 73101.16700099183\n",
      "Epoch 123164: reducing learning rate of group 0 to 2.9802e-03.\n",
      "Epoch 123201, Training Loss: 30345, Validation Loss: 58410, 66183.47696314788\n",
      "Epoch 123265: reducing learning rate of group 0 to 2.9772e-03.\n",
      "Epoch 123301, Training Loss: 30780, Validation Loss: 52524, 64032.1798749072\n",
      "Epoch 123366: reducing learning rate of group 0 to 2.9742e-03.\n",
      "Epoch 123401, Training Loss: 29123, Validation Loss: 54171, 62409.996699198666\n",
      "Epoch 123467: reducing learning rate of group 0 to 2.9712e-03.\n",
      "Epoch 123501, Training Loss: 31492, Validation Loss: 55338, 70519.66640154515\n",
      "Epoch 123568: reducing learning rate of group 0 to 2.9683e-03.\n",
      "Epoch 123601, Training Loss: 31131, Validation Loss: 53947, 63369.43337360216\n",
      "Epoch 123669: reducing learning rate of group 0 to 2.9653e-03.\n",
      "Epoch 123701, Training Loss: 28839, Validation Loss: 52971, 55463.92820165486\n",
      "Epoch 123770: reducing learning rate of group 0 to 2.9623e-03.\n",
      "Epoch 123801, Training Loss: 30469, Validation Loss: 54334, 65511.981167712416\n",
      "Epoch 123871: reducing learning rate of group 0 to 2.9594e-03.\n",
      "Epoch 123901, Training Loss: 34211, Validation Loss: 53593, 72561.82615848014\n",
      "Epoch 123972: reducing learning rate of group 0 to 2.9564e-03.\n",
      "Epoch 124001, Training Loss: 29970, Validation Loss: 52481, 72796.24441676191\n",
      "Epoch 124073: reducing learning rate of group 0 to 2.9535e-03.\n",
      "Epoch 124101, Training Loss: 32857, Validation Loss: 53275, 68861.1755519519\n",
      "Epoch 124174: reducing learning rate of group 0 to 2.9505e-03.\n",
      "Epoch 124201, Training Loss: 29865, Validation Loss: 53431, 63786.79156435813\n",
      "Epoch 124275: reducing learning rate of group 0 to 2.9475e-03.\n",
      "Epoch 124301, Training Loss: 32518, Validation Loss: 51554, 80994.30575091261\n",
      "Epoch 124376: reducing learning rate of group 0 to 2.9446e-03.\n",
      "Epoch 124401, Training Loss: 29027, Validation Loss: 53701, 73448.35570525733\n",
      "Epoch 124477: reducing learning rate of group 0 to 2.9417e-03.\n",
      "Epoch 124501, Training Loss: 31259, Validation Loss: 54401, 73718.93993386037\n",
      "Epoch 124578: reducing learning rate of group 0 to 2.9387e-03.\n",
      "Epoch 124601, Training Loss: 31797, Validation Loss: 55423, 65183.68708544093\n",
      "Epoch 124679: reducing learning rate of group 0 to 2.9358e-03.\n",
      "Epoch 124701, Training Loss: 31260, Validation Loss: 56118, 92059.34872909934\n",
      "Epoch 124780: reducing learning rate of group 0 to 2.9328e-03.\n",
      "Epoch 124801, Training Loss: 29038, Validation Loss: 56116, 62243.32982551293\n",
      "Epoch 124881: reducing learning rate of group 0 to 2.9299e-03.\n",
      "Epoch 124901, Training Loss: 33037, Validation Loss: 56216, 67106.65236175415\n",
      "Epoch 124982: reducing learning rate of group 0 to 2.9270e-03.\n",
      "Epoch 125001, Training Loss: 29794, Validation Loss: 55846, 96568.9941447429\n",
      "Epoch 125083: reducing learning rate of group 0 to 2.9241e-03.\n",
      "Epoch 125101, Training Loss: 28885, Validation Loss: 51608, 66444.09872673311\n",
      "Epoch 125184: reducing learning rate of group 0 to 2.9211e-03.\n",
      "Epoch 125201, Training Loss: 30010, Validation Loss: 54183, 60666.53435854599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125285: reducing learning rate of group 0 to 2.9182e-03.\n",
      "Epoch 125301, Training Loss: 29800, Validation Loss: 52073, 59711.03722850926\n",
      "Epoch 125386: reducing learning rate of group 0 to 2.9153e-03.\n",
      "Epoch 125401, Training Loss: 32068, Validation Loss: 53424, 56336.54592953521\n",
      "Epoch 125487: reducing learning rate of group 0 to 2.9124e-03.\n",
      "Epoch 125501, Training Loss: 29778, Validation Loss: 54863, 69398.67229522746\n",
      "Epoch 125588: reducing learning rate of group 0 to 2.9095e-03.\n",
      "Epoch 125601, Training Loss: 29096, Validation Loss: 51600, 73605.07634427962\n",
      "Epoch 125689: reducing learning rate of group 0 to 2.9066e-03.\n",
      "Epoch 125701, Training Loss: 31628, Validation Loss: 53545, 69281.4498670287\n",
      "Epoch 125790: reducing learning rate of group 0 to 2.9036e-03.\n",
      "Epoch 125801, Training Loss: 30451, Validation Loss: 53662, 75099.27034769875\n",
      "Epoch 125891: reducing learning rate of group 0 to 2.9007e-03.\n",
      "Epoch 125901, Training Loss: 31630, Validation Loss: 54157, 72492.96816219144\n",
      "Epoch 125992: reducing learning rate of group 0 to 2.8978e-03.\n",
      "Epoch 126001, Training Loss: 34138, Validation Loss: 57534, 99644.9067390253\n",
      "Epoch 126093: reducing learning rate of group 0 to 2.8949e-03.\n",
      "Epoch 126101, Training Loss: 30841, Validation Loss: 54204, 60459.88027447969\n",
      "Epoch 126194: reducing learning rate of group 0 to 2.8920e-03.\n",
      "Epoch 126201, Training Loss: 31263, Validation Loss: 54582, 77175.30853480687\n",
      "Epoch 126295: reducing learning rate of group 0 to 2.8892e-03.\n",
      "Epoch 126301, Training Loss: 29825, Validation Loss: 53450, 72359.35117953441\n",
      "Epoch 126396: reducing learning rate of group 0 to 2.8863e-03.\n",
      "Epoch 126401, Training Loss: 31424, Validation Loss: 52700, 76922.59674403748\n",
      "Epoch 126497: reducing learning rate of group 0 to 2.8834e-03.\n",
      "Epoch 126501, Training Loss: 28242, Validation Loss: 50239, 62995.739218124625\n",
      "Epoch 126598: reducing learning rate of group 0 to 2.8805e-03.\n",
      "Epoch 126601, Training Loss: 28536, Validation Loss: 55160, 69805.89358022681\n",
      "Epoch 126699: reducing learning rate of group 0 to 2.8776e-03.\n",
      "Epoch 126701, Training Loss: 30151, Validation Loss: 49748, 71926.3266236864\n",
      "Epoch 126800: reducing learning rate of group 0 to 2.8747e-03.\n",
      "Epoch 126801, Training Loss: 31097, Validation Loss: 51476, 73688.32626051728\n",
      "Epoch 126901: reducing learning rate of group 0 to 2.8719e-03.\n",
      "Epoch 126901, Training Loss: 31139, Validation Loss: 52339, 67073.56653637432\n",
      "Epoch 127001, Training Loss: 31201, Validation Loss: 52588, 81799.82058913149\n",
      "Epoch 127002: reducing learning rate of group 0 to 2.8690e-03.\n",
      "Epoch 127101, Training Loss: 30130, Validation Loss: 53347, 67219.76709442331\n",
      "Epoch 127103: reducing learning rate of group 0 to 2.8661e-03.\n",
      "Epoch 127201, Training Loss: 30410, Validation Loss: 55154, 61594.067129656374\n",
      "Epoch 127204: reducing learning rate of group 0 to 2.8633e-03.\n",
      "Epoch 127301, Training Loss: 31614, Validation Loss: 55181, 70574.09654761762\n",
      "Epoch 127305: reducing learning rate of group 0 to 2.8604e-03.\n",
      "Epoch 127401, Training Loss: 29230, Validation Loss: 55573, 64875.30453802682\n",
      "Epoch 127406: reducing learning rate of group 0 to 2.8575e-03.\n",
      "Epoch 127501, Training Loss: 29715, Validation Loss: 55449, 64821.316298163205\n",
      "Epoch 127507: reducing learning rate of group 0 to 2.8547e-03.\n",
      "Epoch 127601, Training Loss: 29522, Validation Loss: 53226, 58617.378828051915\n",
      "Epoch 127608: reducing learning rate of group 0 to 2.8518e-03.\n",
      "Epoch 127701, Training Loss: 29844, Validation Loss: 53236, 79598.59404793552\n",
      "Epoch 127709: reducing learning rate of group 0 to 2.8490e-03.\n",
      "Epoch 127801, Training Loss: 29246, Validation Loss: 55615, 70257.82705281362\n",
      "Epoch 127810: reducing learning rate of group 0 to 2.8461e-03.\n",
      "Epoch 127901, Training Loss: 34051, Validation Loss: 55807, 76499.10057821586\n",
      "Epoch 127911: reducing learning rate of group 0 to 2.8433e-03.\n",
      "Epoch 128001, Training Loss: 31840, Validation Loss: 53640, 57974.0121830072\n",
      "Epoch 128012: reducing learning rate of group 0 to 2.8404e-03.\n",
      "Epoch 128101, Training Loss: 32007, Validation Loss: 54295, 80752.22594538916\n",
      "Epoch 128113: reducing learning rate of group 0 to 2.8376e-03.\n",
      "Epoch 128201, Training Loss: 30639, Validation Loss: 54449, 73927.2243605267\n",
      "Epoch 128214: reducing learning rate of group 0 to 2.8348e-03.\n",
      "Epoch 128301, Training Loss: 31409, Validation Loss: 52434, 66811.4072699134\n",
      "Epoch 128315: reducing learning rate of group 0 to 2.8319e-03.\n",
      "Epoch 128401, Training Loss: 30233, Validation Loss: 56591, 68771.66280563545\n",
      "Epoch 128416: reducing learning rate of group 0 to 2.8291e-03.\n",
      "Epoch 128501, Training Loss: 30378, Validation Loss: 52068, 78115.43092259225\n",
      "Epoch 128517: reducing learning rate of group 0 to 2.8263e-03.\n",
      "Epoch 128601, Training Loss: 28356, Validation Loss: 54233, 76300.21288591316\n",
      "Epoch 128618: reducing learning rate of group 0 to 2.8234e-03.\n",
      "Epoch 128701, Training Loss: 31195, Validation Loss: 55153, 78268.40353278279\n",
      "Epoch 128719: reducing learning rate of group 0 to 2.8206e-03.\n",
      "Epoch 128801, Training Loss: 29224, Validation Loss: 52839, 65654.01365808187\n",
      "Epoch 128820: reducing learning rate of group 0 to 2.8178e-03.\n",
      "Epoch 128901, Training Loss: 28767, Validation Loss: 57775, 61792.19793324623\n",
      "Epoch 128921: reducing learning rate of group 0 to 2.8150e-03.\n",
      "Epoch 129001, Training Loss: 30370, Validation Loss: 54849, 64970.794506060025\n",
      "Epoch 129022: reducing learning rate of group 0 to 2.8122e-03.\n",
      "Epoch 129101, Training Loss: 31744, Validation Loss: 55327, 70903.24117223352\n",
      "Epoch 129123: reducing learning rate of group 0 to 2.8093e-03.\n",
      "Epoch 129201, Training Loss: 31052, Validation Loss: 52339, 72593.58265302148\n",
      "Epoch 129224: reducing learning rate of group 0 to 2.8065e-03.\n",
      "Epoch 129301, Training Loss: 30317, Validation Loss: 54513, 89796.82747604541\n",
      "Epoch 129325: reducing learning rate of group 0 to 2.8037e-03.\n",
      "Epoch 129401, Training Loss: 31877, Validation Loss: 54497, 54341.28150431491\n",
      "Epoch 129426: reducing learning rate of group 0 to 2.8009e-03.\n",
      "Epoch 129501, Training Loss: 31488, Validation Loss: 54472, 61216.93386759586\n",
      "Epoch 129527: reducing learning rate of group 0 to 2.7981e-03.\n",
      "Epoch 129601, Training Loss: 29480, Validation Loss: 57945, 63053.29205113722\n",
      "Epoch 129628: reducing learning rate of group 0 to 2.7953e-03.\n",
      "Epoch 129701, Training Loss: 30137, Validation Loss: 56747, 70639.25972689538\n",
      "Epoch 129729: reducing learning rate of group 0 to 2.7925e-03.\n",
      "Epoch 129801, Training Loss: 31319, Validation Loss: 51448, 62666.787792886804\n",
      "Epoch 129830: reducing learning rate of group 0 to 2.7897e-03.\n",
      "Epoch 129901, Training Loss: 29178, Validation Loss: 54904, 75022.59198613382\n",
      "Epoch 129931: reducing learning rate of group 0 to 2.7869e-03.\n",
      "Epoch 130001, Training Loss: 31599, Validation Loss: 54284, 92063.87149893116\n",
      "Epoch 130032: reducing learning rate of group 0 to 2.7842e-03.\n",
      "Epoch 130101, Training Loss: 30077, Validation Loss: 50514, 62124.38843054278\n",
      "Epoch 130133: reducing learning rate of group 0 to 2.7814e-03.\n",
      "Epoch 130201, Training Loss: 30554, Validation Loss: 56134, 61258.33618258982\n",
      "Epoch 130234: reducing learning rate of group 0 to 2.7786e-03.\n",
      "Epoch 130301, Training Loss: 30572, Validation Loss: 55158, 71767.82981416742\n",
      "Epoch 130335: reducing learning rate of group 0 to 2.7758e-03.\n",
      "Epoch 130401, Training Loss: 30542, Validation Loss: 57431, 58731.0899569405\n",
      "Epoch 130436: reducing learning rate of group 0 to 2.7730e-03.\n",
      "Epoch 130501, Training Loss: 29770, Validation Loss: 52947, 59311.267589174626\n",
      "Epoch 130537: reducing learning rate of group 0 to 2.7703e-03.\n",
      "Epoch 130601, Training Loss: 30082, Validation Loss: 52276, 75687.1424721903\n",
      "Epoch 130638: reducing learning rate of group 0 to 2.7675e-03.\n",
      "Epoch 130701, Training Loss: 30779, Validation Loss: 54082, 71709.53267861641\n",
      "Epoch 130739: reducing learning rate of group 0 to 2.7647e-03.\n",
      "Epoch 130801, Training Loss: 31073, Validation Loss: 55368, 75088.8749288032\n",
      "Epoch 130840: reducing learning rate of group 0 to 2.7620e-03.\n",
      "Epoch 130901, Training Loss: 30707, Validation Loss: 58198, 62925.8060101378\n",
      "Epoch 130941: reducing learning rate of group 0 to 2.7592e-03.\n",
      "Epoch 131001, Training Loss: 30942, Validation Loss: 53544, 92758.621901532\n",
      "Epoch 131042: reducing learning rate of group 0 to 2.7564e-03.\n",
      "Epoch 131101, Training Loss: 30285, Validation Loss: 53967, 78074.96333913963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131143: reducing learning rate of group 0 to 2.7537e-03.\n",
      "Epoch 131201, Training Loss: 31526, Validation Loss: 54034, 72418.52924878256\n",
      "Epoch 131244: reducing learning rate of group 0 to 2.7509e-03.\n",
      "Epoch 131301, Training Loss: 31915, Validation Loss: 52314, 76224.85004533506\n",
      "Epoch 131345: reducing learning rate of group 0 to 2.7482e-03.\n",
      "Epoch 131401, Training Loss: 29745, Validation Loss: 51866, 70564.37980784332\n",
      "Epoch 131446: reducing learning rate of group 0 to 2.7454e-03.\n",
      "Epoch 131501, Training Loss: 29072, Validation Loss: 52990, 73644.32157576155\n",
      "Epoch 131547: reducing learning rate of group 0 to 2.7427e-03.\n",
      "Epoch 131601, Training Loss: 30088, Validation Loss: 56062, 80283.56914411984\n",
      "Epoch 131648: reducing learning rate of group 0 to 2.7399e-03.\n",
      "Epoch 131701, Training Loss: 29071, Validation Loss: 59764, 75779.4128567126\n",
      "Epoch 131749: reducing learning rate of group 0 to 2.7372e-03.\n",
      "Epoch 131801, Training Loss: 29315, Validation Loss: 52349, 64683.215458799015\n",
      "Epoch 131850: reducing learning rate of group 0 to 2.7345e-03.\n",
      "Epoch 131901, Training Loss: 31444, Validation Loss: 54908, 68215.4279315963\n",
      "Epoch 131951: reducing learning rate of group 0 to 2.7317e-03.\n",
      "Epoch 132001, Training Loss: 30100, Validation Loss: 55002, 78560.10692894243\n",
      "Epoch 132052: reducing learning rate of group 0 to 2.7290e-03.\n",
      "Epoch 132101, Training Loss: 31505, Validation Loss: 54857, 69919.56385612261\n",
      "Epoch 132153: reducing learning rate of group 0 to 2.7263e-03.\n",
      "Epoch 132201, Training Loss: 31637, Validation Loss: 52270, 70295.95470263185\n",
      "Epoch 132254: reducing learning rate of group 0 to 2.7235e-03.\n",
      "Epoch 132301, Training Loss: 31448, Validation Loss: 50517, 68247.31679325235\n",
      "Epoch 132355: reducing learning rate of group 0 to 2.7208e-03.\n",
      "Epoch 132401, Training Loss: 26864, Validation Loss: 52663, 75314.96848047082\n",
      "Epoch 132456: reducing learning rate of group 0 to 2.7181e-03.\n",
      "Epoch 132501, Training Loss: 29237, Validation Loss: 53806, 71057.99841570166\n",
      "Epoch 132557: reducing learning rate of group 0 to 2.7154e-03.\n",
      "Epoch 132601, Training Loss: 31437, Validation Loss: 53333, 77916.78988190445\n",
      "Epoch 132658: reducing learning rate of group 0 to 2.7127e-03.\n",
      "Epoch 132701, Training Loss: 30078, Validation Loss: 53819, 66455.54403423906\n",
      "Epoch 132759: reducing learning rate of group 0 to 2.7100e-03.\n",
      "Epoch 132801, Training Loss: 27995, Validation Loss: 56648, 63612.366539851995\n",
      "Epoch 132860: reducing learning rate of group 0 to 2.7072e-03.\n",
      "Epoch 132901, Training Loss: 30898, Validation Loss: 51903, 69902.03300638111\n",
      "Epoch 132961: reducing learning rate of group 0 to 2.7045e-03.\n",
      "Epoch 133001, Training Loss: 32708, Validation Loss: 52810, 68140.6647112354\n",
      "Epoch 133062: reducing learning rate of group 0 to 2.7018e-03.\n",
      "Epoch 133101, Training Loss: 29884, Validation Loss: 52983, 71098.94120264951\n",
      "Epoch 133163: reducing learning rate of group 0 to 2.6991e-03.\n",
      "Epoch 133201, Training Loss: 30846, Validation Loss: 52556, 68839.69997349575\n",
      "Epoch 133264: reducing learning rate of group 0 to 2.6964e-03.\n",
      "Epoch 133301, Training Loss: 30778, Validation Loss: 52025, 79640.77461925894\n",
      "Epoch 133365: reducing learning rate of group 0 to 2.6937e-03.\n",
      "Epoch 133401, Training Loss: 30823, Validation Loss: 53036, 94858.24015550215\n",
      "Epoch 133466: reducing learning rate of group 0 to 2.6910e-03.\n",
      "Epoch 133501, Training Loss: 31049, Validation Loss: 53818, 72006.3525440683\n",
      "Epoch 133567: reducing learning rate of group 0 to 2.6884e-03.\n",
      "Epoch 133601, Training Loss: 29930, Validation Loss: 53534, 80110.82754427214\n",
      "Epoch 133668: reducing learning rate of group 0 to 2.6857e-03.\n",
      "Epoch 133701, Training Loss: 29220, Validation Loss: 55774, 67502.72333526805\n",
      "Epoch 133769: reducing learning rate of group 0 to 2.6830e-03.\n",
      "Epoch 133801, Training Loss: 29542, Validation Loss: 54583, 77742.8420656601\n",
      "Epoch 133870: reducing learning rate of group 0 to 2.6803e-03.\n",
      "Epoch 133901, Training Loss: 28506, Validation Loss: 52681, 75794.24619286998\n",
      "Epoch 133971: reducing learning rate of group 0 to 2.6776e-03.\n",
      "Epoch 134001, Training Loss: 28575, Validation Loss: 50710, 70719.83795022183\n",
      "Epoch 134072: reducing learning rate of group 0 to 2.6749e-03.\n",
      "Epoch 134101, Training Loss: 32404, Validation Loss: 54461, 66381.05882062101\n",
      "Epoch 134173: reducing learning rate of group 0 to 2.6723e-03.\n",
      "Epoch 134201, Training Loss: 28543, Validation Loss: 50869, 61514.23763501403\n",
      "Epoch 134274: reducing learning rate of group 0 to 2.6696e-03.\n",
      "Epoch 134301, Training Loss: 32523, Validation Loss: 54524, 68166.2780075672\n",
      "Epoch 134375: reducing learning rate of group 0 to 2.6669e-03.\n",
      "Epoch 134401, Training Loss: 28921, Validation Loss: 54447, 69609.35779330233\n",
      "Epoch 134476: reducing learning rate of group 0 to 2.6643e-03.\n",
      "Epoch 134501, Training Loss: 28316, Validation Loss: 52569, 62010.68642580974\n",
      "Epoch 134577: reducing learning rate of group 0 to 2.6616e-03.\n",
      "Epoch 134601, Training Loss: 29691, Validation Loss: 53159, 69685.2420313318\n",
      "Epoch 134678: reducing learning rate of group 0 to 2.6589e-03.\n",
      "Epoch 134701, Training Loss: 30660, Validation Loss: 52311, 67879.50675232829\n",
      "Epoch 134779: reducing learning rate of group 0 to 2.6563e-03.\n",
      "Epoch 134801, Training Loss: 33004, Validation Loss: 54608, 67904.72940377203\n",
      "Epoch 134880: reducing learning rate of group 0 to 2.6536e-03.\n",
      "Epoch 134901, Training Loss: 29912, Validation Loss: 51526, 65502.695710812695\n",
      "Epoch 134981: reducing learning rate of group 0 to 2.6510e-03.\n",
      "Epoch 135001, Training Loss: 31947, Validation Loss: 52257, 62951.765157003254\n",
      "Epoch 135082: reducing learning rate of group 0 to 2.6483e-03.\n",
      "Epoch 135101, Training Loss: 30836, Validation Loss: 52834, 70761.18398955844\n",
      "Epoch 135183: reducing learning rate of group 0 to 2.6457e-03.\n",
      "Epoch 135201, Training Loss: 28635, Validation Loss: 55446, 61114.240171010875\n",
      "Epoch 135284: reducing learning rate of group 0 to 2.6430e-03.\n",
      "Epoch 135301, Training Loss: 29766, Validation Loss: 53775, 82510.65502314676\n",
      "Epoch 135385: reducing learning rate of group 0 to 2.6404e-03.\n",
      "Epoch 135401, Training Loss: 30260, Validation Loss: 51919, 68045.52764345071\n",
      "Epoch 135486: reducing learning rate of group 0 to 2.6377e-03.\n",
      "Epoch 135501, Training Loss: 28521, Validation Loss: 55403, 68286.67194878483\n",
      "Epoch 135587: reducing learning rate of group 0 to 2.6351e-03.\n",
      "Epoch 135601, Training Loss: 31573, Validation Loss: 58870, 62124.78256357757\n",
      "Epoch 135688: reducing learning rate of group 0 to 2.6325e-03.\n",
      "Epoch 135701, Training Loss: 30198, Validation Loss: 58178, 77122.8809965877\n",
      "Epoch 135789: reducing learning rate of group 0 to 2.6298e-03.\n",
      "Epoch 135801, Training Loss: 29974, Validation Loss: 58257, 62953.66288425534\n",
      "Epoch 135890: reducing learning rate of group 0 to 2.6272e-03.\n",
      "Epoch 135901, Training Loss: 31066, Validation Loss: 56628, 71552.11007051902\n",
      "Epoch 135991: reducing learning rate of group 0 to 2.6246e-03.\n",
      "Epoch 136001, Training Loss: 30926, Validation Loss: 51681, 80864.32620643782\n",
      "Epoch 136092: reducing learning rate of group 0 to 2.6219e-03.\n",
      "Epoch 136101, Training Loss: 32249, Validation Loss: 54841, 71885.21264424917\n",
      "Epoch 136193: reducing learning rate of group 0 to 2.6193e-03.\n",
      "Epoch 136201, Training Loss: 30555, Validation Loss: 53267, 79288.18046165215\n",
      "Epoch 136294: reducing learning rate of group 0 to 2.6167e-03.\n",
      "Epoch 136301, Training Loss: 29809, Validation Loss: 51987, 65726.47758240812\n",
      "Epoch 136395: reducing learning rate of group 0 to 2.6141e-03.\n",
      "Epoch 136401, Training Loss: 30713, Validation Loss: 52641, 63398.277307761695\n",
      "Epoch 136496: reducing learning rate of group 0 to 2.6115e-03.\n",
      "Epoch 136501, Training Loss: 31156, Validation Loss: 55662, 76853.62412514818\n",
      "Epoch 136597: reducing learning rate of group 0 to 2.6089e-03.\n",
      "Epoch 136601, Training Loss: 28990, Validation Loss: 53704, 58901.65830146506\n",
      "Epoch 136698: reducing learning rate of group 0 to 2.6063e-03.\n",
      "Epoch 136701, Training Loss: 28573, Validation Loss: 55559, 75810.34858834776\n",
      "Epoch 136799: reducing learning rate of group 0 to 2.6036e-03.\n",
      "Epoch 136801, Training Loss: 29824, Validation Loss: 55487, 82191.46906023637\n",
      "Epoch 136900: reducing learning rate of group 0 to 2.6010e-03.\n",
      "Epoch 136901, Training Loss: 30306, Validation Loss: 51604, 58945.62774602435\n",
      "Epoch 137001: reducing learning rate of group 0 to 2.5984e-03.\n",
      "Epoch 137001, Training Loss: 31638, Validation Loss: 51329, 70293.106511619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137101, Training Loss: 30015, Validation Loss: 54517, 66724.25209671863\n",
      "Epoch 137102: reducing learning rate of group 0 to 2.5958e-03.\n",
      "Epoch 137201, Training Loss: 30204, Validation Loss: 54014, 75317.45304999726\n",
      "Epoch 137203: reducing learning rate of group 0 to 2.5932e-03.\n",
      "Epoch 137301, Training Loss: 30189, Validation Loss: 54121, 87525.92441077338\n",
      "Epoch 137304: reducing learning rate of group 0 to 2.5907e-03.\n",
      "Epoch 137401, Training Loss: 31019, Validation Loss: 55111, 85834.86084222535\n",
      "Epoch 137405: reducing learning rate of group 0 to 2.5881e-03.\n",
      "Epoch 137501, Training Loss: 28034, Validation Loss: 53566, 77157.48613736378\n",
      "Epoch 137506: reducing learning rate of group 0 to 2.5855e-03.\n",
      "Epoch 137601, Training Loss: 30119, Validation Loss: 53952, 66277.831182632\n",
      "Epoch 137607: reducing learning rate of group 0 to 2.5829e-03.\n",
      "Epoch 137701, Training Loss: 32529, Validation Loss: 54162, 63598.55801404238\n",
      "Epoch 137708: reducing learning rate of group 0 to 2.5803e-03.\n",
      "Epoch 137801, Training Loss: 30915, Validation Loss: 54339, 58005.34433684673\n",
      "Epoch 137809: reducing learning rate of group 0 to 2.5777e-03.\n",
      "Epoch 137901, Training Loss: 28073, Validation Loss: 54662, 69233.29515458233\n",
      "Epoch 137910: reducing learning rate of group 0 to 2.5751e-03.\n",
      "Epoch 138001, Training Loss: 29007, Validation Loss: 52437, 83095.20884201115\n",
      "Epoch 138011: reducing learning rate of group 0 to 2.5726e-03.\n",
      "Epoch 138101, Training Loss: 27273, Validation Loss: 53371, 72263.82271928382\n",
      "Epoch 138112: reducing learning rate of group 0 to 2.5700e-03.\n",
      "Epoch 138201, Training Loss: 32672, Validation Loss: 56884, 64948.27945085196\n",
      "Epoch 138213: reducing learning rate of group 0 to 2.5674e-03.\n",
      "Epoch 138301, Training Loss: 31583, Validation Loss: 56358, 55589.525850479746\n",
      "Epoch 138314: reducing learning rate of group 0 to 2.5649e-03.\n",
      "Epoch 138401, Training Loss: 33739, Validation Loss: 52739, 70854.08226102269\n",
      "Epoch 138415: reducing learning rate of group 0 to 2.5623e-03.\n",
      "Epoch 138501, Training Loss: 30706, Validation Loss: 55954, 66228.13249384699\n",
      "Epoch 138516: reducing learning rate of group 0 to 2.5597e-03.\n",
      "Epoch 138601, Training Loss: 31630, Validation Loss: 56845, 66451.92188141822\n",
      "Epoch 138617: reducing learning rate of group 0 to 2.5572e-03.\n",
      "Epoch 138701, Training Loss: 31453, Validation Loss: 55106, 77943.86315737375\n",
      "Epoch 138718: reducing learning rate of group 0 to 2.5546e-03.\n",
      "Epoch 138801, Training Loss: 29513, Validation Loss: 52597, 79669.38910509177\n",
      "Epoch 138819: reducing learning rate of group 0 to 2.5521e-03.\n",
      "Epoch 138901, Training Loss: 31091, Validation Loss: 56883, 69356.3030574238\n",
      "Epoch 138920: reducing learning rate of group 0 to 2.5495e-03.\n",
      "Epoch 139001, Training Loss: 29890, Validation Loss: 52238, 68224.3465489211\n",
      "Epoch 139021: reducing learning rate of group 0 to 2.5470e-03.\n",
      "Epoch 139101, Training Loss: 30537, Validation Loss: 54694, 76900.06051589479\n",
      "Epoch 139122: reducing learning rate of group 0 to 2.5444e-03.\n",
      "Epoch 139201, Training Loss: 29846, Validation Loss: 55448, 70129.9894163184\n",
      "Epoch 139223: reducing learning rate of group 0 to 2.5419e-03.\n",
      "Epoch 139301, Training Loss: 29810, Validation Loss: 52158, 65960.75880095862\n",
      "Epoch 139324: reducing learning rate of group 0 to 2.5393e-03.\n",
      "Epoch 139401, Training Loss: 29185, Validation Loss: 53898, 73493.64100212711\n",
      "Epoch 139425: reducing learning rate of group 0 to 2.5368e-03.\n",
      "Epoch 139501, Training Loss: 30439, Validation Loss: 54303, 77272.59847449826\n",
      "Epoch 139526: reducing learning rate of group 0 to 2.5343e-03.\n",
      "Epoch 139601, Training Loss: 32443, Validation Loss: 52591, 68580.64475218892\n",
      "Epoch 139627: reducing learning rate of group 0 to 2.5317e-03.\n",
      "Epoch 139701, Training Loss: 29954, Validation Loss: 55243, 63132.17124248854\n",
      "Epoch 139728: reducing learning rate of group 0 to 2.5292e-03.\n",
      "Epoch 139801, Training Loss: 29121, Validation Loss: 55152, 78252.8373950143\n",
      "Epoch 139829: reducing learning rate of group 0 to 2.5267e-03.\n",
      "Epoch 139901, Training Loss: 30552, Validation Loss: 52555, 68663.23624959904\n",
      "Epoch 139930: reducing learning rate of group 0 to 2.5241e-03.\n",
      "Epoch 140001, Training Loss: 30156, Validation Loss: 53808, 67027.30490845355\n",
      "Epoch 140031: reducing learning rate of group 0 to 2.5216e-03.\n",
      "Epoch 140101, Training Loss: 30722, Validation Loss: 52605, 67584.17002117244\n",
      "Epoch 140132: reducing learning rate of group 0 to 2.5191e-03.\n",
      "Epoch 140201, Training Loss: 29492, Validation Loss: 55145, 60824.83133696604\n",
      "Epoch 140233: reducing learning rate of group 0 to 2.5166e-03.\n",
      "Epoch 140301, Training Loss: 32765, Validation Loss: 54053, 58222.88679006542\n",
      "Epoch 140334: reducing learning rate of group 0 to 2.5140e-03.\n",
      "Epoch 140401, Training Loss: 32715, Validation Loss: 58683, 74240.81009404482\n",
      "Epoch 140435: reducing learning rate of group 0 to 2.5115e-03.\n",
      "Epoch 140501, Training Loss: 30456, Validation Loss: 54370, 82102.06133008985\n",
      "Epoch 140536: reducing learning rate of group 0 to 2.5090e-03.\n",
      "Epoch 140601, Training Loss: 28701, Validation Loss: 52850, 75424.49382822389\n",
      "Epoch 140637: reducing learning rate of group 0 to 2.5065e-03.\n",
      "Epoch 140701, Training Loss: 31371, Validation Loss: 56734, 83445.23608975923\n",
      "Epoch 140738: reducing learning rate of group 0 to 2.5040e-03.\n",
      "Epoch 140801, Training Loss: 29592, Validation Loss: 55608, 76484.28234768649\n",
      "Epoch 140839: reducing learning rate of group 0 to 2.5015e-03.\n",
      "Epoch 140901, Training Loss: 30587, Validation Loss: 53162, 69547.20467192009\n",
      "Epoch 140940: reducing learning rate of group 0 to 2.4990e-03.\n",
      "Epoch 141001, Training Loss: 30689, Validation Loss: 55694, 68865.71713621753\n",
      "Epoch 141041: reducing learning rate of group 0 to 2.4965e-03.\n",
      "Epoch 141101, Training Loss: 33200, Validation Loss: 55635, 67325.62626166757\n",
      "Epoch 141142: reducing learning rate of group 0 to 2.4940e-03.\n",
      "Epoch 141201, Training Loss: 31685, Validation Loss: 56264, 65972.8869662455\n",
      "Epoch 141243: reducing learning rate of group 0 to 2.4915e-03.\n",
      "Epoch 141301, Training Loss: 30882, Validation Loss: 55311, 87071.09763134172\n",
      "Epoch 141344: reducing learning rate of group 0 to 2.4890e-03.\n",
      "Epoch 141401, Training Loss: 29803, Validation Loss: 55795, 79525.4377624647\n",
      "Epoch 141445: reducing learning rate of group 0 to 2.4865e-03.\n",
      "Epoch 141501, Training Loss: 30807, Validation Loss: 53315, 82184.08242410945\n",
      "Epoch 141546: reducing learning rate of group 0 to 2.4840e-03.\n",
      "Epoch 141601, Training Loss: 29537, Validation Loss: 55302, 73083.67267863946\n",
      "Epoch 141647: reducing learning rate of group 0 to 2.4816e-03.\n",
      "Epoch 141701, Training Loss: 29433, Validation Loss: 59061, 66180.40638606854\n",
      "Epoch 141748: reducing learning rate of group 0 to 2.4791e-03.\n",
      "Epoch 141801, Training Loss: 32851, Validation Loss: 55260, 79668.27502627566\n",
      "Epoch 141849: reducing learning rate of group 0 to 2.4766e-03.\n",
      "Epoch 141901, Training Loss: 31253, Validation Loss: 51138, 66234.36337586095\n",
      "Epoch 141950: reducing learning rate of group 0 to 2.4741e-03.\n",
      "Epoch 142001, Training Loss: 27201, Validation Loss: 51893, 67079.70591002055\n",
      "Epoch 142051: reducing learning rate of group 0 to 2.4717e-03.\n",
      "Epoch 142101, Training Loss: 29057, Validation Loss: 51422, 65392.663583885784\n",
      "Epoch 142152: reducing learning rate of group 0 to 2.4692e-03.\n",
      "Epoch 142201, Training Loss: 29651, Validation Loss: 53189, 88591.4560859119\n",
      "Epoch 142253: reducing learning rate of group 0 to 2.4667e-03.\n",
      "Epoch 142301, Training Loss: 30358, Validation Loss: 51657, 73560.72197974003\n",
      "Epoch 142354: reducing learning rate of group 0 to 2.4642e-03.\n",
      "Epoch 142401, Training Loss: 29023, Validation Loss: 56145, 65475.77655707977\n",
      "Epoch 142455: reducing learning rate of group 0 to 2.4618e-03.\n",
      "Epoch 142501, Training Loss: 28115, Validation Loss: 53362, 64321.325061943906\n",
      "Epoch 142556: reducing learning rate of group 0 to 2.4593e-03.\n",
      "Epoch 142601, Training Loss: 29559, Validation Loss: 52786, 65569.31011539348\n",
      "Epoch 142657: reducing learning rate of group 0 to 2.4569e-03.\n",
      "Epoch 142701, Training Loss: 29955, Validation Loss: 52440, 65585.90422228888\n",
      "Epoch 142758: reducing learning rate of group 0 to 2.4544e-03.\n",
      "Epoch 142801, Training Loss: 30111, Validation Loss: 53949, 62173.776958694914\n",
      "Epoch 142859: reducing learning rate of group 0 to 2.4519e-03.\n",
      "Epoch 142901, Training Loss: 30320, Validation Loss: 60137, 73971.17551928366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142960: reducing learning rate of group 0 to 2.4495e-03.\n",
      "Epoch 143001, Training Loss: 32028, Validation Loss: 54417, 78492.8573496385\n",
      "Epoch 143061: reducing learning rate of group 0 to 2.4470e-03.\n",
      "Epoch 143101, Training Loss: 30503, Validation Loss: 54518, 56823.1813050069\n",
      "Epoch 143162: reducing learning rate of group 0 to 2.4446e-03.\n",
      "Epoch 143201, Training Loss: 31479, Validation Loss: 54354, 67213.14982411753\n",
      "Epoch 143263: reducing learning rate of group 0 to 2.4422e-03.\n",
      "Epoch 143301, Training Loss: 33819, Validation Loss: 54621, 72681.1857428961\n",
      "Epoch 143364: reducing learning rate of group 0 to 2.4397e-03.\n",
      "Epoch 143401, Training Loss: 29919, Validation Loss: 55787, 70973.25722441006\n",
      "Epoch 143465: reducing learning rate of group 0 to 2.4373e-03.\n",
      "Epoch 143501, Training Loss: 31914, Validation Loss: 51899, 84680.43348352381\n",
      "Epoch 143566: reducing learning rate of group 0 to 2.4348e-03.\n",
      "Epoch 143601, Training Loss: 30839, Validation Loss: 54184, 72576.8647810158\n",
      "Epoch 143667: reducing learning rate of group 0 to 2.4324e-03.\n",
      "Epoch 143701, Training Loss: 28655, Validation Loss: 53356, 67380.16984470798\n",
      "Epoch 143768: reducing learning rate of group 0 to 2.4300e-03.\n",
      "Epoch 143801, Training Loss: 35014, Validation Loss: 57220, 88341.82640911452\n",
      "Epoch 143869: reducing learning rate of group 0 to 2.4275e-03.\n",
      "Epoch 143901, Training Loss: 28449, Validation Loss: 53177, 71066.36266871767\n",
      "Epoch 143970: reducing learning rate of group 0 to 2.4251e-03.\n",
      "Epoch 144001, Training Loss: 31643, Validation Loss: 53959, 60012.06346335547\n",
      "Epoch 144071: reducing learning rate of group 0 to 2.4227e-03.\n",
      "Epoch 144101, Training Loss: 30126, Validation Loss: 53319, 78107.61439485673\n",
      "Epoch 144172: reducing learning rate of group 0 to 2.4203e-03.\n",
      "Epoch 144201, Training Loss: 30000, Validation Loss: 54130, 96528.16727531103\n",
      "Epoch 144273: reducing learning rate of group 0 to 2.4178e-03.\n",
      "Epoch 144301, Training Loss: 30959, Validation Loss: 54045, 88592.76512482848\n",
      "Epoch 144374: reducing learning rate of group 0 to 2.4154e-03.\n",
      "Epoch 144401, Training Loss: 27869, Validation Loss: 52569, 75825.26850419819\n",
      "Epoch 144475: reducing learning rate of group 0 to 2.4130e-03.\n",
      "Epoch 144501, Training Loss: 30327, Validation Loss: 52577, 73308.19985644417\n",
      "Epoch 144576: reducing learning rate of group 0 to 2.4106e-03.\n",
      "Epoch 144601, Training Loss: 30989, Validation Loss: 55706, 76270.63670290766\n",
      "Epoch 144677: reducing learning rate of group 0 to 2.4082e-03.\n",
      "Epoch 144701, Training Loss: 32230, Validation Loss: 53989, 75010.16773594276\n",
      "Epoch 144778: reducing learning rate of group 0 to 2.4058e-03.\n",
      "Epoch 144801, Training Loss: 26630, Validation Loss: 53872, 63116.8944757949\n",
      "Epoch 144879: reducing learning rate of group 0 to 2.4034e-03.\n",
      "Epoch 144901, Training Loss: 28316, Validation Loss: 53756, 58888.273608305004\n",
      "Epoch 144980: reducing learning rate of group 0 to 2.4010e-03.\n",
      "Epoch 145001, Training Loss: 31482, Validation Loss: 53410, 71458.42393011245\n",
      "Epoch 145081: reducing learning rate of group 0 to 2.3986e-03.\n",
      "Epoch 145101, Training Loss: 30686, Validation Loss: 54844, 67540.04912327659\n",
      "Epoch 145182: reducing learning rate of group 0 to 2.3962e-03.\n",
      "Epoch 145201, Training Loss: 30157, Validation Loss: 52994, 71947.11224846443\n",
      "Epoch 145283: reducing learning rate of group 0 to 2.3938e-03.\n",
      "Epoch 145301, Training Loss: 31090, Validation Loss: 55017, 67740.6667594502\n",
      "Epoch 145384: reducing learning rate of group 0 to 2.3914e-03.\n",
      "Epoch 145401, Training Loss: 28389, Validation Loss: 53466, 81228.74142052855\n",
      "Epoch 145485: reducing learning rate of group 0 to 2.3890e-03.\n",
      "Epoch 145501, Training Loss: 29813, Validation Loss: 53750, 72575.51963133234\n",
      "Epoch 145586: reducing learning rate of group 0 to 2.3866e-03.\n",
      "Epoch 145601, Training Loss: 29528, Validation Loss: 54011, 80448.31937679368\n",
      "Epoch 145687: reducing learning rate of group 0 to 2.3842e-03.\n",
      "Epoch 145701, Training Loss: 28721, Validation Loss: 54403, 66525.27625929612\n",
      "Epoch 145788: reducing learning rate of group 0 to 2.3818e-03.\n",
      "Epoch 145801, Training Loss: 29316, Validation Loss: 53640, 71211.72729323815\n",
      "Epoch 145889: reducing learning rate of group 0 to 2.3794e-03.\n",
      "Epoch 145901, Training Loss: 30267, Validation Loss: 52192, 71592.69199767243\n",
      "Epoch 145990: reducing learning rate of group 0 to 2.3771e-03.\n",
      "Epoch 146001, Training Loss: 30464, Validation Loss: 54119, 79336.0591959603\n",
      "Epoch 146091: reducing learning rate of group 0 to 2.3747e-03.\n",
      "Epoch 146101, Training Loss: 28190, Validation Loss: 54585, 70714.19546586419\n",
      "Epoch 146192: reducing learning rate of group 0 to 2.3723e-03.\n",
      "Epoch 146201, Training Loss: 30915, Validation Loss: 54678, 71423.15806673629\n",
      "Epoch 146293: reducing learning rate of group 0 to 2.3699e-03.\n",
      "Epoch 146301, Training Loss: 29846, Validation Loss: 57749, 69132.50309967181\n",
      "Epoch 146394: reducing learning rate of group 0 to 2.3676e-03.\n",
      "Epoch 146401, Training Loss: 33303, Validation Loss: 55319, 71782.16261413904\n",
      "Epoch 146495: reducing learning rate of group 0 to 2.3652e-03.\n",
      "Epoch 146501, Training Loss: 30480, Validation Loss: 55035, 69663.02767089284\n",
      "Epoch 146596: reducing learning rate of group 0 to 2.3628e-03.\n",
      "Epoch 146601, Training Loss: 28817, Validation Loss: 51468, 68074.08815607682\n",
      "Epoch 146697: reducing learning rate of group 0 to 2.3605e-03.\n",
      "Epoch 146701, Training Loss: 29242, Validation Loss: 53751, 70978.01393092489\n",
      "Epoch 146798: reducing learning rate of group 0 to 2.3581e-03.\n",
      "Epoch 146801, Training Loss: 29778, Validation Loss: 51916, 66565.49905416754\n",
      "Epoch 146899: reducing learning rate of group 0 to 2.3558e-03.\n",
      "Epoch 146901, Training Loss: 30117, Validation Loss: 57020, 73083.37716396661\n",
      "Epoch 147000: reducing learning rate of group 0 to 2.3534e-03.\n",
      "Epoch 147001, Training Loss: 27870, Validation Loss: 55601, 72229.23471105694\n",
      "Epoch 147101: reducing learning rate of group 0 to 2.3510e-03.\n",
      "Epoch 147101, Training Loss: 28926, Validation Loss: 51598, 62468.06488251698\n",
      "Epoch 147201, Training Loss: 29595, Validation Loss: 51414, 69642.99324390473\n",
      "Epoch 147202: reducing learning rate of group 0 to 2.3487e-03.\n",
      "Epoch 147301, Training Loss: 30216, Validation Loss: 51946, 68212.87009820312\n",
      "Epoch 147303: reducing learning rate of group 0 to 2.3463e-03.\n",
      "Epoch 147401, Training Loss: 31524, Validation Loss: 54663, 71044.01122842001\n",
      "Epoch 147404: reducing learning rate of group 0 to 2.3440e-03.\n",
      "Epoch 147501, Training Loss: 29718, Validation Loss: 53606, 69780.06427288544\n",
      "Epoch 147505: reducing learning rate of group 0 to 2.3417e-03.\n",
      "Epoch 147601, Training Loss: 32180, Validation Loss: 53266, 59146.320996002374\n",
      "Epoch 147606: reducing learning rate of group 0 to 2.3393e-03.\n",
      "Epoch 147701, Training Loss: 29939, Validation Loss: 55166, 76040.39358300556\n",
      "Epoch 147707: reducing learning rate of group 0 to 2.3370e-03.\n",
      "Epoch 147801, Training Loss: 30219, Validation Loss: 57390, 75526.12395643805\n",
      "Epoch 147808: reducing learning rate of group 0 to 2.3346e-03.\n",
      "Epoch 147901, Training Loss: 30286, Validation Loss: 53223, 76804.81603102134\n",
      "Epoch 147909: reducing learning rate of group 0 to 2.3323e-03.\n",
      "Epoch 148001, Training Loss: 29624, Validation Loss: 55656, 81165.59623941063\n",
      "Epoch 148010: reducing learning rate of group 0 to 2.3300e-03.\n",
      "Epoch 148101, Training Loss: 30980, Validation Loss: 55236, 83523.73819913523\n",
      "Epoch 148111: reducing learning rate of group 0 to 2.3276e-03.\n",
      "Epoch 148201, Training Loss: 31414, Validation Loss: 53457, 72130.24157610041\n",
      "Epoch 148212: reducing learning rate of group 0 to 2.3253e-03.\n",
      "Epoch 148301, Training Loss: 31612, Validation Loss: 54610, 75163.36368227855\n",
      "Epoch 148313: reducing learning rate of group 0 to 2.3230e-03.\n",
      "Epoch 148401, Training Loss: 31005, Validation Loss: 56224, 66029.83481755866\n",
      "Epoch 148414: reducing learning rate of group 0 to 2.3207e-03.\n",
      "Epoch 148501, Training Loss: 32643, Validation Loss: 54525, 92776.85335897496\n",
      "Epoch 148515: reducing learning rate of group 0 to 2.3183e-03.\n",
      "Epoch 148601, Training Loss: 30214, Validation Loss: 53809, 70463.07459396879\n",
      "Epoch 148616: reducing learning rate of group 0 to 2.3160e-03.\n",
      "Epoch 148701, Training Loss: 29467, Validation Loss: 53990, 69199.03099993263\n",
      "Epoch 148717: reducing learning rate of group 0 to 2.3137e-03.\n",
      "Epoch 148801, Training Loss: 30788, Validation Loss: 57085, 86673.21974971038\n",
      "Epoch 148818: reducing learning rate of group 0 to 2.3114e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148901, Training Loss: 28838, Validation Loss: 53830, 73843.69841554774\n",
      "Epoch 148919: reducing learning rate of group 0 to 2.3091e-03.\n",
      "Epoch 149001, Training Loss: 31827, Validation Loss: 55132, 75179.149220685\n",
      "Epoch 149020: reducing learning rate of group 0 to 2.3068e-03.\n",
      "Epoch 149101, Training Loss: 30168, Validation Loss: 57054, 71479.8740562888\n",
      "Epoch 149121: reducing learning rate of group 0 to 2.3045e-03.\n",
      "Epoch 149201, Training Loss: 31707, Validation Loss: 55062, 73179.16936001311\n",
      "Epoch 149222: reducing learning rate of group 0 to 2.3022e-03.\n",
      "Epoch 149301, Training Loss: 28794, Validation Loss: 54412, 65751.62500925978\n",
      "Epoch 149323: reducing learning rate of group 0 to 2.2999e-03.\n",
      "Epoch 149401, Training Loss: 28507, Validation Loss: 51911, 64765.84877669195\n",
      "Epoch 149424: reducing learning rate of group 0 to 2.2976e-03.\n",
      "Epoch 149501, Training Loss: 30234, Validation Loss: 54727, 67664.12258746372\n",
      "Epoch 149525: reducing learning rate of group 0 to 2.2953e-03.\n",
      "Epoch 149601, Training Loss: 29840, Validation Loss: 55003, 72297.72501290702\n",
      "Epoch 149626: reducing learning rate of group 0 to 2.2930e-03.\n",
      "Epoch 149701, Training Loss: 29076, Validation Loss: 53168, 58735.903056558396\n",
      "Epoch 149727: reducing learning rate of group 0 to 2.2907e-03.\n",
      "Epoch 149801, Training Loss: 29666, Validation Loss: 54672, 86208.42897854462\n",
      "Epoch 149828: reducing learning rate of group 0 to 2.2884e-03.\n",
      "Epoch 149901, Training Loss: 29986, Validation Loss: 57529, 98657.5876654554\n",
      "Epoch 149929: reducing learning rate of group 0 to 2.2861e-03.\n",
      "Epoch 150001, Training Loss: 30327, Validation Loss: 54949, 69370.67595852222\n",
      "Epoch 150030: reducing learning rate of group 0 to 2.2838e-03.\n",
      "Epoch 150101, Training Loss: 28665, Validation Loss: 52644, 66741.08746771772\n",
      "Epoch 150131: reducing learning rate of group 0 to 2.2815e-03.\n",
      "Epoch 150201, Training Loss: 28565, Validation Loss: 54213, 84817.7002858329\n",
      "Epoch 150232: reducing learning rate of group 0 to 2.2792e-03.\n",
      "Epoch 150301, Training Loss: 29791, Validation Loss: 55886, 76206.24730730435\n",
      "Epoch 150333: reducing learning rate of group 0 to 2.2770e-03.\n",
      "Epoch 150401, Training Loss: 29496, Validation Loss: 52285, 65477.561249867955\n",
      "Epoch 150434: reducing learning rate of group 0 to 2.2747e-03.\n",
      "Epoch 150501, Training Loss: 27513, Validation Loss: 52932, 70611.43738459585\n",
      "Epoch 150535: reducing learning rate of group 0 to 2.2724e-03.\n",
      "Epoch 150601, Training Loss: 31046, Validation Loss: 55672, 73360.91404410357\n",
      "Epoch 150636: reducing learning rate of group 0 to 2.2701e-03.\n",
      "Epoch 150701, Training Loss: 30956, Validation Loss: 53367, 81994.12241509683\n",
      "Epoch 150737: reducing learning rate of group 0 to 2.2679e-03.\n",
      "Epoch 150801, Training Loss: 30786, Validation Loss: 54754, 65530.52232028651\n",
      "Epoch 150838: reducing learning rate of group 0 to 2.2656e-03.\n",
      "Epoch 150901, Training Loss: 30842, Validation Loss: 54797, 74294.64234937995\n",
      "Epoch 150939: reducing learning rate of group 0 to 2.2633e-03.\n",
      "Epoch 151001, Training Loss: 27819, Validation Loss: 53331, 84149.37901599753\n",
      "Epoch 151040: reducing learning rate of group 0 to 2.2611e-03.\n",
      "Epoch 151101, Training Loss: 31412, Validation Loss: 54078, 63333.66903240094\n",
      "Epoch 151141: reducing learning rate of group 0 to 2.2588e-03.\n",
      "Epoch 151201, Training Loss: 29477, Validation Loss: 53720, 78187.83996158787\n",
      "Epoch 151242: reducing learning rate of group 0 to 2.2566e-03.\n",
      "Epoch 151301, Training Loss: 28330, Validation Loss: 52332, 71494.36418840867\n",
      "Epoch 151343: reducing learning rate of group 0 to 2.2543e-03.\n",
      "Epoch 151401, Training Loss: 28753, Validation Loss: 52386, 77997.6446304532\n",
      "Epoch 151444: reducing learning rate of group 0 to 2.2520e-03.\n",
      "Epoch 151501, Training Loss: 29270, Validation Loss: 55174, 68327.43529271027\n",
      "Epoch 151545: reducing learning rate of group 0 to 2.2498e-03.\n",
      "Epoch 151601, Training Loss: 31221, Validation Loss: 55978, 76008.27583500855\n",
      "Epoch 151646: reducing learning rate of group 0 to 2.2475e-03.\n",
      "Epoch 151701, Training Loss: 32600, Validation Loss: 54987, 73873.60057528713\n",
      "Epoch 151747: reducing learning rate of group 0 to 2.2453e-03.\n",
      "Epoch 151801, Training Loss: 27243, Validation Loss: 52279, 70365.35547511194\n",
      "Epoch 151848: reducing learning rate of group 0 to 2.2431e-03.\n",
      "Epoch 151901, Training Loss: 30213, Validation Loss: 52836, 76883.80430634477\n",
      "Epoch 151949: reducing learning rate of group 0 to 2.2408e-03.\n",
      "Epoch 152001, Training Loss: 29976, Validation Loss: 53936, 93926.17466952332\n",
      "Epoch 152050: reducing learning rate of group 0 to 2.2386e-03.\n",
      "Epoch 152101, Training Loss: 29885, Validation Loss: 54591, 72983.22159012634\n",
      "Epoch 152151: reducing learning rate of group 0 to 2.2363e-03.\n",
      "Epoch 152201, Training Loss: 29039, Validation Loss: 55779, 67917.08918983834\n",
      "Epoch 152252: reducing learning rate of group 0 to 2.2341e-03.\n",
      "Epoch 152301, Training Loss: 30377, Validation Loss: 53312, 63365.35460195262\n",
      "Epoch 152353: reducing learning rate of group 0 to 2.2319e-03.\n",
      "Epoch 152401, Training Loss: 30692, Validation Loss: 53554, 60769.75729580055\n",
      "Epoch 152454: reducing learning rate of group 0 to 2.2296e-03.\n",
      "Epoch 152501, Training Loss: 30096, Validation Loss: 53878, 57518.29243227529\n",
      "Epoch 152555: reducing learning rate of group 0 to 2.2274e-03.\n",
      "Epoch 152601, Training Loss: 29116, Validation Loss: 54196, 77416.53692514739\n",
      "Epoch 152656: reducing learning rate of group 0 to 2.2252e-03.\n",
      "Epoch 152701, Training Loss: 27683, Validation Loss: 51791, 61247.454419028894\n",
      "Epoch 152757: reducing learning rate of group 0 to 2.2229e-03.\n",
      "Epoch 152801, Training Loss: 30178, Validation Loss: 53683, 78593.81428801829\n",
      "Epoch 152858: reducing learning rate of group 0 to 2.2207e-03.\n",
      "Epoch 152901, Training Loss: 28881, Validation Loss: 55896, 70168.52105610738\n",
      "Epoch 152959: reducing learning rate of group 0 to 2.2185e-03.\n",
      "Epoch 153001, Training Loss: 28556, Validation Loss: 53831, 67650.12116856685\n",
      "Epoch 153060: reducing learning rate of group 0 to 2.2163e-03.\n",
      "Epoch 153101, Training Loss: 26900, Validation Loss: 54762, 64950.48583629856\n",
      "Epoch 153161: reducing learning rate of group 0 to 2.2141e-03.\n",
      "Epoch 153201, Training Loss: 29399, Validation Loss: 52756, 71526.55821378647\n",
      "Epoch 153262: reducing learning rate of group 0 to 2.2119e-03.\n",
      "Epoch 153301, Training Loss: 29368, Validation Loss: 54443, 66818.67363085026\n",
      "Epoch 153363: reducing learning rate of group 0 to 2.2096e-03.\n",
      "Epoch 153401, Training Loss: 27714, Validation Loss: 53451, 66948.05468022375\n",
      "Epoch 153464: reducing learning rate of group 0 to 2.2074e-03.\n",
      "Epoch 153501, Training Loss: 31178, Validation Loss: 52401, 66248.31359936528\n",
      "Epoch 153565: reducing learning rate of group 0 to 2.2052e-03.\n",
      "Epoch 153601, Training Loss: 30088, Validation Loss: 52157, 70844.9749205688\n",
      "Epoch 153666: reducing learning rate of group 0 to 2.2030e-03.\n",
      "Epoch 153701, Training Loss: 31269, Validation Loss: 54605, 64087.18647177272\n",
      "Epoch 153767: reducing learning rate of group 0 to 2.2008e-03.\n",
      "Epoch 153801, Training Loss: 29402, Validation Loss: 53409, 61686.898985481435\n",
      "Epoch 153868: reducing learning rate of group 0 to 2.1986e-03.\n",
      "Epoch 153901, Training Loss: 29146, Validation Loss: 54366, 79256.10068497852\n",
      "Epoch 153969: reducing learning rate of group 0 to 2.1964e-03.\n",
      "Epoch 154001, Training Loss: 31694, Validation Loss: 53135, 58904.35236030305\n",
      "Epoch 154070: reducing learning rate of group 0 to 2.1942e-03.\n",
      "Epoch 154101, Training Loss: 32184, Validation Loss: 53311, 69475.24418194823\n",
      "Epoch 154171: reducing learning rate of group 0 to 2.1920e-03.\n",
      "Epoch 154201, Training Loss: 29311, Validation Loss: 56957, 74915.82420815883\n",
      "Epoch 154272: reducing learning rate of group 0 to 2.1898e-03.\n",
      "Epoch 154301, Training Loss: 29171, Validation Loss: 55548, 84484.6768923254\n",
      "Epoch 154373: reducing learning rate of group 0 to 2.1876e-03.\n",
      "Epoch 154401, Training Loss: 29391, Validation Loss: 53434, 87061.27154113095\n",
      "Epoch 154474: reducing learning rate of group 0 to 2.1855e-03.\n",
      "Epoch 154501, Training Loss: 32718, Validation Loss: 54548, 67970.59806352416\n",
      "Epoch 154575: reducing learning rate of group 0 to 2.1833e-03.\n",
      "Epoch 154601, Training Loss: 27357, Validation Loss: 54910, 62800.10485367692\n",
      "Epoch 154676: reducing learning rate of group 0 to 2.1811e-03.\n",
      "Epoch 154701, Training Loss: 28816, Validation Loss: 55783, 80794.14402489673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154777: reducing learning rate of group 0 to 2.1789e-03.\n",
      "Epoch 154801, Training Loss: 29234, Validation Loss: 51950, 72906.66530586268\n",
      "Epoch 154878: reducing learning rate of group 0 to 2.1767e-03.\n",
      "Epoch 154901, Training Loss: 30126, Validation Loss: 56334, 66417.91947138173\n",
      "Epoch 154979: reducing learning rate of group 0 to 2.1746e-03.\n",
      "Epoch 155001, Training Loss: 28574, Validation Loss: 52311, 60623.114874888735\n",
      "Epoch 155080: reducing learning rate of group 0 to 2.1724e-03.\n",
      "Epoch 155101, Training Loss: 29751, Validation Loss: 53199, 79163.14727645741\n",
      "Epoch 155181: reducing learning rate of group 0 to 2.1702e-03.\n",
      "Epoch 155201, Training Loss: 29612, Validation Loss: 57421, 72209.1854375121\n",
      "Epoch 155282: reducing learning rate of group 0 to 2.1680e-03.\n",
      "Epoch 155301, Training Loss: 29655, Validation Loss: 54087, 62575.70365880544\n",
      "Epoch 155383: reducing learning rate of group 0 to 2.1659e-03.\n",
      "Epoch 155401, Training Loss: 30443, Validation Loss: 54091, 98466.37406491714\n",
      "Epoch 155484: reducing learning rate of group 0 to 2.1637e-03.\n",
      "Epoch 155501, Training Loss: 28661, Validation Loss: 57640, 73830.639050791\n",
      "Epoch 155585: reducing learning rate of group 0 to 2.1615e-03.\n",
      "Epoch 155601, Training Loss: 29375, Validation Loss: 56183, 66023.67199940562\n",
      "Epoch 155686: reducing learning rate of group 0 to 2.1594e-03.\n",
      "Epoch 155701, Training Loss: 29263, Validation Loss: 55826, 69520.78708369512\n",
      "Epoch 155787: reducing learning rate of group 0 to 2.1572e-03.\n",
      "Epoch 155801, Training Loss: 29788, Validation Loss: 53489, 87661.6313613864\n",
      "Epoch 155888: reducing learning rate of group 0 to 2.1551e-03.\n",
      "Epoch 155901, Training Loss: 28601, Validation Loss: 55945, 65745.67722301617\n",
      "Epoch 155989: reducing learning rate of group 0 to 2.1529e-03.\n",
      "Epoch 156001, Training Loss: 29181, Validation Loss: 53545, 79625.90956690861\n",
      "Epoch 156090: reducing learning rate of group 0 to 2.1507e-03.\n",
      "Epoch 156101, Training Loss: 26902, Validation Loss: 56510, 71936.05477682698\n",
      "Epoch 156191: reducing learning rate of group 0 to 2.1486e-03.\n",
      "Epoch 156201, Training Loss: 31478, Validation Loss: 52709, 72093.93489975917\n",
      "Epoch 156292: reducing learning rate of group 0 to 2.1465e-03.\n",
      "Epoch 156301, Training Loss: 30607, Validation Loss: 54073, 78992.10546589944\n",
      "Epoch 156393: reducing learning rate of group 0 to 2.1443e-03.\n",
      "Epoch 156401, Training Loss: 28269, Validation Loss: 53605, 71643.26350314765\n",
      "Epoch 156494: reducing learning rate of group 0 to 2.1422e-03.\n",
      "Epoch 156501, Training Loss: 27869, Validation Loss: 53391, 67903.12946074373\n",
      "Epoch 156595: reducing learning rate of group 0 to 2.1400e-03.\n",
      "Epoch 156601, Training Loss: 28473, Validation Loss: 55064, 63751.73020814623\n",
      "Epoch 156696: reducing learning rate of group 0 to 2.1379e-03.\n",
      "Epoch 156701, Training Loss: 30856, Validation Loss: 54330, 71291.61714654835\n",
      "Epoch 156797: reducing learning rate of group 0 to 2.1357e-03.\n",
      "Epoch 156801, Training Loss: 30138, Validation Loss: 58377, 59077.97790548886\n",
      "Epoch 156898: reducing learning rate of group 0 to 2.1336e-03.\n",
      "Epoch 156901, Training Loss: 29624, Validation Loss: 54904, 99876.93637177038\n",
      "Epoch 156999: reducing learning rate of group 0 to 2.1315e-03.\n",
      "Epoch 157001, Training Loss: 27028, Validation Loss: 56559, 65620.70191899971\n",
      "Epoch 157100: reducing learning rate of group 0 to 2.1293e-03.\n",
      "Epoch 157101, Training Loss: 27957, Validation Loss: 54845, 78242.34315022116\n",
      "Epoch 157201: reducing learning rate of group 0 to 2.1272e-03.\n",
      "Epoch 157201, Training Loss: 29617, Validation Loss: 54087, 75846.84158791883\n",
      "Epoch 157301, Training Loss: 29263, Validation Loss: 55205, 78355.29874362041\n",
      "Epoch 157302: reducing learning rate of group 0 to 2.1251e-03.\n",
      "Epoch 157401, Training Loss: 29111, Validation Loss: 52956, 56611.300990641954\n",
      "Epoch 157403: reducing learning rate of group 0 to 2.1230e-03.\n",
      "Epoch 157501, Training Loss: 30161, Validation Loss: 54071, 78626.07431901923\n",
      "Epoch 157504: reducing learning rate of group 0 to 2.1208e-03.\n",
      "Epoch 157601, Training Loss: 30858, Validation Loss: 55137, 63113.42263416915\n",
      "Epoch 157605: reducing learning rate of group 0 to 2.1187e-03.\n",
      "Epoch 157701, Training Loss: 28569, Validation Loss: 55399, 70996.93495189726\n",
      "Epoch 157706: reducing learning rate of group 0 to 2.1166e-03.\n",
      "Epoch 157801, Training Loss: 29491, Validation Loss: 53438, 79439.93206989391\n",
      "Epoch 157807: reducing learning rate of group 0 to 2.1145e-03.\n",
      "Epoch 157901, Training Loss: 33168, Validation Loss: 55384, 73769.05991493106\n",
      "Epoch 157908: reducing learning rate of group 0 to 2.1124e-03.\n",
      "Epoch 158001, Training Loss: 29624, Validation Loss: 53097, 106468.73824858344\n",
      "Epoch 158009: reducing learning rate of group 0 to 2.1103e-03.\n",
      "Epoch 158101, Training Loss: 29226, Validation Loss: 54941, 81502.39163316065\n",
      "Epoch 158110: reducing learning rate of group 0 to 2.1081e-03.\n",
      "Epoch 158201, Training Loss: 27501, Validation Loss: 53235, 80275.62408405665\n",
      "Epoch 158211: reducing learning rate of group 0 to 2.1060e-03.\n",
      "Epoch 158301, Training Loss: 31802, Validation Loss: 56191, 90113.35941736975\n",
      "Epoch 158312: reducing learning rate of group 0 to 2.1039e-03.\n",
      "Epoch 158401, Training Loss: 29300, Validation Loss: 55594, 76925.73026900517\n",
      "Epoch 158413: reducing learning rate of group 0 to 2.1018e-03.\n",
      "Epoch 158501, Training Loss: 32611, Validation Loss: 55074, 69712.78380763573\n",
      "Epoch 158514: reducing learning rate of group 0 to 2.0997e-03.\n",
      "Epoch 158601, Training Loss: 28835, Validation Loss: 55949, 72432.71645197796\n",
      "Epoch 158615: reducing learning rate of group 0 to 2.0976e-03.\n",
      "Epoch 158701, Training Loss: 29943, Validation Loss: 51514, 74277.099199848\n",
      "Epoch 158716: reducing learning rate of group 0 to 2.0955e-03.\n",
      "Epoch 158801, Training Loss: 28108, Validation Loss: 54067, 71658.43139134684\n",
      "Epoch 158817: reducing learning rate of group 0 to 2.0934e-03.\n",
      "Epoch 158901, Training Loss: 30193, Validation Loss: 55494, 73971.38222911474\n",
      "Epoch 158918: reducing learning rate of group 0 to 2.0913e-03.\n",
      "Epoch 159001, Training Loss: 28364, Validation Loss: 54952, 65937.79328569472\n",
      "Epoch 159019: reducing learning rate of group 0 to 2.0892e-03.\n",
      "Epoch 159101, Training Loss: 30040, Validation Loss: 54223, 71887.17435468458\n",
      "Epoch 159120: reducing learning rate of group 0 to 2.0872e-03.\n",
      "Epoch 159201, Training Loss: 30554, Validation Loss: 55835, 74374.88214262738\n",
      "Epoch 159221: reducing learning rate of group 0 to 2.0851e-03.\n",
      "Epoch 159301, Training Loss: 28301, Validation Loss: 52942, 74162.63767209598\n",
      "Epoch 159322: reducing learning rate of group 0 to 2.0830e-03.\n",
      "Epoch 159401, Training Loss: 29450, Validation Loss: 54745, 69591.229570753\n",
      "Epoch 159423: reducing learning rate of group 0 to 2.0809e-03.\n",
      "Epoch 159501, Training Loss: 29310, Validation Loss: 53010, 67680.51992178854\n",
      "Epoch 159524: reducing learning rate of group 0 to 2.0788e-03.\n",
      "Epoch 159601, Training Loss: 29212, Validation Loss: 55054, 70667.43225229741\n",
      "Epoch 159625: reducing learning rate of group 0 to 2.0767e-03.\n",
      "Epoch 159701, Training Loss: 29829, Validation Loss: 54865, 79128.68520048771\n",
      "Epoch 159726: reducing learning rate of group 0 to 2.0747e-03.\n",
      "Epoch 159801, Training Loss: 27991, Validation Loss: 55627, 80868.12647149908\n",
      "Epoch 159827: reducing learning rate of group 0 to 2.0726e-03.\n",
      "Epoch 159901, Training Loss: 30209, Validation Loss: 51530, 60849.37769993883\n",
      "Epoch 159928: reducing learning rate of group 0 to 2.0705e-03.\n",
      "Epoch 160001, Training Loss: 29822, Validation Loss: 55198, 86016.66753633415\n",
      "Epoch 160029: reducing learning rate of group 0 to 2.0684e-03.\n",
      "Epoch 160101, Training Loss: 29825, Validation Loss: 55046, 86074.68706040441\n",
      "Epoch 160130: reducing learning rate of group 0 to 2.0664e-03.\n",
      "Epoch 160201, Training Loss: 28637, Validation Loss: 55122, 70125.56104936088\n",
      "Epoch 160231: reducing learning rate of group 0 to 2.0643e-03.\n",
      "Epoch 160301, Training Loss: 30752, Validation Loss: 54552, 73767.07880065746\n",
      "Epoch 160332: reducing learning rate of group 0 to 2.0622e-03.\n",
      "Epoch 160401, Training Loss: 29998, Validation Loss: 52642, 74231.28714650667\n",
      "Epoch 160433: reducing learning rate of group 0 to 2.0602e-03.\n",
      "Epoch 160501, Training Loss: 31619, Validation Loss: 54439, 70105.92753130496\n",
      "Epoch 160534: reducing learning rate of group 0 to 2.0581e-03.\n",
      "Epoch 160601, Training Loss: 28536, Validation Loss: 54886, 82363.52475698497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160635: reducing learning rate of group 0 to 2.0561e-03.\n",
      "Epoch 160701, Training Loss: 29221, Validation Loss: 55284, 63862.85383503657\n",
      "Epoch 160736: reducing learning rate of group 0 to 2.0540e-03.\n",
      "Epoch 160801, Training Loss: 28736, Validation Loss: 52045, 69751.18032067314\n",
      "Epoch 160837: reducing learning rate of group 0 to 2.0520e-03.\n",
      "Epoch 160901, Training Loss: 27877, Validation Loss: 53156, 80171.24445968619\n",
      "Epoch 160938: reducing learning rate of group 0 to 2.0499e-03.\n",
      "Epoch 161001, Training Loss: 29509, Validation Loss: 51647, 74738.7678182051\n",
      "Epoch 161039: reducing learning rate of group 0 to 2.0479e-03.\n",
      "Epoch 161101, Training Loss: 31061, Validation Loss: 53228, 70691.57002839644\n",
      "Epoch 161140: reducing learning rate of group 0 to 2.0458e-03.\n",
      "Epoch 161201, Training Loss: 29667, Validation Loss: 53128, 73411.4467018122\n",
      "Epoch 161241: reducing learning rate of group 0 to 2.0438e-03.\n",
      "Epoch 161301, Training Loss: 30109, Validation Loss: 54281, 75363.22358603953\n",
      "Epoch 161342: reducing learning rate of group 0 to 2.0417e-03.\n",
      "Epoch 161401, Training Loss: 29041, Validation Loss: 58128, 76325.94065469448\n",
      "Epoch 161443: reducing learning rate of group 0 to 2.0397e-03.\n",
      "Epoch 161501, Training Loss: 28150, Validation Loss: 52200, 73855.88928773745\n",
      "Epoch 161544: reducing learning rate of group 0 to 2.0376e-03.\n",
      "Epoch 161601, Training Loss: 30192, Validation Loss: 55655, 80669.22083366149\n",
      "Epoch 161645: reducing learning rate of group 0 to 2.0356e-03.\n",
      "Epoch 161701, Training Loss: 27815, Validation Loss: 57131, 84651.44289767409\n",
      "Epoch 161746: reducing learning rate of group 0 to 2.0336e-03.\n",
      "Epoch 161801, Training Loss: 29741, Validation Loss: 54239, 71602.92602738609\n",
      "Epoch 161847: reducing learning rate of group 0 to 2.0315e-03.\n",
      "Epoch 161901, Training Loss: 30703, Validation Loss: 54833, 70513.17651295585\n",
      "Epoch 161948: reducing learning rate of group 0 to 2.0295e-03.\n",
      "Epoch 162001, Training Loss: 31444, Validation Loss: 52685, 74390.75374669056\n",
      "Epoch 162049: reducing learning rate of group 0 to 2.0275e-03.\n",
      "Epoch 162101, Training Loss: 28236, Validation Loss: 54979, 84619.56929807233\n",
      "Epoch 162150: reducing learning rate of group 0 to 2.0254e-03.\n",
      "Epoch 162201, Training Loss: 29011, Validation Loss: 55271, 84124.9179451102\n",
      "Epoch 162251: reducing learning rate of group 0 to 2.0234e-03.\n",
      "Epoch 162301, Training Loss: 28371, Validation Loss: 55750, 65149.193023111264\n",
      "Epoch 162352: reducing learning rate of group 0 to 2.0214e-03.\n",
      "Epoch 162401, Training Loss: 30101, Validation Loss: 53452, 72638.2735917461\n",
      "Epoch 162453: reducing learning rate of group 0 to 2.0194e-03.\n",
      "Epoch 162501, Training Loss: 29249, Validation Loss: 56926, 61059.36617150538\n",
      "Epoch 162554: reducing learning rate of group 0 to 2.0173e-03.\n",
      "Epoch 162601, Training Loss: 29642, Validation Loss: 53588, 89617.3815518315\n",
      "Epoch 162655: reducing learning rate of group 0 to 2.0153e-03.\n",
      "Epoch 162701, Training Loss: 30884, Validation Loss: 53867, 80424.13597509972\n",
      "Epoch 162756: reducing learning rate of group 0 to 2.0133e-03.\n",
      "Epoch 162801, Training Loss: 31070, Validation Loss: 53524, 81772.47845550056\n",
      "Epoch 162857: reducing learning rate of group 0 to 2.0113e-03.\n",
      "Epoch 162901, Training Loss: 27554, Validation Loss: 53979, 80399.82679115252\n",
      "Epoch 162958: reducing learning rate of group 0 to 2.0093e-03.\n",
      "Epoch 163001, Training Loss: 27838, Validation Loss: 55781, 80222.02890851536\n",
      "Epoch 163059: reducing learning rate of group 0 to 2.0073e-03.\n",
      "Epoch 163101, Training Loss: 26704, Validation Loss: 54117, 78062.54806197358\n",
      "Epoch 163160: reducing learning rate of group 0 to 2.0053e-03.\n",
      "Epoch 163201, Training Loss: 28881, Validation Loss: 56215, 75718.3142265859\n",
      "Epoch 163261: reducing learning rate of group 0 to 2.0033e-03.\n",
      "Epoch 163301, Training Loss: 31900, Validation Loss: 54604, 80534.17989797701\n",
      "Epoch 163362: reducing learning rate of group 0 to 2.0013e-03.\n",
      "Epoch 163401, Training Loss: 28041, Validation Loss: 54014, 68221.34326738001\n",
      "Epoch 163463: reducing learning rate of group 0 to 1.9993e-03.\n",
      "Epoch 163501, Training Loss: 28275, Validation Loss: 51784, 82842.37923924881\n",
      "Epoch 163564: reducing learning rate of group 0 to 1.9973e-03.\n",
      "Epoch 163601, Training Loss: 27337, Validation Loss: 55779, 78508.72473950092\n",
      "Epoch 163665: reducing learning rate of group 0 to 1.9953e-03.\n",
      "Epoch 163701, Training Loss: 32367, Validation Loss: 54830, 70181.44158661521\n",
      "Epoch 163766: reducing learning rate of group 0 to 1.9933e-03.\n",
      "Epoch 163801, Training Loss: 29909, Validation Loss: 55463, 75678.0558553342\n",
      "Epoch 163867: reducing learning rate of group 0 to 1.9913e-03.\n",
      "Epoch 163901, Training Loss: 28750, Validation Loss: 55235, 69625.43769498104\n",
      "Epoch 163968: reducing learning rate of group 0 to 1.9893e-03.\n",
      "Epoch 164001, Training Loss: 30091, Validation Loss: 55219, 79668.94802900711\n",
      "Epoch 164069: reducing learning rate of group 0 to 1.9873e-03.\n",
      "Epoch 164101, Training Loss: 30098, Validation Loss: 53893, 63744.94777044294\n",
      "Epoch 164170: reducing learning rate of group 0 to 1.9853e-03.\n",
      "Epoch 164201, Training Loss: 27401, Validation Loss: 54856, 98894.6439668348\n",
      "Epoch 164271: reducing learning rate of group 0 to 1.9833e-03.\n",
      "Epoch 164301, Training Loss: 28177, Validation Loss: 54781, 78325.45951977275\n",
      "Epoch 164372: reducing learning rate of group 0 to 1.9813e-03.\n",
      "Epoch 164401, Training Loss: 28703, Validation Loss: 53541, 78996.93493339639\n",
      "Epoch 164473: reducing learning rate of group 0 to 1.9794e-03.\n",
      "Epoch 164501, Training Loss: 29967, Validation Loss: 53287, 66599.02474992425\n",
      "Epoch 164574: reducing learning rate of group 0 to 1.9774e-03.\n",
      "Epoch 164601, Training Loss: 28405, Validation Loss: 51758, 85266.15100768617\n",
      "Epoch 164675: reducing learning rate of group 0 to 1.9754e-03.\n",
      "Epoch 164701, Training Loss: 31862, Validation Loss: 55231, 69933.41738224713\n",
      "Epoch 164776: reducing learning rate of group 0 to 1.9734e-03.\n",
      "Epoch 164801, Training Loss: 29989, Validation Loss: 54438, 84597.61247429207\n",
      "Epoch 164877: reducing learning rate of group 0 to 1.9715e-03.\n",
      "Epoch 164901, Training Loss: 29720, Validation Loss: 54923, 82054.40251409503\n",
      "Epoch 164978: reducing learning rate of group 0 to 1.9695e-03.\n",
      "Epoch 165001, Training Loss: 28143, Validation Loss: 53269, 64619.316277079786\n",
      "Epoch 165079: reducing learning rate of group 0 to 1.9675e-03.\n",
      "Epoch 165101, Training Loss: 31904, Validation Loss: 53391, 73131.47491997799\n",
      "Epoch 165180: reducing learning rate of group 0 to 1.9655e-03.\n",
      "Epoch 165201, Training Loss: 30608, Validation Loss: 54295, 56615.15399774909\n",
      "Epoch 165281: reducing learning rate of group 0 to 1.9636e-03.\n",
      "Epoch 165301, Training Loss: 33707, Validation Loss: 54737, 74688.6280148742\n",
      "Epoch 165382: reducing learning rate of group 0 to 1.9616e-03.\n",
      "Epoch 165401, Training Loss: 28248, Validation Loss: 53103, 74363.15699918235\n",
      "Epoch 165483: reducing learning rate of group 0 to 1.9597e-03.\n",
      "Epoch 165501, Training Loss: 29111, Validation Loss: 54559, 66840.05680651408\n",
      "Epoch 165584: reducing learning rate of group 0 to 1.9577e-03.\n",
      "Epoch 165601, Training Loss: 29358, Validation Loss: 51212, 75277.66081373952\n",
      "Epoch 165685: reducing learning rate of group 0 to 1.9557e-03.\n",
      "Epoch 165701, Training Loss: 30433, Validation Loss: 54975, 79155.37603184253\n",
      "Epoch 165786: reducing learning rate of group 0 to 1.9538e-03.\n",
      "Epoch 165801, Training Loss: 29771, Validation Loss: 55574, 67931.63481829858\n",
      "Epoch 165887: reducing learning rate of group 0 to 1.9518e-03.\n",
      "Epoch 165901, Training Loss: 30407, Validation Loss: 57272, 69573.55353338993\n",
      "Epoch 165988: reducing learning rate of group 0 to 1.9499e-03.\n",
      "Epoch 166001, Training Loss: 30613, Validation Loss: 55273, 74147.13860020206\n",
      "Epoch 166089: reducing learning rate of group 0 to 1.9479e-03.\n",
      "Epoch 166101, Training Loss: 29534, Validation Loss: 54836, 66644.10690159899\n",
      "Epoch 166190: reducing learning rate of group 0 to 1.9460e-03.\n",
      "Epoch 166201, Training Loss: 29904, Validation Loss: 55397, 72164.74888723854\n",
      "Epoch 166291: reducing learning rate of group 0 to 1.9440e-03.\n",
      "Epoch 166301, Training Loss: 28808, Validation Loss: 56554, 85164.87853385367\n",
      "Epoch 166392: reducing learning rate of group 0 to 1.9421e-03.\n",
      "Epoch 166401, Training Loss: 29438, Validation Loss: 55119, 60106.261561170024\n",
      "Epoch 166493: reducing learning rate of group 0 to 1.9401e-03.\n",
      "Epoch 166501, Training Loss: 30238, Validation Loss: 55025, 67584.70958230087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166594: reducing learning rate of group 0 to 1.9382e-03.\n",
      "Epoch 166601, Training Loss: 29976, Validation Loss: 54018, 72719.95964001263\n",
      "Epoch 166695: reducing learning rate of group 0 to 1.9363e-03.\n",
      "Epoch 166701, Training Loss: 29662, Validation Loss: 53794, 70295.7839718467\n",
      "Epoch 166796: reducing learning rate of group 0 to 1.9343e-03.\n",
      "Epoch 166801, Training Loss: 29065, Validation Loss: 52309, 83046.87861303044\n",
      "Epoch 166897: reducing learning rate of group 0 to 1.9324e-03.\n",
      "Epoch 166901, Training Loss: 27910, Validation Loss: 55998, 70273.22595997382\n",
      "Epoch 166998: reducing learning rate of group 0 to 1.9305e-03.\n",
      "Epoch 167001, Training Loss: 29973, Validation Loss: 51886, 79188.13872542538\n",
      "Epoch 167099: reducing learning rate of group 0 to 1.9285e-03.\n",
      "Epoch 167101, Training Loss: 28634, Validation Loss: 53392, 67842.96720020537\n",
      "Epoch 167200: reducing learning rate of group 0 to 1.9266e-03.\n",
      "Epoch 167201, Training Loss: 28857, Validation Loss: 53522, 77308.0996839736\n",
      "Epoch 167301: reducing learning rate of group 0 to 1.9247e-03.\n",
      "Epoch 167301, Training Loss: 29352, Validation Loss: 54723, 78603.43374210573\n",
      "Epoch 167401, Training Loss: 29911, Validation Loss: 55772, 78203.28857651637\n",
      "Epoch 167402: reducing learning rate of group 0 to 1.9228e-03.\n",
      "Epoch 167501, Training Loss: 30003, Validation Loss: 52668, 66445.88350999536\n",
      "Epoch 167503: reducing learning rate of group 0 to 1.9208e-03.\n",
      "Epoch 167601, Training Loss: 29931, Validation Loss: 53389, 65050.76857765587\n",
      "Epoch 167604: reducing learning rate of group 0 to 1.9189e-03.\n",
      "Epoch 167701, Training Loss: 27282, Validation Loss: 52261, 91616.73924646775\n",
      "Epoch 167705: reducing learning rate of group 0 to 1.9170e-03.\n",
      "Epoch 167801, Training Loss: 30218, Validation Loss: 53997, 70601.44388553125\n",
      "Epoch 167806: reducing learning rate of group 0 to 1.9151e-03.\n",
      "Epoch 167901, Training Loss: 28286, Validation Loss: 54835, 78752.61236840802\n",
      "Epoch 167907: reducing learning rate of group 0 to 1.9132e-03.\n",
      "Epoch 168001, Training Loss: 27455, Validation Loss: 55126, 69438.90126603354\n",
      "Epoch 168008: reducing learning rate of group 0 to 1.9113e-03.\n",
      "Epoch 168101, Training Loss: 28742, Validation Loss: 52480, 67495.65016876337\n",
      "Epoch 168109: reducing learning rate of group 0 to 1.9093e-03.\n",
      "Epoch 168201, Training Loss: 29808, Validation Loss: 53757, 73716.2639384029\n",
      "Epoch 168210: reducing learning rate of group 0 to 1.9074e-03.\n",
      "Epoch 168301, Training Loss: 29036, Validation Loss: 54132, 72481.66622423376\n",
      "Epoch 168311: reducing learning rate of group 0 to 1.9055e-03.\n",
      "Epoch 168401, Training Loss: 29793, Validation Loss: 54666, 63913.45363811224\n",
      "Epoch 168412: reducing learning rate of group 0 to 1.9036e-03.\n",
      "Epoch 168501, Training Loss: 29809, Validation Loss: 52308, 68599.03973371681\n",
      "Epoch 168513: reducing learning rate of group 0 to 1.9017e-03.\n",
      "Epoch 168601, Training Loss: 30765, Validation Loss: 55182, 71194.5205258158\n",
      "Epoch 168614: reducing learning rate of group 0 to 1.8998e-03.\n",
      "Epoch 168701, Training Loss: 27919, Validation Loss: 53924, 70655.29843709648\n",
      "Epoch 168715: reducing learning rate of group 0 to 1.8979e-03.\n",
      "Epoch 168801, Training Loss: 29681, Validation Loss: 54080, 95265.80413765386\n",
      "Epoch 168816: reducing learning rate of group 0 to 1.8960e-03.\n",
      "Epoch 168901, Training Loss: 32236, Validation Loss: 52645, 79894.65708480978\n",
      "Epoch 168917: reducing learning rate of group 0 to 1.8941e-03.\n",
      "Epoch 169001, Training Loss: 33652, Validation Loss: 51370, 75646.14005503895\n",
      "Epoch 169018: reducing learning rate of group 0 to 1.8922e-03.\n",
      "Epoch 169101, Training Loss: 33816, Validation Loss: 53466, 81616.84224791884\n",
      "Epoch 169119: reducing learning rate of group 0 to 1.8903e-03.\n",
      "Epoch 169201, Training Loss: 28615, Validation Loss: 53902, 72315.2716068827\n",
      "Epoch 169220: reducing learning rate of group 0 to 1.8884e-03.\n",
      "Epoch 169301, Training Loss: 29860, Validation Loss: 54518, 68349.9393209191\n",
      "Epoch 169321: reducing learning rate of group 0 to 1.8866e-03.\n",
      "Epoch 169401, Training Loss: 29156, Validation Loss: 55029, 68180.98008253705\n",
      "Epoch 169422: reducing learning rate of group 0 to 1.8847e-03.\n",
      "Epoch 169501, Training Loss: 30330, Validation Loss: 53385, 82796.11751560128\n",
      "Epoch 169523: reducing learning rate of group 0 to 1.8828e-03.\n",
      "Epoch 169601, Training Loss: 28982, Validation Loss: 54325, 61592.50992357376\n",
      "Epoch 169624: reducing learning rate of group 0 to 1.8809e-03.\n",
      "Epoch 169701, Training Loss: 26800, Validation Loss: 55026, 76653.8981767082\n",
      "Epoch 169725: reducing learning rate of group 0 to 1.8790e-03.\n",
      "Epoch 169801, Training Loss: 29805, Validation Loss: 53402, 94774.90657619097\n",
      "Epoch 169826: reducing learning rate of group 0 to 1.8771e-03.\n",
      "Epoch 169901, Training Loss: 29418, Validation Loss: 52324, 73314.27164896576\n",
      "Epoch 169927: reducing learning rate of group 0 to 1.8753e-03.\n",
      "Epoch 170001, Training Loss: 28303, Validation Loss: 55701, 83005.77259412564\n",
      "Epoch 170028: reducing learning rate of group 0 to 1.8734e-03.\n",
      "Epoch 170101, Training Loss: 29417, Validation Loss: 55384, 88768.68359556746\n",
      "Epoch 170129: reducing learning rate of group 0 to 1.8715e-03.\n",
      "Epoch 170201, Training Loss: 28174, Validation Loss: 53526, 74351.17232906827\n",
      "Epoch 170230: reducing learning rate of group 0 to 1.8696e-03.\n",
      "Epoch 170301, Training Loss: 29635, Validation Loss: 56223, 67044.4399337302\n",
      "Epoch 170331: reducing learning rate of group 0 to 1.8678e-03.\n",
      "Epoch 170401, Training Loss: 28068, Validation Loss: 56969, 70377.43189977635\n",
      "Epoch 170432: reducing learning rate of group 0 to 1.8659e-03.\n",
      "Epoch 170501, Training Loss: 30202, Validation Loss: 52998, 84699.30869388225\n",
      "Epoch 170533: reducing learning rate of group 0 to 1.8640e-03.\n",
      "Epoch 170601, Training Loss: 29972, Validation Loss: 54544, 74904.73095143087\n",
      "Epoch 170634: reducing learning rate of group 0 to 1.8622e-03.\n",
      "Epoch 170701, Training Loss: 28929, Validation Loss: 54818, 80094.62741310555\n",
      "Epoch 170735: reducing learning rate of group 0 to 1.8603e-03.\n",
      "Epoch 170801, Training Loss: 30824, Validation Loss: 56225, 65561.85120090308\n",
      "Epoch 170836: reducing learning rate of group 0 to 1.8585e-03.\n",
      "Epoch 170901, Training Loss: 29020, Validation Loss: 57057, 68003.24016280608\n",
      "Epoch 170937: reducing learning rate of group 0 to 1.8566e-03.\n",
      "Epoch 171001, Training Loss: 28245, Validation Loss: 54209, 66163.92273366214\n",
      "Epoch 171038: reducing learning rate of group 0 to 1.8547e-03.\n",
      "Epoch 171101, Training Loss: 31227, Validation Loss: 55129, 63937.87664367367\n",
      "Epoch 171139: reducing learning rate of group 0 to 1.8529e-03.\n",
      "Epoch 171201, Training Loss: 30453, Validation Loss: 53607, 67852.27788284315\n",
      "Epoch 171240: reducing learning rate of group 0 to 1.8510e-03.\n",
      "Epoch 171301, Training Loss: 30479, Validation Loss: 52892, 83291.0174316052\n",
      "Epoch 171341: reducing learning rate of group 0 to 1.8492e-03.\n",
      "Epoch 171401, Training Loss: 28167, Validation Loss: 54934, 71981.93107652111\n",
      "Epoch 171442: reducing learning rate of group 0 to 1.8473e-03.\n",
      "Epoch 171501, Training Loss: 30695, Validation Loss: 56300, 76192.19917726156\n",
      "Epoch 171543: reducing learning rate of group 0 to 1.8455e-03.\n",
      "Epoch 171601, Training Loss: 28293, Validation Loss: 54742, 73488.08112807879\n",
      "Epoch 171644: reducing learning rate of group 0 to 1.8436e-03.\n",
      "Epoch 171701, Training Loss: 32321, Validation Loss: 53936, 79038.18381905036\n",
      "Epoch 171745: reducing learning rate of group 0 to 1.8418e-03.\n",
      "Epoch 171801, Training Loss: 30370, Validation Loss: 52554, 82880.74206935085\n",
      "Epoch 171846: reducing learning rate of group 0 to 1.8400e-03.\n",
      "Epoch 171901, Training Loss: 31356, Validation Loss: 52579, 86092.20888485353\n",
      "Epoch 171947: reducing learning rate of group 0 to 1.8381e-03.\n",
      "Epoch 172001, Training Loss: 29261, Validation Loss: 57170, 76874.7174607918\n",
      "Epoch 172048: reducing learning rate of group 0 to 1.8363e-03.\n",
      "Epoch 172101, Training Loss: 30984, Validation Loss: 52566, 68632.70956509771\n",
      "Epoch 172149: reducing learning rate of group 0 to 1.8344e-03.\n",
      "Epoch 172201, Training Loss: 29887, Validation Loss: 55743, 72347.41066004922\n",
      "Epoch 172250: reducing learning rate of group 0 to 1.8326e-03.\n",
      "Epoch 172301, Training Loss: 30291, Validation Loss: 52276, 84149.62910862047\n",
      "Epoch 172351: reducing learning rate of group 0 to 1.8308e-03.\n",
      "Epoch 172401, Training Loss: 28545, Validation Loss: 53247, 66257.45806458265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172452: reducing learning rate of group 0 to 1.8289e-03.\n",
      "Epoch 172501, Training Loss: 32658, Validation Loss: 55230, 94783.31151352776\n",
      "Epoch 172553: reducing learning rate of group 0 to 1.8271e-03.\n",
      "Epoch 172601, Training Loss: 30599, Validation Loss: 55285, 84401.42343184461\n",
      "Epoch 172654: reducing learning rate of group 0 to 1.8253e-03.\n",
      "Epoch 172701, Training Loss: 28951, Validation Loss: 56140, 86550.75779262201\n",
      "Epoch 172755: reducing learning rate of group 0 to 1.8235e-03.\n",
      "Epoch 172801, Training Loss: 30194, Validation Loss: 54331, 74349.39993183978\n",
      "Epoch 172856: reducing learning rate of group 0 to 1.8216e-03.\n",
      "Epoch 172901, Training Loss: 28548, Validation Loss: 55082, 76818.2335673973\n",
      "Epoch 172957: reducing learning rate of group 0 to 1.8198e-03.\n",
      "Epoch 173001, Training Loss: 30005, Validation Loss: 52944, 74640.82034344964\n",
      "Epoch 173058: reducing learning rate of group 0 to 1.8180e-03.\n",
      "Epoch 173101, Training Loss: 29913, Validation Loss: 57358, 85824.78433716431\n",
      "Epoch 173159: reducing learning rate of group 0 to 1.8162e-03.\n",
      "Epoch 173201, Training Loss: 26903, Validation Loss: 53449, 66856.43676286264\n",
      "Epoch 173260: reducing learning rate of group 0 to 1.8144e-03.\n",
      "Epoch 173301, Training Loss: 28013, Validation Loss: 55069, 82059.59389472095\n",
      "Epoch 173361: reducing learning rate of group 0 to 1.8125e-03.\n",
      "Epoch 173401, Training Loss: 30320, Validation Loss: 54062, 72240.23891953252\n",
      "Epoch 173462: reducing learning rate of group 0 to 1.8107e-03.\n",
      "Epoch 173501, Training Loss: 29229, Validation Loss: 53598, 79954.79051773864\n",
      "Epoch 173563: reducing learning rate of group 0 to 1.8089e-03.\n",
      "Epoch 173601, Training Loss: 28134, Validation Loss: 54518, 88578.46958131324\n",
      "Epoch 173664: reducing learning rate of group 0 to 1.8071e-03.\n",
      "Epoch 173701, Training Loss: 29536, Validation Loss: 55201, 69046.2967905936\n",
      "Epoch 173765: reducing learning rate of group 0 to 1.8053e-03.\n",
      "Epoch 173801, Training Loss: 30158, Validation Loss: 54062, 83496.99998640465\n",
      "Epoch 173866: reducing learning rate of group 0 to 1.8035e-03.\n",
      "Epoch 173901, Training Loss: 28873, Validation Loss: 56518, 82204.79028532855\n",
      "Epoch 173967: reducing learning rate of group 0 to 1.8017e-03.\n",
      "Epoch 174001, Training Loss: 29174, Validation Loss: 52795, 69370.01216262784\n",
      "Epoch 174068: reducing learning rate of group 0 to 1.7999e-03.\n",
      "Epoch 174101, Training Loss: 29449, Validation Loss: 54785, 69785.2267587141\n",
      "Epoch 174169: reducing learning rate of group 0 to 1.7981e-03.\n",
      "Epoch 174201, Training Loss: 29481, Validation Loss: 54159, 85678.35274359917\n",
      "Epoch 174270: reducing learning rate of group 0 to 1.7963e-03.\n",
      "Epoch 174301, Training Loss: 31043, Validation Loss: 56419, 68087.11941702278\n",
      "Epoch 174371: reducing learning rate of group 0 to 1.7945e-03.\n",
      "Epoch 174401, Training Loss: 30390, Validation Loss: 51165, 63882.27287893788\n",
      "Epoch 174472: reducing learning rate of group 0 to 1.7927e-03.\n",
      "Epoch 174501, Training Loss: 28056, Validation Loss: 53698, 56661.062611091\n",
      "Epoch 174573: reducing learning rate of group 0 to 1.7909e-03.\n",
      "Epoch 174601, Training Loss: 28526, Validation Loss: 53239, 66550.32019078909\n",
      "Epoch 174674: reducing learning rate of group 0 to 1.7891e-03.\n",
      "Epoch 174701, Training Loss: 29011, Validation Loss: 55800, 69264.20539041114\n",
      "Epoch 174775: reducing learning rate of group 0 to 1.7873e-03.\n",
      "Epoch 174801, Training Loss: 27651, Validation Loss: 52527, 79490.92164241515\n",
      "Epoch 174876: reducing learning rate of group 0 to 1.7855e-03.\n",
      "Epoch 174901, Training Loss: 28463, Validation Loss: 55343, 85853.7295209405\n",
      "Epoch 174977: reducing learning rate of group 0 to 1.7838e-03.\n",
      "Epoch 175001, Training Loss: 28714, Validation Loss: 53643, 71764.0459541361\n",
      "Epoch 175078: reducing learning rate of group 0 to 1.7820e-03.\n",
      "Epoch 175101, Training Loss: 27742, Validation Loss: 55178, 84796.70297574387\n",
      "Epoch 175179: reducing learning rate of group 0 to 1.7802e-03.\n",
      "Epoch 175201, Training Loss: 30712, Validation Loss: 53298, 74737.8847446813\n",
      "Epoch 175280: reducing learning rate of group 0 to 1.7784e-03.\n",
      "Epoch 175301, Training Loss: 29230, Validation Loss: 54006, 71783.68147692447\n",
      "Epoch 175381: reducing learning rate of group 0 to 1.7766e-03.\n",
      "Epoch 175401, Training Loss: 28936, Validation Loss: 55473, 64818.43661859174\n",
      "Epoch 175482: reducing learning rate of group 0 to 1.7749e-03.\n",
      "Epoch 175501, Training Loss: 27220, Validation Loss: 53312, 70205.01482704932\n",
      "Epoch 175583: reducing learning rate of group 0 to 1.7731e-03.\n",
      "Epoch 175601, Training Loss: 28086, Validation Loss: 56292, 83724.69643567876\n",
      "Epoch 175684: reducing learning rate of group 0 to 1.7713e-03.\n",
      "Epoch 175701, Training Loss: 27504, Validation Loss: 54337, 68511.63473710766\n",
      "Epoch 175785: reducing learning rate of group 0 to 1.7695e-03.\n",
      "Epoch 175801, Training Loss: 28892, Validation Loss: 56940, 63420.79013801381\n",
      "Epoch 175886: reducing learning rate of group 0 to 1.7678e-03.\n",
      "Epoch 175901, Training Loss: 28573, Validation Loss: 57165, 56964.252505095326\n",
      "Epoch 175987: reducing learning rate of group 0 to 1.7660e-03.\n",
      "Epoch 176001, Training Loss: 30753, Validation Loss: 53081, 72525.7255775387\n",
      "Epoch 176088: reducing learning rate of group 0 to 1.7642e-03.\n",
      "Epoch 176101, Training Loss: 29957, Validation Loss: 54888, 62280.61961493798\n",
      "Epoch 176189: reducing learning rate of group 0 to 1.7625e-03.\n",
      "Epoch 176201, Training Loss: 31603, Validation Loss: 53113, 84913.42514220031\n",
      "Epoch 176290: reducing learning rate of group 0 to 1.7607e-03.\n",
      "Epoch 176301, Training Loss: 29021, Validation Loss: 54344, 69920.54083683829\n",
      "Epoch 176391: reducing learning rate of group 0 to 1.7589e-03.\n",
      "Epoch 176401, Training Loss: 30101, Validation Loss: 54611, 79055.09327040371\n",
      "Epoch 176492: reducing learning rate of group 0 to 1.7572e-03.\n",
      "Epoch 176501, Training Loss: 32706, Validation Loss: 53251, 91831.35690379055\n",
      "Epoch 176593: reducing learning rate of group 0 to 1.7554e-03.\n",
      "Epoch 176601, Training Loss: 27905, Validation Loss: 52786, 63940.559550947335\n",
      "Epoch 176694: reducing learning rate of group 0 to 1.7537e-03.\n",
      "Epoch 176701, Training Loss: 28822, Validation Loss: 53418, 89708.85216578173\n",
      "Epoch 176795: reducing learning rate of group 0 to 1.7519e-03.\n",
      "Epoch 176801, Training Loss: 27958, Validation Loss: 56190, 81903.55519978177\n",
      "Epoch 176896: reducing learning rate of group 0 to 1.7502e-03.\n",
      "Epoch 176901, Training Loss: 29008, Validation Loss: 53999, 72832.3259569335\n",
      "Epoch 176997: reducing learning rate of group 0 to 1.7484e-03.\n",
      "Epoch 177001, Training Loss: 27462, Validation Loss: 54935, 74869.67910770337\n",
      "Epoch 177098: reducing learning rate of group 0 to 1.7467e-03.\n",
      "Epoch 177101, Training Loss: 30049, Validation Loss: 54807, 86985.54092784021\n",
      "Epoch 177199: reducing learning rate of group 0 to 1.7449e-03.\n",
      "Epoch 177201, Training Loss: 29562, Validation Loss: 52396, 68972.23865487122\n",
      "Epoch 177300: reducing learning rate of group 0 to 1.7432e-03.\n",
      "Epoch 177301, Training Loss: 31160, Validation Loss: 54212, 81383.62021791107\n",
      "Epoch 177401: reducing learning rate of group 0 to 1.7414e-03.\n",
      "Epoch 177401, Training Loss: 29660, Validation Loss: 54103, 85100.96666467025\n",
      "Epoch 177501, Training Loss: 29073, Validation Loss: 52984, 71347.54208483388\n",
      "Epoch 177502: reducing learning rate of group 0 to 1.7397e-03.\n",
      "Epoch 177601, Training Loss: 28714, Validation Loss: 54145, 81502.56544633307\n",
      "Epoch 177603: reducing learning rate of group 0 to 1.7380e-03.\n",
      "Epoch 177701, Training Loss: 30410, Validation Loss: 53638, 82182.2659294356\n",
      "Epoch 177704: reducing learning rate of group 0 to 1.7362e-03.\n",
      "Epoch 177801, Training Loss: 27902, Validation Loss: 52015, 76757.18025890937\n",
      "Epoch 177805: reducing learning rate of group 0 to 1.7345e-03.\n",
      "Epoch 177901, Training Loss: 27682, Validation Loss: 54972, 78309.22638782549\n",
      "Epoch 177906: reducing learning rate of group 0 to 1.7327e-03.\n",
      "Epoch 178001, Training Loss: 27386, Validation Loss: 54866, 67177.18244461993\n",
      "Epoch 178007: reducing learning rate of group 0 to 1.7310e-03.\n",
      "Epoch 178101, Training Loss: 29281, Validation Loss: 55494, 64703.90689949193\n",
      "Epoch 178108: reducing learning rate of group 0 to 1.7293e-03.\n",
      "Epoch 178201, Training Loss: 29512, Validation Loss: 53605, 68615.067614134\n",
      "Epoch 178209: reducing learning rate of group 0 to 1.7276e-03.\n",
      "Epoch 178301, Training Loss: 28828, Validation Loss: 53570, 61762.615489219555\n",
      "Epoch 178310: reducing learning rate of group 0 to 1.7258e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178401, Training Loss: 30069, Validation Loss: 56446, 82816.54458389738\n",
      "Epoch 178411: reducing learning rate of group 0 to 1.7241e-03.\n",
      "Epoch 178501, Training Loss: 28763, Validation Loss: 53752, 95133.28640030767\n",
      "Epoch 178512: reducing learning rate of group 0 to 1.7224e-03.\n",
      "Epoch 178601, Training Loss: 28777, Validation Loss: 58652, 73662.00745985779\n",
      "Epoch 178613: reducing learning rate of group 0 to 1.7207e-03.\n",
      "Epoch 178701, Training Loss: 30989, Validation Loss: 55680, 90353.3897825061\n",
      "Epoch 178714: reducing learning rate of group 0 to 1.7189e-03.\n",
      "Epoch 178801, Training Loss: 29857, Validation Loss: 53174, 83501.97659190545\n",
      "Epoch 178815: reducing learning rate of group 0 to 1.7172e-03.\n",
      "Epoch 178901, Training Loss: 29355, Validation Loss: 54731, 66715.3901544622\n",
      "Epoch 178916: reducing learning rate of group 0 to 1.7155e-03.\n",
      "Epoch 179001, Training Loss: 27599, Validation Loss: 54417, 78417.10582518211\n",
      "Epoch 179017: reducing learning rate of group 0 to 1.7138e-03.\n",
      "Epoch 179101, Training Loss: 29163, Validation Loss: 55825, 69952.17859505549\n",
      "Epoch 179118: reducing learning rate of group 0 to 1.7121e-03.\n",
      "Epoch 179201, Training Loss: 29125, Validation Loss: 54453, 79166.29836565307\n",
      "Epoch 179219: reducing learning rate of group 0 to 1.7104e-03.\n",
      "Epoch 179301, Training Loss: 31246, Validation Loss: 56707, 76587.90249977032\n",
      "Epoch 179320: reducing learning rate of group 0 to 1.7086e-03.\n",
      "Epoch 179401, Training Loss: 29136, Validation Loss: 53833, 77879.8359686724\n",
      "Epoch 179421: reducing learning rate of group 0 to 1.7069e-03.\n",
      "Epoch 179501, Training Loss: 29177, Validation Loss: 53626, 70578.80368566536\n",
      "Epoch 179522: reducing learning rate of group 0 to 1.7052e-03.\n",
      "Epoch 179601, Training Loss: 26651, Validation Loss: 54775, 87517.67304212287\n",
      "Epoch 179623: reducing learning rate of group 0 to 1.7035e-03.\n",
      "Epoch 179701, Training Loss: 27806, Validation Loss: 54327, 74985.80884621246\n",
      "Epoch 179724: reducing learning rate of group 0 to 1.7018e-03.\n",
      "Epoch 179801, Training Loss: 32098, Validation Loss: 54276, 73732.84798904111\n",
      "Epoch 179825: reducing learning rate of group 0 to 1.7001e-03.\n",
      "Epoch 179901, Training Loss: 28356, Validation Loss: 54879, 86807.58814062255\n",
      "Epoch 179926: reducing learning rate of group 0 to 1.6984e-03.\n",
      "Epoch 180001, Training Loss: 28023, Validation Loss: 56305, 75189.99581368324\n",
      "Epoch 180027: reducing learning rate of group 0 to 1.6967e-03.\n",
      "Epoch 180101, Training Loss: 28162, Validation Loss: 54108, 75508.94238523122\n",
      "Epoch 180128: reducing learning rate of group 0 to 1.6950e-03.\n",
      "Epoch 180201, Training Loss: 29715, Validation Loss: 55783, 93829.5897622532\n",
      "Epoch 180229: reducing learning rate of group 0 to 1.6933e-03.\n",
      "Epoch 180301, Training Loss: 28146, Validation Loss: 53142, 56438.554026389436\n",
      "Epoch 180330: reducing learning rate of group 0 to 1.6916e-03.\n",
      "Epoch 180401, Training Loss: 27453, Validation Loss: 55536, 67606.64118585248\n",
      "Epoch 180431: reducing learning rate of group 0 to 1.6899e-03.\n",
      "Epoch 180501, Training Loss: 29104, Validation Loss: 53614, 70051.30683152967\n",
      "Epoch 180532: reducing learning rate of group 0 to 1.6883e-03.\n",
      "Epoch 180601, Training Loss: 28111, Validation Loss: 52998, 77084.3050444573\n",
      "Epoch 180633: reducing learning rate of group 0 to 1.6866e-03.\n",
      "Epoch 180701, Training Loss: 28637, Validation Loss: 53212, 76101.63171816892\n",
      "Epoch 180734: reducing learning rate of group 0 to 1.6849e-03.\n",
      "Epoch 180801, Training Loss: 28437, Validation Loss: 54849, 65762.61073310337\n",
      "Epoch 180835: reducing learning rate of group 0 to 1.6832e-03.\n",
      "Epoch 180901, Training Loss: 28595, Validation Loss: 56022, 62493.52523040783\n",
      "Epoch 180936: reducing learning rate of group 0 to 1.6815e-03.\n",
      "Epoch 181001, Training Loss: 28205, Validation Loss: 54136, 71434.96293799381\n",
      "Epoch 181037: reducing learning rate of group 0 to 1.6798e-03.\n",
      "Epoch 181101, Training Loss: 27441, Validation Loss: 54771, 80272.07897316788\n",
      "Epoch 181138: reducing learning rate of group 0 to 1.6782e-03.\n",
      "Epoch 181201, Training Loss: 27800, Validation Loss: 52802, 72182.47094957506\n",
      "Epoch 181239: reducing learning rate of group 0 to 1.6765e-03.\n",
      "Epoch 181301, Training Loss: 29474, Validation Loss: 54569, 73802.71463037863\n",
      "Epoch 181340: reducing learning rate of group 0 to 1.6748e-03.\n",
      "Epoch 181401, Training Loss: 30543, Validation Loss: 54538, 66013.1148934109\n",
      "Epoch 181441: reducing learning rate of group 0 to 1.6731e-03.\n",
      "Epoch 181501, Training Loss: 27532, Validation Loss: 53709, 62797.53367217337\n",
      "Epoch 181542: reducing learning rate of group 0 to 1.6714e-03.\n",
      "Epoch 181601, Training Loss: 29206, Validation Loss: 52761, 70551.69195162461\n",
      "Epoch 181643: reducing learning rate of group 0 to 1.6698e-03.\n",
      "Epoch 181701, Training Loss: 27979, Validation Loss: 53287, 65965.57863573705\n",
      "Epoch 181744: reducing learning rate of group 0 to 1.6681e-03.\n",
      "Epoch 181801, Training Loss: 30056, Validation Loss: 55495, 72185.14894581033\n",
      "Epoch 181845: reducing learning rate of group 0 to 1.6664e-03.\n",
      "Epoch 181901, Training Loss: 30090, Validation Loss: 55034, 71268.98784531973\n",
      "Epoch 181946: reducing learning rate of group 0 to 1.6648e-03.\n",
      "Epoch 182001, Training Loss: 29355, Validation Loss: 54331, 77366.9968427843\n",
      "Epoch 182047: reducing learning rate of group 0 to 1.6631e-03.\n",
      "Epoch 182101, Training Loss: 27849, Validation Loss: 54710, 81463.66044381006\n",
      "Epoch 182148: reducing learning rate of group 0 to 1.6614e-03.\n",
      "Epoch 182201, Training Loss: 30304, Validation Loss: 53901, 73252.09956027333\n",
      "Epoch 182249: reducing learning rate of group 0 to 1.6598e-03.\n",
      "Epoch 182301, Training Loss: 30027, Validation Loss: 53842, 76371.28302119276\n",
      "Epoch 182350: reducing learning rate of group 0 to 1.6581e-03.\n",
      "Epoch 182401, Training Loss: 29191, Validation Loss: 53456, 80660.24636279304\n",
      "Epoch 182451: reducing learning rate of group 0 to 1.6565e-03.\n",
      "Epoch 182501, Training Loss: 27567, Validation Loss: 54719, 75589.25335743294\n",
      "Epoch 182552: reducing learning rate of group 0 to 1.6548e-03.\n",
      "Epoch 182601, Training Loss: 28852, Validation Loss: 53641, 87658.39761879559\n",
      "Epoch 182653: reducing learning rate of group 0 to 1.6532e-03.\n",
      "Epoch 182701, Training Loss: 30448, Validation Loss: 53896, 71204.18125655831\n",
      "Epoch 182754: reducing learning rate of group 0 to 1.6515e-03.\n",
      "Epoch 182801, Training Loss: 30091, Validation Loss: 55085, 81434.25400103912\n",
      "Epoch 182855: reducing learning rate of group 0 to 1.6498e-03.\n",
      "Epoch 182901, Training Loss: 29079, Validation Loss: 53811, 79569.82095992938\n",
      "Epoch 182956: reducing learning rate of group 0 to 1.6482e-03.\n",
      "Epoch 183001, Training Loss: 30321, Validation Loss: 52111, 90759.18046651734\n",
      "Epoch 183057: reducing learning rate of group 0 to 1.6466e-03.\n",
      "Epoch 183101, Training Loss: 29480, Validation Loss: 51866, 73308.88834098524\n",
      "Epoch 183158: reducing learning rate of group 0 to 1.6449e-03.\n",
      "Epoch 183201, Training Loss: 27083, Validation Loss: 54865, 77077.07111714942\n",
      "Epoch 183259: reducing learning rate of group 0 to 1.6433e-03.\n",
      "Epoch 183301, Training Loss: 29935, Validation Loss: 55086, 87330.60924896189\n",
      "Epoch 183360: reducing learning rate of group 0 to 1.6416e-03.\n",
      "Epoch 183401, Training Loss: 27272, Validation Loss: 55110, 84973.10190517495\n",
      "Epoch 183461: reducing learning rate of group 0 to 1.6400e-03.\n",
      "Epoch 183501, Training Loss: 28517, Validation Loss: 57351, 58908.81323200002\n",
      "Epoch 183562: reducing learning rate of group 0 to 1.6383e-03.\n",
      "Epoch 183601, Training Loss: 28774, Validation Loss: 53519, 77596.05071903342\n",
      "Epoch 183663: reducing learning rate of group 0 to 1.6367e-03.\n",
      "Epoch 183701, Training Loss: 27415, Validation Loss: 54837, 85651.42802928593\n",
      "Epoch 183764: reducing learning rate of group 0 to 1.6351e-03.\n",
      "Epoch 183801, Training Loss: 29004, Validation Loss: 53534, 80006.2831136305\n",
      "Epoch 183865: reducing learning rate of group 0 to 1.6334e-03.\n",
      "Epoch 183901, Training Loss: 28358, Validation Loss: 56369, 99876.34342512871\n",
      "Epoch 183966: reducing learning rate of group 0 to 1.6318e-03.\n",
      "Epoch 184001, Training Loss: 28134, Validation Loss: 51852, 72102.51933393307\n",
      "Epoch 184067: reducing learning rate of group 0 to 1.6302e-03.\n",
      "Epoch 184101, Training Loss: 29048, Validation Loss: 56118, 139733.74059878543\n",
      "Epoch 184168: reducing learning rate of group 0 to 1.6285e-03.\n",
      "Epoch 184201, Training Loss: 28860, Validation Loss: 52746, 100980.20003639052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184269: reducing learning rate of group 0 to 1.6269e-03.\n",
      "Epoch 184301, Training Loss: 29609, Validation Loss: 55184, 67055.0920511073\n",
      "Epoch 184370: reducing learning rate of group 0 to 1.6253e-03.\n",
      "Epoch 184401, Training Loss: 28302, Validation Loss: 56324, 84791.96918411514\n",
      "Epoch 184471: reducing learning rate of group 0 to 1.6236e-03.\n",
      "Epoch 184501, Training Loss: 28845, Validation Loss: 53754, 81171.15204668867\n",
      "Epoch 184572: reducing learning rate of group 0 to 1.6220e-03.\n",
      "Epoch 184601, Training Loss: 28357, Validation Loss: 55020, 87637.58980155429\n",
      "Epoch 184673: reducing learning rate of group 0 to 1.6204e-03.\n",
      "Epoch 184701, Training Loss: 31034, Validation Loss: 53848, 71804.14561615793\n",
      "Epoch 184774: reducing learning rate of group 0 to 1.6188e-03.\n",
      "Epoch 184801, Training Loss: 29140, Validation Loss: 55026, 84599.94645226283\n",
      "Epoch 184875: reducing learning rate of group 0 to 1.6172e-03.\n",
      "Epoch 184901, Training Loss: 29320, Validation Loss: 54515, 77423.67452024137\n",
      "Epoch 184976: reducing learning rate of group 0 to 1.6155e-03.\n",
      "Epoch 185001, Training Loss: 28728, Validation Loss: 52971, 82250.92048669391\n",
      "Epoch 185077: reducing learning rate of group 0 to 1.6139e-03.\n",
      "Epoch 185101, Training Loss: 29095, Validation Loss: 54378, 77518.31547035904\n",
      "Epoch 185178: reducing learning rate of group 0 to 1.6123e-03.\n",
      "Epoch 185201, Training Loss: 29054, Validation Loss: 55661, 58073.03993485946\n",
      "Epoch 185279: reducing learning rate of group 0 to 1.6107e-03.\n",
      "Epoch 185301, Training Loss: 30597, Validation Loss: 53743, 81904.71132767417\n",
      "Epoch 185380: reducing learning rate of group 0 to 1.6091e-03.\n",
      "Epoch 185401, Training Loss: 29093, Validation Loss: 54464, 63602.1283414287\n",
      "Epoch 185481: reducing learning rate of group 0 to 1.6075e-03.\n",
      "Epoch 185501, Training Loss: 30742, Validation Loss: 53127, 78550.98778104603\n",
      "Epoch 185582: reducing learning rate of group 0 to 1.6059e-03.\n",
      "Epoch 185601, Training Loss: 27019, Validation Loss: 52951, 65042.27261993124\n",
      "Epoch 185683: reducing learning rate of group 0 to 1.6043e-03.\n",
      "Epoch 185701, Training Loss: 30190, Validation Loss: 53931, 86272.58147131548\n",
      "Epoch 185784: reducing learning rate of group 0 to 1.6027e-03.\n",
      "Epoch 185801, Training Loss: 29522, Validation Loss: 54206, 75299.75091761992\n",
      "Epoch 185885: reducing learning rate of group 0 to 1.6011e-03.\n",
      "Epoch 185901, Training Loss: 30376, Validation Loss: 57064, 73330.30699631492\n",
      "Epoch 185986: reducing learning rate of group 0 to 1.5995e-03.\n",
      "Epoch 186001, Training Loss: 28256, Validation Loss: 53895, 75811.54947627992\n",
      "Epoch 186087: reducing learning rate of group 0 to 1.5979e-03.\n",
      "Epoch 186101, Training Loss: 28786, Validation Loss: 56367, 74178.61353801009\n",
      "Epoch 186188: reducing learning rate of group 0 to 1.5963e-03.\n",
      "Epoch 186201, Training Loss: 27360, Validation Loss: 54876, 82389.7346276559\n",
      "Epoch 186289: reducing learning rate of group 0 to 1.5947e-03.\n",
      "Epoch 186301, Training Loss: 29626, Validation Loss: 54891, 73712.24564732128\n",
      "Epoch 186390: reducing learning rate of group 0 to 1.5931e-03.\n",
      "Epoch 186401, Training Loss: 27120, Validation Loss: 58094, 69114.50671544678\n",
      "Epoch 186491: reducing learning rate of group 0 to 1.5915e-03.\n",
      "Epoch 186501, Training Loss: 28715, Validation Loss: 53370, 62724.17884500936\n",
      "Epoch 186592: reducing learning rate of group 0 to 1.5899e-03.\n",
      "Epoch 186601, Training Loss: 28688, Validation Loss: 53395, 70913.50166441801\n",
      "Epoch 186693: reducing learning rate of group 0 to 1.5883e-03.\n",
      "Epoch 186701, Training Loss: 27165, Validation Loss: 53625, 77582.75784823854\n",
      "Epoch 186794: reducing learning rate of group 0 to 1.5867e-03.\n",
      "Epoch 186801, Training Loss: 27954, Validation Loss: 55942, 75715.26435933438\n",
      "Epoch 186895: reducing learning rate of group 0 to 1.5851e-03.\n",
      "Epoch 186901, Training Loss: 27830, Validation Loss: 53339, 73578.83300513869\n",
      "Epoch 186996: reducing learning rate of group 0 to 1.5835e-03.\n",
      "Epoch 187001, Training Loss: 30038, Validation Loss: 53409, 74511.07250852996\n",
      "Epoch 187097: reducing learning rate of group 0 to 1.5820e-03.\n",
      "Epoch 187101, Training Loss: 29826, Validation Loss: 52968, 68589.33019166527\n",
      "Epoch 187198: reducing learning rate of group 0 to 1.5804e-03.\n",
      "Epoch 187201, Training Loss: 28711, Validation Loss: 54688, 70447.86153299622\n",
      "Epoch 187299: reducing learning rate of group 0 to 1.5788e-03.\n",
      "Epoch 187301, Training Loss: 28988, Validation Loss: 53743, 68728.80808608964\n",
      "Epoch 187400: reducing learning rate of group 0 to 1.5772e-03.\n",
      "Epoch 187401, Training Loss: 31043, Validation Loss: 55424, 74631.76251232882\n",
      "Epoch 187501: reducing learning rate of group 0 to 1.5756e-03.\n",
      "Epoch 187501, Training Loss: 27556, Validation Loss: 55311, 86277.45847052954\n",
      "Epoch 187601, Training Loss: 34269, Validation Loss: 53163, 70116.36384173365\n",
      "Epoch 187602: reducing learning rate of group 0 to 1.5741e-03.\n",
      "Epoch 187701, Training Loss: 28400, Validation Loss: 54706, 63589.88069219415\n",
      "Epoch 187703: reducing learning rate of group 0 to 1.5725e-03.\n",
      "Epoch 187801, Training Loss: 28422, Validation Loss: 53305, 83811.41395505222\n",
      "Epoch 187804: reducing learning rate of group 0 to 1.5709e-03.\n",
      "Epoch 187901, Training Loss: 28772, Validation Loss: 54480, 76323.8904043048\n",
      "Epoch 187905: reducing learning rate of group 0 to 1.5693e-03.\n",
      "Epoch 188001, Training Loss: 28741, Validation Loss: 55627, 80934.04501325794\n",
      "Epoch 188006: reducing learning rate of group 0 to 1.5678e-03.\n",
      "Epoch 188101, Training Loss: 29397, Validation Loss: 53719, 72371.42637904546\n",
      "Epoch 188107: reducing learning rate of group 0 to 1.5662e-03.\n",
      "Epoch 188201, Training Loss: 27869, Validation Loss: 52660, 74262.29857334116\n",
      "Epoch 188208: reducing learning rate of group 0 to 1.5646e-03.\n",
      "Epoch 188301, Training Loss: 28190, Validation Loss: 54157, 68903.25478464282\n",
      "Epoch 188309: reducing learning rate of group 0 to 1.5631e-03.\n",
      "Epoch 188401, Training Loss: 26982, Validation Loss: 54083, 87831.66961427209\n",
      "Epoch 188410: reducing learning rate of group 0 to 1.5615e-03.\n",
      "Epoch 188501, Training Loss: 29611, Validation Loss: 55604, 75286.79628874088\n",
      "Epoch 188511: reducing learning rate of group 0 to 1.5600e-03.\n",
      "Epoch 188601, Training Loss: 29639, Validation Loss: 56251, 76012.94453562277\n",
      "Epoch 188612: reducing learning rate of group 0 to 1.5584e-03.\n",
      "Epoch 188701, Training Loss: 28707, Validation Loss: 54222, 76916.59270326058\n",
      "Epoch 188713: reducing learning rate of group 0 to 1.5568e-03.\n",
      "Epoch 188801, Training Loss: 27972, Validation Loss: 54208, 67776.83651980075\n",
      "Epoch 188814: reducing learning rate of group 0 to 1.5553e-03.\n",
      "Epoch 188901, Training Loss: 31040, Validation Loss: 55001, 76424.46271812168\n",
      "Epoch 188915: reducing learning rate of group 0 to 1.5537e-03.\n",
      "Epoch 189001, Training Loss: 29652, Validation Loss: 53367, 73946.04847539436\n",
      "Epoch 189016: reducing learning rate of group 0 to 1.5522e-03.\n",
      "Epoch 189101, Training Loss: 28727, Validation Loss: 55941, 88125.62610252433\n",
      "Epoch 189117: reducing learning rate of group 0 to 1.5506e-03.\n",
      "Epoch 189201, Training Loss: 25625, Validation Loss: 56401, 67249.0800040372\n",
      "Epoch 189218: reducing learning rate of group 0 to 1.5491e-03.\n",
      "Epoch 189301, Training Loss: 34870, Validation Loss: 54288, 88454.38416439605\n",
      "Epoch 189319: reducing learning rate of group 0 to 1.5475e-03.\n",
      "Epoch 189401, Training Loss: 31353, Validation Loss: 56977, 72813.6900818344\n",
      "Epoch 189420: reducing learning rate of group 0 to 1.5460e-03.\n",
      "Epoch 189501, Training Loss: 31942, Validation Loss: 53685, 77058.72962831444\n",
      "Epoch 189521: reducing learning rate of group 0 to 1.5444e-03.\n",
      "Epoch 189601, Training Loss: 27854, Validation Loss: 52481, 60043.17651947274\n",
      "Epoch 189622: reducing learning rate of group 0 to 1.5429e-03.\n",
      "Epoch 189701, Training Loss: 27579, Validation Loss: 57720, 72918.24710700834\n",
      "Epoch 189723: reducing learning rate of group 0 to 1.5413e-03.\n",
      "Epoch 189801, Training Loss: 30218, Validation Loss: 54105, 71184.44195754568\n",
      "Epoch 189824: reducing learning rate of group 0 to 1.5398e-03.\n",
      "Epoch 189901, Training Loss: 28645, Validation Loss: 55860, 87019.43001342227\n",
      "Epoch 189925: reducing learning rate of group 0 to 1.5383e-03.\n",
      "Epoch 190001, Training Loss: 28284, Validation Loss: 54303, 71560.77460197006\n",
      "Epoch 190026: reducing learning rate of group 0 to 1.5367e-03.\n",
      "Epoch 190101, Training Loss: 27676, Validation Loss: 52068, 78553.42318761263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190127: reducing learning rate of group 0 to 1.5352e-03.\n",
      "Epoch 190201, Training Loss: 27928, Validation Loss: 55044, 63655.0980156513\n",
      "Epoch 190228: reducing learning rate of group 0 to 1.5336e-03.\n",
      "Epoch 190301, Training Loss: 28563, Validation Loss: 54691, 76237.80797824886\n",
      "Epoch 190329: reducing learning rate of group 0 to 1.5321e-03.\n",
      "Epoch 190401, Training Loss: 29137, Validation Loss: 54259, 87053.64834493228\n",
      "Epoch 190430: reducing learning rate of group 0 to 1.5306e-03.\n",
      "Epoch 190501, Training Loss: 27489, Validation Loss: 52915, 106370.42611650395\n",
      "Epoch 190531: reducing learning rate of group 0 to 1.5290e-03.\n",
      "Epoch 190601, Training Loss: 29599, Validation Loss: 54574, 84550.08253328534\n",
      "Epoch 190632: reducing learning rate of group 0 to 1.5275e-03.\n",
      "Epoch 190701, Training Loss: 31478, Validation Loss: 55668, 77186.74265124678\n",
      "Epoch 190733: reducing learning rate of group 0 to 1.5260e-03.\n",
      "Epoch 190801, Training Loss: 30079, Validation Loss: 54519, 66915.77549120523\n",
      "Epoch 190834: reducing learning rate of group 0 to 1.5245e-03.\n",
      "Epoch 190901, Training Loss: 29576, Validation Loss: 54640, 81878.25025582247\n",
      "Epoch 190935: reducing learning rate of group 0 to 1.5229e-03.\n",
      "Epoch 191001, Training Loss: 27855, Validation Loss: 57001, 109965.17988097963\n",
      "Epoch 191036: reducing learning rate of group 0 to 1.5214e-03.\n",
      "Epoch 191101, Training Loss: 32662, Validation Loss: 54788, 77935.60193382937\n",
      "Epoch 191137: reducing learning rate of group 0 to 1.5199e-03.\n",
      "Epoch 191201, Training Loss: 27906, Validation Loss: 54283, 71019.92808563558\n",
      "Epoch 191238: reducing learning rate of group 0 to 1.5184e-03.\n",
      "Epoch 191301, Training Loss: 28977, Validation Loss: 54655, 78526.83926542349\n",
      "Epoch 191339: reducing learning rate of group 0 to 1.5169e-03.\n",
      "Epoch 191401, Training Loss: 29153, Validation Loss: 55589, 63899.77879576348\n",
      "Epoch 191440: reducing learning rate of group 0 to 1.5153e-03.\n",
      "Epoch 191501, Training Loss: 27757, Validation Loss: 55106, 81800.38870687662\n",
      "Epoch 191541: reducing learning rate of group 0 to 1.5138e-03.\n",
      "Epoch 191601, Training Loss: 28371, Validation Loss: 54494, 80628.88188649659\n",
      "Epoch 191642: reducing learning rate of group 0 to 1.5123e-03.\n",
      "Epoch 191701, Training Loss: 30010, Validation Loss: 51809, 87781.0839387377\n",
      "Epoch 191743: reducing learning rate of group 0 to 1.5108e-03.\n",
      "Epoch 191801, Training Loss: 27562, Validation Loss: 53069, 78987.75234837194\n",
      "Epoch 191844: reducing learning rate of group 0 to 1.5093e-03.\n",
      "Epoch 191901, Training Loss: 30934, Validation Loss: 53483, 62455.22535450739\n",
      "Epoch 191945: reducing learning rate of group 0 to 1.5078e-03.\n",
      "Epoch 192001, Training Loss: 31542, Validation Loss: 55289, 98632.98371819254\n",
      "Epoch 192046: reducing learning rate of group 0 to 1.5063e-03.\n",
      "Epoch 192101, Training Loss: 28295, Validation Loss: 55283, 72666.08238308592\n",
      "Epoch 192147: reducing learning rate of group 0 to 1.5048e-03.\n",
      "Epoch 192201, Training Loss: 30345, Validation Loss: 54897, 104028.74441624556\n",
      "Epoch 192248: reducing learning rate of group 0 to 1.5033e-03.\n",
      "Epoch 192301, Training Loss: 27673, Validation Loss: 54156, 65179.708756738495\n",
      "Epoch 192349: reducing learning rate of group 0 to 1.5018e-03.\n",
      "Epoch 192401, Training Loss: 29365, Validation Loss: 54850, 77116.71277665671\n",
      "Epoch 192450: reducing learning rate of group 0 to 1.5003e-03.\n",
      "Epoch 192501, Training Loss: 27074, Validation Loss: 54554, 82888.65924384278\n",
      "Epoch 192551: reducing learning rate of group 0 to 1.4988e-03.\n",
      "Epoch 192601, Training Loss: 28426, Validation Loss: 54166, 79794.07471768484\n",
      "Epoch 192652: reducing learning rate of group 0 to 1.4973e-03.\n",
      "Epoch 192701, Training Loss: 28568, Validation Loss: 55253, 64262.62257842813\n",
      "Epoch 192753: reducing learning rate of group 0 to 1.4958e-03.\n",
      "Epoch 192801, Training Loss: 29604, Validation Loss: 53421, 76785.79630338041\n",
      "Epoch 192854: reducing learning rate of group 0 to 1.4943e-03.\n",
      "Epoch 192901, Training Loss: 27828, Validation Loss: 54447, 67215.69902638777\n",
      "Epoch 192955: reducing learning rate of group 0 to 1.4928e-03.\n",
      "Epoch 193001, Training Loss: 28556, Validation Loss: 53957, 79329.56188521748\n",
      "Epoch 193056: reducing learning rate of group 0 to 1.4913e-03.\n",
      "Epoch 193101, Training Loss: 29368, Validation Loss: 55706, 70891.9691000608\n",
      "Epoch 193157: reducing learning rate of group 0 to 1.4898e-03.\n",
      "Epoch 193201, Training Loss: 27211, Validation Loss: 53919, 80621.31723147696\n",
      "Epoch 193258: reducing learning rate of group 0 to 1.4883e-03.\n",
      "Epoch 193301, Training Loss: 26019, Validation Loss: 55942, 89488.17559099903\n",
      "Epoch 193359: reducing learning rate of group 0 to 1.4868e-03.\n",
      "Epoch 193401, Training Loss: 29594, Validation Loss: 54067, 93002.55000762096\n",
      "Epoch 193460: reducing learning rate of group 0 to 1.4853e-03.\n",
      "Epoch 193501, Training Loss: 30451, Validation Loss: 55543, 83638.86031574832\n",
      "Epoch 193561: reducing learning rate of group 0 to 1.4838e-03.\n",
      "Epoch 193601, Training Loss: 28681, Validation Loss: 55460, 80658.37776960329\n",
      "Epoch 193662: reducing learning rate of group 0 to 1.4824e-03.\n",
      "Epoch 193701, Training Loss: 28161, Validation Loss: 53912, 75546.38039341498\n",
      "Epoch 193763: reducing learning rate of group 0 to 1.4809e-03.\n",
      "Epoch 193801, Training Loss: 29750, Validation Loss: 53195, 79919.3352015644\n",
      "Epoch 193864: reducing learning rate of group 0 to 1.4794e-03.\n",
      "Epoch 193901, Training Loss: 31039, Validation Loss: 54536, 74963.23465027513\n",
      "Epoch 193965: reducing learning rate of group 0 to 1.4779e-03.\n",
      "Epoch 194001, Training Loss: 30391, Validation Loss: 55822, 79524.01872486305\n",
      "Epoch 194066: reducing learning rate of group 0 to 1.4764e-03.\n",
      "Epoch 194101, Training Loss: 26274, Validation Loss: 54271, 79960.95383481929\n",
      "Epoch 194167: reducing learning rate of group 0 to 1.4750e-03.\n",
      "Epoch 194201, Training Loss: 30148, Validation Loss: 55002, 93592.01912443755\n",
      "Epoch 194268: reducing learning rate of group 0 to 1.4735e-03.\n",
      "Epoch 194301, Training Loss: 27487, Validation Loss: 55060, 76787.02234780336\n",
      "Epoch 194369: reducing learning rate of group 0 to 1.4720e-03.\n",
      "Epoch 194401, Training Loss: 28416, Validation Loss: 53388, 80459.60309922391\n",
      "Epoch 194470: reducing learning rate of group 0 to 1.4705e-03.\n",
      "Epoch 194501, Training Loss: 26858, Validation Loss: 53351, 78267.59458927717\n",
      "Epoch 194571: reducing learning rate of group 0 to 1.4691e-03.\n",
      "Epoch 194601, Training Loss: 32274, Validation Loss: 54308, 78213.24874069844\n",
      "Epoch 194672: reducing learning rate of group 0 to 1.4676e-03.\n",
      "Epoch 194701, Training Loss: 27010, Validation Loss: 52962, 74382.56244849013\n",
      "Epoch 194773: reducing learning rate of group 0 to 1.4661e-03.\n",
      "Epoch 194801, Training Loss: 27178, Validation Loss: 54927, 57864.94137766938\n",
      "Epoch 194874: reducing learning rate of group 0 to 1.4647e-03.\n",
      "Epoch 194901, Training Loss: 26807, Validation Loss: 53874, 73724.8687273098\n",
      "Epoch 194975: reducing learning rate of group 0 to 1.4632e-03.\n",
      "Epoch 195001, Training Loss: 29027, Validation Loss: 56131, 96732.86458117166\n",
      "Epoch 195076: reducing learning rate of group 0 to 1.4617e-03.\n",
      "Epoch 195101, Training Loss: 31400, Validation Loss: 54297, 81233.15337913786\n",
      "Epoch 195177: reducing learning rate of group 0 to 1.4603e-03.\n",
      "Epoch 195201, Training Loss: 28521, Validation Loss: 55591, 68132.39387483669\n",
      "Epoch 195278: reducing learning rate of group 0 to 1.4588e-03.\n",
      "Epoch 195301, Training Loss: 29058, Validation Loss: 53281, 72887.90069532115\n",
      "Epoch 195379: reducing learning rate of group 0 to 1.4574e-03.\n",
      "Epoch 195401, Training Loss: 27916, Validation Loss: 53670, 76851.82310788399\n",
      "Epoch 195480: reducing learning rate of group 0 to 1.4559e-03.\n",
      "Epoch 195501, Training Loss: 28964, Validation Loss: 53430, 76412.81040764322\n",
      "Epoch 195581: reducing learning rate of group 0 to 1.4544e-03.\n",
      "Epoch 195601, Training Loss: 32961, Validation Loss: 55484, 79057.8254244593\n",
      "Epoch 195682: reducing learning rate of group 0 to 1.4530e-03.\n",
      "Epoch 195701, Training Loss: 29781, Validation Loss: 56783, 68799.42871698202\n",
      "Epoch 195783: reducing learning rate of group 0 to 1.4515e-03.\n",
      "Epoch 195801, Training Loss: 28274, Validation Loss: 55884, 71675.46451186521\n",
      "Epoch 195884: reducing learning rate of group 0 to 1.4501e-03.\n",
      "Epoch 195901, Training Loss: 27296, Validation Loss: 56360, 76933.44770575562\n",
      "Epoch 195985: reducing learning rate of group 0 to 1.4486e-03.\n",
      "Epoch 196001, Training Loss: 28764, Validation Loss: 53984, 77977.42990829778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196086: reducing learning rate of group 0 to 1.4472e-03.\n",
      "Epoch 196101, Training Loss: 29945, Validation Loss: 54892, 84670.30748652639\n",
      "Epoch 196187: reducing learning rate of group 0 to 1.4457e-03.\n",
      "Epoch 196201, Training Loss: 30323, Validation Loss: 52494, 79660.69280335057\n",
      "Epoch 196288: reducing learning rate of group 0 to 1.4443e-03.\n",
      "Epoch 196301, Training Loss: 27842, Validation Loss: 53863, 79101.00743181138\n",
      "Epoch 196389: reducing learning rate of group 0 to 1.4428e-03.\n",
      "Epoch 196401, Training Loss: 31721, Validation Loss: 53563, 88621.69310411671\n",
      "Epoch 196490: reducing learning rate of group 0 to 1.4414e-03.\n",
      "Epoch 196501, Training Loss: 26540, Validation Loss: 53589, 75769.33176071347\n",
      "Epoch 196591: reducing learning rate of group 0 to 1.4400e-03.\n",
      "Epoch 196601, Training Loss: 29673, Validation Loss: 54943, 79881.87764689243\n",
      "Epoch 196692: reducing learning rate of group 0 to 1.4385e-03.\n",
      "Epoch 196701, Training Loss: 27876, Validation Loss: 55595, 84002.45018040696\n",
      "Epoch 196793: reducing learning rate of group 0 to 1.4371e-03.\n",
      "Epoch 196801, Training Loss: 30802, Validation Loss: 53560, 98508.67567636522\n",
      "Epoch 196894: reducing learning rate of group 0 to 1.4356e-03.\n",
      "Epoch 196901, Training Loss: 31126, Validation Loss: 53329, 86413.8516759026\n",
      "Epoch 196995: reducing learning rate of group 0 to 1.4342e-03.\n",
      "Epoch 197001, Training Loss: 28100, Validation Loss: 54448, 72492.73161019135\n",
      "Epoch 197096: reducing learning rate of group 0 to 1.4328e-03.\n",
      "Epoch 197101, Training Loss: 27829, Validation Loss: 54715, 71470.34228686454\n",
      "Epoch 197197: reducing learning rate of group 0 to 1.4313e-03.\n",
      "Epoch 197201, Training Loss: 26974, Validation Loss: 54309, 72973.81770355583\n",
      "Epoch 197298: reducing learning rate of group 0 to 1.4299e-03.\n",
      "Epoch 197301, Training Loss: 30973, Validation Loss: 55747, 83125.77835657769\n",
      "Epoch 197399: reducing learning rate of group 0 to 1.4285e-03.\n",
      "Epoch 197401, Training Loss: 28358, Validation Loss: 57326, 84538.37796085536\n",
      "Epoch 197500: reducing learning rate of group 0 to 1.4271e-03.\n",
      "Epoch 197501, Training Loss: 28727, Validation Loss: 53667, 78491.74879363913\n",
      "Epoch 197601: reducing learning rate of group 0 to 1.4256e-03.\n",
      "Epoch 197601, Training Loss: 27634, Validation Loss: 54882, 74855.09783999075\n",
      "Epoch 197701, Training Loss: 28785, Validation Loss: 53791, 66976.33922464716\n",
      "Epoch 197702: reducing learning rate of group 0 to 1.4242e-03.\n",
      "Epoch 197801, Training Loss: 29007, Validation Loss: 55950, 68883.1928478244\n",
      "Epoch 197803: reducing learning rate of group 0 to 1.4228e-03.\n",
      "Epoch 197901, Training Loss: 26091, Validation Loss: 53050, 69799.86620480975\n",
      "Epoch 197904: reducing learning rate of group 0 to 1.4214e-03.\n",
      "Epoch 198001, Training Loss: 29020, Validation Loss: 53814, 73594.31143855366\n",
      "Epoch 198005: reducing learning rate of group 0 to 1.4199e-03.\n",
      "Epoch 198101, Training Loss: 29513, Validation Loss: 52637, 72515.99727822407\n",
      "Epoch 198106: reducing learning rate of group 0 to 1.4185e-03.\n",
      "Epoch 198201, Training Loss: 30690, Validation Loss: 53749, 83859.14548868785\n",
      "Epoch 198207: reducing learning rate of group 0 to 1.4171e-03.\n",
      "Epoch 198301, Training Loss: 27038, Validation Loss: 53369, 75825.46062073261\n",
      "Epoch 198308: reducing learning rate of group 0 to 1.4157e-03.\n",
      "Epoch 198401, Training Loss: 28120, Validation Loss: 53837, 89852.20688622445\n",
      "Epoch 198409: reducing learning rate of group 0 to 1.4143e-03.\n",
      "Epoch 198501, Training Loss: 28838, Validation Loss: 52079, 72587.07604087802\n",
      "Epoch 198510: reducing learning rate of group 0 to 1.4128e-03.\n",
      "Epoch 198601, Training Loss: 29452, Validation Loss: 53142, 79791.15373149923\n",
      "Epoch 198611: reducing learning rate of group 0 to 1.4114e-03.\n",
      "Epoch 198701, Training Loss: 28456, Validation Loss: 55730, 82422.81891060666\n",
      "Epoch 198712: reducing learning rate of group 0 to 1.4100e-03.\n",
      "Epoch 198801, Training Loss: 27665, Validation Loss: 55557, 92155.33509854697\n",
      "Epoch 198813: reducing learning rate of group 0 to 1.4086e-03.\n",
      "Epoch 198901, Training Loss: 28011, Validation Loss: 53436, 89389.28349284174\n",
      "Epoch 198914: reducing learning rate of group 0 to 1.4072e-03.\n",
      "Epoch 199001, Training Loss: 30151, Validation Loss: 54259, 74334.85515307156\n",
      "Epoch 199015: reducing learning rate of group 0 to 1.4058e-03.\n",
      "Epoch 199101, Training Loss: 29806, Validation Loss: 56275, 75219.94583687746\n",
      "Epoch 199116: reducing learning rate of group 0 to 1.4044e-03.\n",
      "Epoch 199201, Training Loss: 31618, Validation Loss: 53413, 70323.42978037013\n",
      "Epoch 199217: reducing learning rate of group 0 to 1.4030e-03.\n",
      "Epoch 199301, Training Loss: 27582, Validation Loss: 55308, 82750.87736452681\n",
      "Epoch 199318: reducing learning rate of group 0 to 1.4016e-03.\n",
      "Epoch 199401, Training Loss: 29261, Validation Loss: 52628, 82093.0038422665\n",
      "Epoch 199419: reducing learning rate of group 0 to 1.4002e-03.\n",
      "Epoch 199501, Training Loss: 30056, Validation Loss: 54277, 73354.29360062855\n",
      "Epoch 199520: reducing learning rate of group 0 to 1.3988e-03.\n",
      "Epoch 199601, Training Loss: 27430, Validation Loss: 54116, 71399.3236753473\n",
      "Epoch 199621: reducing learning rate of group 0 to 1.3974e-03.\n",
      "Epoch 199701, Training Loss: 29332, Validation Loss: 54127, 63064.96600294416\n",
      "Epoch 199722: reducing learning rate of group 0 to 1.3960e-03.\n",
      "Epoch 199801, Training Loss: 29736, Validation Loss: 54362, 88762.53438512374\n",
      "Epoch 199823: reducing learning rate of group 0 to 1.3946e-03.\n",
      "Epoch 199901, Training Loss: 31801, Validation Loss: 52861, 73102.99476310068\n",
      "Epoch 199924: reducing learning rate of group 0 to 1.3932e-03.\n",
      "Epoch 200001, Training Loss: 29017, Validation Loss: 56568, 66805.6613454115\n",
      "Epoch 200025: reducing learning rate of group 0 to 1.3918e-03.\n",
      "Epoch 200101, Training Loss: 28120, Validation Loss: 53369, 87087.80465221053\n",
      "Epoch 200126: reducing learning rate of group 0 to 1.3904e-03.\n",
      "Epoch 200201, Training Loss: 28292, Validation Loss: 54596, 60893.16351213807\n",
      "Epoch 200227: reducing learning rate of group 0 to 1.3890e-03.\n",
      "Epoch 200301, Training Loss: 27554, Validation Loss: 52663, 85382.0263762236\n",
      "Epoch 200328: reducing learning rate of group 0 to 1.3876e-03.\n",
      "Epoch 200401, Training Loss: 27452, Validation Loss: 53960, 78562.47171081445\n",
      "Epoch 200429: reducing learning rate of group 0 to 1.3862e-03.\n",
      "Epoch 200501, Training Loss: 27970, Validation Loss: 55956, 83028.58712928956\n",
      "Epoch 200530: reducing learning rate of group 0 to 1.3849e-03.\n",
      "Epoch 200601, Training Loss: 28053, Validation Loss: 53639, 68732.39531056532\n",
      "Epoch 200631: reducing learning rate of group 0 to 1.3835e-03.\n",
      "Epoch 200701, Training Loss: 32475, Validation Loss: 55015, 82176.63413539746\n",
      "Epoch 200732: reducing learning rate of group 0 to 1.3821e-03.\n",
      "Epoch 200801, Training Loss: 29934, Validation Loss: 56591, 75898.94835961558\n",
      "Epoch 200833: reducing learning rate of group 0 to 1.3807e-03.\n",
      "Epoch 200901, Training Loss: 27424, Validation Loss: 56214, 82688.75258613152\n",
      "Epoch 200934: reducing learning rate of group 0 to 1.3793e-03.\n",
      "Epoch 201001, Training Loss: 29694, Validation Loss: 55809, 72519.52481756617\n",
      "Epoch 201035: reducing learning rate of group 0 to 1.3779e-03.\n",
      "Epoch 201101, Training Loss: 30374, Validation Loss: 56508, 84357.24808980922\n",
      "Epoch 201136: reducing learning rate of group 0 to 1.3766e-03.\n",
      "Epoch 201201, Training Loss: 27742, Validation Loss: 55476, 70366.74027829923\n",
      "Epoch 201237: reducing learning rate of group 0 to 1.3752e-03.\n",
      "Epoch 201301, Training Loss: 26887, Validation Loss: 53894, 88205.38981155572\n",
      "Epoch 201338: reducing learning rate of group 0 to 1.3738e-03.\n",
      "Epoch 201401, Training Loss: 29146, Validation Loss: 56249, 85991.21508738627\n",
      "Epoch 201439: reducing learning rate of group 0 to 1.3724e-03.\n",
      "Epoch 201501, Training Loss: 27276, Validation Loss: 53283, 82356.7674710041\n",
      "Epoch 201540: reducing learning rate of group 0 to 1.3711e-03.\n",
      "Epoch 201601, Training Loss: 29034, Validation Loss: 57020, 79990.15161387091\n",
      "Epoch 201641: reducing learning rate of group 0 to 1.3697e-03.\n",
      "Epoch 201701, Training Loss: 29356, Validation Loss: 54542, 87508.88722399513\n",
      "Epoch 201742: reducing learning rate of group 0 to 1.3683e-03.\n",
      "Epoch 201801, Training Loss: 27191, Validation Loss: 54513, 75080.11600076381\n",
      "Epoch 201843: reducing learning rate of group 0 to 1.3670e-03.\n",
      "Epoch 201901, Training Loss: 29361, Validation Loss: 54787, 86350.02882641004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201944: reducing learning rate of group 0 to 1.3656e-03.\n",
      "Epoch 202001, Training Loss: 31042, Validation Loss: 54522, 75686.431664169\n",
      "Epoch 202045: reducing learning rate of group 0 to 1.3642e-03.\n",
      "Epoch 202101, Training Loss: 28408, Validation Loss: 53071, 77823.8930709708\n",
      "Epoch 202146: reducing learning rate of group 0 to 1.3629e-03.\n",
      "Epoch 202201, Training Loss: 33341, Validation Loss: 55778, 86893.19812211554\n",
      "Epoch 202247: reducing learning rate of group 0 to 1.3615e-03.\n",
      "Epoch 202301, Training Loss: 27515, Validation Loss: 54104, 74491.70222075819\n",
      "Epoch 202348: reducing learning rate of group 0 to 1.3601e-03.\n",
      "Epoch 202401, Training Loss: 27809, Validation Loss: 51259, 79370.36968372668\n",
      "Epoch 202449: reducing learning rate of group 0 to 1.3588e-03.\n",
      "Epoch 202501, Training Loss: 29801, Validation Loss: 55486, 83008.23113432186\n",
      "Epoch 202550: reducing learning rate of group 0 to 1.3574e-03.\n",
      "Epoch 202601, Training Loss: 28810, Validation Loss: 53083, 81276.34476858392\n",
      "Epoch 202651: reducing learning rate of group 0 to 1.3561e-03.\n",
      "Epoch 202701, Training Loss: 27964, Validation Loss: 54967, 69742.6335823706\n",
      "Epoch 202752: reducing learning rate of group 0 to 1.3547e-03.\n",
      "Epoch 202801, Training Loss: 30488, Validation Loss: 56139, 79607.3744692672\n",
      "Epoch 202853: reducing learning rate of group 0 to 1.3534e-03.\n",
      "Epoch 202901, Training Loss: 29202, Validation Loss: 53820, 80191.58571754697\n",
      "Epoch 202954: reducing learning rate of group 0 to 1.3520e-03.\n",
      "Epoch 203001, Training Loss: 26504, Validation Loss: 52563, 101900.7915744972\n",
      "Epoch 203055: reducing learning rate of group 0 to 1.3506e-03.\n",
      "Epoch 203101, Training Loss: 28538, Validation Loss: 53422, 75880.027926594\n",
      "Epoch 203156: reducing learning rate of group 0 to 1.3493e-03.\n",
      "Epoch 203201, Training Loss: 29338, Validation Loss: 52711, 78908.22601423728\n",
      "Epoch 203257: reducing learning rate of group 0 to 1.3479e-03.\n",
      "Epoch 203301, Training Loss: 29677, Validation Loss: 53524, 89676.73156826566\n",
      "Epoch 203358: reducing learning rate of group 0 to 1.3466e-03.\n",
      "Epoch 203401, Training Loss: 28406, Validation Loss: 54618, 98387.76509222008\n",
      "Epoch 203459: reducing learning rate of group 0 to 1.3453e-03.\n",
      "Epoch 203501, Training Loss: 29386, Validation Loss: 54930, 79443.93619943799\n",
      "Epoch 203560: reducing learning rate of group 0 to 1.3439e-03.\n",
      "Epoch 203601, Training Loss: 29358, Validation Loss: 52927, 83101.14504228103\n",
      "Epoch 203661: reducing learning rate of group 0 to 1.3426e-03.\n",
      "Epoch 203701, Training Loss: 29557, Validation Loss: 56206, 84621.04004001473\n",
      "Epoch 203762: reducing learning rate of group 0 to 1.3412e-03.\n",
      "Epoch 203801, Training Loss: 28664, Validation Loss: 55289, 88167.19838226191\n",
      "Epoch 203863: reducing learning rate of group 0 to 1.3399e-03.\n",
      "Epoch 203901, Training Loss: 30981, Validation Loss: 56392, 88393.10097976122\n",
      "Epoch 203964: reducing learning rate of group 0 to 1.3385e-03.\n",
      "Epoch 204001, Training Loss: 29809, Validation Loss: 53907, 83423.76051367322\n",
      "Epoch 204065: reducing learning rate of group 0 to 1.3372e-03.\n",
      "Epoch 204101, Training Loss: 27177, Validation Loss: 53783, 84069.14653799708\n",
      "Epoch 204166: reducing learning rate of group 0 to 1.3359e-03.\n",
      "Epoch 204201, Training Loss: 27775, Validation Loss: 55708, 77037.21312750982\n",
      "Epoch 204267: reducing learning rate of group 0 to 1.3345e-03.\n",
      "Epoch 204301, Training Loss: 28445, Validation Loss: 51692, 80961.04179317928\n",
      "Epoch 204368: reducing learning rate of group 0 to 1.3332e-03.\n",
      "Epoch 204401, Training Loss: 29812, Validation Loss: 52992, 91225.45743795426\n",
      "Epoch 204469: reducing learning rate of group 0 to 1.3319e-03.\n",
      "Epoch 204501, Training Loss: 29177, Validation Loss: 54044, 78662.45078390987\n",
      "Epoch 204570: reducing learning rate of group 0 to 1.3305e-03.\n",
      "Epoch 204601, Training Loss: 27418, Validation Loss: 54913, 73900.34337325196\n",
      "Epoch 204671: reducing learning rate of group 0 to 1.3292e-03.\n",
      "Epoch 204701, Training Loss: 27704, Validation Loss: 52445, 94237.7172955606\n",
      "Epoch 204772: reducing learning rate of group 0 to 1.3279e-03.\n",
      "Epoch 204801, Training Loss: 29757, Validation Loss: 54543, 83882.33529783988\n",
      "Epoch 204873: reducing learning rate of group 0 to 1.3265e-03.\n",
      "Epoch 204901, Training Loss: 27945, Validation Loss: 54040, 69719.11660838299\n",
      "Epoch 204974: reducing learning rate of group 0 to 1.3252e-03.\n",
      "Epoch 205001, Training Loss: 28852, Validation Loss: 52756, 78147.06846460757\n",
      "Epoch 205075: reducing learning rate of group 0 to 1.3239e-03.\n",
      "Epoch 205101, Training Loss: 32470, Validation Loss: 54667, 88623.92171725951\n",
      "Epoch 205176: reducing learning rate of group 0 to 1.3226e-03.\n",
      "Epoch 205201, Training Loss: 28978, Validation Loss: 53539, 63878.01143800269\n",
      "Epoch 205277: reducing learning rate of group 0 to 1.3212e-03.\n",
      "Epoch 205301, Training Loss: 29823, Validation Loss: 53399, 76788.23454867884\n",
      "Epoch 205378: reducing learning rate of group 0 to 1.3199e-03.\n",
      "Epoch 205401, Training Loss: 29214, Validation Loss: 55467, 70613.64284106313\n",
      "Epoch 205479: reducing learning rate of group 0 to 1.3186e-03.\n",
      "Epoch 205501, Training Loss: 30263, Validation Loss: 55679, 64117.645760117266\n",
      "Epoch 205580: reducing learning rate of group 0 to 1.3173e-03.\n",
      "Epoch 205601, Training Loss: 28045, Validation Loss: 54483, 82073.37466205187\n",
      "Epoch 205681: reducing learning rate of group 0 to 1.3160e-03.\n",
      "Epoch 205701, Training Loss: 27755, Validation Loss: 54794, 64334.98516234673\n",
      "Epoch 205782: reducing learning rate of group 0 to 1.3146e-03.\n",
      "Epoch 205801, Training Loss: 27122, Validation Loss: 54247, 79953.94249566646\n",
      "Epoch 205883: reducing learning rate of group 0 to 1.3133e-03.\n",
      "Epoch 205901, Training Loss: 28836, Validation Loss: 56992, 79957.96510714675\n",
      "Epoch 205984: reducing learning rate of group 0 to 1.3120e-03.\n",
      "Epoch 206001, Training Loss: 26745, Validation Loss: 54668, 80558.10324879753\n",
      "Epoch 206085: reducing learning rate of group 0 to 1.3107e-03.\n",
      "Epoch 206101, Training Loss: 28708, Validation Loss: 53551, 97561.87712541372\n",
      "Epoch 206186: reducing learning rate of group 0 to 1.3094e-03.\n",
      "Epoch 206201, Training Loss: 28624, Validation Loss: 56016, 74028.37141870848\n",
      "Epoch 206287: reducing learning rate of group 0 to 1.3081e-03.\n",
      "Epoch 206301, Training Loss: 28830, Validation Loss: 54399, 77990.88330168974\n",
      "Epoch 206388: reducing learning rate of group 0 to 1.3068e-03.\n",
      "Epoch 206401, Training Loss: 30041, Validation Loss: 54090, 87450.598069902\n",
      "Epoch 206489: reducing learning rate of group 0 to 1.3055e-03.\n",
      "Epoch 206501, Training Loss: 27622, Validation Loss: 53532, 81962.8952321078\n",
      "Epoch 206590: reducing learning rate of group 0 to 1.3042e-03.\n",
      "Epoch 206601, Training Loss: 27250, Validation Loss: 52514, 78754.84229551857\n",
      "Epoch 206691: reducing learning rate of group 0 to 1.3029e-03.\n",
      "Epoch 206701, Training Loss: 30265, Validation Loss: 55043, 70325.18563190784\n",
      "Epoch 206792: reducing learning rate of group 0 to 1.3016e-03.\n",
      "Epoch 206801, Training Loss: 27371, Validation Loss: 55234, 90727.3570674046\n",
      "Epoch 206893: reducing learning rate of group 0 to 1.3003e-03.\n",
      "Epoch 206901, Training Loss: 28077, Validation Loss: 53045, 70869.94805539997\n",
      "Epoch 206994: reducing learning rate of group 0 to 1.2990e-03.\n",
      "Epoch 207001, Training Loss: 28785, Validation Loss: 53445, 94087.36631513499\n",
      "Epoch 207095: reducing learning rate of group 0 to 1.2977e-03.\n",
      "Epoch 207101, Training Loss: 29245, Validation Loss: 55031, 74834.45971117295\n",
      "Epoch 207196: reducing learning rate of group 0 to 1.2964e-03.\n",
      "Epoch 207201, Training Loss: 28392, Validation Loss: 54603, 101187.57807102136\n",
      "Epoch 207297: reducing learning rate of group 0 to 1.2951e-03.\n",
      "Epoch 207301, Training Loss: 28185, Validation Loss: 55590, 101874.43607216222\n",
      "Epoch 207398: reducing learning rate of group 0 to 1.2938e-03.\n",
      "Epoch 207401, Training Loss: 27312, Validation Loss: 53171, 76197.73369694653\n",
      "Epoch 207499: reducing learning rate of group 0 to 1.2925e-03.\n",
      "Epoch 207501, Training Loss: 28709, Validation Loss: 54779, 72520.45223632285\n",
      "Epoch 207600: reducing learning rate of group 0 to 1.2912e-03.\n",
      "Epoch 207601, Training Loss: 30126, Validation Loss: 54882, 88708.11478860937\n",
      "Epoch 207701: reducing learning rate of group 0 to 1.2899e-03.\n",
      "Epoch 207701, Training Loss: 29166, Validation Loss: 52162, 74636.44496176952\n",
      "Epoch 207801, Training Loss: 27331, Validation Loss: 52974, 73698.78878301627\n",
      "Epoch 207802: reducing learning rate of group 0 to 1.2886e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207901, Training Loss: 28938, Validation Loss: 54550, 90743.39450747013\n",
      "Epoch 207903: reducing learning rate of group 0 to 1.2873e-03.\n",
      "Epoch 208001, Training Loss: 27003, Validation Loss: 54438, 82021.7308953753\n",
      "Epoch 208004: reducing learning rate of group 0 to 1.2860e-03.\n",
      "Epoch 208101, Training Loss: 28496, Validation Loss: 55457, 87163.60662149938\n",
      "Epoch 208105: reducing learning rate of group 0 to 1.2847e-03.\n",
      "Epoch 208201, Training Loss: 28122, Validation Loss: 55928, 74969.30345319893\n",
      "Epoch 208206: reducing learning rate of group 0 to 1.2835e-03.\n",
      "Epoch 208301, Training Loss: 27660, Validation Loss: 53174, 82499.19470211815\n",
      "Epoch 208307: reducing learning rate of group 0 to 1.2822e-03.\n",
      "Epoch 208401, Training Loss: 30033, Validation Loss: 56123, 77774.6989846139\n",
      "Epoch 208408: reducing learning rate of group 0 to 1.2809e-03.\n",
      "Epoch 208501, Training Loss: 28737, Validation Loss: 54584, 81940.24571639951\n",
      "Epoch 208509: reducing learning rate of group 0 to 1.2796e-03.\n",
      "Epoch 208601, Training Loss: 29572, Validation Loss: 55179, 96211.30701089569\n",
      "Epoch 208610: reducing learning rate of group 0 to 1.2783e-03.\n",
      "Epoch 208701, Training Loss: 25380, Validation Loss: 55379, 85849.31394822874\n",
      "Epoch 208711: reducing learning rate of group 0 to 1.2771e-03.\n",
      "Epoch 208801, Training Loss: 29311, Validation Loss: 54158, 82257.34003362835\n",
      "Epoch 208812: reducing learning rate of group 0 to 1.2758e-03.\n",
      "Epoch 208901, Training Loss: 27284, Validation Loss: 55473, 84866.12572225141\n",
      "Epoch 208913: reducing learning rate of group 0 to 1.2745e-03.\n",
      "Epoch 209001, Training Loss: 29523, Validation Loss: 55254, 87054.13356810618\n",
      "Epoch 209014: reducing learning rate of group 0 to 1.2732e-03.\n",
      "Epoch 209101, Training Loss: 30235, Validation Loss: 52754, 84287.24459313285\n",
      "Epoch 209115: reducing learning rate of group 0 to 1.2720e-03.\n",
      "Epoch 209201, Training Loss: 27611, Validation Loss: 55939, 80865.59514148535\n",
      "Epoch 209216: reducing learning rate of group 0 to 1.2707e-03.\n",
      "Epoch 209301, Training Loss: 29670, Validation Loss: 55985, 73743.16560383921\n",
      "Epoch 209317: reducing learning rate of group 0 to 1.2694e-03.\n",
      "Epoch 209401, Training Loss: 29169, Validation Loss: 55872, 85037.81615316228\n",
      "Epoch 209418: reducing learning rate of group 0 to 1.2681e-03.\n",
      "Epoch 209501, Training Loss: 29769, Validation Loss: 53688, 94811.93674565801\n",
      "Epoch 209519: reducing learning rate of group 0 to 1.2669e-03.\n",
      "Epoch 209601, Training Loss: 30391, Validation Loss: 52358, 76086.96453573018\n",
      "Epoch 209620: reducing learning rate of group 0 to 1.2656e-03.\n",
      "Epoch 209701, Training Loss: 27018, Validation Loss: 53881, 81120.14909244404\n",
      "Epoch 209721: reducing learning rate of group 0 to 1.2643e-03.\n",
      "Epoch 209801, Training Loss: 27161, Validation Loss: 54857, 80098.63316049495\n",
      "Epoch 209822: reducing learning rate of group 0 to 1.2631e-03.\n",
      "Epoch 209901, Training Loss: 27036, Validation Loss: 54268, 64344.85517487858\n",
      "Epoch 209923: reducing learning rate of group 0 to 1.2618e-03.\n",
      "Epoch 210001, Training Loss: 29893, Validation Loss: 53971, 78758.06923725751\n",
      "Epoch 210024: reducing learning rate of group 0 to 1.2606e-03.\n",
      "Epoch 210101, Training Loss: 32381, Validation Loss: 52230, 110249.84736762417\n",
      "Epoch 210125: reducing learning rate of group 0 to 1.2593e-03.\n",
      "Epoch 210201, Training Loss: 28742, Validation Loss: 53882, 70296.99502416405\n",
      "Epoch 210226: reducing learning rate of group 0 to 1.2580e-03.\n",
      "Epoch 210301, Training Loss: 28116, Validation Loss: 54094, 69754.8202835066\n",
      "Epoch 210327: reducing learning rate of group 0 to 1.2568e-03.\n",
      "Epoch 210401, Training Loss: 28845, Validation Loss: 54151, 74950.27597676539\n",
      "Epoch 210428: reducing learning rate of group 0 to 1.2555e-03.\n",
      "Epoch 210501, Training Loss: 29591, Validation Loss: 56270, 71001.82403389865\n",
      "Epoch 210529: reducing learning rate of group 0 to 1.2543e-03.\n",
      "Epoch 210601, Training Loss: 31672, Validation Loss: 57757, 88679.04119399602\n",
      "Epoch 210630: reducing learning rate of group 0 to 1.2530e-03.\n",
      "Epoch 210701, Training Loss: 31598, Validation Loss: 55256, 83674.46117810553\n",
      "Epoch 210731: reducing learning rate of group 0 to 1.2518e-03.\n",
      "Epoch 210801, Training Loss: 26512, Validation Loss: 52858, 75784.89648771765\n",
      "Epoch 210832: reducing learning rate of group 0 to 1.2505e-03.\n",
      "Epoch 210901, Training Loss: 26255, Validation Loss: 53830, 77682.30227992107\n",
      "Epoch 210933: reducing learning rate of group 0 to 1.2493e-03.\n",
      "Epoch 211001, Training Loss: 28360, Validation Loss: 53736, 78042.14652246276\n",
      "Epoch 211034: reducing learning rate of group 0 to 1.2480e-03.\n",
      "Epoch 211101, Training Loss: 29031, Validation Loss: 53540, 76764.11929329892\n",
      "Epoch 211135: reducing learning rate of group 0 to 1.2468e-03.\n",
      "Epoch 211201, Training Loss: 28577, Validation Loss: 55655, 83109.06634460039\n",
      "Epoch 211236: reducing learning rate of group 0 to 1.2455e-03.\n",
      "Epoch 211301, Training Loss: 27666, Validation Loss: 53718, 86970.6462956345\n",
      "Epoch 211337: reducing learning rate of group 0 to 1.2443e-03.\n",
      "Epoch 211401, Training Loss: 25749, Validation Loss: 53999, 75482.68067852674\n",
      "Epoch 211438: reducing learning rate of group 0 to 1.2430e-03.\n",
      "Epoch 211501, Training Loss: 30459, Validation Loss: 56382, 76633.2444929476\n",
      "Epoch 211539: reducing learning rate of group 0 to 1.2418e-03.\n",
      "Epoch 211601, Training Loss: 28108, Validation Loss: 54142, 78874.17760072986\n",
      "Epoch 211640: reducing learning rate of group 0 to 1.2405e-03.\n",
      "Epoch 211701, Training Loss: 29973, Validation Loss: 53845, 73500.52844110668\n",
      "Epoch 211741: reducing learning rate of group 0 to 1.2393e-03.\n",
      "Epoch 211801, Training Loss: 29172, Validation Loss: 56061, 81260.83229972525\n",
      "Epoch 211842: reducing learning rate of group 0 to 1.2381e-03.\n",
      "Epoch 211901, Training Loss: 30447, Validation Loss: 55840, 92042.58799726849\n",
      "Epoch 211943: reducing learning rate of group 0 to 1.2368e-03.\n",
      "Epoch 212001, Training Loss: 28353, Validation Loss: 52048, 98256.71853518345\n",
      "Epoch 212044: reducing learning rate of group 0 to 1.2356e-03.\n",
      "Epoch 212101, Training Loss: 28029, Validation Loss: 53851, 92027.23097910169\n",
      "Epoch 212145: reducing learning rate of group 0 to 1.2343e-03.\n",
      "Epoch 212201, Training Loss: 27804, Validation Loss: 55132, 74842.16641119753\n",
      "Epoch 212246: reducing learning rate of group 0 to 1.2331e-03.\n",
      "Epoch 212301, Training Loss: 31850, Validation Loss: 53702, 71564.39126748532\n",
      "Epoch 212347: reducing learning rate of group 0 to 1.2319e-03.\n",
      "Epoch 212401, Training Loss: 27775, Validation Loss: 55313, 77273.58740679928\n",
      "Epoch 212448: reducing learning rate of group 0 to 1.2306e-03.\n",
      "Epoch 212501, Training Loss: 28516, Validation Loss: 55034, 71768.12509022135\n",
      "Epoch 212549: reducing learning rate of group 0 to 1.2294e-03.\n",
      "Epoch 212601, Training Loss: 30181, Validation Loss: 55326, 87174.6150400602\n",
      "Epoch 212650: reducing learning rate of group 0 to 1.2282e-03.\n",
      "Epoch 212701, Training Loss: 29398, Validation Loss: 53552, 76772.05507762071\n",
      "Epoch 212751: reducing learning rate of group 0 to 1.2270e-03.\n",
      "Epoch 212801, Training Loss: 28154, Validation Loss: 55355, 76757.89995112082\n",
      "Epoch 212852: reducing learning rate of group 0 to 1.2257e-03.\n",
      "Epoch 212901, Training Loss: 31001, Validation Loss: 55529, 66214.37183024206\n",
      "Epoch 212953: reducing learning rate of group 0 to 1.2245e-03.\n",
      "Epoch 213001, Training Loss: 26570, Validation Loss: 52724, 75887.69075909042\n",
      "Epoch 213054: reducing learning rate of group 0 to 1.2233e-03.\n",
      "Epoch 213101, Training Loss: 29134, Validation Loss: 54040, 87028.29538720856\n",
      "Epoch 213155: reducing learning rate of group 0 to 1.2221e-03.\n",
      "Epoch 213201, Training Loss: 30438, Validation Loss: 54162, 88438.80512957663\n",
      "Epoch 213256: reducing learning rate of group 0 to 1.2208e-03.\n",
      "Epoch 213301, Training Loss: 29753, Validation Loss: 53836, 76270.68481410529\n",
      "Epoch 213357: reducing learning rate of group 0 to 1.2196e-03.\n",
      "Epoch 213401, Training Loss: 27206, Validation Loss: 54303, 71274.35441214051\n",
      "Epoch 213458: reducing learning rate of group 0 to 1.2184e-03.\n",
      "Epoch 213501, Training Loss: 30352, Validation Loss: 53680, 92309.78284167558\n",
      "Epoch 213559: reducing learning rate of group 0 to 1.2172e-03.\n",
      "Epoch 213601, Training Loss: 28728, Validation Loss: 52288, 67868.62103083749\n",
      "Epoch 213660: reducing learning rate of group 0 to 1.2160e-03.\n",
      "Epoch 213701, Training Loss: 28397, Validation Loss: 53384, 69434.52553507709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213761: reducing learning rate of group 0 to 1.2147e-03.\n",
      "Epoch 213801, Training Loss: 28433, Validation Loss: 53541, 79506.33006984596\n",
      "Epoch 213862: reducing learning rate of group 0 to 1.2135e-03.\n",
      "Epoch 213901, Training Loss: 28195, Validation Loss: 52879, 69706.80930582274\n",
      "Epoch 213963: reducing learning rate of group 0 to 1.2123e-03.\n",
      "Epoch 214001, Training Loss: 27637, Validation Loss: 54825, 82696.55104581009\n",
      "Epoch 214064: reducing learning rate of group 0 to 1.2111e-03.\n",
      "Epoch 214101, Training Loss: 28769, Validation Loss: 54196, 70992.5869799713\n",
      "Epoch 214165: reducing learning rate of group 0 to 1.2099e-03.\n",
      "Epoch 214201, Training Loss: 25848, Validation Loss: 53290, 88472.70359084748\n",
      "Epoch 214266: reducing learning rate of group 0 to 1.2087e-03.\n",
      "Epoch 214301, Training Loss: 28879, Validation Loss: 54159, 107384.33765426709\n",
      "Epoch 214367: reducing learning rate of group 0 to 1.2075e-03.\n",
      "Epoch 214401, Training Loss: 28034, Validation Loss: 54304, 73311.79950470215\n",
      "Epoch 214468: reducing learning rate of group 0 to 1.2063e-03.\n",
      "Epoch 214501, Training Loss: 29586, Validation Loss: 53448, 77373.83833773059\n",
      "Epoch 214569: reducing learning rate of group 0 to 1.2051e-03.\n",
      "Epoch 214601, Training Loss: 28141, Validation Loss: 52309, 106580.60565855932\n",
      "Epoch 214670: reducing learning rate of group 0 to 1.2039e-03.\n",
      "Epoch 214701, Training Loss: 26804, Validation Loss: 55650, 64552.95535728077\n",
      "Epoch 214771: reducing learning rate of group 0 to 1.2026e-03.\n",
      "Epoch 214801, Training Loss: 27787, Validation Loss: 55911, 71343.18128107018\n",
      "Epoch 214872: reducing learning rate of group 0 to 1.2014e-03.\n",
      "Epoch 214901, Training Loss: 26875, Validation Loss: 56610, 94985.02217059217\n",
      "Epoch 214973: reducing learning rate of group 0 to 1.2002e-03.\n",
      "Epoch 215001, Training Loss: 25942, Validation Loss: 54812, 97030.04795804824\n",
      "Epoch 215074: reducing learning rate of group 0 to 1.1990e-03.\n",
      "Epoch 215101, Training Loss: 27221, Validation Loss: 54100, 83213.63622637802\n",
      "Epoch 215175: reducing learning rate of group 0 to 1.1978e-03.\n",
      "Epoch 215201, Training Loss: 26865, Validation Loss: 54615, 81958.38548532127\n",
      "Epoch 215276: reducing learning rate of group 0 to 1.1966e-03.\n",
      "Epoch 215301, Training Loss: 27093, Validation Loss: 54984, 75030.15589540784\n",
      "Epoch 215377: reducing learning rate of group 0 to 1.1955e-03.\n",
      "Epoch 215401, Training Loss: 28114, Validation Loss: 52675, 69681.3743818036\n",
      "Epoch 215478: reducing learning rate of group 0 to 1.1943e-03.\n",
      "Epoch 215501, Training Loss: 31427, Validation Loss: 54717, 75284.5070640564\n",
      "Epoch 215579: reducing learning rate of group 0 to 1.1931e-03.\n",
      "Epoch 215601, Training Loss: 28624, Validation Loss: 54586, 70316.97715093376\n",
      "Epoch 215680: reducing learning rate of group 0 to 1.1919e-03.\n",
      "Epoch 215701, Training Loss: 27968, Validation Loss: 54222, 66167.83241491693\n",
      "Epoch 215781: reducing learning rate of group 0 to 1.1907e-03.\n",
      "Epoch 215801, Training Loss: 28889, Validation Loss: 54391, 84313.48565837916\n",
      "Epoch 215882: reducing learning rate of group 0 to 1.1895e-03.\n",
      "Epoch 215901, Training Loss: 27767, Validation Loss: 53520, 71300.91691216275\n",
      "Epoch 215983: reducing learning rate of group 0 to 1.1883e-03.\n",
      "Epoch 216001, Training Loss: 28138, Validation Loss: 53453, 88543.18062655341\n",
      "Epoch 216084: reducing learning rate of group 0 to 1.1871e-03.\n",
      "Epoch 216101, Training Loss: 30341, Validation Loss: 54406, 85539.05273724566\n",
      "Epoch 216185: reducing learning rate of group 0 to 1.1859e-03.\n",
      "Epoch 216201, Training Loss: 27414, Validation Loss: 53079, 66521.70352720519\n",
      "Epoch 216286: reducing learning rate of group 0 to 1.1847e-03.\n",
      "Epoch 216301, Training Loss: 27048, Validation Loss: 54300, 88023.36172003068\n",
      "Epoch 216387: reducing learning rate of group 0 to 1.1835e-03.\n",
      "Epoch 216401, Training Loss: 30139, Validation Loss: 55846, 91196.14572600771\n",
      "Epoch 216488: reducing learning rate of group 0 to 1.1824e-03.\n",
      "Epoch 216501, Training Loss: 28163, Validation Loss: 54606, 75704.70912965275\n",
      "Epoch 216589: reducing learning rate of group 0 to 1.1812e-03.\n",
      "Epoch 216601, Training Loss: 27910, Validation Loss: 53013, 66443.15881211626\n",
      "Epoch 216690: reducing learning rate of group 0 to 1.1800e-03.\n",
      "Epoch 216701, Training Loss: 31217, Validation Loss: 53327, 80360.56714253443\n",
      "Epoch 216791: reducing learning rate of group 0 to 1.1788e-03.\n",
      "Epoch 216801, Training Loss: 30621, Validation Loss: 54170, 83562.81690645848\n",
      "Epoch 216892: reducing learning rate of group 0 to 1.1776e-03.\n",
      "Epoch 216901, Training Loss: 28852, Validation Loss: 55800, 94641.82628543382\n",
      "Epoch 216993: reducing learning rate of group 0 to 1.1765e-03.\n",
      "Epoch 217001, Training Loss: 30069, Validation Loss: 56721, 83017.01727713387\n",
      "Epoch 217094: reducing learning rate of group 0 to 1.1753e-03.\n",
      "Epoch 217101, Training Loss: 29730, Validation Loss: 53497, 76337.44972978918\n",
      "Epoch 217195: reducing learning rate of group 0 to 1.1741e-03.\n",
      "Epoch 217201, Training Loss: 28407, Validation Loss: 54749, 69917.0663084387\n",
      "Epoch 217296: reducing learning rate of group 0 to 1.1729e-03.\n",
      "Epoch 217301, Training Loss: 27904, Validation Loss: 56014, 76779.13451447852\n",
      "Epoch 217397: reducing learning rate of group 0 to 1.1718e-03.\n",
      "Epoch 217401, Training Loss: 28504, Validation Loss: 56097, 116519.00997930417\n",
      "Epoch 217498: reducing learning rate of group 0 to 1.1706e-03.\n",
      "Epoch 217501, Training Loss: 30390, Validation Loss: 53293, 86889.40505948637\n",
      "Epoch 217599: reducing learning rate of group 0 to 1.1694e-03.\n",
      "Epoch 217601, Training Loss: 30446, Validation Loss: 54724, 64741.421288148405\n",
      "Epoch 217700: reducing learning rate of group 0 to 1.1683e-03.\n",
      "Epoch 217701, Training Loss: 28973, Validation Loss: 55104, 88964.1035587925\n",
      "Epoch 217801: reducing learning rate of group 0 to 1.1671e-03.\n",
      "Epoch 217801, Training Loss: 28286, Validation Loss: 53329, 87115.37277370846\n",
      "Epoch 217901, Training Loss: 28674, Validation Loss: 55460, 73615.24391273451\n",
      "Epoch 217902: reducing learning rate of group 0 to 1.1659e-03.\n",
      "Epoch 218001, Training Loss: 26108, Validation Loss: 55850, 85935.09044584453\n",
      "Epoch 218003: reducing learning rate of group 0 to 1.1648e-03.\n",
      "Epoch 218101, Training Loss: 30391, Validation Loss: 55545, 81874.05280907727\n",
      "Epoch 218104: reducing learning rate of group 0 to 1.1636e-03.\n",
      "Epoch 218201, Training Loss: 29424, Validation Loss: 54948, 94988.04674403247\n",
      "Epoch 218205: reducing learning rate of group 0 to 1.1624e-03.\n",
      "Epoch 218301, Training Loss: 28543, Validation Loss: 52264, 68384.54992791692\n",
      "Epoch 218306: reducing learning rate of group 0 to 1.1613e-03.\n",
      "Epoch 218401, Training Loss: 27256, Validation Loss: 53667, 70994.69689867734\n",
      "Epoch 218407: reducing learning rate of group 0 to 1.1601e-03.\n",
      "Epoch 218501, Training Loss: 28598, Validation Loss: 53855, 67773.2921857531\n",
      "Epoch 218508: reducing learning rate of group 0 to 1.1589e-03.\n",
      "Epoch 218601, Training Loss: 28196, Validation Loss: 55630, 76943.89436200516\n",
      "Epoch 218609: reducing learning rate of group 0 to 1.1578e-03.\n",
      "Epoch 218701, Training Loss: 28018, Validation Loss: 55667, 71762.6723697517\n",
      "Epoch 218710: reducing learning rate of group 0 to 1.1566e-03.\n",
      "Epoch 218801, Training Loss: 29780, Validation Loss: 55278, 68903.32838226091\n",
      "Epoch 218811: reducing learning rate of group 0 to 1.1555e-03.\n",
      "Epoch 218901, Training Loss: 28871, Validation Loss: 53647, 82547.8420979524\n",
      "Epoch 218912: reducing learning rate of group 0 to 1.1543e-03.\n",
      "Epoch 219001, Training Loss: 27048, Validation Loss: 53723, 87142.95695696108\n",
      "Epoch 219013: reducing learning rate of group 0 to 1.1532e-03.\n",
      "Epoch 219101, Training Loss: 26943, Validation Loss: 55601, 71132.72325908084\n",
      "Epoch 219114: reducing learning rate of group 0 to 1.1520e-03.\n",
      "Epoch 219201, Training Loss: 27340, Validation Loss: 57125, 68412.99949462416\n",
      "Epoch 219215: reducing learning rate of group 0 to 1.1509e-03.\n",
      "Epoch 219301, Training Loss: 27699, Validation Loss: 54067, 79922.49679103536\n",
      "Epoch 219316: reducing learning rate of group 0 to 1.1497e-03.\n",
      "Epoch 219401, Training Loss: 27972, Validation Loss: 55238, 80775.71595216286\n",
      "Epoch 219417: reducing learning rate of group 0 to 1.1486e-03.\n",
      "Epoch 219501, Training Loss: 30323, Validation Loss: 53898, 80672.21040374994\n",
      "Epoch 219518: reducing learning rate of group 0 to 1.1474e-03.\n",
      "Epoch 219601, Training Loss: 27720, Validation Loss: 56350, 92856.22645496468\n",
      "Epoch 219619: reducing learning rate of group 0 to 1.1463e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219701, Training Loss: 28925, Validation Loss: 55285, 72342.08011676207\n",
      "Epoch 219720: reducing learning rate of group 0 to 1.1451e-03.\n",
      "Epoch 219801, Training Loss: 28474, Validation Loss: 54689, 78170.06736612938\n",
      "Epoch 219821: reducing learning rate of group 0 to 1.1440e-03.\n",
      "Epoch 219901, Training Loss: 27948, Validation Loss: 54707, 71630.26421163585\n",
      "Epoch 219922: reducing learning rate of group 0 to 1.1428e-03.\n",
      "Epoch 220001, Training Loss: 27280, Validation Loss: 55499, 79212.78349319327\n",
      "Epoch 220023: reducing learning rate of group 0 to 1.1417e-03.\n",
      "Epoch 220101, Training Loss: 27632, Validation Loss: 54790, 82341.18837633119\n",
      "Epoch 220124: reducing learning rate of group 0 to 1.1405e-03.\n",
      "Epoch 220201, Training Loss: 25991, Validation Loss: 54451, 87500.60858793021\n",
      "Epoch 220225: reducing learning rate of group 0 to 1.1394e-03.\n",
      "Epoch 220301, Training Loss: 29444, Validation Loss: 54059, 72872.79177792148\n",
      "Epoch 220326: reducing learning rate of group 0 to 1.1383e-03.\n",
      "Epoch 220401, Training Loss: 27958, Validation Loss: 52320, 97248.35550232469\n",
      "Epoch 220427: reducing learning rate of group 0 to 1.1371e-03.\n",
      "Epoch 220501, Training Loss: 30189, Validation Loss: 56054, 70431.97668375156\n",
      "Epoch 220528: reducing learning rate of group 0 to 1.1360e-03.\n",
      "Epoch 220601, Training Loss: 25959, Validation Loss: 54433, 71282.88075328043\n",
      "Epoch 220629: reducing learning rate of group 0 to 1.1348e-03.\n",
      "Epoch 220701, Training Loss: 28400, Validation Loss: 54205, 80075.8687785222\n",
      "Epoch 220730: reducing learning rate of group 0 to 1.1337e-03.\n",
      "Epoch 220801, Training Loss: 28751, Validation Loss: 52114, 75860.49719660683\n",
      "Epoch 220831: reducing learning rate of group 0 to 1.1326e-03.\n",
      "Epoch 220901, Training Loss: 27120, Validation Loss: 53312, 68310.63507414544\n",
      "Epoch 220932: reducing learning rate of group 0 to 1.1314e-03.\n",
      "Epoch 221001, Training Loss: 28061, Validation Loss: 56656, 87870.93083101876\n",
      "Epoch 221033: reducing learning rate of group 0 to 1.1303e-03.\n",
      "Epoch 221101, Training Loss: 28830, Validation Loss: 53822, 82667.53793531952\n",
      "Epoch 221134: reducing learning rate of group 0 to 1.1292e-03.\n",
      "Epoch 221201, Training Loss: 29250, Validation Loss: 53871, 74199.17575126693\n",
      "Epoch 221235: reducing learning rate of group 0 to 1.1281e-03.\n",
      "Epoch 221301, Training Loss: 26948, Validation Loss: 54531, 123071.39738752565\n",
      "Epoch 221336: reducing learning rate of group 0 to 1.1269e-03.\n",
      "Epoch 221401, Training Loss: 31484, Validation Loss: 53760, 73161.66927261336\n",
      "Epoch 221437: reducing learning rate of group 0 to 1.1258e-03.\n",
      "Epoch 221501, Training Loss: 28505, Validation Loss: 56749, 74862.2060934936\n",
      "Epoch 221538: reducing learning rate of group 0 to 1.1247e-03.\n",
      "Epoch 221601, Training Loss: 28410, Validation Loss: 54038, 75652.51056762938\n",
      "Epoch 221639: reducing learning rate of group 0 to 1.1235e-03.\n",
      "Epoch 221701, Training Loss: 27929, Validation Loss: 54760, 74973.08735874515\n",
      "Epoch 221740: reducing learning rate of group 0 to 1.1224e-03.\n",
      "Epoch 221801, Training Loss: 26962, Validation Loss: 54312, 84387.75479570219\n",
      "Epoch 221841: reducing learning rate of group 0 to 1.1213e-03.\n",
      "Epoch 221901, Training Loss: 29110, Validation Loss: 54231, 73600.32638172177\n",
      "Epoch 221942: reducing learning rate of group 0 to 1.1202e-03.\n",
      "Epoch 222001, Training Loss: 29905, Validation Loss: 53208, 82285.5815454954\n",
      "Epoch 222043: reducing learning rate of group 0 to 1.1191e-03.\n",
      "Epoch 222101, Training Loss: 28435, Validation Loss: 54980, 71424.244446476\n",
      "Epoch 222144: reducing learning rate of group 0 to 1.1179e-03.\n",
      "Epoch 222201, Training Loss: 27821, Validation Loss: 54602, 68662.11247027201\n",
      "Epoch 222245: reducing learning rate of group 0 to 1.1168e-03.\n",
      "Epoch 222301, Training Loss: 28210, Validation Loss: 56050, 88204.89479262824\n",
      "Epoch 222346: reducing learning rate of group 0 to 1.1157e-03.\n",
      "Epoch 222401, Training Loss: 26663, Validation Loss: 54199, 86834.1074596746\n",
      "Epoch 222447: reducing learning rate of group 0 to 1.1146e-03.\n",
      "Epoch 222501, Training Loss: 27916, Validation Loss: 52979, 73765.63592588197\n",
      "Epoch 222548: reducing learning rate of group 0 to 1.1135e-03.\n",
      "Epoch 222601, Training Loss: 28024, Validation Loss: 55527, 74174.46675374727\n",
      "Epoch 222649: reducing learning rate of group 0 to 1.1124e-03.\n",
      "Epoch 222701, Training Loss: 27136, Validation Loss: 53629, 63565.09286842819\n",
      "Epoch 222750: reducing learning rate of group 0 to 1.1113e-03.\n",
      "Epoch 222801, Training Loss: 28729, Validation Loss: 53368, 95972.55570279389\n",
      "Epoch 222851: reducing learning rate of group 0 to 1.1101e-03.\n",
      "Epoch 222901, Training Loss: 25730, Validation Loss: 54178, 76618.61559541232\n",
      "Epoch 222952: reducing learning rate of group 0 to 1.1090e-03.\n",
      "Epoch 223001, Training Loss: 25983, Validation Loss: 54078, 63534.675708990406\n",
      "Epoch 223053: reducing learning rate of group 0 to 1.1079e-03.\n",
      "Epoch 223101, Training Loss: 28140, Validation Loss: 55486, 80873.61283616368\n",
      "Epoch 223154: reducing learning rate of group 0 to 1.1068e-03.\n",
      "Epoch 223201, Training Loss: 26873, Validation Loss: 53572, 89812.29622403729\n",
      "Epoch 223255: reducing learning rate of group 0 to 1.1057e-03.\n",
      "Epoch 223301, Training Loss: 29845, Validation Loss: 53657, 117893.79780327913\n",
      "Epoch 223356: reducing learning rate of group 0 to 1.1046e-03.\n",
      "Epoch 223401, Training Loss: 29056, Validation Loss: 54091, 80997.21957171905\n",
      "Epoch 223457: reducing learning rate of group 0 to 1.1035e-03.\n",
      "Epoch 223501, Training Loss: 28411, Validation Loss: 54401, 82333.27883058596\n",
      "Epoch 223558: reducing learning rate of group 0 to 1.1024e-03.\n",
      "Epoch 223601, Training Loss: 27280, Validation Loss: 55978, 75857.2540688064\n",
      "Epoch 223659: reducing learning rate of group 0 to 1.1013e-03.\n",
      "Epoch 223701, Training Loss: 27895, Validation Loss: 53519, 78232.53860496845\n",
      "Epoch 223760: reducing learning rate of group 0 to 1.1002e-03.\n",
      "Epoch 223801, Training Loss: 27375, Validation Loss: 54336, 92397.32584590996\n",
      "Epoch 223861: reducing learning rate of group 0 to 1.0991e-03.\n",
      "Epoch 223901, Training Loss: 32340, Validation Loss: 55068, 80796.54461465296\n",
      "Epoch 223962: reducing learning rate of group 0 to 1.0980e-03.\n",
      "Epoch 224001, Training Loss: 27160, Validation Loss: 54420, 105237.30053398758\n",
      "Epoch 224063: reducing learning rate of group 0 to 1.0969e-03.\n",
      "Epoch 224101, Training Loss: 28479, Validation Loss: 53089, 80050.76455564021\n",
      "Epoch 224164: reducing learning rate of group 0 to 1.0958e-03.\n",
      "Epoch 224201, Training Loss: 27830, Validation Loss: 56387, 71620.34403181214\n",
      "Epoch 224265: reducing learning rate of group 0 to 1.0947e-03.\n",
      "Epoch 224301, Training Loss: 32652, Validation Loss: 55379, 81541.89535996581\n",
      "Epoch 224366: reducing learning rate of group 0 to 1.0936e-03.\n",
      "Epoch 224401, Training Loss: 30075, Validation Loss: 54941, 98337.65530497108\n",
      "Epoch 224467: reducing learning rate of group 0 to 1.0925e-03.\n",
      "Epoch 224501, Training Loss: 28893, Validation Loss: 55166, 85693.8931594515\n",
      "Epoch 224568: reducing learning rate of group 0 to 1.0914e-03.\n",
      "Epoch 224601, Training Loss: 27725, Validation Loss: 53537, 75031.78896423013\n",
      "Epoch 224669: reducing learning rate of group 0 to 1.0903e-03.\n",
      "Epoch 224701, Training Loss: 26460, Validation Loss: 55085, 95254.99248453847\n",
      "Epoch 224770: reducing learning rate of group 0 to 1.0892e-03.\n",
      "Epoch 224801, Training Loss: 26588, Validation Loss: 56076, 79175.31027103687\n",
      "Epoch 224871: reducing learning rate of group 0 to 1.0881e-03.\n",
      "Epoch 224901, Training Loss: 27705, Validation Loss: 55462, 79359.55830292718\n",
      "Epoch 224972: reducing learning rate of group 0 to 1.0871e-03.\n",
      "Epoch 225001, Training Loss: 31237, Validation Loss: 55506, 87639.40683562889\n",
      "Epoch 225073: reducing learning rate of group 0 to 1.0860e-03.\n",
      "Epoch 225101, Training Loss: 28981, Validation Loss: 54105, 65978.62170878335\n",
      "Epoch 225174: reducing learning rate of group 0 to 1.0849e-03.\n",
      "Epoch 225201, Training Loss: 29234, Validation Loss: 55087, 68048.27224570117\n",
      "Epoch 225275: reducing learning rate of group 0 to 1.0838e-03.\n",
      "Epoch 225301, Training Loss: 27617, Validation Loss: 54011, 89400.41103354038\n",
      "Epoch 225376: reducing learning rate of group 0 to 1.0827e-03.\n",
      "Epoch 225401, Training Loss: 28844, Validation Loss: 54244, 85399.06772744544\n",
      "Epoch 225477: reducing learning rate of group 0 to 1.0816e-03.\n",
      "Epoch 225501, Training Loss: 27954, Validation Loss: 53775, 80394.90454685985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225578: reducing learning rate of group 0 to 1.0806e-03.\n",
      "Epoch 225601, Training Loss: 26675, Validation Loss: 56107, 80438.02937276867\n",
      "Epoch 225679: reducing learning rate of group 0 to 1.0795e-03.\n",
      "Epoch 225701, Training Loss: 27908, Validation Loss: 53811, 78729.14714578915\n",
      "Epoch 225780: reducing learning rate of group 0 to 1.0784e-03.\n",
      "Epoch 225801, Training Loss: 29489, Validation Loss: 53029, 94136.266827123\n",
      "Epoch 225881: reducing learning rate of group 0 to 1.0773e-03.\n",
      "Epoch 225901, Training Loss: 29029, Validation Loss: 54854, 80217.748331761\n",
      "Epoch 225982: reducing learning rate of group 0 to 1.0762e-03.\n",
      "Epoch 226001, Training Loss: 28264, Validation Loss: 55213, 88450.35868099757\n",
      "Epoch 226083: reducing learning rate of group 0 to 1.0752e-03.\n",
      "Epoch 226101, Training Loss: 27250, Validation Loss: 53650, 68303.7729497346\n",
      "Epoch 226184: reducing learning rate of group 0 to 1.0741e-03.\n",
      "Epoch 226201, Training Loss: 27105, Validation Loss: 55338, 85580.18611806324\n",
      "Epoch 226285: reducing learning rate of group 0 to 1.0730e-03.\n",
      "Epoch 226301, Training Loss: 28051, Validation Loss: 54800, 80211.79326149411\n",
      "Epoch 226386: reducing learning rate of group 0 to 1.0719e-03.\n",
      "Epoch 226401, Training Loss: 27459, Validation Loss: 55946, 80432.80451177231\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-2,\n",
    "    weight_decay=3e-4\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999, \n",
    "    patience=100, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
