{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 64\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "#             nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.18),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 238563, Validation Loss: 236194, 10798.70111039525\n",
      "Epoch 101, Training Loss: 69653, Validation Loss: 63633, 197560.07368488796\n",
      "Epoch 201, Training Loss: 68983, Validation Loss: 61380, 230791.000837439\n",
      "Epoch 301, Training Loss: 61254, Validation Loss: 63030, 167431.80344639896\n",
      "Epoch 401, Training Loss: 64386, Validation Loss: 74858, 429479.81182723143\n",
      "Epoch 501, Training Loss: 60586, Validation Loss: 56993, 256397.04659626642\n",
      "Epoch 601, Training Loss: 59629, Validation Loss: 60467, 177360.06948324808\n",
      "Epoch 701, Training Loss: 65753, Validation Loss: 83845, 450211.0507746108\n",
      "Epoch 801, Training Loss: 57319, Validation Loss: 54115, 262667.91565399326\n",
      "Epoch 901, Training Loss: 57339, Validation Loss: 63300, 276699.22940436756\n",
      "Epoch 1001, Training Loss: 59990, Validation Loss: 58691, 417080.5974599185\n",
      "Epoch 1101, Training Loss: 55093, Validation Loss: 58419, 159074.53022960926\n",
      "Epoch 1201, Training Loss: 56359, Validation Loss: 63986, 232622.6724903876\n",
      "Epoch 1301, Training Loss: 60933, Validation Loss: 82589, 414990.941161119\n",
      "Epoch 1401, Training Loss: 58827, Validation Loss: 65429, 273740.66275690746\n",
      "Epoch 1501, Training Loss: 57496, Validation Loss: 68514, 221026.22594419317\n",
      "Epoch 1601, Training Loss: 56537, Validation Loss: 69999, 382262.74648595747\n",
      "Epoch 1701, Training Loss: 56299, Validation Loss: 70500, 351384.95633814996\n",
      "Epoch 1801, Training Loss: 58361, Validation Loss: 70595, 212724.89038521636\n",
      "Epoch 1901, Training Loss: 60576, Validation Loss: 82576, 697742.243617762\n",
      "Epoch 2001, Training Loss: 57751, Validation Loss: 74122, 226987.25575811288\n",
      "Epoch 2101, Training Loss: 54100, Validation Loss: 62357, 398846.29881589004\n",
      "Epoch 2201, Training Loss: 57246, Validation Loss: 67488, 293350.35550782457\n",
      "Epoch 2301, Training Loss: 55295, Validation Loss: 63636, 384750.59766555746\n",
      "Epoch 2401, Training Loss: 57380, Validation Loss: 64022, 370145.472396255\n",
      "Epoch 2501, Training Loss: 57743, Validation Loss: 68962, 328705.43460615637\n",
      "Epoch 2601, Training Loss: 53178, Validation Loss: 68181, 355365.1862434519\n",
      "Epoch 2701, Training Loss: 55278, Validation Loss: 69466, 414555.35775189666\n",
      "Epoch 2801, Training Loss: 53516, Validation Loss: 70955, 283724.97937179444\n",
      "Epoch 2901, Training Loss: 53759, Validation Loss: 65800, 382928.2893080307\n",
      "Epoch 3001, Training Loss: 54357, Validation Loss: 71658, 354662.77630281757\n",
      "Epoch 3101, Training Loss: 53589, Validation Loss: 73070, 287235.6953262102\n",
      "Epoch 3201, Training Loss: 53416, Validation Loss: 65216, 294511.3534180701\n",
      "Epoch 3301, Training Loss: 56462, Validation Loss: 72302, 513637.0280967427\n",
      "Epoch 3401, Training Loss: 56254, Validation Loss: 73021, 713369.9905039635\n",
      "Epoch 3501, Training Loss: 51236, Validation Loss: 77586, 250120.2386618777\n",
      "Epoch 3601, Training Loss: 55684, Validation Loss: 74693, 516838.42228476884\n",
      "Epoch 3701, Training Loss: 52789, Validation Loss: 68229, 424213.4225027608\n",
      "Epoch 3801, Training Loss: 53266, Validation Loss: 75993, 341176.0867460413\n",
      "Epoch 3901, Training Loss: 56432, Validation Loss: 79375, 735574.4992628945\n",
      "Epoch 4001, Training Loss: 52450, Validation Loss: 72712, 366879.9846401236\n",
      "Epoch 4101, Training Loss: 51745, Validation Loss: 67280, 283097.3064086837\n",
      "Epoch 4201, Training Loss: 52913, Validation Loss: 76295, 296462.0016039553\n",
      "Epoch 4301, Training Loss: 54493, Validation Loss: 75633, 285930.47318704904\n",
      "Epoch 4401, Training Loss: 53487, Validation Loss: 61337, 375005.1605304941\n",
      "Epoch 4501, Training Loss: 55020, Validation Loss: 65802, 339558.7777134865\n",
      "Epoch 4601, Training Loss: 57895, Validation Loss: 75367, 476792.6697493961\n",
      "Epoch 4701, Training Loss: 53133, Validation Loss: 77825, 271584.6835170421\n",
      "Epoch 4801, Training Loss: 51469, Validation Loss: 71889, 284137.16475076426\n",
      "Epoch 4901, Training Loss: 53824, Validation Loss: 76799, 457332.3407769869\n",
      "Epoch 5001, Training Loss: 52110, Validation Loss: 70672, 404722.1370009033\n",
      "Epoch 5101, Training Loss: 51750, Validation Loss: 71106, 264811.8851050617\n",
      "Epoch 5201, Training Loss: 54451, Validation Loss: 73872, 476443.40968869743\n",
      "Epoch 5301, Training Loss: 53922, Validation Loss: 72334, 268964.892511035\n",
      "Epoch 5401, Training Loss: 52703, Validation Loss: 73416, 480492.33248545486\n",
      "Epoch 5501, Training Loss: 51834, Validation Loss: 67271, 396248.12905162544\n",
      "Epoch 5601, Training Loss: 52913, Validation Loss: 78164, 537357.2902622019\n",
      "Epoch 5701, Training Loss: 56192, Validation Loss: 71265, 208535.11256548818\n",
      "Epoch 5801, Training Loss: 52384, Validation Loss: 77468, 295747.6332398095\n",
      "Epoch 5901, Training Loss: 53932, Validation Loss: 78517, 379404.04947822174\n",
      "Epoch 6001, Training Loss: 52332, Validation Loss: 64971, 530064.9150889978\n",
      "Epoch 6101, Training Loss: 51368, Validation Loss: 71660, 301787.7557318397\n",
      "Epoch 6201, Training Loss: 52961, Validation Loss: 78101, 370699.4648024556\n",
      "Epoch 6301, Training Loss: 55750, Validation Loss: 73448, 495771.11712406314\n",
      "Epoch 6401, Training Loss: 55827, Validation Loss: 78104, 340591.16026818944\n",
      "Epoch 6501, Training Loss: 55575, Validation Loss: 75340, 236664.3696338071\n",
      "Epoch 6601, Training Loss: 52679, Validation Loss: 72857, 178673.10126103982\n",
      "Epoch 6701, Training Loss: 54013, Validation Loss: 71924, 321614.0493043678\n",
      "Epoch 6801, Training Loss: 50444, Validation Loss: 76503, 201799.9348424822\n",
      "Epoch 6901, Training Loss: 54158, Validation Loss: 68376, 316274.5916582724\n",
      "Epoch 7001, Training Loss: 55275, Validation Loss: 67085, 220121.13722052274\n",
      "Epoch 7101, Training Loss: 53618, Validation Loss: 64463, 320483.4235513416\n",
      "Epoch 7201, Training Loss: 49606, Validation Loss: 79267, 543831.3400108082\n",
      "Epoch 7301, Training Loss: 53342, Validation Loss: 75091, 236447.99066980436\n",
      "Epoch 7401, Training Loss: 55593, Validation Loss: 72746, 263306.5955466739\n",
      "Epoch 7501, Training Loss: 51090, Validation Loss: 72884, 235772.1694592155\n",
      "Epoch 7601, Training Loss: 57352, Validation Loss: 68868, 227181.99622246483\n",
      "Epoch 7701, Training Loss: 55413, Validation Loss: 75902, 312744.8745250258\n",
      "Epoch 7801, Training Loss: 55820, Validation Loss: 72390, 300136.91091312445\n",
      "Epoch 7901, Training Loss: 53747, Validation Loss: 71123, 346065.7105810407\n",
      "Epoch 8001, Training Loss: 54503, Validation Loss: 76323, 414987.46457588044\n",
      "Epoch 8101, Training Loss: 54751, Validation Loss: 74703, 348831.77972605155\n",
      "Epoch 8201, Training Loss: 53183, Validation Loss: 73120, 412163.8464783964\n",
      "Epoch 8301, Training Loss: 50858, Validation Loss: 66685, 554008.7701799301\n",
      "Epoch 8401, Training Loss: 54349, Validation Loss: 63147, 382453.93381130934\n",
      "Epoch 8501, Training Loss: 55599, Validation Loss: 84730, 808419.1855631407\n",
      "Epoch 8601, Training Loss: 51915, Validation Loss: 69939, 456438.47785918415\n",
      "Epoch 8701, Training Loss: 55357, Validation Loss: 68726, 356516.7202697336\n",
      "Epoch 8801, Training Loss: 57661, Validation Loss: 71818, 394159.50613738433\n",
      "Epoch 8901, Training Loss: 56492, Validation Loss: 77553, 358429.69621270243\n",
      "Epoch 9001, Training Loss: 50611, Validation Loss: 66692, 474273.1271906239\n",
      "Epoch 9101, Training Loss: 51541, Validation Loss: 67009, 298217.0224174823\n",
      "Epoch 9201, Training Loss: 56408, Validation Loss: 67501, 641380.5265539805\n",
      "Epoch 9301, Training Loss: 53610, Validation Loss: 70259, 427349.2519745122\n",
      "Epoch 9401, Training Loss: 53385, Validation Loss: 69022, 174598.68112497762\n",
      "Epoch 9501, Training Loss: 53932, Validation Loss: 70604, 443362.5633729247\n",
      "Epoch 9601, Training Loss: 50477, Validation Loss: 73646, 389825.7616463609\n",
      "Epoch 9701, Training Loss: 56908, Validation Loss: 70953, 587100.6131472111\n",
      "Epoch 9801, Training Loss: 52238, Validation Loss: 67872, 436193.5842759635\n",
      "Epoch 9901, Training Loss: 51784, Validation Loss: 72170, 518887.3997877299\n",
      "Epoch 10001, Training Loss: 55682, Validation Loss: 75726, 618955.9084006102\n",
      "Epoch 10101, Training Loss: 51905, Validation Loss: 69192, 303244.13675290724\n",
      "Epoch 10201, Training Loss: 52658, Validation Loss: 70885, 292128.87448171177\n",
      "Epoch 10301, Training Loss: 49976, Validation Loss: 66848, 327882.3421703063\n",
      "Epoch 10401, Training Loss: 55998, Validation Loss: 71009, 545180.597462678\n",
      "Epoch 10501, Training Loss: 55147, Validation Loss: 76879, 463952.4089369404\n",
      "Epoch 10601, Training Loss: 53214, Validation Loss: 64171, 319569.48110659834\n",
      "Epoch 10701, Training Loss: 54645, Validation Loss: 73878, 261691.99373836338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10801, Training Loss: 55977, Validation Loss: 67091, 545644.4335251119\n",
      "Epoch 10901, Training Loss: 53070, Validation Loss: 75635, 391825.4771722706\n",
      "Epoch 11001, Training Loss: 50468, Validation Loss: 78568, 328380.1868201297\n",
      "Epoch 11101, Training Loss: 55767, Validation Loss: 67727, 212489.31914042833\n",
      "Epoch 11201, Training Loss: 56044, Validation Loss: 63164, 281466.61590254604\n",
      "Epoch 11301, Training Loss: 55839, Validation Loss: 76650, 570620.8662997888\n",
      "Epoch 11401, Training Loss: 55357, Validation Loss: 76760, 354088.30167406175\n",
      "Epoch 11501, Training Loss: 56586, Validation Loss: 66833, 483883.4402954292\n",
      "Epoch 11601, Training Loss: 55962, Validation Loss: 74655, 388820.64835901064\n",
      "Epoch 11701, Training Loss: 54260, Validation Loss: 79577, 378527.5461072466\n",
      "Epoch 11801, Training Loss: 56491, Validation Loss: 64375, 491001.56081084424\n",
      "Epoch 11901, Training Loss: 55059, Validation Loss: 64324, 662801.3963795904\n",
      "Epoch 12001, Training Loss: 55449, Validation Loss: 73933, 354484.9167092759\n",
      "Epoch 12101, Training Loss: 55923, Validation Loss: 73041, 311307.8841309934\n",
      "Epoch 12201, Training Loss: 57432, Validation Loss: 71860, 428741.51514798385\n",
      "Epoch 12301, Training Loss: 53416, Validation Loss: 66172, 239147.47520042528\n",
      "Epoch 12401, Training Loss: 52316, Validation Loss: 78332, 267730.98840055795\n",
      "Epoch 12501, Training Loss: 53036, Validation Loss: 64974, 289591.2267303927\n",
      "Epoch 12601, Training Loss: 52585, Validation Loss: 69737, 495082.1052197255\n",
      "Epoch 12701, Training Loss: 58696, Validation Loss: 71059, 382315.30351739126\n",
      "Epoch 12801, Training Loss: 55619, Validation Loss: 75308, 260290.09798276462\n",
      "Epoch 12901, Training Loss: 50787, Validation Loss: 65874, 300929.17119659873\n",
      "Epoch 13001, Training Loss: 54352, Validation Loss: 68746, 782055.9323685052\n",
      "Epoch 13101, Training Loss: 56696, Validation Loss: 65304, 205018.8707258531\n",
      "Epoch 13201, Training Loss: 52230, Validation Loss: 68176, 190647.10505860043\n",
      "Epoch 13301, Training Loss: 55036, Validation Loss: 72235, 254506.19973254856\n",
      "Epoch 13401, Training Loss: 59461, Validation Loss: 73940, 418459.31287085946\n",
      "Epoch 13501, Training Loss: 57612, Validation Loss: 66200, 381320.5891305848\n",
      "Epoch 13601, Training Loss: 57216, Validation Loss: 66534, 216820.63512731736\n",
      "Epoch 13701, Training Loss: 55877, Validation Loss: 66439, 153813.98248088258\n",
      "Epoch 13801, Training Loss: 57596, Validation Loss: 65138, 334090.6291111915\n",
      "Epoch 13901, Training Loss: 57350, Validation Loss: 65469, 253230.99877129417\n",
      "Epoch 14001, Training Loss: 53746, Validation Loss: 70370, 257822.85836150384\n",
      "Epoch 14101, Training Loss: 53136, Validation Loss: 67730, 360503.67618002585\n",
      "Epoch 14201, Training Loss: 58132, Validation Loss: 74088, 239573.4336187317\n",
      "Epoch 14301, Training Loss: 57826, Validation Loss: 82557, 460449.4802091145\n",
      "Epoch 14401, Training Loss: 54364, Validation Loss: 71526, 277958.4467899909\n",
      "Epoch 14501, Training Loss: 58205, Validation Loss: 65705, 307765.28694541025\n",
      "Epoch 14601, Training Loss: 65019, Validation Loss: 74865, 417561.31658315397\n",
      "Epoch 14701, Training Loss: 63059, Validation Loss: 68159, 304596.3213878109\n",
      "Epoch 14801, Training Loss: 65890, Validation Loss: 79698, 231421.1363705398\n",
      "Epoch 14901, Training Loss: 68344, Validation Loss: 62921, 269936.2617191667\n",
      "Epoch 15001, Training Loss: 67943, Validation Loss: 73584, 375928.56910279585\n",
      "Epoch 15101, Training Loss: 64667, Validation Loss: 76100, 191735.30451067016\n",
      "Epoch 15201, Training Loss: 65048, Validation Loss: 67799, 182708.37266950772\n",
      "Epoch 15301, Training Loss: 66787, Validation Loss: 73572, 101195.48929057509\n",
      "Epoch 15401, Training Loss: 67275, Validation Loss: 70302, 80905.80209630712\n",
      "Epoch 15501, Training Loss: 63244, Validation Loss: 62783, 572520.7663323783\n",
      "Epoch 15601, Training Loss: 65534, Validation Loss: 77693, 244617.59965549156\n",
      "Epoch 15701, Training Loss: 65245, Validation Loss: 70043, 470857.2185398559\n",
      "Epoch 15801, Training Loss: 63960, Validation Loss: 69848, 319565.6460681514\n",
      "Epoch 15901, Training Loss: 64513, Validation Loss: 79561, 216739.83525027614\n",
      "Epoch 16001, Training Loss: 63381, Validation Loss: 71760, 229813.6212862069\n",
      "Epoch 16101, Training Loss: 64814, Validation Loss: 73062, 265864.3170058788\n",
      "Epoch 16201, Training Loss: 65639, Validation Loss: 68696, 540811.9002360954\n",
      "Epoch 16301, Training Loss: 62658, Validation Loss: 69852, 137566.2967731155\n",
      "Epoch 16401, Training Loss: 62763, Validation Loss: 69231, 135220.4647576216\n",
      "Epoch 16501, Training Loss: 66925, Validation Loss: 75505, 109638.74948627048\n",
      "Epoch 16601, Training Loss: 63735, Validation Loss: 72604, 181615.34689881428\n",
      "Epoch 16701, Training Loss: 63938, Validation Loss: 67590, 147993.35547508832\n",
      "Epoch 16801, Training Loss: 64227, Validation Loss: 72902, 222364.78275079653\n",
      "Epoch 16901, Training Loss: 64708, Validation Loss: 75264, 175317.0799897176\n",
      "Epoch 17001, Training Loss: 61044, Validation Loss: 71619, 341821.24340569094\n",
      "Epoch 17101, Training Loss: 67692, Validation Loss: 72632, 306501.33193624514\n",
      "Epoch 17201, Training Loss: 73038, Validation Loss: 68897, 445145.52459646185\n",
      "Epoch 17301, Training Loss: 71732, Validation Loss: 69359, 444220.30464980315\n",
      "Epoch 17401, Training Loss: 71162, Validation Loss: 74457, 274232.34262710484\n",
      "Epoch 17501, Training Loss: 66074, Validation Loss: 69570, 304364.0206003925\n",
      "Epoch 17601, Training Loss: 65700, Validation Loss: 73368, 147248.0934816112\n",
      "Epoch 17701, Training Loss: 65595, Validation Loss: 68872, 118039.07154759485\n",
      "Epoch 17801, Training Loss: 66470, Validation Loss: 68659, 480879.54373145284\n",
      "Epoch 17901, Training Loss: 66433, Validation Loss: 66453, 270862.7280263923\n",
      "Epoch 18001, Training Loss: 59297, Validation Loss: 68054, 99071.61422990041\n",
      "Epoch 18101, Training Loss: 63870, Validation Loss: 70145, 56062.91566204218\n",
      "Epoch 18201, Training Loss: 65015, Validation Loss: 66952, 292516.5716633198\n",
      "Epoch 18301, Training Loss: 65247, Validation Loss: 73507, 147637.47874871996\n",
      "Epoch 18401, Training Loss: 69656, Validation Loss: 69343, 409695.22812620265\n",
      "Epoch 18501, Training Loss: 63326, Validation Loss: 71477, 134905.66077069446\n",
      "Epoch 18601, Training Loss: 63370, Validation Loss: 76696, 268408.3786018546\n",
      "Epoch 18701, Training Loss: 73848, Validation Loss: 83655, 735349.1132143134\n",
      "Epoch 18801, Training Loss: 66675, Validation Loss: 70375, 237562.78457697397\n",
      "Epoch 18901, Training Loss: 69147, Validation Loss: 70321, 205116.2254830381\n",
      "Epoch 19001, Training Loss: 66696, Validation Loss: 82588, 451252.0089409693\n",
      "Epoch 19101, Training Loss: 65637, Validation Loss: 64387, 130557.01573906436\n",
      "Epoch 19201, Training Loss: 63167, Validation Loss: 67681, 221268.4339527874\n",
      "Epoch 19301, Training Loss: 63705, Validation Loss: 70977, 256390.3949063417\n",
      "Epoch 19401, Training Loss: 61331, Validation Loss: 66747, 73078.46515699387\n",
      "Epoch 19501, Training Loss: 64244, Validation Loss: 69739, 309102.65397196845\n",
      "Epoch 19601, Training Loss: 68671, Validation Loss: 64533, 404261.6022658713\n",
      "Epoch 19701, Training Loss: 63451, Validation Loss: 71296, 361265.4643261614\n",
      "Epoch 19801, Training Loss: 65283, Validation Loss: 71534, 357812.94782598736\n",
      "Epoch 19901, Training Loss: 64760, Validation Loss: 71423, 534389.2203045998\n",
      "Epoch 20001, Training Loss: 63745, Validation Loss: 69643, 506257.5755764333\n",
      "Epoch 20101, Training Loss: 60101, Validation Loss: 72534, 261665.83460572935\n",
      "Epoch 20201, Training Loss: 65554, Validation Loss: 70263, 539249.2566889784\n",
      "Epoch 20301, Training Loss: 62650, Validation Loss: 71763, 184553.5492244511\n",
      "Epoch 20401, Training Loss: 67736, Validation Loss: 72275, 332987.1879148392\n",
      "Epoch 20501, Training Loss: 65077, Validation Loss: 72233, 50995.90735961655\n",
      "Epoch 20601, Training Loss: 61851, Validation Loss: 76289, 295667.92197203165\n",
      "Epoch 20701, Training Loss: 63229, Validation Loss: 74504, 172878.66773370255\n",
      "Epoch 20801, Training Loss: 61779, Validation Loss: 70344, 83886.8338279767\n",
      "Epoch 20901, Training Loss: 67877, Validation Loss: 60400, 902988.3815427389\n",
      "Epoch 21001, Training Loss: 62867, Validation Loss: 70620, 130688.53181146459\n",
      "Epoch 21101, Training Loss: 65647, Validation Loss: 74433, 137709.62094094296\n",
      "Epoch 21201, Training Loss: 60933, Validation Loss: 75690, 316220.9976513575\n",
      "Epoch 21301, Training Loss: 65030, Validation Loss: 68991, 67304.0022510438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21401, Training Loss: 62952, Validation Loss: 69374, 115368.71466200531\n",
      "Epoch 21501, Training Loss: 72452, Validation Loss: 73958, 532308.2504864834\n",
      "Epoch 21601, Training Loss: 107944, Validation Loss: 84053, 1605374.0756320935\n",
      "Epoch 21701, Training Loss: 63155, Validation Loss: 67352, 149804.5090670701\n",
      "Epoch 21801, Training Loss: 63143, Validation Loss: 69289, 210024.13840091848\n",
      "Epoch 21901, Training Loss: 61682, Validation Loss: 79231, 125686.55288063863\n",
      "Epoch 22001, Training Loss: 63271, Validation Loss: 70649, 350801.17523346655\n",
      "Epoch 22101, Training Loss: 68418, Validation Loss: 82818, 621502.9447876915\n",
      "Epoch 22201, Training Loss: 60562, Validation Loss: 73066, 255880.84146190167\n",
      "Epoch 22301, Training Loss: 62627, Validation Loss: 72087, 210266.52951987798\n",
      "Epoch 22401, Training Loss: 65340, Validation Loss: 79067, 208613.29318551222\n",
      "Epoch 22501, Training Loss: 72106, Validation Loss: 59591, 1042829.5983035034\n",
      "Epoch 22601, Training Loss: 62550, Validation Loss: 75361, 82694.14671516436\n",
      "Epoch 22701, Training Loss: 63440, Validation Loss: 63668, 535515.1620811092\n",
      "Epoch 22801, Training Loss: 65965, Validation Loss: 60595, 880111.8965286762\n",
      "Epoch 22901, Training Loss: 67204, Validation Loss: 69614, 129270.00935948755\n",
      "Epoch 23001, Training Loss: 73444, Validation Loss: 84447, 654462.4011809343\n",
      "Epoch 23101, Training Loss: 62776, Validation Loss: 64924, 464007.41425988293\n",
      "Epoch 23201, Training Loss: 63902, Validation Loss: 72380, 190737.4368896353\n",
      "Epoch 23301, Training Loss: 65324, Validation Loss: 57367, 2268847.9555433406\n",
      "Epoch 23401, Training Loss: 65908, Validation Loss: 69445, 119804.30309731443\n",
      "Epoch 23501, Training Loss: 62454, Validation Loss: 75468, 323107.7425285757\n",
      "Epoch 23601, Training Loss: 67266, Validation Loss: 62715, 869636.8674328392\n",
      "Epoch 23701, Training Loss: 61945, Validation Loss: 67449, 205731.3913337161\n",
      "Epoch 23801, Training Loss: 65608, Validation Loss: 71911, 165035.6012759822\n",
      "Epoch 23901, Training Loss: 63530, Validation Loss: 75888, 228497.88381235546\n",
      "Epoch 24001, Training Loss: 67290, Validation Loss: 66401, 275329.20625589794\n",
      "Epoch 24101, Training Loss: 62292, Validation Loss: 72893, 125585.92862372776\n",
      "Epoch 24201, Training Loss: 66871, Validation Loss: 76888, 291363.2061417732\n",
      "Epoch 24301, Training Loss: 68007, Validation Loss: 67008, 555325.4475286658\n",
      "Epoch 24401, Training Loss: 67234, Validation Loss: 66305, 524236.92250572215\n",
      "Epoch 24501, Training Loss: 61892, Validation Loss: 66159, 136241.05005822374\n",
      "Epoch 24601, Training Loss: 63669, Validation Loss: 69054, 67420.9693051295\n",
      "Epoch 24701, Training Loss: 66340, Validation Loss: 76295, 232354.79043791734\n",
      "Epoch 24801, Training Loss: 67940, Validation Loss: 69871, 170480.83901529555\n",
      "Epoch 24901, Training Loss: 66228, Validation Loss: 68980, 156774.2122917679\n",
      "Epoch 25001, Training Loss: 65386, Validation Loss: 63207, 819964.3362843086\n",
      "Epoch 25101, Training Loss: 65463, Validation Loss: 77741, 171283.405166601\n",
      "Epoch 25201, Training Loss: 65179, Validation Loss: 68653, 103008.92130316205\n",
      "Epoch 25301, Training Loss: 63893, Validation Loss: 68779, 144776.12672984632\n",
      "Epoch 25401, Training Loss: 63697, Validation Loss: 70727, 83675.78508408232\n",
      "Epoch 25501, Training Loss: 63961, Validation Loss: 67517, 96683.75449428531\n",
      "Epoch 25601, Training Loss: 64561, Validation Loss: 76039, 157046.51455343145\n",
      "Epoch 25701, Training Loss: 78742, Validation Loss: 93736, 96998.1348461586\n",
      "Epoch 25801, Training Loss: 62360, Validation Loss: 71688, 199070.6673811957\n",
      "Epoch 25901, Training Loss: 67321, Validation Loss: 64467, 518487.6474024787\n",
      "Epoch 26001, Training Loss: 67159, Validation Loss: 74497, 250853.3005227192\n",
      "Epoch 26101, Training Loss: 62606, Validation Loss: 67036, 56213.26263717404\n",
      "Epoch 26201, Training Loss: 65306, Validation Loss: 70082, 202512.8093008039\n",
      "Epoch 26301, Training Loss: 63863, Validation Loss: 64157, 156253.76671691152\n",
      "Epoch 26401, Training Loss: 67152, Validation Loss: 70993, 151558.38838184826\n",
      "Epoch 26501, Training Loss: 67148, Validation Loss: 62398, 597885.5414685355\n",
      "Epoch 26601, Training Loss: 65689, Validation Loss: 68948, 196879.62733046667\n",
      "Epoch 26701, Training Loss: 68336, Validation Loss: 70118, 283691.33191908756\n",
      "Epoch 26801, Training Loss: 62726, Validation Loss: 70507, 210249.5621177332\n",
      "Epoch 26901, Training Loss: 64644, Validation Loss: 69072, 216152.82370594586\n",
      "Epoch 27001, Training Loss: 65615, Validation Loss: 62590, 375200.3372955544\n",
      "Epoch 27101, Training Loss: 68320, Validation Loss: 67165, 433909.8612127415\n",
      "Epoch 27201, Training Loss: 65258, Validation Loss: 72979, 185875.83133628257\n",
      "Epoch 27301, Training Loss: 60413, Validation Loss: 77202, 104054.02414755791\n",
      "Epoch 27401, Training Loss: 77268, Validation Loss: 95671, 644379.1645171623\n",
      "Epoch 27501, Training Loss: 65996, Validation Loss: 61842, 198554.8054842737\n",
      "Epoch 27601, Training Loss: 99832, Validation Loss: 94107, 2562.521994310657\n",
      "Epoch 27701, Training Loss: 98949, Validation Loss: 93853, 2579.288479222689\n",
      "Epoch 27801, Training Loss: 98805, Validation Loss: 93970, 2485.484056550599\n",
      "Epoch 27901, Training Loss: 93981, Validation Loss: 94066, 3823.0880844246117\n",
      "Epoch 28001, Training Loss: 95659, Validation Loss: 94195, 2065.9479056071973\n",
      "Epoch 28101, Training Loss: 93492, Validation Loss: 92297, 120089.5194266356\n",
      "Epoch 28201, Training Loss: 72002, Validation Loss: 68031, 550253.6729261376\n",
      "Epoch 28301, Training Loss: 73323, Validation Loss: 68940, 466992.644955749\n",
      "Epoch 28401, Training Loss: 70686, Validation Loss: 68631, 1258597.4173267414\n",
      "Epoch 28501, Training Loss: 69054, Validation Loss: 68759, 1002946.048560725\n",
      "Epoch 28601, Training Loss: 71475, Validation Loss: 80934, 810950.140131947\n",
      "Epoch 28701, Training Loss: 68318, Validation Loss: 71177, 694574.3624730528\n",
      "Epoch 28801, Training Loss: 70995, Validation Loss: 63966, 440825.1784579999\n",
      "Epoch 28901, Training Loss: 66972, Validation Loss: 71618, 355371.6034867658\n",
      "Epoch 29001, Training Loss: 69020, Validation Loss: 73524, 888170.6859316449\n",
      "Epoch 29101, Training Loss: 70718, Validation Loss: 77431, 514462.62311313394\n",
      "Epoch 29201, Training Loss: 68490, Validation Loss: 75891, 687267.1182812363\n",
      "Epoch 29301, Training Loss: 69755, Validation Loss: 69415, 571765.598782008\n",
      "Epoch 29401, Training Loss: 66793, Validation Loss: 61323, 661838.6196222606\n",
      "Epoch 29501, Training Loss: 65702, Validation Loss: 66908, 1464883.1383650082\n",
      "Epoch 29601, Training Loss: 64917, Validation Loss: 67496, 637531.1183503256\n",
      "Epoch 29701, Training Loss: 65268, Validation Loss: 73155, 244824.26398175428\n",
      "Epoch 29801, Training Loss: 63370, Validation Loss: 73353, 397138.93434927077\n",
      "Epoch 29901, Training Loss: 69017, Validation Loss: 80660, 692571.3007326946\n",
      "Epoch 30001, Training Loss: 65531, Validation Loss: 63292, 746805.4847513507\n",
      "Epoch 30101, Training Loss: 66856, Validation Loss: 79788, 209954.3938131151\n",
      "Epoch 30201, Training Loss: 69094, Validation Loss: 71765, 1910765.7110229752\n",
      "Epoch 30301, Training Loss: 69923, Validation Loss: 65110, 1257564.0627754584\n",
      "Epoch 30401, Training Loss: 66622, Validation Loss: 66479, 1590242.9836946211\n",
      "Epoch 30501, Training Loss: 65407, Validation Loss: 67226, 442209.87595813465\n",
      "Epoch 30601, Training Loss: 63481, Validation Loss: 68021, 437274.55513639824\n",
      "Epoch 30701, Training Loss: 62027, Validation Loss: 72890, 652661.8318467041\n",
      "Epoch 30801, Training Loss: 67816, Validation Loss: 73065, 1293110.5585778777\n",
      "Epoch 30901, Training Loss: 102603, Validation Loss: 94020, 4514.230501424582\n",
      "Epoch 31001, Training Loss: 97817, Validation Loss: 94059, 3355.3096986953956\n",
      "Epoch 31101, Training Loss: 98784, Validation Loss: 94053, 2729.0929076526977\n",
      "Epoch 31201, Training Loss: 97261, Validation Loss: 94091, 3590.3838920998296\n",
      "Epoch 31301, Training Loss: 73391, Validation Loss: 78444, 779001.975396499\n",
      "Epoch 31401, Training Loss: 72453, Validation Loss: 66321, 1094965.0358787824\n",
      "Epoch 31501, Training Loss: 65758, Validation Loss: 78104, 214074.3104311713\n",
      "Epoch 31601, Training Loss: 70207, Validation Loss: 85970, 1278901.099073442\n",
      "Epoch 31701, Training Loss: 66033, Validation Loss: 66729, 1049977.1823938745\n",
      "Epoch 31801, Training Loss: 67527, Validation Loss: 73306, 590756.2472762043\n",
      "Epoch 31901, Training Loss: 67570, Validation Loss: 67029, 571679.1924004873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32001, Training Loss: 70156, Validation Loss: 83511, 1658901.3742590535\n",
      "Epoch 32101, Training Loss: 74025, Validation Loss: 74209, 1557697.749852325\n",
      "Epoch 32201, Training Loss: 69408, Validation Loss: 66261, 830403.7337979126\n",
      "Epoch 32301, Training Loss: 65025, Validation Loss: 65598, 1056199.557867622\n",
      "Epoch 32401, Training Loss: 66867, Validation Loss: 70548, 326909.76444607985\n",
      "Epoch 32501, Training Loss: 67056, Validation Loss: 75195, 868396.0219451515\n",
      "Epoch 32601, Training Loss: 67114, Validation Loss: 69348, 375965.06502998085\n",
      "Epoch 32701, Training Loss: 67814, Validation Loss: 77072, 435151.7806768047\n",
      "Epoch 32801, Training Loss: 61430, Validation Loss: 79050, 524197.6859772697\n",
      "Epoch 32901, Training Loss: 63736, Validation Loss: 66582, 760368.5050342651\n",
      "Epoch 33001, Training Loss: 64256, Validation Loss: 74507, 890411.4634234267\n",
      "Epoch 33101, Training Loss: 63567, Validation Loss: 79573, 870560.4943919806\n",
      "Epoch 33201, Training Loss: 65708, Validation Loss: 74368, 1628825.6031673339\n",
      "Epoch 33301, Training Loss: 65100, Validation Loss: 73513, 231929.66307961292\n",
      "Epoch 33401, Training Loss: 68997, Validation Loss: 93287, 2055707.2174536018\n",
      "Epoch 33501, Training Loss: 65642, Validation Loss: 76829, 482883.0569875256\n",
      "Epoch 33601, Training Loss: 62676, Validation Loss: 77405, 380287.6691130609\n",
      "Epoch 33701, Training Loss: 66327, Validation Loss: 61661, 1211128.2700784432\n",
      "Epoch 33801, Training Loss: 61816, Validation Loss: 64625, 659352.9929931558\n",
      "Epoch 33901, Training Loss: 62666, Validation Loss: 77457, 624161.6048782923\n",
      "Epoch 34001, Training Loss: 60779, Validation Loss: 74855, 321722.3929293444\n",
      "Epoch 34101, Training Loss: 65768, Validation Loss: 82634, 565202.876254465\n",
      "Epoch 34201, Training Loss: 62772, Validation Loss: 80730, 270669.7529628342\n",
      "Epoch 34301, Training Loss: 64725, Validation Loss: 79252, 1052077.8139136457\n",
      "Epoch 34401, Training Loss: 68661, Validation Loss: 72930, 1582942.8939664338\n",
      "Epoch 34501, Training Loss: 66431, Validation Loss: 75911, 817199.5782154095\n",
      "Epoch 34601, Training Loss: 63432, Validation Loss: 85062, 916148.4553437815\n",
      "Epoch 34701, Training Loss: 59458, Validation Loss: 75362, 578475.0912941601\n",
      "Epoch 34801, Training Loss: 69656, Validation Loss: 74694, 1064275.9923809783\n",
      "Epoch 34901, Training Loss: 66860, Validation Loss: 86901, 1413883.2683170035\n",
      "Epoch 35001, Training Loss: 65020, Validation Loss: 81363, 857554.3279970608\n",
      "Epoch 35101, Training Loss: 63924, Validation Loss: 75297, 750256.3350981049\n",
      "Epoch 35201, Training Loss: 63542, Validation Loss: 72432, 631652.6910803191\n",
      "Epoch 35301, Training Loss: 60821, Validation Loss: 78996, 715421.3594728648\n",
      "Epoch 35401, Training Loss: 61573, Validation Loss: 70787, 246895.96968700938\n",
      "Epoch 35501, Training Loss: 65273, Validation Loss: 77972, 708153.4781116778\n",
      "Epoch 35601, Training Loss: 59180, Validation Loss: 75909, 181565.33107059472\n",
      "Epoch 35701, Training Loss: 63407, Validation Loss: 68187, 1224073.005559324\n",
      "Epoch 35801, Training Loss: 66160, Validation Loss: 78233, 935030.0122200088\n",
      "Epoch 35901, Training Loss: 62574, Validation Loss: 88079, 736468.2898012238\n",
      "Epoch 36001, Training Loss: 65248, Validation Loss: 71890, 569789.9612633489\n",
      "Epoch 36101, Training Loss: 66332, Validation Loss: 71603, 714152.0865908246\n",
      "Epoch 36201, Training Loss: 63352, Validation Loss: 79569, 588404.1142657025\n",
      "Epoch 36301, Training Loss: 61176, Validation Loss: 81176, 717484.6999380299\n",
      "Epoch 36401, Training Loss: 59286, Validation Loss: 82499, 463387.659733742\n",
      "Epoch 36501, Training Loss: 61998, Validation Loss: 69852, 596847.4246014818\n",
      "Epoch 36601, Training Loss: 59188, Validation Loss: 58827, 1519602.1298173517\n",
      "Epoch 36701, Training Loss: 62530, Validation Loss: 80767, 727432.5397831467\n",
      "Epoch 36801, Training Loss: 67294, Validation Loss: 65691, 998161.2650090769\n",
      "Epoch 36901, Training Loss: 59843, Validation Loss: 77295, 1072976.9129028376\n",
      "Epoch 37001, Training Loss: 62423, Validation Loss: 78793, 444984.73819164885\n",
      "Epoch 37101, Training Loss: 75596, Validation Loss: 66383, 671474.5603150622\n",
      "Epoch 37201, Training Loss: 65983, Validation Loss: 75538, 874934.5452264621\n",
      "Epoch 37301, Training Loss: 65653, Validation Loss: 80075, 427529.9957447687\n",
      "Epoch 37401, Training Loss: 65709, Validation Loss: 81760, 500747.35495683335\n",
      "Epoch 37501, Training Loss: 62622, Validation Loss: 66856, 812824.7010402327\n",
      "Epoch 37601, Training Loss: 63998, Validation Loss: 81309, 650845.4695815939\n",
      "Epoch 37701, Training Loss: 64426, Validation Loss: 70683, 300022.5758125691\n",
      "Epoch 37801, Training Loss: 61621, Validation Loss: 79858, 970501.7601259564\n",
      "Epoch 37901, Training Loss: 64320, Validation Loss: 72210, 272662.86486244487\n",
      "Epoch 38001, Training Loss: 79362, Validation Loss: 68177, 2633136.1710278383\n",
      "Epoch 38101, Training Loss: 57255, Validation Loss: 74771, 421524.7798778545\n",
      "Epoch 38201, Training Loss: 65075, Validation Loss: 71273, 717342.8437384157\n",
      "Epoch 38301, Training Loss: 64788, Validation Loss: 65874, 310560.80749130476\n",
      "Epoch 38401, Training Loss: 63735, Validation Loss: 74026, 792821.8688436759\n",
      "Epoch 38501, Training Loss: 61619, Validation Loss: 82277, 862665.6907613041\n",
      "Epoch 38601, Training Loss: 64886, Validation Loss: 66743, 1266408.7882791732\n",
      "Epoch 38701, Training Loss: 66141, Validation Loss: 70941, 679012.8010076024\n",
      "Epoch 38801, Training Loss: 61740, Validation Loss: 70322, 228548.91887144404\n",
      "Epoch 38901, Training Loss: 67112, Validation Loss: 72369, 485210.40015113394\n",
      "Epoch 39001, Training Loss: 99492, Validation Loss: 94804, 18728.99166660534\n",
      "Epoch 39101, Training Loss: 97122, Validation Loss: 94212, 2128.949840676813\n",
      "Epoch 39201, Training Loss: 96231, Validation Loss: 94046, 2054.5832855315125\n",
      "Epoch 39301, Training Loss: 96856, Validation Loss: 94061, 4498.760524545874\n",
      "Epoch 39401, Training Loss: 100022, Validation Loss: 93931, 2157.1337756891417\n",
      "Epoch 39501, Training Loss: 100716, Validation Loss: 93958, 3262.9035245649115\n",
      "Epoch 39601, Training Loss: 101127, Validation Loss: 94051, 2305.9155227294327\n",
      "Epoch 39701, Training Loss: 96480, Validation Loss: 93778, 4520.035289967577\n",
      "Epoch 39801, Training Loss: 97537, Validation Loss: 93771, 2117.656570611431\n",
      "Epoch 39901, Training Loss: 98736, Validation Loss: 94143, 3539.9981594795076\n",
      "Epoch 40001, Training Loss: 97075, Validation Loss: 93760, 2674.6162859345864\n",
      "Epoch 40101, Training Loss: 103317, Validation Loss: 93989, 2122.7647208895864\n",
      "Epoch 40201, Training Loss: 96811, Validation Loss: 94030, 4935.0423599950045\n",
      "Epoch 40301, Training Loss: 97869, Validation Loss: 93953, 2165.2631421613128\n",
      "Epoch 40401, Training Loss: 96863, Validation Loss: 93982, 2034.5161910262038\n",
      "Epoch 40501, Training Loss: 100465, Validation Loss: 93944, 3605.1277264298624\n",
      "Epoch 40601, Training Loss: 97953, Validation Loss: 94049, 2335.6299863194618\n",
      "Epoch 40701, Training Loss: 102051, Validation Loss: 93992, 5075.055599741451\n",
      "Epoch 40801, Training Loss: 99170, Validation Loss: 93996, 4105.596229584506\n",
      "Epoch 40901, Training Loss: 96758, Validation Loss: 94026, 2441.034969814786\n",
      "Epoch 41001, Training Loss: 98596, Validation Loss: 93769, 1990.8334546404349\n",
      "Epoch 41101, Training Loss: 95842, Validation Loss: 94137, 2408.864532980728\n",
      "Epoch 41201, Training Loss: 99300, Validation Loss: 93898, 2825.948457713246\n",
      "Epoch 41301, Training Loss: 96700, Validation Loss: 93956, 2575.260295754699\n",
      "Epoch 41401, Training Loss: 100236, Validation Loss: 93793, 3447.811277235423\n",
      "Epoch 41501, Training Loss: 94459, Validation Loss: 94010, 3736.5880673590564\n",
      "Epoch 41601, Training Loss: 100948, Validation Loss: 93789, 3022.3480672704804\n",
      "Epoch 41701, Training Loss: 102223, Validation Loss: 94238, 5952.647227877991\n",
      "Epoch 41801, Training Loss: 101452, Validation Loss: 94009, 4471.755494736796\n",
      "Epoch 41901, Training Loss: 102105, Validation Loss: 94095, 2107.302312922273\n",
      "Epoch 42001, Training Loss: 94832, Validation Loss: 93724, 3414.4896022250286\n",
      "Epoch 42101, Training Loss: 100959, Validation Loss: 93730, 4047.790061595573\n",
      "Epoch 42201, Training Loss: 99633, Validation Loss: 93947, 4728.123654602945\n",
      "Epoch 42301, Training Loss: 96260, Validation Loss: 94460, 4542.264172083291\n",
      "Epoch 42401, Training Loss: 98174, Validation Loss: 93889, 3745.620087228747\n",
      "Epoch 42501, Training Loss: 98827, Validation Loss: 94109, 4487.546852643342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42601, Training Loss: 95715, Validation Loss: 93824, 2281.3337173699347\n",
      "Epoch 42701, Training Loss: 99657, Validation Loss: 93982, 1555.4811430458576\n",
      "Epoch 42801, Training Loss: 96469, Validation Loss: 93965, 2496.400727277467\n",
      "Epoch 42901, Training Loss: 97946, Validation Loss: 94048, 3143.5050368304924\n",
      "Epoch 43001, Training Loss: 101641, Validation Loss: 93964, 5078.109879007279\n",
      "Epoch 43101, Training Loss: 95988, Validation Loss: 94195, 3329.343134452794\n",
      "Epoch 43201, Training Loss: 101275, Validation Loss: 94292, 6414.105500319412\n",
      "Epoch 43301, Training Loss: 101405, Validation Loss: 93679, 5028.496457837724\n",
      "Epoch 43401, Training Loss: 97944, Validation Loss: 93918, 3938.2150623313246\n",
      "Epoch 43501, Training Loss: 99367, Validation Loss: 93725, 2132.1682974565024\n",
      "Epoch 43601, Training Loss: 98176, Validation Loss: 93887, 2042.6575275594266\n",
      "Epoch 43701, Training Loss: 98492, Validation Loss: 93788, 2519.4715977689284\n",
      "Epoch 43801, Training Loss: 104283, Validation Loss: 93814, 4183.124421471217\n",
      "Epoch 43901, Training Loss: 99156, Validation Loss: 93766, 2554.277718852097\n",
      "Epoch 44001, Training Loss: 94558, Validation Loss: 94223, 2305.606868044921\n",
      "Epoch 44101, Training Loss: 94742, Validation Loss: 94351, 1774.236725951728\n",
      "Epoch 44201, Training Loss: 97987, Validation Loss: 94258, 3565.5249412573917\n",
      "Epoch 44301, Training Loss: 98355, Validation Loss: 93680, 3958.06261092273\n",
      "Epoch 44401, Training Loss: 96899, Validation Loss: 93891, 2375.5067470200775\n",
      "Epoch 44501, Training Loss: 98965, Validation Loss: 94354, 2127.526765722241\n",
      "Epoch 44601, Training Loss: 102729, Validation Loss: 94419, 5017.5519482421705\n",
      "Epoch 44701, Training Loss: 89100, Validation Loss: 100801, 79156.88413927928\n",
      "Epoch 44801, Training Loss: 75630, Validation Loss: 110274, 906604.8599052288\n",
      "Epoch 44901, Training Loss: 70874, Validation Loss: 74362, 233178.12053823433\n",
      "Epoch 45001, Training Loss: 71015, Validation Loss: 68136, 463188.99100614834\n",
      "Epoch 45101, Training Loss: 67467, Validation Loss: 68341, 558632.8648322535\n",
      "Epoch 45201, Training Loss: 73105, Validation Loss: 78197, 871047.3620273382\n",
      "Epoch 45301, Training Loss: 72455, Validation Loss: 79884, 861311.4316725886\n",
      "Epoch 45401, Training Loss: 66273, Validation Loss: 73327, 487175.46562978026\n",
      "Epoch 45501, Training Loss: 66843, Validation Loss: 81487, 775603.5825894582\n",
      "Epoch 45601, Training Loss: 70574, Validation Loss: 83416, 901501.3813311718\n",
      "Epoch 45701, Training Loss: 66058, Validation Loss: 73564, 1460600.2584235957\n",
      "Epoch 45801, Training Loss: 77907, Validation Loss: 68928, 508670.3980363135\n",
      "Epoch 45901, Training Loss: 70649, Validation Loss: 76412, 688328.5179408374\n",
      "Epoch 46001, Training Loss: 65081, Validation Loss: 69410, 463602.8020828685\n",
      "Epoch 46101, Training Loss: 66291, Validation Loss: 65228, 878890.0468524414\n",
      "Epoch 46201, Training Loss: 68171, Validation Loss: 77677, 375238.8669800453\n",
      "Epoch 46301, Training Loss: 64141, Validation Loss: 74756, 544161.3163904543\n",
      "Epoch 46401, Training Loss: 65189, Validation Loss: 60845, 1046393.6476127881\n",
      "Epoch 46501, Training Loss: 69418, Validation Loss: 69859, 977783.9272040533\n",
      "Epoch 46601, Training Loss: 64335, Validation Loss: 62519, 577674.8772105224\n",
      "Epoch 46701, Training Loss: 64641, Validation Loss: 74435, 882636.4114750158\n",
      "Epoch 46801, Training Loss: 64309, Validation Loss: 77039, 377036.6404082181\n",
      "Epoch 46901, Training Loss: 66842, Validation Loss: 68816, 1201846.1774188178\n",
      "Epoch 47001, Training Loss: 69336, Validation Loss: 72289, 433903.01509405306\n",
      "Epoch 47101, Training Loss: 67012, Validation Loss: 72166, 420483.8402270705\n",
      "Epoch 47201, Training Loss: 69972, Validation Loss: 70685, 831500.9922899096\n",
      "Epoch 47301, Training Loss: 66515, Validation Loss: 72819, 1087162.3099346713\n",
      "Epoch 47401, Training Loss: 66572, Validation Loss: 67991, 489470.5572343616\n",
      "Epoch 47501, Training Loss: 65097, Validation Loss: 70283, 622458.4737644126\n",
      "Epoch 47601, Training Loss: 73002, Validation Loss: 71792, 1259284.865824488\n",
      "Epoch 47701, Training Loss: 73670, Validation Loss: 76066, 2129800.6458204794\n",
      "Epoch 47801, Training Loss: 64503, Validation Loss: 77182, 753771.2129929954\n",
      "Epoch 47901, Training Loss: 62492, Validation Loss: 67202, 839729.4881800524\n",
      "Epoch 48001, Training Loss: 64429, Validation Loss: 73106, 916944.1260438198\n",
      "Epoch 48101, Training Loss: 68004, Validation Loss: 77565, 1046541.8170083928\n",
      "Epoch 48201, Training Loss: 66721, Validation Loss: 71843, 237104.55222565052\n",
      "Epoch 48301, Training Loss: 65241, Validation Loss: 71204, 409522.4136076197\n",
      "Epoch 48401, Training Loss: 67794, Validation Loss: 70106, 1034504.675125261\n",
      "Epoch 48501, Training Loss: 65425, Validation Loss: 61296, 324147.46022208105\n",
      "Epoch 48601, Training Loss: 69048, Validation Loss: 70954, 669167.8889178183\n",
      "Epoch 48701, Training Loss: 70300, Validation Loss: 77241, 733776.273990003\n",
      "Epoch 48801, Training Loss: 68225, Validation Loss: 70474, 186601.04162503255\n",
      "Epoch 48901, Training Loss: 67327, Validation Loss: 71564, 738424.4855119883\n",
      "Epoch 49001, Training Loss: 73081, Validation Loss: 66207, 907237.6333166873\n",
      "Epoch 49101, Training Loss: 65715, Validation Loss: 71749, 771128.0109883491\n",
      "Epoch 49201, Training Loss: 64744, Validation Loss: 74350, 188425.4856230784\n",
      "Epoch 49301, Training Loss: 64689, Validation Loss: 69287, 333616.7202060733\n",
      "Epoch 49401, Training Loss: 69153, Validation Loss: 71105, 794105.4281875525\n",
      "Epoch 49501, Training Loss: 69252, Validation Loss: 82094, 521523.5754140066\n",
      "Epoch 49601, Training Loss: 65858, Validation Loss: 66201, 518277.98528068216\n",
      "Epoch 49701, Training Loss: 66279, Validation Loss: 80290, 437124.61147215235\n",
      "Epoch 49801, Training Loss: 70260, Validation Loss: 83539, 1056555.1356933108\n",
      "Epoch 49901, Training Loss: 68496, Validation Loss: 74079, 407544.1464039003\n",
      "Epoch 50001, Training Loss: 62263, Validation Loss: 70307, 359252.538918771\n",
      "Epoch 50101, Training Loss: 65311, Validation Loss: 66644, 528527.4089950105\n",
      "Epoch 50201, Training Loss: 67670, Validation Loss: 71978, 851337.2252566778\n",
      "Epoch 50301, Training Loss: 66666, Validation Loss: 82428, 1085097.7736575583\n",
      "Epoch 50401, Training Loss: 65170, Validation Loss: 74358, 375363.8757594805\n",
      "Epoch 50501, Training Loss: 61949, Validation Loss: 64743, 662329.9059367403\n",
      "Epoch 50601, Training Loss: 63504, Validation Loss: 68549, 713231.4101632657\n",
      "Epoch 50701, Training Loss: 62735, Validation Loss: 62867, 456434.86125060945\n",
      "Epoch 50801, Training Loss: 63291, Validation Loss: 72853, 137342.52028098435\n",
      "Epoch 50901, Training Loss: 64123, Validation Loss: 66114, 731642.6564574657\n",
      "Epoch 51001, Training Loss: 67289, Validation Loss: 67381, 680343.759862872\n",
      "Epoch 51101, Training Loss: 62863, Validation Loss: 72339, 448497.56179667945\n",
      "Epoch 51201, Training Loss: 68255, Validation Loss: 70694, 1860163.4263570402\n",
      "Epoch 51301, Training Loss: 67714, Validation Loss: 72351, 1200535.6408934733\n",
      "Epoch 51401, Training Loss: 65803, Validation Loss: 80832, 788249.7272244152\n",
      "Epoch 51501, Training Loss: 63934, Validation Loss: 79309, 285332.49066587246\n",
      "Epoch 51601, Training Loss: 99815, Validation Loss: 93829, 3595.688788139711\n",
      "Epoch 51701, Training Loss: 100586, Validation Loss: 93997, 4667.584000712579\n",
      "Epoch 51801, Training Loss: 102263, Validation Loss: 94050, 2602.1533787573003\n",
      "Epoch 51901, Training Loss: 99242, Validation Loss: 93870, 1775.0453528414437\n",
      "Epoch 52001, Training Loss: 99856, Validation Loss: 93814, 1821.4357453880439\n",
      "Epoch 52101, Training Loss: 101662, Validation Loss: 94523, 3151.555671028878\n",
      "Epoch 52201, Training Loss: 99133, Validation Loss: 94070, 2123.8986933096126\n",
      "Epoch 52301, Training Loss: 101907, Validation Loss: 93815, 2695.920856934717\n",
      "Epoch 52401, Training Loss: 96791, Validation Loss: 93709, 2539.1071873807227\n",
      "Epoch 52501, Training Loss: 96563, Validation Loss: 93930, 1780.1499900544993\n",
      "Epoch 52601, Training Loss: 98314, Validation Loss: 94151, 2159.122153910072\n",
      "Epoch 52701, Training Loss: 102235, Validation Loss: 94017, 3975.048048842325\n",
      "Epoch 52801, Training Loss: 95428, Validation Loss: 94116, 3092.557801571888\n",
      "Epoch 52901, Training Loss: 100526, Validation Loss: 93913, 2550.235817458411\n",
      "Epoch 53001, Training Loss: 98803, Validation Loss: 93792, 2191.574918070304\n",
      "Epoch 53101, Training Loss: 99116, Validation Loss: 93844, 1685.7044751189114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53201, Training Loss: 99485, Validation Loss: 93743, 2777.955808356454\n",
      "Epoch 53301, Training Loss: 99903, Validation Loss: 94581, 3283.7807590750635\n",
      "Epoch 53401, Training Loss: 99347, Validation Loss: 94141, 2052.790260806481\n",
      "Epoch 53501, Training Loss: 100507, Validation Loss: 94495, 2028.0589146198897\n",
      "Epoch 53601, Training Loss: 93699, Validation Loss: 94042, 2072.422819313529\n",
      "Epoch 53701, Training Loss: 95731, Validation Loss: 93839, 3399.216880382444\n",
      "Epoch 53801, Training Loss: 97626, Validation Loss: 93887, 1595.698054599901\n",
      "Epoch 53901, Training Loss: 99655, Validation Loss: 93862, 1742.9439993831677\n",
      "Epoch 54001, Training Loss: 95477, Validation Loss: 94231, 2154.1169689285734\n",
      "Epoch 54101, Training Loss: 98097, Validation Loss: 94088, 1704.3435946188913\n",
      "Epoch 54201, Training Loss: 98057, Validation Loss: 94100, 2199.591638248483\n",
      "Epoch 54301, Training Loss: 100172, Validation Loss: 93957, 2808.380731862289\n",
      "Epoch 54401, Training Loss: 97784, Validation Loss: 94017, 3807.7091254979023\n",
      "Epoch 54501, Training Loss: 101781, Validation Loss: 93759, 3572.618263990415\n",
      "Epoch 54601, Training Loss: 97249, Validation Loss: 94007, 1781.2492640451321\n",
      "Epoch 54701, Training Loss: 96500, Validation Loss: 94150, 4300.197022673855\n",
      "Epoch 54801, Training Loss: 98857, Validation Loss: 93814, 1810.295482660499\n",
      "Epoch 54901, Training Loss: 97939, Validation Loss: 93748, 4291.350376645578\n",
      "Epoch 55001, Training Loss: 98200, Validation Loss: 93948, 2234.1404207275355\n",
      "Epoch 55101, Training Loss: 98609, Validation Loss: 94186, 4268.462232645615\n",
      "Epoch 55201, Training Loss: 102349, Validation Loss: 94246, 2945.9817727428704\n",
      "Epoch 55301, Training Loss: 95692, Validation Loss: 93946, 3538.0420817228382\n",
      "Epoch 55401, Training Loss: 102689, Validation Loss: 93791, 2604.6345066498866\n",
      "Epoch 55501, Training Loss: 94394, Validation Loss: 94102, 2101.1788475884496\n",
      "Epoch 55601, Training Loss: 99849, Validation Loss: 93895, 3732.6110242052346\n",
      "Epoch 55701, Training Loss: 97681, Validation Loss: 94210, 2273.539890573038\n",
      "Epoch 55801, Training Loss: 95849, Validation Loss: 94412, 3464.107364121804\n",
      "Epoch 55901, Training Loss: 100404, Validation Loss: 94115, 3577.777514362644\n",
      "Epoch 56001, Training Loss: 98410, Validation Loss: 93825, 2693.438692657572\n",
      "Epoch 56101, Training Loss: 95163, Validation Loss: 93926, 2091.557733824338\n",
      "Epoch 56201, Training Loss: 102132, Validation Loss: 94434, 3816.5530383515215\n",
      "Epoch 56301, Training Loss: 95865, Validation Loss: 94520, 1899.8181442463865\n",
      "Epoch 56401, Training Loss: 100468, Validation Loss: 93799, 3023.954802294582\n",
      "Epoch 56501, Training Loss: 97294, Validation Loss: 94061, 2195.7540490204324\n",
      "Epoch 56601, Training Loss: 97130, Validation Loss: 93743, 2385.759729047421\n",
      "Epoch 56701, Training Loss: 96745, Validation Loss: 94522, 2282.928298378489\n",
      "Epoch 56801, Training Loss: 95233, Validation Loss: 94024, 4516.138504027072\n",
      "Epoch 56901, Training Loss: 98705, Validation Loss: 93951, 2595.9821938772366\n",
      "Epoch 57001, Training Loss: 98174, Validation Loss: 94058, 1432.9595702626666\n",
      "Epoch 57101, Training Loss: 97106, Validation Loss: 94037, 6021.000370154204\n",
      "Epoch 57201, Training Loss: 83481, Validation Loss: 88942, 200427.37261031553\n",
      "Epoch 57301, Training Loss: 71423, Validation Loss: 71355, 502216.8358059355\n",
      "Epoch 57401, Training Loss: 69086, Validation Loss: 75079, 300465.5607756658\n",
      "Epoch 57501, Training Loss: 70696, Validation Loss: 79461, 517378.1364160381\n",
      "Epoch 57601, Training Loss: 68893, Validation Loss: 78465, 397022.700337527\n",
      "Epoch 57701, Training Loss: 71031, Validation Loss: 83839, 548008.1488300947\n",
      "Epoch 57801, Training Loss: 68087, Validation Loss: 69035, 246927.38211312777\n",
      "Epoch 57901, Training Loss: 68088, Validation Loss: 77508, 1055137.96673652\n",
      "Epoch 58001, Training Loss: 64024, Validation Loss: 78903, 513575.35269660526\n",
      "Epoch 58101, Training Loss: 68429, Validation Loss: 73170, 509035.60153226735\n",
      "Epoch 58201, Training Loss: 64031, Validation Loss: 71877, 687291.0395903544\n",
      "Epoch 58301, Training Loss: 66077, Validation Loss: 79255, 426046.04335265333\n",
      "Epoch 58401, Training Loss: 65349, Validation Loss: 70003, 734537.2474035857\n",
      "Epoch 58501, Training Loss: 66627, Validation Loss: 69016, 846952.3714911966\n",
      "Epoch 58601, Training Loss: 70117, Validation Loss: 69454, 1123627.037708949\n",
      "Epoch 58701, Training Loss: 63681, Validation Loss: 76800, 480452.8863534489\n",
      "Epoch 58801, Training Loss: 65089, Validation Loss: 77191, 496809.44701689575\n",
      "Epoch 58901, Training Loss: 67347, Validation Loss: 76750, 482430.29631410626\n",
      "Epoch 59001, Training Loss: 67981, Validation Loss: 75011, 674458.8814206789\n",
      "Epoch 59101, Training Loss: 65085, Validation Loss: 67143, 910737.6760012539\n",
      "Epoch 59201, Training Loss: 66916, Validation Loss: 81921, 720434.7756957565\n",
      "Epoch 59301, Training Loss: 68308, Validation Loss: 68311, 528695.7945589134\n",
      "Epoch 59401, Training Loss: 68353, Validation Loss: 80229, 284170.54144424485\n",
      "Epoch 59501, Training Loss: 67972, Validation Loss: 84697, 1641842.5836161412\n",
      "Epoch 59601, Training Loss: 63519, Validation Loss: 86227, 763133.7129266177\n",
      "Epoch 59701, Training Loss: 67603, Validation Loss: 72862, 985758.2913405722\n",
      "Epoch 59801, Training Loss: 63018, Validation Loss: 75644, 850238.5236394427\n",
      "Epoch 59901, Training Loss: 63094, Validation Loss: 75935, 426779.2385368365\n",
      "Epoch 60001, Training Loss: 67256, Validation Loss: 83160, 608473.8307656838\n",
      "Epoch 60101, Training Loss: 67422, Validation Loss: 78302, 856019.2142702201\n",
      "Epoch 60201, Training Loss: 65629, Validation Loss: 81535, 933744.4231567835\n",
      "Epoch 60301, Training Loss: 65799, Validation Loss: 82414, 256533.6575057703\n",
      "Epoch 60401, Training Loss: 65732, Validation Loss: 78809, 544737.0329922701\n",
      "Epoch 60501, Training Loss: 66991, Validation Loss: 66247, 186487.96656310573\n",
      "Epoch 60601, Training Loss: 67808, Validation Loss: 68702, 684669.3523495212\n",
      "Epoch 60701, Training Loss: 64110, Validation Loss: 71926, 598677.4380881024\n",
      "Epoch 60801, Training Loss: 63275, Validation Loss: 83684, 604581.0467274442\n",
      "Epoch 60901, Training Loss: 61785, Validation Loss: 79219, 665535.0333621434\n",
      "Epoch 61001, Training Loss: 62018, Validation Loss: 75312, 656365.8330901312\n",
      "Epoch 61101, Training Loss: 65971, Validation Loss: 74398, 555720.5369406504\n",
      "Epoch 61201, Training Loss: 66577, Validation Loss: 80168, 247684.6779855451\n",
      "Epoch 61301, Training Loss: 66146, Validation Loss: 79720, 750338.9587636208\n",
      "Epoch 61401, Training Loss: 66820, Validation Loss: 65358, 313380.1411519803\n",
      "Epoch 61501, Training Loss: 65007, Validation Loss: 68713, 688514.3748787761\n",
      "Epoch 61601, Training Loss: 63996, Validation Loss: 75926, 807447.1725392323\n",
      "Epoch 61701, Training Loss: 65451, Validation Loss: 71381, 197834.0513652045\n",
      "Epoch 61801, Training Loss: 65729, Validation Loss: 82738, 462593.69716056035\n",
      "Epoch 61901, Training Loss: 63085, Validation Loss: 72148, 924680.0234802832\n",
      "Epoch 62001, Training Loss: 66005, Validation Loss: 69781, 468665.7832879711\n",
      "Epoch 62101, Training Loss: 67185, Validation Loss: 70198, 481805.9163608316\n",
      "Epoch 62201, Training Loss: 72598, Validation Loss: 77793, 857553.0616106185\n",
      "Epoch 62301, Training Loss: 63583, Validation Loss: 76699, 398993.6039193021\n",
      "Epoch 62401, Training Loss: 61740, Validation Loss: 75828, 361436.5375404488\n",
      "Epoch 62501, Training Loss: 65805, Validation Loss: 73999, 483955.8910749761\n",
      "Epoch 62601, Training Loss: 64351, Validation Loss: 85910, 947376.5086391777\n",
      "Epoch 62701, Training Loss: 66528, Validation Loss: 73527, 428230.2961805442\n",
      "Epoch 62801, Training Loss: 66875, Validation Loss: 75102, 1095389.4512342403\n",
      "Epoch 62901, Training Loss: 65394, Validation Loss: 73287, 682332.0010794273\n",
      "Epoch 63001, Training Loss: 61765, Validation Loss: 71344, 430006.31416980765\n",
      "Epoch 63101, Training Loss: 65686, Validation Loss: 75673, 676459.5691242366\n",
      "Epoch 63201, Training Loss: 67814, Validation Loss: 73401, 795588.9220996127\n",
      "Epoch 63301, Training Loss: 64478, Validation Loss: 73382, 542027.2586716221\n",
      "Epoch 63401, Training Loss: 64106, Validation Loss: 63355, 830876.082574162\n",
      "Epoch 63501, Training Loss: 68488, Validation Loss: 69172, 344914.1305051566\n",
      "Epoch 63601, Training Loss: 66696, Validation Loss: 67379, 568664.6463691869\n",
      "Epoch 63701, Training Loss: 67437, Validation Loss: 74070, 163033.37887126967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63801, Training Loss: 63460, Validation Loss: 69219, 862833.3367782716\n",
      "Epoch 63901, Training Loss: 64619, Validation Loss: 73823, 569828.253913273\n",
      "Epoch 64001, Training Loss: 64019, Validation Loss: 78556, 728256.2404182581\n",
      "Epoch 64101, Training Loss: 66506, Validation Loss: 78907, 453924.9578636059\n",
      "Epoch 64201, Training Loss: 63898, Validation Loss: 78273, 532652.0059290623\n",
      "Epoch 64301, Training Loss: 66764, Validation Loss: 79315, 560894.9874076423\n",
      "Epoch 64401, Training Loss: 64390, Validation Loss: 66595, 285002.14530211565\n",
      "Epoch 64501, Training Loss: 67627, Validation Loss: 68603, 534141.294591573\n",
      "Epoch 64601, Training Loss: 64330, Validation Loss: 79733, 518786.063680452\n",
      "Epoch 64701, Training Loss: 72326, Validation Loss: 70920, 944060.7839581813\n",
      "Epoch 64801, Training Loss: 64054, Validation Loss: 79789, 201360.06165168143\n",
      "Epoch 64901, Training Loss: 64278, Validation Loss: 79112, 981474.6913564866\n",
      "Epoch 65001, Training Loss: 71118, Validation Loss: 94989, 883386.9748211396\n",
      "Epoch 65101, Training Loss: 64816, Validation Loss: 62931, 681967.109454963\n",
      "Epoch 65201, Training Loss: 67990, Validation Loss: 60499, 1103574.5967628544\n",
      "Epoch 65301, Training Loss: 68879, Validation Loss: 73577, 261964.87540005447\n",
      "Epoch 65401, Training Loss: 63513, Validation Loss: 69312, 672400.9053964101\n",
      "Epoch 65501, Training Loss: 68573, Validation Loss: 67767, 840297.0744948812\n",
      "Epoch 65601, Training Loss: 62052, Validation Loss: 70612, 507628.2703702855\n",
      "Epoch 65701, Training Loss: 60545, Validation Loss: 66703, 168646.91705605152\n",
      "Epoch 65801, Training Loss: 63169, Validation Loss: 61709, 470883.66953568603\n",
      "Epoch 65901, Training Loss: 64853, Validation Loss: 68734, 771971.3496588804\n",
      "Epoch 66001, Training Loss: 61121, Validation Loss: 70291, 287807.55308611266\n",
      "Epoch 66101, Training Loss: 65742, Validation Loss: 76317, 381417.0781830254\n",
      "Epoch 66201, Training Loss: 63799, Validation Loss: 67140, 380536.2840475416\n",
      "Epoch 66301, Training Loss: 65753, Validation Loss: 66290, 333790.3482026206\n",
      "Epoch 66401, Training Loss: 72860, Validation Loss: 64029, 835329.854209214\n",
      "Epoch 66501, Training Loss: 65195, Validation Loss: 64074, 452501.265678375\n",
      "Epoch 66601, Training Loss: 65384, Validation Loss: 65952, 1129356.1416439966\n",
      "Epoch 66701, Training Loss: 66000, Validation Loss: 78089, 354055.49837253644\n",
      "Epoch 66801, Training Loss: 66805, Validation Loss: 76498, 339785.2167470691\n",
      "Epoch 66901, Training Loss: 65114, Validation Loss: 71371, 272156.18072406\n",
      "Epoch 67001, Training Loss: 69264, Validation Loss: 71195, 679216.3215894481\n",
      "Epoch 67101, Training Loss: 64068, Validation Loss: 77911, 86790.60977611528\n",
      "Epoch 67201, Training Loss: 95887, Validation Loss: 81947, 461607.76412333356\n",
      "Epoch 67301, Training Loss: 64891, Validation Loss: 69070, 654537.6523719431\n",
      "Epoch 67401, Training Loss: 66694, Validation Loss: 69293, 758348.0758256145\n",
      "Epoch 67501, Training Loss: 61199, Validation Loss: 77591, 157461.66446544987\n",
      "Epoch 67601, Training Loss: 63574, Validation Loss: 71926, 163274.0602115118\n",
      "Epoch 67701, Training Loss: 66453, Validation Loss: 72476, 198789.4061833136\n",
      "Epoch 67801, Training Loss: 67701, Validation Loss: 67834, 177801.85043094188\n",
      "Epoch 67901, Training Loss: 64551, Validation Loss: 73866, 713232.065257255\n",
      "Epoch 68001, Training Loss: 85593, Validation Loss: 75453, 658137.1941282782\n",
      "Epoch 68101, Training Loss: 75250, Validation Loss: 93595, 904267.2384872878\n",
      "Epoch 68201, Training Loss: 65703, Validation Loss: 74114, 708729.4704266576\n",
      "Epoch 68301, Training Loss: 65687, Validation Loss: 70188, 356310.47245180234\n",
      "Epoch 68401, Training Loss: 66249, Validation Loss: 69598, 347667.0456265492\n",
      "Epoch 68501, Training Loss: 66944, Validation Loss: 69344, 236531.12310576937\n",
      "Epoch 68601, Training Loss: 61817, Validation Loss: 65630, 186541.20079563835\n",
      "Epoch 68701, Training Loss: 66392, Validation Loss: 69140, 240148.59300023256\n",
      "Epoch 68801, Training Loss: 65740, Validation Loss: 85766, 1124218.3268490268\n",
      "Epoch 68901, Training Loss: 63605, Validation Loss: 72418, 468831.2466991632\n",
      "Epoch 69001, Training Loss: 64746, Validation Loss: 78597, 203075.33837727818\n",
      "Epoch 69101, Training Loss: 99994, Validation Loss: 93729, 2681.740061914383\n",
      "Epoch 69201, Training Loss: 97688, Validation Loss: 94058, 1700.7496613212006\n",
      "Epoch 69301, Training Loss: 99659, Validation Loss: 93890, 3619.2553285717477\n",
      "Epoch 69401, Training Loss: 100821, Validation Loss: 93974, 2855.2921315113977\n",
      "Epoch 69501, Training Loss: 96094, Validation Loss: 93784, 2188.094495497182\n",
      "Epoch 69601, Training Loss: 98579, Validation Loss: 93842, 3564.3829607949315\n",
      "Epoch 69701, Training Loss: 96107, Validation Loss: 93786, 2196.594429933202\n",
      "Epoch 69801, Training Loss: 95736, Validation Loss: 93855, 2021.0450249051644\n",
      "Epoch 69901, Training Loss: 100021, Validation Loss: 93817, 2659.1646820571127\n",
      "Epoch 70001, Training Loss: 99195, Validation Loss: 94017, 1544.5528127357584\n",
      "Epoch 70101, Training Loss: 100228, Validation Loss: 93978, 2112.2543470506566\n",
      "Epoch 70201, Training Loss: 98112, Validation Loss: 94101, 1807.373361936697\n",
      "Epoch 70301, Training Loss: 103837, Validation Loss: 93820, 2326.944556421892\n",
      "Epoch 70401, Training Loss: 96170, Validation Loss: 93780, 2607.193123133089\n",
      "Epoch 70501, Training Loss: 100143, Validation Loss: 93705, 1799.8768086395567\n",
      "Epoch 70601, Training Loss: 97075, Validation Loss: 94413, 2510.784072213373\n",
      "Epoch 70701, Training Loss: 103573, Validation Loss: 93905, 2471.6536213755826\n",
      "Epoch 70801, Training Loss: 97439, Validation Loss: 94127, 1522.5617486064032\n",
      "Epoch 70901, Training Loss: 97423, Validation Loss: 94022, 2031.5773273821367\n",
      "Epoch 71001, Training Loss: 99094, Validation Loss: 94202, 4167.733483153337\n",
      "Epoch 71101, Training Loss: 99834, Validation Loss: 94052, 2003.31424982069\n",
      "Epoch 71201, Training Loss: 96405, Validation Loss: 93843, 2812.05111324937\n",
      "Epoch 71301, Training Loss: 99742, Validation Loss: 94038, 1880.9417266438948\n",
      "Epoch 71401, Training Loss: 95836, Validation Loss: 93783, 5682.296650075869\n",
      "Epoch 71501, Training Loss: 101197, Validation Loss: 93879, 1806.5455826753223\n",
      "Epoch 71601, Training Loss: 97145, Validation Loss: 93958, 3098.5282810129943\n",
      "Epoch 71701, Training Loss: 102861, Validation Loss: 93835, 3103.0404503540353\n",
      "Epoch 71801, Training Loss: 98340, Validation Loss: 93970, 2881.626144966544\n",
      "Epoch 71901, Training Loss: 96148, Validation Loss: 93947, 2702.006173475236\n",
      "Epoch 72001, Training Loss: 98030, Validation Loss: 93809, 1747.3107040809089\n",
      "Epoch 72101, Training Loss: 95683, Validation Loss: 94276, 3295.8136903669947\n",
      "Epoch 72201, Training Loss: 94495, Validation Loss: 94136, 1923.4941931113487\n",
      "Epoch 72301, Training Loss: 94478, Validation Loss: 93855, 2424.0521041545458\n",
      "Epoch 72401, Training Loss: 95932, Validation Loss: 94086, 1404.5804961226465\n",
      "Epoch 72501, Training Loss: 97242, Validation Loss: 93937, 2193.3521621813975\n",
      "Epoch 72601, Training Loss: 98102, Validation Loss: 94364, 2663.960581079408\n",
      "Epoch 72701, Training Loss: 98604, Validation Loss: 93902, 3569.989603556341\n",
      "Epoch 72801, Training Loss: 95748, Validation Loss: 94192, 1783.0839977584146\n",
      "Epoch 72901, Training Loss: 96962, Validation Loss: 93784, 3815.6246674009903\n",
      "Epoch 73001, Training Loss: 95928, Validation Loss: 94003, 4626.167851097948\n",
      "Epoch 73101, Training Loss: 97467, Validation Loss: 93776, 4559.97515003629\n",
      "Epoch 73201, Training Loss: 96125, Validation Loss: 93960, 2243.9082704864427\n",
      "Epoch 73301, Training Loss: 100264, Validation Loss: 93848, 1554.114137869464\n",
      "Epoch 73401, Training Loss: 99283, Validation Loss: 94060, 2257.9446232082805\n",
      "Epoch 73501, Training Loss: 96983, Validation Loss: 93967, 1928.3063347942596\n",
      "Epoch 73601, Training Loss: 97542, Validation Loss: 94182, 3037.1874638380427\n",
      "Epoch 73701, Training Loss: 95831, Validation Loss: 94316, 3468.7291024823485\n",
      "Epoch 73801, Training Loss: 97846, Validation Loss: 93693, 2727.350674976297\n",
      "Epoch 73901, Training Loss: 98383, Validation Loss: 93981, 2485.6176270908713\n",
      "Epoch 74001, Training Loss: 99475, Validation Loss: 94264, 2739.629527856925\n",
      "Epoch 74101, Training Loss: 95386, Validation Loss: 94144, 4424.422511908458\n",
      "Epoch 74201, Training Loss: 100971, Validation Loss: 93907, 2040.1801607447308\n",
      "Epoch 74301, Training Loss: 99034, Validation Loss: 94152, 2666.858850019123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74401, Training Loss: 101438, Validation Loss: 93978, 3769.5300163990564\n",
      "Epoch 74501, Training Loss: 110395, Validation Loss: 93697, 376852.2189056619\n",
      "Epoch 74601, Training Loss: 98967, Validation Loss: 94080, 1785.5269568637996\n",
      "Epoch 74701, Training Loss: 96427, Validation Loss: 93971, 2877.4834733803473\n",
      "Epoch 74801, Training Loss: 96611, Validation Loss: 94241, 4937.828982748809\n",
      "Epoch 74901, Training Loss: 94953, Validation Loss: 93931, 4614.876092410771\n",
      "Epoch 75001, Training Loss: 100098, Validation Loss: 93975, 4647.502996397275\n",
      "Epoch 75101, Training Loss: 98417, Validation Loss: 94283, 1372.5084758916798\n",
      "Epoch 75201, Training Loss: 101135, Validation Loss: 94223, 1720.0341485733995\n",
      "Epoch 75301, Training Loss: 100491, Validation Loss: 93966, 1574.5844906357652\n",
      "Epoch 75401, Training Loss: 100052, Validation Loss: 93918, 2437.0891999308196\n",
      "Epoch 75501, Training Loss: 99128, Validation Loss: 93721, 2039.35715061664\n",
      "Epoch 75601, Training Loss: 99781, Validation Loss: 93801, 2635.02228482893\n",
      "Epoch 75701, Training Loss: 101162, Validation Loss: 94415, 2304.6992778524655\n",
      "Epoch 75801, Training Loss: 101697, Validation Loss: 94109, 2443.572137694905\n",
      "Epoch 75901, Training Loss: 97078, Validation Loss: 93877, 2509.833893398591\n",
      "Epoch 76001, Training Loss: 99453, Validation Loss: 93792, 2013.9527175797787\n",
      "Epoch 76101, Training Loss: 99637, Validation Loss: 93937, 2593.938831436395\n",
      "Epoch 76201, Training Loss: 98686, Validation Loss: 94346, 2745.8983221991152\n",
      "Epoch 76301, Training Loss: 95056, Validation Loss: 93888, 2088.002596354772\n",
      "Epoch 76401, Training Loss: 103949, Validation Loss: 93809, 2867.6975723103774\n",
      "Epoch 76501, Training Loss: 98182, Validation Loss: 93807, 2996.0238793353924\n",
      "Epoch 76601, Training Loss: 94680, Validation Loss: 94220, 2043.767314686879\n",
      "Epoch 76701, Training Loss: 97455, Validation Loss: 93764, 3231.0608185246238\n",
      "Epoch 76801, Training Loss: 101886, Validation Loss: 94117, 2323.4066087433853\n",
      "Epoch 76901, Training Loss: 80268, Validation Loss: 81015, 660419.4719972181\n",
      "Epoch 77001, Training Loss: 77059, Validation Loss: 74490, 948194.3301127885\n",
      "Epoch 77101, Training Loss: 72134, Validation Loss: 80691, 1055594.9379046806\n",
      "Epoch 77201, Training Loss: 71729, Validation Loss: 70607, 798621.3842451113\n",
      "Epoch 77301, Training Loss: 72793, Validation Loss: 68951, 1029705.2910483623\n",
      "Epoch 77401, Training Loss: 71025, Validation Loss: 70736, 563688.351192996\n",
      "Epoch 77501, Training Loss: 70455, Validation Loss: 70399, 430730.2651443363\n",
      "Epoch 77601, Training Loss: 65322, Validation Loss: 67142, 971393.1361675983\n",
      "Epoch 77701, Training Loss: 66549, Validation Loss: 73721, 538456.926812386\n",
      "Epoch 77801, Training Loss: 71265, Validation Loss: 68713, 407429.09985900525\n",
      "Epoch 77901, Training Loss: 68261, Validation Loss: 76476, 440957.6633674037\n",
      "Epoch 78001, Training Loss: 68613, Validation Loss: 72545, 1995050.1958046146\n",
      "Epoch 78101, Training Loss: 71464, Validation Loss: 74537, 409717.4306398416\n",
      "Epoch 78201, Training Loss: 66207, Validation Loss: 70784, 699840.6815021083\n",
      "Epoch 78301, Training Loss: 69585, Validation Loss: 74512, 786136.8315050573\n",
      "Epoch 78401, Training Loss: 70793, Validation Loss: 72516, 587775.472464349\n",
      "Epoch 78501, Training Loss: 69713, Validation Loss: 64438, 669058.5735935672\n",
      "Epoch 78601, Training Loss: 65656, Validation Loss: 76765, 1036213.5386064262\n",
      "Epoch 78701, Training Loss: 67901, Validation Loss: 74113, 1087405.2382847052\n",
      "Epoch 78801, Training Loss: 64092, Validation Loss: 74515, 1031195.2589791106\n",
      "Epoch 78901, Training Loss: 63428, Validation Loss: 68547, 503117.77878268925\n",
      "Epoch 79001, Training Loss: 65083, Validation Loss: 76357, 224685.4674327752\n",
      "Epoch 79101, Training Loss: 69049, Validation Loss: 83757, 1025017.3121406325\n",
      "Epoch 79201, Training Loss: 65224, Validation Loss: 71655, 263312.9585117624\n",
      "Epoch 79301, Training Loss: 66165, Validation Loss: 76368, 421472.0276649869\n",
      "Epoch 79401, Training Loss: 68362, Validation Loss: 70344, 873583.8494312578\n",
      "Epoch 79501, Training Loss: 61966, Validation Loss: 64250, 1008013.5965946357\n",
      "Epoch 79601, Training Loss: 63875, Validation Loss: 69871, 554848.2471900876\n",
      "Epoch 79701, Training Loss: 65303, Validation Loss: 80801, 796401.730702852\n",
      "Epoch 79801, Training Loss: 63994, Validation Loss: 80557, 1119428.813205262\n",
      "Epoch 79901, Training Loss: 64232, Validation Loss: 66135, 660020.1590616779\n",
      "Epoch 80001, Training Loss: 63673, Validation Loss: 75477, 730754.6586626247\n",
      "Epoch 80101, Training Loss: 63146, Validation Loss: 70056, 1017563.3670032803\n",
      "Epoch 80201, Training Loss: 68918, Validation Loss: 71264, 401201.726957189\n",
      "Epoch 80301, Training Loss: 63038, Validation Loss: 71249, 850311.5506160986\n",
      "Epoch 80401, Training Loss: 62372, Validation Loss: 81404, 610657.6359793202\n",
      "Epoch 80501, Training Loss: 63644, Validation Loss: 68676, 437150.7841926931\n",
      "Epoch 80601, Training Loss: 67916, Validation Loss: 77275, 550371.0816004643\n",
      "Epoch 80701, Training Loss: 67531, Validation Loss: 87619, 488494.52064598864\n",
      "Epoch 80801, Training Loss: 64020, Validation Loss: 71169, 518898.2705855483\n",
      "Epoch 80901, Training Loss: 64547, Validation Loss: 70534, 1401842.5509836015\n",
      "Epoch 81001, Training Loss: 67273, Validation Loss: 72586, 1498408.6717950094\n",
      "Epoch 81101, Training Loss: 67829, Validation Loss: 68310, 635892.236739518\n",
      "Epoch 81201, Training Loss: 64848, Validation Loss: 74292, 302841.1418302141\n",
      "Epoch 81301, Training Loss: 64980, Validation Loss: 78321, 932912.9484425414\n",
      "Epoch 81401, Training Loss: 66344, Validation Loss: 73854, 655977.8876976535\n",
      "Epoch 81501, Training Loss: 65195, Validation Loss: 79446, 842381.5697307349\n",
      "Epoch 81601, Training Loss: 64421, Validation Loss: 80280, 1331262.9134848074\n",
      "Epoch 81701, Training Loss: 66403, Validation Loss: 77978, 843062.0136632514\n",
      "Epoch 81801, Training Loss: 61679, Validation Loss: 72544, 461351.9610149388\n",
      "Epoch 81901, Training Loss: 68577, Validation Loss: 63985, 1085053.2480156096\n",
      "Epoch 82001, Training Loss: 64425, Validation Loss: 85980, 412264.2409714193\n",
      "Epoch 82101, Training Loss: 62519, Validation Loss: 71378, 395788.92307924625\n",
      "Epoch 82201, Training Loss: 64121, Validation Loss: 79900, 1168822.617737718\n",
      "Epoch 82301, Training Loss: 67107, Validation Loss: 64437, 725566.923666994\n",
      "Epoch 82401, Training Loss: 68774, Validation Loss: 73636, 481434.8863119848\n",
      "Epoch 82501, Training Loss: 65714, Validation Loss: 64022, 575991.5293189563\n",
      "Epoch 82601, Training Loss: 65516, Validation Loss: 69409, 253919.74331391449\n",
      "Epoch 82701, Training Loss: 67378, Validation Loss: 81294, 340778.0234065267\n",
      "Epoch 82801, Training Loss: 66156, Validation Loss: 69909, 920615.9071222424\n",
      "Epoch 82901, Training Loss: 66272, Validation Loss: 69904, 575384.1371438178\n",
      "Epoch 83001, Training Loss: 62120, Validation Loss: 68767, 711350.7526657503\n",
      "Epoch 83101, Training Loss: 65595, Validation Loss: 69749, 736517.6106682735\n",
      "Epoch 83201, Training Loss: 62853, Validation Loss: 72068, 927242.2219352793\n",
      "Epoch 83301, Training Loss: 66103, Validation Loss: 61894, 2843145.822791187\n",
      "Epoch 83401, Training Loss: 65720, Validation Loss: 76552, 1127767.4811274584\n",
      "Epoch 83501, Training Loss: 68822, Validation Loss: 69947, 346221.95052317163\n",
      "Epoch 83601, Training Loss: 63076, Validation Loss: 78987, 423114.94678798405\n",
      "Epoch 83701, Training Loss: 65650, Validation Loss: 85362, 980381.7205163519\n",
      "Epoch 83801, Training Loss: 65008, Validation Loss: 71166, 444230.66958190576\n",
      "Epoch 83901, Training Loss: 62843, Validation Loss: 69312, 653755.9440606904\n",
      "Epoch 84001, Training Loss: 69045, Validation Loss: 80888, 485003.81141744676\n",
      "Epoch 84101, Training Loss: 65131, Validation Loss: 65014, 599491.5488423968\n",
      "Epoch 84201, Training Loss: 63888, Validation Loss: 73752, 322508.3936720907\n",
      "Epoch 84301, Training Loss: 64478, Validation Loss: 70008, 420626.76533275074\n",
      "Epoch 84401, Training Loss: 62948, Validation Loss: 73751, 200712.362622973\n",
      "Epoch 84501, Training Loss: 65296, Validation Loss: 75991, 265591.5965710391\n",
      "Epoch 84601, Training Loss: 64838, Validation Loss: 75711, 432054.94738385914\n",
      "Epoch 84701, Training Loss: 68210, Validation Loss: 70499, 194471.6727243671\n",
      "Epoch 84801, Training Loss: 66572, Validation Loss: 65810, 504087.5623316681\n",
      "Epoch 84901, Training Loss: 100241, Validation Loss: 93902, 2145.1248440512213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85001, Training Loss: 94913, Validation Loss: 94171, 1956.949261219066\n",
      "Epoch 85101, Training Loss: 104021, Validation Loss: 94117, 1921.1275255027056\n",
      "Epoch 85201, Training Loss: 99784, Validation Loss: 93936, 2101.0268793118425\n",
      "Epoch 85301, Training Loss: 99604, Validation Loss: 94277, 2745.076568170279\n",
      "Epoch 85401, Training Loss: 94717, Validation Loss: 94021, 1763.0138535481826\n",
      "Epoch 85501, Training Loss: 96276, Validation Loss: 94207, 2143.1707911514363\n",
      "Epoch 85601, Training Loss: 96789, Validation Loss: 94413, 5018.699240435227\n",
      "Epoch 85701, Training Loss: 97729, Validation Loss: 93798, 2669.993021966977\n",
      "Epoch 85801, Training Loss: 102586, Validation Loss: 94054, 3162.392857311656\n",
      "Epoch 85901, Training Loss: 95947, Validation Loss: 93944, 3930.802550916988\n",
      "Epoch 86001, Training Loss: 98245, Validation Loss: 93788, 2373.4213505587995\n",
      "Epoch 86101, Training Loss: 102746, Validation Loss: 93708, 5237.231426607096\n",
      "Epoch 86201, Training Loss: 96050, Validation Loss: 94545, 4534.808949263753\n",
      "Epoch 86301, Training Loss: 100669, Validation Loss: 93683, 3768.4600182162635\n",
      "Epoch 86401, Training Loss: 101400, Validation Loss: 93958, 1884.2634733177972\n",
      "Epoch 86501, Training Loss: 98828, Validation Loss: 93946, 1714.7999456853577\n",
      "Epoch 86601, Training Loss: 97210, Validation Loss: 94378, 1831.1620342776469\n",
      "Epoch 86701, Training Loss: 96875, Validation Loss: 94305, 2447.3046071025624\n",
      "Epoch 86801, Training Loss: 97410, Validation Loss: 94140, 2891.759518886136\n",
      "Epoch 86901, Training Loss: 98293, Validation Loss: 94140, 1678.0998982346657\n",
      "Epoch 87001, Training Loss: 96174, Validation Loss: 93961, 5670.816007450204\n",
      "Epoch 87101, Training Loss: 98643, Validation Loss: 93921, 3686.9973114381196\n",
      "Epoch 87201, Training Loss: 94971, Validation Loss: 92863, 327717.7748452456\n",
      "Epoch 87301, Training Loss: 75742, Validation Loss: 77115, 563810.4338356144\n",
      "Epoch 87401, Training Loss: 74082, Validation Loss: 72237, 789504.5302004948\n",
      "Epoch 87501, Training Loss: 71426, Validation Loss: 68129, 981678.9144801246\n",
      "Epoch 87601, Training Loss: 70797, Validation Loss: 67593, 653662.2405544152\n",
      "Epoch 87701, Training Loss: 67184, Validation Loss: 72221, 397280.50159021514\n",
      "Epoch 87801, Training Loss: 72310, Validation Loss: 64926, 337277.936541163\n",
      "Epoch 87901, Training Loss: 66459, Validation Loss: 68766, 494191.49382309895\n",
      "Epoch 88001, Training Loss: 67334, Validation Loss: 68548, 455307.8463048809\n",
      "Epoch 88101, Training Loss: 68859, Validation Loss: 68555, 390891.28731958853\n",
      "Epoch 88201, Training Loss: 70173, Validation Loss: 66108, 759884.9466104344\n",
      "Epoch 88301, Training Loss: 66162, Validation Loss: 79803, 830355.2333371235\n",
      "Epoch 88401, Training Loss: 64920, Validation Loss: 70387, 286086.6546947807\n",
      "Epoch 88501, Training Loss: 72017, Validation Loss: 76445, 549796.1255739847\n",
      "Epoch 88601, Training Loss: 64913, Validation Loss: 68312, 535455.7621946693\n",
      "Epoch 88701, Training Loss: 72154, Validation Loss: 74503, 387241.04392713803\n",
      "Epoch 88801, Training Loss: 71125, Validation Loss: 68348, 576972.7504722516\n",
      "Epoch 88901, Training Loss: 72833, Validation Loss: 65755, 550195.8581847184\n",
      "Epoch 89001, Training Loss: 70824, Validation Loss: 70337, 470663.75614374416\n",
      "Epoch 89101, Training Loss: 70623, Validation Loss: 102690, 2367762.89859518\n",
      "Epoch 89201, Training Loss: 70092, Validation Loss: 69392, 831017.0510597773\n",
      "Epoch 89301, Training Loss: 68033, Validation Loss: 66651, 665940.1878041815\n",
      "Epoch 89401, Training Loss: 69211, Validation Loss: 67966, 584131.5995259313\n",
      "Epoch 89501, Training Loss: 96286, Validation Loss: 93933, 2502.238267505228\n",
      "Epoch 89601, Training Loss: 100044, Validation Loss: 93955, 2869.1540433429586\n",
      "Epoch 89701, Training Loss: 99610, Validation Loss: 94106, 3140.3135576770965\n",
      "Epoch 89801, Training Loss: 93616, Validation Loss: 93813, 2500.895552330772\n",
      "Epoch 89901, Training Loss: 98450, Validation Loss: 94362, 2443.377251958393\n",
      "Epoch 90001, Training Loss: 95898, Validation Loss: 94037, 2815.394865023454\n",
      "Epoch 90101, Training Loss: 97007, Validation Loss: 94134, 1959.4968779714407\n",
      "Epoch 90201, Training Loss: 99979, Validation Loss: 94121, 1789.2377606871912\n",
      "Epoch 90301, Training Loss: 96268, Validation Loss: 93835, 5136.537683332309\n",
      "Epoch 90401, Training Loss: 98285, Validation Loss: 93793, 5285.0058135453155\n",
      "Epoch 90501, Training Loss: 97959, Validation Loss: 94152, 4351.888411491978\n",
      "Epoch 90601, Training Loss: 98374, Validation Loss: 93959, 2094.4425243215956\n",
      "Epoch 90701, Training Loss: 99888, Validation Loss: 94101, 3243.9706608580987\n",
      "Epoch 90801, Training Loss: 102120, Validation Loss: 94054, 3344.9755767082274\n",
      "Epoch 90901, Training Loss: 98005, Validation Loss: 94229, 3498.2672469871927\n",
      "Epoch 91001, Training Loss: 97724, Validation Loss: 94525, 2152.0713280376144\n",
      "Epoch 91101, Training Loss: 101106, Validation Loss: 93796, 2249.3282144200602\n",
      "Epoch 91201, Training Loss: 99453, Validation Loss: 94080, 2677.1622388176806\n",
      "Epoch 91301, Training Loss: 99299, Validation Loss: 94028, 4570.343769239407\n",
      "Epoch 91401, Training Loss: 98601, Validation Loss: 93958, 2902.4735843773346\n",
      "Epoch 91501, Training Loss: 96422, Validation Loss: 93854, 3446.6331064056467\n",
      "Epoch 91601, Training Loss: 96241, Validation Loss: 93810, 3368.367705156055\n",
      "Epoch 91701, Training Loss: 96159, Validation Loss: 94018, 1860.712187546537\n",
      "Epoch 91801, Training Loss: 101673, Validation Loss: 94126, 1567.8441208732002\n",
      "Epoch 91901, Training Loss: 103010, Validation Loss: 94284, 3894.438036140702\n",
      "Epoch 92001, Training Loss: 95669, Validation Loss: 94159, 1548.1658007334372\n",
      "Epoch 92101, Training Loss: 95803, Validation Loss: 93914, 2617.3133976576933\n",
      "Epoch 92201, Training Loss: 95285, Validation Loss: 94601, 2673.1021041129748\n",
      "Epoch 92301, Training Loss: 96207, Validation Loss: 94145, 2403.311897698662\n",
      "Epoch 92401, Training Loss: 98665, Validation Loss: 93708, 2952.9080988065994\n",
      "Epoch 92501, Training Loss: 96552, Validation Loss: 93979, 1617.5696615403504\n",
      "Epoch 92601, Training Loss: 100020, Validation Loss: 94329, 2040.9064189546837\n",
      "Epoch 92701, Training Loss: 96535, Validation Loss: 93779, 2299.934227441768\n",
      "Epoch 92801, Training Loss: 98424, Validation Loss: 94081, 1934.5505042230097\n",
      "Epoch 92901, Training Loss: 97128, Validation Loss: 93966, 1838.8130861343616\n",
      "Epoch 93001, Training Loss: 90569, Validation Loss: 93936, 5186.992190321012\n",
      "Epoch 93101, Training Loss: 99008, Validation Loss: 95285, 3959.5299514664925\n",
      "Epoch 93201, Training Loss: 100242, Validation Loss: 94027, 3879.6793866280927\n",
      "Epoch 93301, Training Loss: 99885, Validation Loss: 93779, 4562.741197746495\n",
      "Epoch 93401, Training Loss: 100334, Validation Loss: 94382, 2204.1349910846434\n",
      "Epoch 93501, Training Loss: 94401, Validation Loss: 97928, 210748.24505914515\n",
      "Epoch 93601, Training Loss: 75880, Validation Loss: 67131, 614429.3153631024\n",
      "Epoch 93701, Training Loss: 69984, Validation Loss: 69241, 651684.4684681284\n",
      "Epoch 93801, Training Loss: 66446, Validation Loss: 76740, 694101.5548184426\n",
      "Epoch 93901, Training Loss: 69394, Validation Loss: 75374, 485007.10498927074\n",
      "Epoch 94001, Training Loss: 71056, Validation Loss: 74586, 713761.6327002366\n",
      "Epoch 94101, Training Loss: 71188, Validation Loss: 73990, 753270.0994308084\n",
      "Epoch 94201, Training Loss: 70946, Validation Loss: 73714, 452243.36413143034\n",
      "Epoch 94301, Training Loss: 65631, Validation Loss: 63614, 418805.3454971765\n",
      "Epoch 94401, Training Loss: 67739, Validation Loss: 66699, 383117.70261612086\n",
      "Epoch 94501, Training Loss: 65783, Validation Loss: 72947, 400519.058063011\n",
      "Epoch 94601, Training Loss: 62073, Validation Loss: 72829, 393142.1807858112\n",
      "Epoch 94701, Training Loss: 69526, Validation Loss: 82128, 342591.63262733445\n",
      "Epoch 94801, Training Loss: 70078, Validation Loss: 69672, 404316.7415925958\n",
      "Epoch 94901, Training Loss: 65730, Validation Loss: 74992, 395287.7544274722\n",
      "Epoch 95001, Training Loss: 63380, Validation Loss: 67542, 420714.1570417963\n",
      "Epoch 95101, Training Loss: 69172, Validation Loss: 77366, 456643.6670358774\n",
      "Epoch 95201, Training Loss: 72439, Validation Loss: 83240, 963901.0873766983\n",
      "Epoch 95301, Training Loss: 65667, Validation Loss: 81764, 1069213.7639123953\n",
      "Epoch 95401, Training Loss: 62318, Validation Loss: 67126, 371669.3620976278\n",
      "Epoch 95501, Training Loss: 66774, Validation Loss: 79989, 870784.2323186137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95601, Training Loss: 62033, Validation Loss: 65842, 794938.8040044139\n",
      "Epoch 95701, Training Loss: 65923, Validation Loss: 64115, 908542.9705859289\n",
      "Epoch 95801, Training Loss: 66951, Validation Loss: 73582, 675977.9126378841\n",
      "Epoch 95901, Training Loss: 66751, Validation Loss: 75342, 189933.14875108047\n",
      "Epoch 96001, Training Loss: 65098, Validation Loss: 71693, 640816.749794595\n",
      "Epoch 96101, Training Loss: 68941, Validation Loss: 76682, 489430.7460001979\n",
      "Epoch 96201, Training Loss: 63206, Validation Loss: 65796, 247582.12912151788\n",
      "Epoch 96301, Training Loss: 67117, Validation Loss: 72483, 1098464.285355083\n",
      "Epoch 96401, Training Loss: 63564, Validation Loss: 72648, 264274.8152698384\n",
      "Epoch 96501, Training Loss: 74254, Validation Loss: 76448, 1358719.4506500682\n",
      "Epoch 96601, Training Loss: 68414, Validation Loss: 66098, 788401.7266213461\n",
      "Epoch 96701, Training Loss: 63478, Validation Loss: 64548, 1059001.9947434803\n",
      "Epoch 96801, Training Loss: 62694, Validation Loss: 66581, 534768.3887425293\n",
      "Epoch 96901, Training Loss: 77023, Validation Loss: 91894, 1307452.7642607891\n",
      "Epoch 97001, Training Loss: 64064, Validation Loss: 67824, 703481.3207986258\n",
      "Epoch 97101, Training Loss: 66616, Validation Loss: 64001, 1166658.2390321458\n",
      "Epoch 97201, Training Loss: 68747, Validation Loss: 77157, 510109.0464935428\n",
      "Epoch 97301, Training Loss: 95823, Validation Loss: 62805, 2797722.655530195\n",
      "Epoch 97401, Training Loss: 64551, Validation Loss: 64025, 1006539.3195135413\n",
      "Epoch 97501, Training Loss: 60152, Validation Loss: 60275, 1066073.3205175302\n",
      "Epoch 97601, Training Loss: 63580, Validation Loss: 67577, 240399.88851502686\n",
      "Epoch 97701, Training Loss: 66531, Validation Loss: 73100, 347964.0688543681\n",
      "Epoch 97801, Training Loss: 65897, Validation Loss: 71618, 431356.73574229405\n",
      "Epoch 97901, Training Loss: 67530, Validation Loss: 71554, 135843.6097611064\n",
      "Epoch 98001, Training Loss: 67704, Validation Loss: 84221, 295064.4180595845\n",
      "Epoch 98101, Training Loss: 63168, Validation Loss: 73647, 443354.0783309599\n",
      "Epoch 98201, Training Loss: 78292, Validation Loss: 68587, 1699451.1993895962\n",
      "Epoch 98301, Training Loss: 64498, Validation Loss: 67882, 857214.7238106976\n",
      "Epoch 98401, Training Loss: 67764, Validation Loss: 67162, 412890.15139950876\n",
      "Epoch 98501, Training Loss: 67327, Validation Loss: 76128, 391446.90090433625\n",
      "Epoch 98601, Training Loss: 66720, Validation Loss: 69882, 236299.44656930305\n",
      "Epoch 98701, Training Loss: 65937, Validation Loss: 68283, 383520.9758298977\n",
      "Epoch 98801, Training Loss: 64600, Validation Loss: 67767, 841682.7421983298\n",
      "Epoch 98901, Training Loss: 68218, Validation Loss: 68099, 319931.4233090834\n",
      "Epoch 99001, Training Loss: 63690, Validation Loss: 68752, 346396.28067735967\n",
      "Epoch 99101, Training Loss: 76272, Validation Loss: 74252, 324584.27045101783\n",
      "Epoch 99201, Training Loss: 63465, Validation Loss: 75540, 559987.5449803489\n",
      "Epoch 99301, Training Loss: 58738, Validation Loss: 67186, 642936.8767387966\n",
      "Epoch 99401, Training Loss: 59741, Validation Loss: 66930, 359487.8627125136\n",
      "Epoch 99501, Training Loss: 62873, Validation Loss: 83555, 690327.2017284372\n",
      "Epoch 99601, Training Loss: 57161, Validation Loss: 72434, 397097.632931795\n",
      "Epoch 99701, Training Loss: 59539, Validation Loss: 68626, 420791.3464042304\n",
      "Epoch 99801, Training Loss: 57756, Validation Loss: 70740, 308310.6585727457\n",
      "Epoch 99901, Training Loss: 67277, Validation Loss: 63768, 1902247.568149219\n",
      "Epoch 100001, Training Loss: 57107, Validation Loss: 64883, 737350.6621214091\n",
      "Epoch 100101, Training Loss: 64058, Validation Loss: 73053, 593127.9528713059\n",
      "Epoch 100201, Training Loss: 60466, Validation Loss: 66223, 410365.7381573056\n",
      "Epoch 100301, Training Loss: 60529, Validation Loss: 70813, 672169.4013551646\n",
      "Epoch 100401, Training Loss: 56644, Validation Loss: 65268, 328125.0378882129\n",
      "Epoch 100501, Training Loss: 67067, Validation Loss: 63339, 986770.1522186128\n",
      "Epoch 100601, Training Loss: 58133, Validation Loss: 67143, 359839.2114912076\n",
      "Epoch 100701, Training Loss: 58467, Validation Loss: 69653, 443570.4411694356\n",
      "Epoch 100801, Training Loss: 57783, Validation Loss: 65093, 365875.11346844584\n",
      "Epoch 100901, Training Loss: 58121, Validation Loss: 68388, 519604.6859457485\n",
      "Epoch 101001, Training Loss: 59827, Validation Loss: 63994, 315415.98821794847\n",
      "Epoch 101101, Training Loss: 60156, Validation Loss: 71145, 513962.31393338955\n",
      "Epoch 101201, Training Loss: 57067, Validation Loss: 65009, 387331.06725017517\n",
      "Epoch 101301, Training Loss: 57388, Validation Loss: 67729, 333667.5203238069\n",
      "Epoch 101401, Training Loss: 57504, Validation Loss: 72445, 312716.8826794878\n",
      "Epoch 101501, Training Loss: 55259, Validation Loss: 63155, 282465.9342289536\n",
      "Epoch 101601, Training Loss: 69145, Validation Loss: 61100, 200092.75497707608\n",
      "Epoch 101701, Training Loss: 68614, Validation Loss: 62066, 384231.4590967453\n",
      "Epoch 101801, Training Loss: 72742, Validation Loss: 60566, 541761.9864883695\n",
      "Epoch 101901, Training Loss: 70086, Validation Loss: 59059, 550001.7259962962\n",
      "Epoch 102001, Training Loss: 67877, Validation Loss: 58793, 233368.0728636987\n",
      "Epoch 102101, Training Loss: 69748, Validation Loss: 62836, 301839.03692784073\n",
      "Epoch 102201, Training Loss: 70267, Validation Loss: 59346, 334899.8505684971\n",
      "Epoch 102301, Training Loss: 71187, Validation Loss: 66138, 565933.0941058216\n",
      "Epoch 102401, Training Loss: 73152, Validation Loss: 64240, 686393.3853098989\n",
      "Epoch 102501, Training Loss: 69195, Validation Loss: 62361, 220262.31562085147\n",
      "Epoch 102601, Training Loss: 72609, Validation Loss: 61636, 686834.360211\n",
      "Epoch 102701, Training Loss: 65626, Validation Loss: 59187, 242254.8913360485\n",
      "Epoch 102801, Training Loss: 69323, Validation Loss: 62905, 692786.3441702089\n",
      "Epoch 102901, Training Loss: 74053, Validation Loss: 60897, 527182.8210149092\n",
      "Epoch 103001, Training Loss: 75336, Validation Loss: 58698, 448466.8664925829\n",
      "Epoch 103101, Training Loss: 69118, Validation Loss: 60505, 282822.9083413251\n",
      "Epoch 103201, Training Loss: 75190, Validation Loss: 67099, 685808.3446676641\n",
      "Epoch 103301, Training Loss: 67857, Validation Loss: 60651, 223418.65577213478\n",
      "Epoch 103401, Training Loss: 66855, Validation Loss: 59694, 369775.56121978624\n",
      "Epoch 103501, Training Loss: 68132, Validation Loss: 60811, 452768.4895809849\n",
      "Epoch 103601, Training Loss: 65972, Validation Loss: 62657, 349872.93849275185\n",
      "Epoch 103701, Training Loss: 63971, Validation Loss: 65385, 680381.5929888239\n",
      "Epoch 103801, Training Loss: 62241, Validation Loss: 59331, 1059799.0705454382\n",
      "Epoch 103901, Training Loss: 61582, Validation Loss: 70991, 668266.683858011\n",
      "Epoch 104001, Training Loss: 62473, Validation Loss: 58782, 830907.8479105862\n",
      "Epoch 104101, Training Loss: 59229, Validation Loss: 57527, 927236.6214776355\n",
      "Epoch 104201, Training Loss: 57643, Validation Loss: 61608, 435987.20421348495\n",
      "Epoch 104301, Training Loss: 62054, Validation Loss: 61776, 629946.3526032596\n",
      "Epoch 104401, Training Loss: 56426, Validation Loss: 64045, 745127.5423401598\n",
      "Epoch 104501, Training Loss: 63042, Validation Loss: 63122, 1006117.7289594575\n",
      "Epoch 104601, Training Loss: 57671, Validation Loss: 60154, 404913.61387744645\n",
      "Epoch 104701, Training Loss: 59241, Validation Loss: 63703, 371396.3757430326\n",
      "Epoch 104801, Training Loss: 61135, Validation Loss: 61256, 1232855.9002683158\n",
      "Epoch 104901, Training Loss: 57243, Validation Loss: 60703, 292687.6760088259\n",
      "Epoch 105001, Training Loss: 58052, Validation Loss: 64281, 347013.5974947064\n",
      "Epoch 105101, Training Loss: 56293, Validation Loss: 75079, 649604.8154867728\n",
      "Epoch 105201, Training Loss: 55584, Validation Loss: 64200, 242369.14593449107\n",
      "Epoch 105301, Training Loss: 58829, Validation Loss: 59805, 477845.5189152185\n",
      "Epoch 105401, Training Loss: 59484, Validation Loss: 64877, 702533.1746951455\n",
      "Epoch 105501, Training Loss: 61246, Validation Loss: 62021, 513244.91008311417\n",
      "Epoch 105601, Training Loss: 58735, Validation Loss: 68897, 966336.3173599369\n",
      "Epoch 105701, Training Loss: 58647, Validation Loss: 68609, 625982.632112855\n",
      "Epoch 105801, Training Loss: 57434, Validation Loss: 64615, 729426.5471859174\n",
      "Epoch 105901, Training Loss: 61644, Validation Loss: 63376, 461763.7890970202\n",
      "Epoch 106001, Training Loss: 57743, Validation Loss: 56784, 454846.89104309166\n",
      "Epoch 106101, Training Loss: 60582, Validation Loss: 56507, 1328726.9663723994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106201, Training Loss: 60528, Validation Loss: 66121, 685010.3698228889\n",
      "Epoch 106301, Training Loss: 63221, Validation Loss: 80865, 595542.1435458864\n",
      "Epoch 106401, Training Loss: 57347, Validation Loss: 65061, 394529.6907316085\n",
      "Epoch 106501, Training Loss: 61525, Validation Loss: 64086, 388239.80392319616\n",
      "Epoch 106601, Training Loss: 59234, Validation Loss: 61735, 412860.51927820226\n",
      "Epoch 106701, Training Loss: 57373, Validation Loss: 62296, 360213.63499049545\n",
      "Epoch 106801, Training Loss: 59812, Validation Loss: 68091, 822522.5267864079\n",
      "Epoch 106901, Training Loss: 67425, Validation Loss: 69392, 1401018.5028117448\n",
      "Epoch 107001, Training Loss: 72114, Validation Loss: 59354, 651015.2126993528\n",
      "Epoch 107101, Training Loss: 72232, Validation Loss: 61469, 876472.4935348779\n",
      "Epoch 107201, Training Loss: 70741, Validation Loss: 68247, 1106425.2113617293\n",
      "Epoch 107301, Training Loss: 67184, Validation Loss: 63713, 728586.5777853788\n",
      "Epoch 107401, Training Loss: 71996, Validation Loss: 65023, 1563743.4569818585\n",
      "Epoch 107501, Training Loss: 75470, Validation Loss: 80432, 1398118.0119690169\n",
      "Epoch 107601, Training Loss: 69658, Validation Loss: 59709, 1294245.7465917477\n",
      "Epoch 107701, Training Loss: 68813, Validation Loss: 62060, 1048273.5964158302\n",
      "Epoch 107801, Training Loss: 69993, Validation Loss: 69340, 253914.09742851116\n",
      "Epoch 107901, Training Loss: 68766, Validation Loss: 59305, 1060173.608994108\n",
      "Epoch 108001, Training Loss: 66359, Validation Loss: 62300, 942748.0622304244\n",
      "Epoch 108101, Training Loss: 80581, Validation Loss: 62894, 1445885.685971246\n",
      "Epoch 108201, Training Loss: 67378, Validation Loss: 62066, 779064.2088127406\n",
      "Epoch 108301, Training Loss: 70008, Validation Loss: 74960, 315597.83349894895\n",
      "Epoch 108401, Training Loss: 67690, Validation Loss: 73380, 717489.9764235221\n",
      "Epoch 108501, Training Loss: 67549, Validation Loss: 67077, 743717.1084113438\n",
      "Epoch 108601, Training Loss: 70155, Validation Loss: 61629, 890111.7278232261\n",
      "Epoch 108701, Training Loss: 66527, Validation Loss: 62561, 360063.61243370705\n",
      "Epoch 108801, Training Loss: 70860, Validation Loss: 95109, 4595809.678963036\n",
      "Epoch 108901, Training Loss: 95965, Validation Loss: 93737, 6839.388850686989\n",
      "Epoch 109001, Training Loss: 103588, Validation Loss: 93799, 7523.818768606147\n",
      "Epoch 109101, Training Loss: 98860, Validation Loss: 93857, 7737.612399065726\n",
      "Epoch 109201, Training Loss: 99680, Validation Loss: 93886, 7287.923044037062\n",
      "Epoch 109301, Training Loss: 97692, Validation Loss: 94316, 4126.5310637270895\n",
      "Epoch 109401, Training Loss: 98524, Validation Loss: 94027, 7062.617971668296\n",
      "Epoch 109501, Training Loss: 97647, Validation Loss: 93960, 4058.8924237398874\n",
      "Epoch 109601, Training Loss: 99590, Validation Loss: 94183, 7201.953687299933\n",
      "Epoch 109701, Training Loss: 99853, Validation Loss: 94349, 9008.799539930056\n",
      "Epoch 109801, Training Loss: 100216, Validation Loss: 93756, 5686.638446424131\n",
      "Epoch 109901, Training Loss: 98050, Validation Loss: 93819, 5170.447004251385\n",
      "Epoch 110001, Training Loss: 99355, Validation Loss: 94139, 9149.464656104108\n",
      "Epoch 110101, Training Loss: 97143, Validation Loss: 94596, 4871.8302698336\n",
      "Epoch 110201, Training Loss: 96623, Validation Loss: 93963, 4803.891265831838\n",
      "Epoch 110301, Training Loss: 103629, Validation Loss: 113830, 22945.063334447565\n",
      "Epoch 110401, Training Loss: 78524, Validation Loss: 77794, 132094.79893238106\n",
      "Epoch 110501, Training Loss: 70832, Validation Loss: 77159, 163108.76348947446\n",
      "Epoch 110601, Training Loss: 67528, Validation Loss: 68178, 340475.8086618556\n",
      "Epoch 110701, Training Loss: 69990, Validation Loss: 65457, 289163.99344389455\n",
      "Epoch 110801, Training Loss: 70583, Validation Loss: 74171, 624906.919845549\n",
      "Epoch 110901, Training Loss: 69137, Validation Loss: 71704, 280143.2228092501\n",
      "Epoch 111001, Training Loss: 71776, Validation Loss: 69342, 686727.7398563732\n",
      "Epoch 111101, Training Loss: 69210, Validation Loss: 74848, 97995.28711269733\n",
      "Epoch 111201, Training Loss: 71129, Validation Loss: 69480, 397350.14702969114\n",
      "Epoch 111301, Training Loss: 69457, Validation Loss: 75463, 332940.0319806085\n",
      "Epoch 111401, Training Loss: 70247, Validation Loss: 70556, 591549.5366530223\n",
      "Epoch 111501, Training Loss: 68748, Validation Loss: 65277, 245916.03361124836\n",
      "Epoch 111601, Training Loss: 69285, Validation Loss: 75701, 259599.11695117003\n",
      "Epoch 111701, Training Loss: 69292, Validation Loss: 68888, 310150.11470025516\n",
      "Epoch 111801, Training Loss: 68014, Validation Loss: 75620, 218269.02032897688\n",
      "Epoch 111901, Training Loss: 72245, Validation Loss: 82309, 1410109.2399073646\n",
      "Epoch 112001, Training Loss: 65644, Validation Loss: 65192, 525332.2701005928\n",
      "Epoch 112101, Training Loss: 66581, Validation Loss: 68004, 179771.38320330964\n",
      "Epoch 112201, Training Loss: 68819, Validation Loss: 63750, 281059.94982156117\n",
      "Epoch 112301, Training Loss: 79514, Validation Loss: 76896, 873972.3052560169\n",
      "Epoch 112401, Training Loss: 70836, Validation Loss: 63778, 595558.3152029394\n",
      "Epoch 112501, Training Loss: 65951, Validation Loss: 82779, 282280.11272560834\n",
      "Epoch 112601, Training Loss: 74032, Validation Loss: 90816, 468845.86066047865\n",
      "Epoch 112701, Training Loss: 66093, Validation Loss: 70887, 370132.19291946216\n",
      "Epoch 112801, Training Loss: 68736, Validation Loss: 73753, 859895.9833515962\n",
      "Epoch 112901, Training Loss: 68254, Validation Loss: 69810, 652953.4296079875\n",
      "Epoch 113001, Training Loss: 66685, Validation Loss: 75630, 455575.9536812434\n",
      "Epoch 113101, Training Loss: 70083, Validation Loss: 72288, 195808.87526088426\n",
      "Epoch 113201, Training Loss: 62781, Validation Loss: 75804, 279669.84754420165\n",
      "Epoch 113301, Training Loss: 67187, Validation Loss: 65401, 565057.4065324414\n",
      "Epoch 113401, Training Loss: 70943, Validation Loss: 82111, 589529.265351997\n",
      "Epoch 113501, Training Loss: 61009, Validation Loss: 111001, 2943287.6844474305\n",
      "Epoch 113601, Training Loss: 102495, Validation Loss: 94074, 6354.85994543547\n",
      "Epoch 113701, Training Loss: 94874, Validation Loss: 93815, 4351.498585696561\n",
      "Epoch 113801, Training Loss: 97657, Validation Loss: 94329, 6030.9551711971\n",
      "Epoch 113901, Training Loss: 97367, Validation Loss: 94021, 7693.944776110103\n",
      "Epoch 114001, Training Loss: 98737, Validation Loss: 93740, 4949.302420226933\n",
      "Epoch 114101, Training Loss: 96352, Validation Loss: 94077, 10880.532953987355\n",
      "Epoch 114201, Training Loss: 95706, Validation Loss: 93804, 4931.358609650373\n",
      "Epoch 114301, Training Loss: 98195, Validation Loss: 94067, 7682.3509610962765\n",
      "Epoch 114401, Training Loss: 97306, Validation Loss: 93688, 6698.761195586521\n",
      "Epoch 114501, Training Loss: 99017, Validation Loss: 93822, 5723.362999920732\n",
      "Epoch 114601, Training Loss: 93405, Validation Loss: 93954, 4542.320213287912\n",
      "Epoch 114701, Training Loss: 97926, Validation Loss: 94132, 4310.542383053421\n",
      "Epoch 114801, Training Loss: 99426, Validation Loss: 94359, 8196.410866532235\n",
      "Epoch 114901, Training Loss: 93176, Validation Loss: 93774, 4247.5063183790735\n",
      "Epoch 115001, Training Loss: 98235, Validation Loss: 93898, 4759.833913661595\n",
      "Epoch 115101, Training Loss: 99381, Validation Loss: 94014, 5011.467549528042\n",
      "Epoch 115201, Training Loss: 98692, Validation Loss: 94089, 4366.724531708234\n",
      "Epoch 115301, Training Loss: 94536, Validation Loss: 93978, 7161.205320997084\n",
      "Epoch 115401, Training Loss: 98810, Validation Loss: 94458, 6358.77828985214\n",
      "Epoch 115501, Training Loss: 97811, Validation Loss: 93733, 8074.566495808768\n",
      "Epoch 115601, Training Loss: 96668, Validation Loss: 94072, 8486.7466801041\n",
      "Epoch 115701, Training Loss: 98408, Validation Loss: 93704, 6550.9559380918\n",
      "Epoch 115801, Training Loss: 94304, Validation Loss: 93804, 6893.622610813778\n",
      "Epoch 115901, Training Loss: 93835, Validation Loss: 94202, 5103.106931839077\n",
      "Epoch 116001, Training Loss: 95983, Validation Loss: 93884, 5650.964034746063\n",
      "Epoch 116101, Training Loss: 102643, Validation Loss: 97622, 19129.134853753483\n",
      "Epoch 116201, Training Loss: 95902, Validation Loss: 94014, 3440.8458920187204\n",
      "Epoch 116301, Training Loss: 100356, Validation Loss: 93837, 6160.753345494751\n",
      "Epoch 116401, Training Loss: 101480, Validation Loss: 93812, 2889.17514042598\n",
      "Epoch 116501, Training Loss: 95897, Validation Loss: 93964, 7816.102143582997\n",
      "Epoch 116601, Training Loss: 97870, Validation Loss: 93730, 7191.620611654995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116701, Training Loss: 95481, Validation Loss: 94597, 4524.398892894055\n",
      "Epoch 116801, Training Loss: 99961, Validation Loss: 94136, 7298.857277780279\n",
      "Epoch 116901, Training Loss: 98834, Validation Loss: 93819, 5734.083451397718\n",
      "Epoch 117001, Training Loss: 102211, Validation Loss: 94329, 6951.52898006071\n",
      "Epoch 117101, Training Loss: 98612, Validation Loss: 93681, 5323.618149733385\n",
      "Epoch 117201, Training Loss: 95199, Validation Loss: 94478, 6117.159254192381\n",
      "Epoch 117301, Training Loss: 96971, Validation Loss: 94017, 5441.092145740057\n",
      "Epoch 117401, Training Loss: 101788, Validation Loss: 94066, 4276.125417085766\n",
      "Epoch 117501, Training Loss: 96879, Validation Loss: 93715, 5461.079377595044\n",
      "Epoch 117601, Training Loss: 98629, Validation Loss: 94968, 5790.2017684142775\n",
      "Epoch 117701, Training Loss: 96740, Validation Loss: 94184, 6275.954157127421\n",
      "Epoch 117801, Training Loss: 99888, Validation Loss: 93874, 4276.473391896532\n",
      "Epoch 117901, Training Loss: 96800, Validation Loss: 94131, 4458.170135985078\n",
      "Epoch 118001, Training Loss: 95926, Validation Loss: 93768, 6248.92097185319\n",
      "Epoch 118101, Training Loss: 97615, Validation Loss: 93810, 4784.963535645173\n",
      "Epoch 118201, Training Loss: 96181, Validation Loss: 94261, 7425.475783140129\n",
      "Epoch 118301, Training Loss: 96814, Validation Loss: 93723, 5266.865445543357\n",
      "Epoch 118401, Training Loss: 97650, Validation Loss: 94165, 6911.1525403934065\n",
      "Epoch 118501, Training Loss: 96811, Validation Loss: 94407, 4256.616957274269\n",
      "Epoch 118601, Training Loss: 94720, Validation Loss: 94211, 3883.102866243488\n",
      "Epoch 118701, Training Loss: 96604, Validation Loss: 93836, 8739.976640957844\n",
      "Epoch 118801, Training Loss: 98969, Validation Loss: 94101, 7084.524200442868\n",
      "Epoch 118901, Training Loss: 100691, Validation Loss: 94067, 3289.3974724474247\n",
      "Epoch 119001, Training Loss: 99024, Validation Loss: 93799, 4890.458007891091\n",
      "Epoch 119101, Training Loss: 95876, Validation Loss: 93904, 5965.978131765453\n",
      "Epoch 119201, Training Loss: 99415, Validation Loss: 93864, 6549.46408216175\n",
      "Epoch 119301, Training Loss: 97001, Validation Loss: 94235, 3198.289451203309\n",
      "Epoch 119401, Training Loss: 100628, Validation Loss: 93810, 7578.226308578157\n",
      "Epoch 119501, Training Loss: 94717, Validation Loss: 93705, 7528.557083406548\n",
      "Epoch 119601, Training Loss: 94523, Validation Loss: 93815, 3028.261113390318\n",
      "Epoch 119701, Training Loss: 95949, Validation Loss: 94596, 3913.977926871612\n",
      "Epoch 119801, Training Loss: 96827, Validation Loss: 93772, 3571.822960011645\n",
      "Epoch 119901, Training Loss: 95637, Validation Loss: 94408, 5458.638630545091\n",
      "Epoch 120001, Training Loss: 97820, Validation Loss: 93911, 4480.583639590475\n",
      "Epoch 120101, Training Loss: 94918, Validation Loss: 93746, 7470.641982020358\n",
      "Epoch 120201, Training Loss: 99907, Validation Loss: 94026, 3413.531118171502\n",
      "Epoch 120301, Training Loss: 97961, Validation Loss: 93721, 6305.286092127615\n",
      "Epoch 120401, Training Loss: 103992, Validation Loss: 94242, 4562.697638916588\n",
      "Epoch 120501, Training Loss: 95346, Validation Loss: 93781, 4173.120901491689\n",
      "Epoch 120601, Training Loss: 98652, Validation Loss: 93821, 4626.901889467804\n",
      "Epoch 120701, Training Loss: 96818, Validation Loss: 94036, 2564.2505356812026\n",
      "Epoch 120801, Training Loss: 96017, Validation Loss: 94369, 6675.519154656616\n",
      "Epoch 120901, Training Loss: 102795, Validation Loss: 94477, 5317.750730690491\n",
      "Epoch 121001, Training Loss: 97667, Validation Loss: 93759, 6204.340342234948\n",
      "Epoch 121101, Training Loss: 97215, Validation Loss: 94096, 3226.7575238426457\n",
      "Epoch 121201, Training Loss: 94799, Validation Loss: 94164, 4143.1336561127255\n",
      "Epoch 121301, Training Loss: 100175, Validation Loss: 94343, 4046.8060960511175\n",
      "Epoch 121401, Training Loss: 97084, Validation Loss: 94355, 3486.4080166259314\n",
      "Epoch 121501, Training Loss: 95038, Validation Loss: 94255, 7346.1954765763085\n",
      "Epoch 121601, Training Loss: 100024, Validation Loss: 93767, 7356.895029110154\n",
      "Epoch 121701, Training Loss: 103607, Validation Loss: 94526, 9066.407271868286\n",
      "Epoch 121801, Training Loss: 98216, Validation Loss: 93810, 14978.75033561613\n",
      "Epoch 121901, Training Loss: 97550, Validation Loss: 94127, 2958.1408383217326\n",
      "Epoch 122001, Training Loss: 102570, Validation Loss: 93690, 3802.9850292300616\n",
      "Epoch 122101, Training Loss: 92910, Validation Loss: 93726, 8172.154889514546\n",
      "Epoch 122201, Training Loss: 98793, Validation Loss: 94170, 5741.906135493915\n",
      "Epoch 122301, Training Loss: 99454, Validation Loss: 94315, 4380.269187985371\n",
      "Epoch 122401, Training Loss: 97945, Validation Loss: 93921, 5029.742183103716\n",
      "Epoch 122501, Training Loss: 99015, Validation Loss: 94088, 2859.9141220222677\n",
      "Epoch 122601, Training Loss: 93093, Validation Loss: 94649, 5147.043144740927\n",
      "Epoch 122701, Training Loss: 99278, Validation Loss: 93710, 6102.298605010411\n",
      "Epoch 122801, Training Loss: 98264, Validation Loss: 94384, 5558.813479578013\n",
      "Epoch 122901, Training Loss: 100939, Validation Loss: 93702, 5073.822651679789\n",
      "Epoch 123001, Training Loss: 97650, Validation Loss: 93715, 3615.7166473277334\n",
      "Epoch 123101, Training Loss: 96482, Validation Loss: 94306, 3244.9799189187543\n",
      "Epoch 123201, Training Loss: 96939, Validation Loss: 93977, 2672.5146015075734\n",
      "Epoch 123301, Training Loss: 99417, Validation Loss: 94013, 2830.909478236747\n",
      "Epoch 123401, Training Loss: 97118, Validation Loss: 93761, 4226.316164928525\n",
      "Epoch 123501, Training Loss: 92859, Validation Loss: 93867, 5886.093896873051\n",
      "Epoch 123601, Training Loss: 98034, Validation Loss: 94147, 5386.1438552726695\n",
      "Epoch 123701, Training Loss: 96869, Validation Loss: 93815, 5508.491024315689\n",
      "Epoch 123801, Training Loss: 97302, Validation Loss: 93757, 9256.866319198862\n",
      "Epoch 123901, Training Loss: 102285, Validation Loss: 93745, 5805.442153945415\n",
      "Epoch 124001, Training Loss: 99210, Validation Loss: 94052, 2950.4464161331166\n",
      "Epoch 124101, Training Loss: 97418, Validation Loss: 94292, 3484.670330198797\n",
      "Epoch 124201, Training Loss: 99170, Validation Loss: 93981, 2667.4508905537227\n",
      "Epoch 124301, Training Loss: 97688, Validation Loss: 94476, 2787.092768562303\n",
      "Epoch 124401, Training Loss: 96417, Validation Loss: 94019, 2720.7937157293395\n",
      "Epoch 124501, Training Loss: 99997, Validation Loss: 93772, 5340.864494283032\n",
      "Epoch 124601, Training Loss: 97020, Validation Loss: 94366, 4590.265775448957\n",
      "Epoch 124701, Training Loss: 97503, Validation Loss: 94676, 3983.409442837828\n",
      "Epoch 124801, Training Loss: 97517, Validation Loss: 94063, 3834.980704774373\n",
      "Epoch 124901, Training Loss: 98147, Validation Loss: 94183, 5753.531788202718\n",
      "Epoch 125001, Training Loss: 93258, Validation Loss: 93776, 6568.53165644057\n",
      "Epoch 125101, Training Loss: 96649, Validation Loss: 94634, 7361.075064238117\n",
      "Epoch 125201, Training Loss: 96580, Validation Loss: 93982, 3439.7507040370997\n",
      "Epoch 125301, Training Loss: 100115, Validation Loss: 93729, 4553.253797590735\n",
      "Epoch 125401, Training Loss: 102107, Validation Loss: 94509, 3077.509500843922\n",
      "Epoch 125501, Training Loss: 96237, Validation Loss: 93722, 7335.235939127997\n",
      "Epoch 125601, Training Loss: 98957, Validation Loss: 93887, 3164.413258598768\n",
      "Epoch 125701, Training Loss: 101357, Validation Loss: 94194, 4385.6338138245155\n",
      "Epoch 125801, Training Loss: 100030, Validation Loss: 94010, 6327.624073814238\n",
      "Epoch 125901, Training Loss: 98207, Validation Loss: 94266, 4570.0525689558635\n",
      "Epoch 126001, Training Loss: 98128, Validation Loss: 94052, 2243.2048097222\n",
      "Epoch 126101, Training Loss: 95043, Validation Loss: 93910, 3761.5415134323594\n",
      "Epoch 126201, Training Loss: 97902, Validation Loss: 94742, 4537.172529259303\n",
      "Epoch 126301, Training Loss: 96380, Validation Loss: 94000, 2994.272357429915\n",
      "Epoch 126401, Training Loss: 101823, Validation Loss: 94072, 3275.286824329971\n",
      "Epoch 126501, Training Loss: 99013, Validation Loss: 93736, 5586.570030395848\n",
      "Epoch 126601, Training Loss: 101136, Validation Loss: 94026, 5207.202862042725\n",
      "Epoch 126701, Training Loss: 83457, Validation Loss: 90280, 82346.40263749189\n",
      "Epoch 126801, Training Loss: 70119, Validation Loss: 82190, 301859.2556922448\n",
      "Epoch 126901, Training Loss: 67126, Validation Loss: 86632, 664775.0490076089\n",
      "Epoch 127001, Training Loss: 67646, Validation Loss: 70776, 466758.8364695218\n",
      "Epoch 127101, Training Loss: 72109, Validation Loss: 73660, 471383.85465545097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127201, Training Loss: 66665, Validation Loss: 77914, 181175.46062829762\n",
      "Epoch 127301, Training Loss: 68147, Validation Loss: 79247, 240255.53609880016\n",
      "Epoch 127401, Training Loss: 73086, Validation Loss: 68664, 1070686.9891323876\n",
      "Epoch 127501, Training Loss: 71268, Validation Loss: 78681, 143085.4503715119\n",
      "Epoch 127601, Training Loss: 67776, Validation Loss: 66660, 705066.7283168201\n",
      "Epoch 127701, Training Loss: 66949, Validation Loss: 71658, 243409.7821979459\n",
      "Epoch 127801, Training Loss: 63272, Validation Loss: 73169, 371141.9088555433\n",
      "Epoch 127901, Training Loss: 65043, Validation Loss: 77466, 649180.8521540825\n",
      "Epoch 128001, Training Loss: 66249, Validation Loss: 86504, 338419.359388611\n",
      "Epoch 128101, Training Loss: 64347, Validation Loss: 66518, 754772.7388098316\n",
      "Epoch 128201, Training Loss: 64293, Validation Loss: 72629, 269245.2433118842\n",
      "Epoch 128301, Training Loss: 66348, Validation Loss: 71142, 446768.16276262613\n",
      "Epoch 128401, Training Loss: 71162, Validation Loss: 72680, 797396.2635840261\n",
      "Epoch 128501, Training Loss: 68333, Validation Loss: 69334, 446712.85858195275\n",
      "Epoch 128601, Training Loss: 65187, Validation Loss: 73138, 700981.6085507296\n",
      "Epoch 128701, Training Loss: 65687, Validation Loss: 70570, 483315.2148321097\n",
      "Epoch 128801, Training Loss: 66214, Validation Loss: 76691, 433296.4732119131\n",
      "Epoch 128901, Training Loss: 64823, Validation Loss: 67316, 292274.76768085855\n",
      "Epoch 129001, Training Loss: 70649, Validation Loss: 87495, 1603438.3881932758\n",
      "Epoch 129101, Training Loss: 66235, Validation Loss: 65512, 1008036.2450282391\n",
      "Epoch 129201, Training Loss: 62957, Validation Loss: 70893, 465981.5761681178\n",
      "Epoch 129301, Training Loss: 64335, Validation Loss: 73230, 501842.59677554487\n",
      "Epoch 129401, Training Loss: 64973, Validation Loss: 77221, 569780.5920909053\n",
      "Epoch 129501, Training Loss: 69587, Validation Loss: 67637, 2172751.602953265\n",
      "Epoch 129601, Training Loss: 71406, Validation Loss: 81423, 559623.9093727118\n",
      "Epoch 129701, Training Loss: 66087, Validation Loss: 79283, 926704.4584575839\n",
      "Epoch 129801, Training Loss: 63815, Validation Loss: 73416, 573948.7651820556\n",
      "Epoch 129901, Training Loss: 68522, Validation Loss: 65862, 1097502.871193093\n",
      "Epoch 130001, Training Loss: 68812, Validation Loss: 67715, 683341.1593045015\n",
      "Epoch 130101, Training Loss: 66200, Validation Loss: 77853, 204211.92321520534\n",
      "Epoch 130201, Training Loss: 67286, Validation Loss: 83023, 304037.041944181\n",
      "Epoch 130301, Training Loss: 69530, Validation Loss: 59394, 1780901.3933491223\n",
      "Epoch 130401, Training Loss: 63294, Validation Loss: 71644, 483269.02760643215\n",
      "Epoch 130501, Training Loss: 72554, Validation Loss: 73415, 657121.0816085589\n",
      "Epoch 130601, Training Loss: 62418, Validation Loss: 76707, 331479.8647050537\n",
      "Epoch 130701, Training Loss: 60110, Validation Loss: 72543, 305139.3164696349\n",
      "Epoch 130801, Training Loss: 64233, Validation Loss: 68257, 338962.55281224847\n",
      "Epoch 130901, Training Loss: 61708, Validation Loss: 80499, 250155.64268508306\n",
      "Epoch 131001, Training Loss: 65314, Validation Loss: 74580, 1214005.6478362468\n",
      "Epoch 131101, Training Loss: 65285, Validation Loss: 78524, 745875.7654674682\n",
      "Epoch 131201, Training Loss: 66320, Validation Loss: 66097, 1204810.047785239\n",
      "Epoch 131301, Training Loss: 65471, Validation Loss: 67028, 1164817.695226607\n",
      "Epoch 131401, Training Loss: 64730, Validation Loss: 73241, 477107.321403249\n",
      "Epoch 131501, Training Loss: 67234, Validation Loss: 66301, 238324.08250102692\n",
      "Epoch 131601, Training Loss: 67044, Validation Loss: 81017, 1280425.901882497\n",
      "Epoch 131701, Training Loss: 63718, Validation Loss: 71981, 264409.2503131335\n",
      "Epoch 131801, Training Loss: 97974, Validation Loss: 94525, 67905.42935477769\n",
      "Epoch 131901, Training Loss: 97960, Validation Loss: 93958, 1982.9511824551985\n",
      "Epoch 132001, Training Loss: 96911, Validation Loss: 93868, 2592.1020769891707\n",
      "Epoch 132101, Training Loss: 97715, Validation Loss: 94016, 3003.4273452557827\n",
      "Epoch 132201, Training Loss: 99344, Validation Loss: 94152, 2039.9296567599524\n",
      "Epoch 132301, Training Loss: 101812, Validation Loss: 95781, 4200.6777038846785\n",
      "Epoch 132401, Training Loss: 98796, Validation Loss: 94243, 2230.857326967179\n",
      "Epoch 132501, Training Loss: 100117, Validation Loss: 94213, 1596.923617767672\n",
      "Epoch 132601, Training Loss: 98585, Validation Loss: 94362, 5350.955866635867\n",
      "Epoch 132701, Training Loss: 99242, Validation Loss: 93992, 1833.565056279019\n",
      "Epoch 132801, Training Loss: 98354, Validation Loss: 93765, 2596.7757002172734\n",
      "Epoch 132901, Training Loss: 103301, Validation Loss: 93981, 4214.6771961375625\n",
      "Epoch 133001, Training Loss: 102771, Validation Loss: 93953, 3290.004776773725\n",
      "Epoch 133101, Training Loss: 93902, Validation Loss: 93805, 1837.9232746606078\n",
      "Epoch 133201, Training Loss: 99491, Validation Loss: 94436, 2401.411270422744\n",
      "Epoch 133301, Training Loss: 96892, Validation Loss: 96952, 61582.21623840206\n",
      "Epoch 133401, Training Loss: 71270, Validation Loss: 81471, 77119.43740197488\n",
      "Epoch 133501, Training Loss: 70367, Validation Loss: 65734, 652161.6815557256\n",
      "Epoch 133601, Training Loss: 70913, Validation Loss: 75827, 563316.5084680717\n",
      "Epoch 133701, Training Loss: 68305, Validation Loss: 81398, 412633.1980480002\n",
      "Epoch 133801, Training Loss: 70867, Validation Loss: 78303, 1048626.5711314224\n",
      "Epoch 133901, Training Loss: 74646, Validation Loss: 67757, 606354.6666640133\n",
      "Epoch 134001, Training Loss: 68597, Validation Loss: 65766, 615507.6666147019\n",
      "Epoch 134101, Training Loss: 72071, Validation Loss: 68945, 525130.8948966232\n",
      "Epoch 134201, Training Loss: 67726, Validation Loss: 74160, 367927.87335561734\n",
      "Epoch 134301, Training Loss: 65723, Validation Loss: 68683, 225295.98975613632\n",
      "Epoch 134401, Training Loss: 66051, Validation Loss: 76973, 659847.1258245276\n",
      "Epoch 134501, Training Loss: 71628, Validation Loss: 66242, 905012.1737763352\n",
      "Epoch 134601, Training Loss: 67389, Validation Loss: 80049, 775746.4685731226\n",
      "Epoch 134701, Training Loss: 71162, Validation Loss: 64820, 556739.7048696412\n",
      "Epoch 134801, Training Loss: 65016, Validation Loss: 62776, 577356.0779480635\n",
      "Epoch 134901, Training Loss: 63779, Validation Loss: 74337, 157775.18224444217\n",
      "Epoch 135001, Training Loss: 72064, Validation Loss: 66021, 1316096.6000425634\n",
      "Epoch 135101, Training Loss: 66776, Validation Loss: 73671, 681412.1562385772\n",
      "Epoch 135201, Training Loss: 65175, Validation Loss: 80997, 1253766.3835176537\n",
      "Epoch 135301, Training Loss: 62348, Validation Loss: 82058, 389171.05527952575\n",
      "Epoch 135401, Training Loss: 70445, Validation Loss: 72590, 741974.2776026592\n",
      "Epoch 135501, Training Loss: 66827, Validation Loss: 81601, 835233.8551826109\n",
      "Epoch 135601, Training Loss: 65437, Validation Loss: 73128, 349859.54937159683\n",
      "Epoch 135701, Training Loss: 67157, Validation Loss: 78583, 720108.8676825633\n",
      "Epoch 135801, Training Loss: 70976, Validation Loss: 73691, 707463.1729066828\n",
      "Epoch 135901, Training Loss: 71785, Validation Loss: 78710, 975868.7028553177\n",
      "Epoch 136001, Training Loss: 63251, Validation Loss: 74348, 384131.40737020137\n",
      "Epoch 136101, Training Loss: 63622, Validation Loss: 80510, 395097.5682904422\n",
      "Epoch 136201, Training Loss: 67228, Validation Loss: 61877, 1063726.0028155753\n",
      "Epoch 136301, Training Loss: 67524, Validation Loss: 59469, 1542187.2959625751\n",
      "Epoch 136401, Training Loss: 66854, Validation Loss: 68776, 637519.7760665271\n",
      "Epoch 136501, Training Loss: 67913, Validation Loss: 69340, 171317.98101296162\n",
      "Epoch 136601, Training Loss: 60117, Validation Loss: 73719, 333023.2309884012\n",
      "Epoch 136701, Training Loss: 68757, Validation Loss: 77384, 620418.9417883855\n",
      "Epoch 136801, Training Loss: 76728, Validation Loss: 63936, 824667.9174393509\n",
      "Epoch 136901, Training Loss: 66585, Validation Loss: 75297, 904191.7903832941\n",
      "Epoch 137001, Training Loss: 64916, Validation Loss: 68218, 479804.13841713406\n",
      "Epoch 137101, Training Loss: 63672, Validation Loss: 65286, 165396.59486461533\n",
      "Epoch 137201, Training Loss: 65578, Validation Loss: 68948, 300845.72330581\n",
      "Epoch 137301, Training Loss: 61804, Validation Loss: 73979, 525100.9826539031\n",
      "Epoch 137401, Training Loss: 68925, Validation Loss: 80415, 775602.7700999711\n",
      "Epoch 137501, Training Loss: 65275, Validation Loss: 74999, 792464.4313886272\n",
      "Epoch 137601, Training Loss: 68222, Validation Loss: 71385, 598861.2361775441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137701, Training Loss: 64882, Validation Loss: 67471, 239097.0378617594\n",
      "Epoch 137801, Training Loss: 64664, Validation Loss: 68158, 279797.4219140312\n",
      "Epoch 137901, Training Loss: 62471, Validation Loss: 66623, 924278.4831408149\n",
      "Epoch 138001, Training Loss: 68371, Validation Loss: 67097, 752145.23111811\n",
      "Epoch 138101, Training Loss: 66059, Validation Loss: 66284, 558417.1613635086\n",
      "Epoch 138201, Training Loss: 68496, Validation Loss: 87258, 395938.43230651645\n",
      "Epoch 138301, Training Loss: 64871, Validation Loss: 69563, 309456.2353187422\n",
      "Epoch 138401, Training Loss: 66222, Validation Loss: 72328, 261600.18545081106\n",
      "Epoch 138501, Training Loss: 65284, Validation Loss: 68347, 529916.5929896153\n",
      "Epoch 138601, Training Loss: 61353, Validation Loss: 71652, 904476.5350853493\n",
      "Epoch 138701, Training Loss: 65363, Validation Loss: 70601, 544967.9563309755\n",
      "Epoch 138801, Training Loss: 87984, Validation Loss: 87115, 2637182.794781905\n",
      "Epoch 138901, Training Loss: 66385, Validation Loss: 69223, 476811.0281643146\n",
      "Epoch 139001, Training Loss: 71329, Validation Loss: 75302, 701648.817357696\n",
      "Epoch 139101, Training Loss: 70554, Validation Loss: 69850, 381138.01350992935\n",
      "Epoch 139201, Training Loss: 68197, Validation Loss: 66084, 505894.0668212774\n",
      "Epoch 139301, Training Loss: 62856, Validation Loss: 67889, 403431.2634266303\n",
      "Epoch 139401, Training Loss: 69655, Validation Loss: 71399, 552989.4013081089\n",
      "Epoch 139501, Training Loss: 100219, Validation Loss: 93892, 3758.528495611537\n",
      "Epoch 139601, Training Loss: 97151, Validation Loss: 93888, 3462.332749891659\n",
      "Epoch 139701, Training Loss: 102935, Validation Loss: 93846, 6023.560579095385\n",
      "Epoch 139801, Training Loss: 96488, Validation Loss: 93777, 3005.8286345042925\n",
      "Epoch 139901, Training Loss: 99807, Validation Loss: 93938, 5447.4014642721895\n",
      "Epoch 140001, Training Loss: 97761, Validation Loss: 93916, 2301.2803400178686\n",
      "Epoch 140101, Training Loss: 92190, Validation Loss: 94333, 2250.6971324616165\n",
      "Epoch 140201, Training Loss: 101810, Validation Loss: 93817, 2212.4905166568547\n",
      "Epoch 140301, Training Loss: 96529, Validation Loss: 94038, 4649.554970922959\n",
      "Epoch 140401, Training Loss: 99024, Validation Loss: 93887, 4159.484345271416\n",
      "Epoch 140501, Training Loss: 93443, Validation Loss: 93809, 2822.395576713379\n",
      "Epoch 140601, Training Loss: 100862, Validation Loss: 93833, 6874.811344477999\n",
      "Epoch 140701, Training Loss: 100857, Validation Loss: 93731, 3002.3168666216857\n",
      "Epoch 140801, Training Loss: 101841, Validation Loss: 94287, 5351.854917337886\n",
      "Epoch 140901, Training Loss: 96457, Validation Loss: 94170, 5893.848335174164\n",
      "Epoch 141001, Training Loss: 96511, Validation Loss: 93692, 3470.6499562397366\n",
      "Epoch 141101, Training Loss: 108298, Validation Loss: 116618, 26111.548009366717\n",
      "Epoch 141201, Training Loss: 91694, Validation Loss: 92402, 79125.88448157063\n",
      "Epoch 141301, Training Loss: 84579, Validation Loss: 83581, 204463.05247168886\n",
      "Epoch 141401, Training Loss: 75692, Validation Loss: 88146, 728857.6414248314\n",
      "Epoch 141501, Training Loss: 72043, Validation Loss: 67593, 492189.81170949666\n",
      "Epoch 141601, Training Loss: 66877, Validation Loss: 67586, 424169.60456806887\n",
      "Epoch 141701, Training Loss: 72315, Validation Loss: 84966, 760904.4170070399\n",
      "Epoch 141801, Training Loss: 72857, Validation Loss: 79077, 1219234.859981243\n",
      "Epoch 141901, Training Loss: 68261, Validation Loss: 67248, 182113.08718952295\n",
      "Epoch 142001, Training Loss: 74960, Validation Loss: 66700, 1279506.1399798577\n",
      "Epoch 142101, Training Loss: 70499, Validation Loss: 63680, 346694.4555896724\n",
      "Epoch 142201, Training Loss: 68337, Validation Loss: 66157, 715633.0195409739\n",
      "Epoch 142301, Training Loss: 81925, Validation Loss: 63221, 1903579.6237450968\n",
      "Epoch 142401, Training Loss: 87653, Validation Loss: 99102, 1472801.3477617747\n",
      "Epoch 142501, Training Loss: 62763, Validation Loss: 72115, 112431.30190429483\n",
      "Epoch 142601, Training Loss: 67862, Validation Loss: 69633, 481787.2766417477\n",
      "Epoch 142701, Training Loss: 67287, Validation Loss: 73183, 545924.8156130527\n",
      "Epoch 142801, Training Loss: 72385, Validation Loss: 78594, 883600.7529663859\n",
      "Epoch 142901, Training Loss: 73074, Validation Loss: 72499, 141632.0385966843\n",
      "Epoch 143001, Training Loss: 64429, Validation Loss: 69704, 128286.61255190724\n",
      "Epoch 143101, Training Loss: 65974, Validation Loss: 69532, 479007.6975486548\n",
      "Epoch 143201, Training Loss: 62579, Validation Loss: 67803, 404602.8953392003\n",
      "Epoch 143301, Training Loss: 74249, Validation Loss: 71059, 715595.6815414033\n",
      "Epoch 143401, Training Loss: 65387, Validation Loss: 90085, 607542.7100533639\n",
      "Epoch 143501, Training Loss: 76375, Validation Loss: 59004, 1519921.8207717182\n",
      "Epoch 143601, Training Loss: 69108, Validation Loss: 75848, 365151.1887609246\n",
      "Epoch 143701, Training Loss: 67889, Validation Loss: 70939, 154183.97373937818\n",
      "Epoch 143801, Training Loss: 69185, Validation Loss: 69601, 680198.9536258263\n",
      "Epoch 143901, Training Loss: 68630, Validation Loss: 68884, 402410.04325511307\n",
      "Epoch 144001, Training Loss: 68385, Validation Loss: 78055, 732554.8419828435\n",
      "Epoch 144101, Training Loss: 62890, Validation Loss: 64284, 970017.6218634387\n",
      "Epoch 144201, Training Loss: 63013, Validation Loss: 70120, 529280.5279955176\n",
      "Epoch 144301, Training Loss: 71416, Validation Loss: 67948, 253897.57985005298\n",
      "Epoch 144401, Training Loss: 67896, Validation Loss: 62090, 280586.85328157566\n",
      "Epoch 144501, Training Loss: 62479, Validation Loss: 64557, 1098674.7668326332\n",
      "Epoch 144601, Training Loss: 65234, Validation Loss: 69412, 505577.5005258582\n",
      "Epoch 144701, Training Loss: 66458, Validation Loss: 63776, 1012450.2470160244\n",
      "Epoch 144801, Training Loss: 64235, Validation Loss: 69607, 617422.9487287098\n",
      "Epoch 144901, Training Loss: 66504, Validation Loss: 66732, 621291.1196388017\n",
      "Epoch 145001, Training Loss: 100635, Validation Loss: 93815, 2673.7434543201894\n",
      "Epoch 145101, Training Loss: 101703, Validation Loss: 94103, 3822.079710325644\n",
      "Epoch 145201, Training Loss: 105031, Validation Loss: 93991, 2492.16355214623\n",
      "Epoch 145301, Training Loss: 97187, Validation Loss: 93771, 2550.455755500218\n",
      "Epoch 145401, Training Loss: 98391, Validation Loss: 93953, 1752.253139482714\n",
      "Epoch 145501, Training Loss: 96979, Validation Loss: 93928, 2983.844307714197\n",
      "Epoch 145601, Training Loss: 97145, Validation Loss: 94222, 5210.68496290417\n",
      "Epoch 145701, Training Loss: 76557, Validation Loss: 77033, 207551.35635008404\n",
      "Epoch 145801, Training Loss: 72045, Validation Loss: 72327, 545358.2187299363\n",
      "Epoch 145901, Training Loss: 70642, Validation Loss: 81908, 405394.71043342695\n",
      "Epoch 146001, Training Loss: 70778, Validation Loss: 76350, 582896.6995905482\n",
      "Epoch 146101, Training Loss: 71381, Validation Loss: 73542, 796433.3221454303\n",
      "Epoch 146201, Training Loss: 69062, Validation Loss: 74950, 436054.11321075395\n",
      "Epoch 146301, Training Loss: 68108, Validation Loss: 69722, 390808.72950980323\n",
      "Epoch 146401, Training Loss: 69360, Validation Loss: 67929, 701747.9675874462\n",
      "Epoch 146501, Training Loss: 67192, Validation Loss: 68302, 714681.2142217195\n",
      "Epoch 146601, Training Loss: 66980, Validation Loss: 78883, 437932.21541790315\n",
      "Epoch 146701, Training Loss: 66880, Validation Loss: 74992, 480180.982020413\n",
      "Epoch 146801, Training Loss: 67999, Validation Loss: 75570, 761320.3799823424\n",
      "Epoch 146901, Training Loss: 71840, Validation Loss: 75745, 681041.6675389009\n",
      "Epoch 147001, Training Loss: 69748, Validation Loss: 86402, 1254942.8220591296\n",
      "Epoch 147101, Training Loss: 66830, Validation Loss: 67303, 569886.2070096793\n",
      "Epoch 147201, Training Loss: 70387, Validation Loss: 71175, 335505.215555897\n",
      "Epoch 147301, Training Loss: 66884, Validation Loss: 71226, 410467.8378114893\n",
      "Epoch 147401, Training Loss: 62989, Validation Loss: 71758, 330468.508747284\n",
      "Epoch 147501, Training Loss: 60474, Validation Loss: 80769, 453096.648593967\n",
      "Epoch 147601, Training Loss: 66047, Validation Loss: 67054, 897226.0898991809\n",
      "Epoch 147701, Training Loss: 61568, Validation Loss: 69701, 206038.9579812209\n",
      "Epoch 147801, Training Loss: 69510, Validation Loss: 63515, 870493.1536794348\n",
      "Epoch 147901, Training Loss: 65152, Validation Loss: 70966, 511149.7654846216\n",
      "Epoch 148001, Training Loss: 69508, Validation Loss: 80263, 1040556.3195346413\n",
      "Epoch 148101, Training Loss: 65033, Validation Loss: 67175, 249152.19256817424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148201, Training Loss: 64846, Validation Loss: 78699, 338958.15323787386\n",
      "Epoch 148301, Training Loss: 63845, Validation Loss: 75043, 882821.6321308069\n",
      "Epoch 148401, Training Loss: 66786, Validation Loss: 65846, 627208.0247179816\n",
      "Epoch 148501, Training Loss: 64562, Validation Loss: 69359, 847075.477560493\n",
      "Epoch 148601, Training Loss: 65862, Validation Loss: 71537, 627560.2094726902\n",
      "Epoch 148701, Training Loss: 67824, Validation Loss: 64015, 848228.4078145024\n",
      "Epoch 148801, Training Loss: 63965, Validation Loss: 75121, 793946.9743618782\n",
      "Epoch 148901, Training Loss: 65420, Validation Loss: 65280, 772048.0616776821\n",
      "Epoch 149001, Training Loss: 62987, Validation Loss: 81666, 673707.0097583625\n",
      "Epoch 149101, Training Loss: 67388, Validation Loss: 74448, 849526.2275105979\n",
      "Epoch 149201, Training Loss: 69045, Validation Loss: 70650, 1227506.9884931755\n",
      "Epoch 149301, Training Loss: 63661, Validation Loss: 72880, 660150.397700366\n",
      "Epoch 149401, Training Loss: 63728, Validation Loss: 67957, 573695.6003807714\n",
      "Epoch 149501, Training Loss: 68980, Validation Loss: 73720, 1339523.2388749805\n",
      "Epoch 149601, Training Loss: 64899, Validation Loss: 70807, 310731.8411189133\n",
      "Epoch 149701, Training Loss: 65277, Validation Loss: 79548, 736109.8841799544\n",
      "Epoch 149801, Training Loss: 65257, Validation Loss: 78574, 509774.428178257\n",
      "Epoch 149901, Training Loss: 65127, Validation Loss: 69796, 612964.5430413572\n",
      "Epoch 150001, Training Loss: 68427, Validation Loss: 66360, 545790.8595753369\n",
      "Epoch 150101, Training Loss: 61906, Validation Loss: 69114, 540310.0762911694\n",
      "Epoch 150201, Training Loss: 64690, Validation Loss: 72058, 217039.30405101503\n",
      "Epoch 150301, Training Loss: 67954, Validation Loss: 72720, 662865.1909133145\n",
      "Epoch 150401, Training Loss: 62551, Validation Loss: 69334, 685764.2303055382\n",
      "Epoch 150501, Training Loss: 62254, Validation Loss: 59251, 2603916.3216559687\n",
      "Epoch 150601, Training Loss: 64372, Validation Loss: 71922, 445721.02029748145\n",
      "Epoch 150701, Training Loss: 70512, Validation Loss: 71305, 400831.3206210723\n",
      "Epoch 150801, Training Loss: 67410, Validation Loss: 69206, 532433.2369814928\n",
      "Epoch 150901, Training Loss: 65949, Validation Loss: 77124, 721603.8462376014\n",
      "Epoch 151001, Training Loss: 67229, Validation Loss: 61713, 795875.5682856814\n",
      "Epoch 151101, Training Loss: 66449, Validation Loss: 65200, 576345.6487935912\n",
      "Epoch 151201, Training Loss: 65842, Validation Loss: 71455, 552578.9339169273\n",
      "Epoch 151301, Training Loss: 64919, Validation Loss: 69275, 210778.61205855114\n",
      "Epoch 151401, Training Loss: 59970, Validation Loss: 73170, 679584.3832247683\n",
      "Epoch 151501, Training Loss: 69043, Validation Loss: 72848, 377091.87461793673\n",
      "Epoch 151601, Training Loss: 62309, Validation Loss: 70722, 261792.08661655863\n",
      "Epoch 151701, Training Loss: 65108, Validation Loss: 68044, 472097.4548724541\n",
      "Epoch 151801, Training Loss: 70239, Validation Loss: 90584, 736280.9979718734\n",
      "Epoch 151901, Training Loss: 67352, Validation Loss: 73494, 670865.6256783608\n",
      "Epoch 152001, Training Loss: 63247, Validation Loss: 75015, 215971.0515879634\n",
      "Epoch 152101, Training Loss: 64840, Validation Loss: 69657, 1064257.8451821946\n",
      "Epoch 152201, Training Loss: 91243, Validation Loss: 83856, 734385.6460417048\n",
      "Epoch 152301, Training Loss: 58680, Validation Loss: 72993, 434124.5507266828\n",
      "Epoch 152401, Training Loss: 63165, Validation Loss: 70601, 108547.28209597606\n",
      "Epoch 152501, Training Loss: 66593, Validation Loss: 73169, 776785.8068939666\n",
      "Epoch 152601, Training Loss: 67945, Validation Loss: 71422, 711295.3845890338\n",
      "Epoch 152701, Training Loss: 61496, Validation Loss: 78211, 330523.7443408792\n",
      "Epoch 152801, Training Loss: 88729, Validation Loss: 59662, 2101053.8791633155\n",
      "Epoch 152901, Training Loss: 62076, Validation Loss: 74044, 331015.9087203744\n",
      "Epoch 153001, Training Loss: 62976, Validation Loss: 65490, 311196.9274634684\n",
      "Epoch 153101, Training Loss: 64115, Validation Loss: 71180, 174291.0806415762\n",
      "Epoch 153201, Training Loss: 68010, Validation Loss: 65791, 250376.9749049282\n",
      "Epoch 153301, Training Loss: 67495, Validation Loss: 71665, 245341.3303650564\n",
      "Epoch 153401, Training Loss: 70224, Validation Loss: 72770, 128548.96242942351\n",
      "Epoch 153501, Training Loss: 66464, Validation Loss: 80426, 410598.9033445854\n",
      "Epoch 153601, Training Loss: 68805, Validation Loss: 73879, 495049.95219806\n",
      "Epoch 153701, Training Loss: 62387, Validation Loss: 80267, 213902.94539322352\n",
      "Epoch 153801, Training Loss: 65237, Validation Loss: 68322, 428351.1663513657\n",
      "Epoch 153901, Training Loss: 68534, Validation Loss: 69315, 549537.8512905025\n",
      "Epoch 154001, Training Loss: 66554, Validation Loss: 62044, 1552930.8942938873\n",
      "Epoch 154101, Training Loss: 66690, Validation Loss: 80631, 662055.9476173335\n",
      "Epoch 154201, Training Loss: 65897, Validation Loss: 74069, 272389.43693110335\n",
      "Epoch 154301, Training Loss: 62091, Validation Loss: 71017, 274376.54252141854\n",
      "Epoch 154401, Training Loss: 65597, Validation Loss: 77097, 130341.2547886169\n",
      "Epoch 154501, Training Loss: 63822, Validation Loss: 78767, 361727.4207722207\n",
      "Epoch 154601, Training Loss: 67120, Validation Loss: 60091, 851432.6397594413\n",
      "Epoch 154701, Training Loss: 62858, Validation Loss: 68120, 769198.7344357301\n",
      "Epoch 154801, Training Loss: 65876, Validation Loss: 70907, 806691.2881956785\n",
      "Epoch 154901, Training Loss: 64635, Validation Loss: 68749, 195808.50612052178\n",
      "Epoch 155001, Training Loss: 66643, Validation Loss: 86189, 475546.1939596869\n",
      "Epoch 155101, Training Loss: 63609, Validation Loss: 66494, 501153.3010365963\n",
      "Epoch 155201, Training Loss: 67300, Validation Loss: 70820, 653675.8982886669\n",
      "Epoch 155301, Training Loss: 66664, Validation Loss: 69200, 417808.4105648158\n",
      "Epoch 155401, Training Loss: 68629, Validation Loss: 68949, 952402.7348243695\n",
      "Epoch 155501, Training Loss: 64880, Validation Loss: 71288, 412346.06429635035\n",
      "Epoch 155601, Training Loss: 65743, Validation Loss: 72746, 147686.7832930026\n",
      "Epoch 155701, Training Loss: 67823, Validation Loss: 76188, 247309.02812357116\n",
      "Epoch 155801, Training Loss: 60949, Validation Loss: 69679, 530134.4746592259\n",
      "Epoch 155901, Training Loss: 68122, Validation Loss: 83082, 421279.8763277533\n",
      "Epoch 156001, Training Loss: 75244, Validation Loss: 75130, 202019.18804235724\n",
      "Epoch 156101, Training Loss: 65781, Validation Loss: 75075, 80711.25808553073\n",
      "Epoch 156201, Training Loss: 64937, Validation Loss: 74941, 443103.4192466533\n",
      "Epoch 156301, Training Loss: 64191, Validation Loss: 78788, 420835.820590328\n",
      "Epoch 156401, Training Loss: 65245, Validation Loss: 74432, 284308.2099018896\n",
      "Epoch 156501, Training Loss: 65618, Validation Loss: 81703, 412380.28236096306\n",
      "Epoch 156601, Training Loss: 67631, Validation Loss: 63629, 456637.6104529495\n",
      "Epoch 156701, Training Loss: 64014, Validation Loss: 67919, 197767.97347681187\n",
      "Epoch 156801, Training Loss: 67283, Validation Loss: 70998, 252700.5874867939\n",
      "Epoch 156901, Training Loss: 68460, Validation Loss: 75410, 774395.6768093406\n",
      "Epoch 157001, Training Loss: 68846, Validation Loss: 90543, 360886.012147571\n",
      "Epoch 157101, Training Loss: 66194, Validation Loss: 70933, 463200.3309721386\n",
      "Epoch 157201, Training Loss: 67556, Validation Loss: 75393, 444208.03172173473\n",
      "Epoch 157301, Training Loss: 62444, Validation Loss: 65960, 135444.73084332483\n",
      "Epoch 157401, Training Loss: 62954, Validation Loss: 74444, 251297.91628088724\n",
      "Epoch 157501, Training Loss: 100907, Validation Loss: 93759, 2744.8947000529456\n",
      "Epoch 157601, Training Loss: 92011, Validation Loss: 93789, 5524.735739638761\n",
      "Epoch 157701, Training Loss: 94901, Validation Loss: 93892, 2425.293099050479\n",
      "Epoch 157801, Training Loss: 96733, Validation Loss: 93711, 3991.5142337286616\n",
      "Epoch 157901, Training Loss: 97876, Validation Loss: 93739, 2091.9467735289704\n",
      "Epoch 158001, Training Loss: 95740, Validation Loss: 93795, 2568.731932062479\n",
      "Epoch 158101, Training Loss: 96273, Validation Loss: 94074, 6537.384396359025\n",
      "Epoch 158201, Training Loss: 97178, Validation Loss: 94179, 2318.461267238818\n",
      "Epoch 158301, Training Loss: 101081, Validation Loss: 94090, 2301.1529979897873\n",
      "Epoch 158401, Training Loss: 95844, Validation Loss: 94171, 2230.3576125602735\n",
      "Epoch 158501, Training Loss: 95768, Validation Loss: 94172, 4234.036638483357\n",
      "Epoch 158601, Training Loss: 97336, Validation Loss: 93831, 3263.6816565550052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158701, Training Loss: 98533, Validation Loss: 93818, 2738.2471947718386\n",
      "Epoch 158801, Training Loss: 97306, Validation Loss: 93954, 2052.750196326632\n",
      "Epoch 158901, Training Loss: 99344, Validation Loss: 94200, 2679.2926392045824\n",
      "Epoch 159001, Training Loss: 101948, Validation Loss: 93938, 3793.0145013127403\n",
      "Epoch 159101, Training Loss: 97235, Validation Loss: 94122, 2014.891641997264\n",
      "Epoch 159201, Training Loss: 93587, Validation Loss: 94040, 6419.888833320379\n",
      "Epoch 159301, Training Loss: 98804, Validation Loss: 94305, 4467.454403721933\n",
      "Epoch 159401, Training Loss: 97703, Validation Loss: 93968, 1943.085067702688\n",
      "Epoch 159501, Training Loss: 98575, Validation Loss: 94431, 2532.7610440870394\n",
      "Epoch 159601, Training Loss: 92720, Validation Loss: 94423, 4569.325697111987\n",
      "Epoch 159701, Training Loss: 96711, Validation Loss: 94374, 3729.6666045940256\n",
      "Epoch 159801, Training Loss: 97800, Validation Loss: 94174, 2719.7452483079073\n",
      "Epoch 159901, Training Loss: 96860, Validation Loss: 93802, 2558.6988194038804\n",
      "Epoch 160001, Training Loss: 98569, Validation Loss: 94116, 1812.5971751756967\n",
      "Epoch 160101, Training Loss: 97756, Validation Loss: 94077, 3165.2764858606083\n",
      "Epoch 160201, Training Loss: 99656, Validation Loss: 94059, 3019.0162914114717\n",
      "Epoch 160301, Training Loss: 97709, Validation Loss: 94001, 2367.427280727023\n",
      "Epoch 160401, Training Loss: 96541, Validation Loss: 93791, 2557.0274149134834\n",
      "Epoch 160501, Training Loss: 101028, Validation Loss: 94035, 2883.2475860010313\n",
      "Epoch 160601, Training Loss: 96207, Validation Loss: 93749, 1620.7060376255695\n",
      "Epoch 160701, Training Loss: 94402, Validation Loss: 94239, 3492.8787405729204\n",
      "Epoch 160801, Training Loss: 100656, Validation Loss: 93722, 2674.869700373703\n",
      "Epoch 160901, Training Loss: 103930, Validation Loss: 94000, 2046.0748564840185\n",
      "Epoch 161001, Training Loss: 92894, Validation Loss: 93954, 2073.952991484377\n",
      "Epoch 161101, Training Loss: 100848, Validation Loss: 93923, 2768.587658741338\n",
      "Epoch 161201, Training Loss: 99341, Validation Loss: 94046, 2934.5205744846785\n",
      "Epoch 161301, Training Loss: 98920, Validation Loss: 93847, 2410.6367840713087\n",
      "Epoch 161401, Training Loss: 98258, Validation Loss: 93800, 1651.9527302264585\n",
      "Epoch 161501, Training Loss: 98630, Validation Loss: 93798, 2343.0999981910027\n",
      "Epoch 161601, Training Loss: 101215, Validation Loss: 93823, 1530.057134619606\n",
      "Epoch 161701, Training Loss: 95104, Validation Loss: 94122, 4199.002326472911\n",
      "Epoch 161801, Training Loss: 101294, Validation Loss: 93804, 1478.09918286576\n",
      "Epoch 161901, Training Loss: 98747, Validation Loss: 93871, 2315.8087335414953\n",
      "Epoch 162001, Training Loss: 98960, Validation Loss: 93904, 4368.065998667339\n",
      "Epoch 162101, Training Loss: 96075, Validation Loss: 93748, 2097.5617374947838\n",
      "Epoch 162201, Training Loss: 100989, Validation Loss: 94125, 2889.6260920599198\n",
      "Epoch 162301, Training Loss: 99786, Validation Loss: 94011, 1799.9140196912888\n",
      "Epoch 162401, Training Loss: 97143, Validation Loss: 93741, 2323.3239114322537\n",
      "Epoch 162501, Training Loss: 97509, Validation Loss: 93807, 2935.9203630329484\n",
      "Epoch 162601, Training Loss: 100712, Validation Loss: 94042, 2494.224800643618\n",
      "Epoch 162701, Training Loss: 98305, Validation Loss: 93964, 4471.801212572273\n",
      "Epoch 162801, Training Loss: 97165, Validation Loss: 93733, 2229.1514094589475\n",
      "Epoch 162901, Training Loss: 100863, Validation Loss: 95073, 2878.946491739806\n",
      "Epoch 163001, Training Loss: 99242, Validation Loss: 94053, 2177.3484588887545\n",
      "Epoch 163101, Training Loss: 98992, Validation Loss: 93965, 2427.4143739915785\n",
      "Epoch 163201, Training Loss: 96787, Validation Loss: 93803, 2831.078415478462\n",
      "Epoch 163301, Training Loss: 94874, Validation Loss: 93983, 2253.988223764912\n",
      "Epoch 163401, Training Loss: 99015, Validation Loss: 93907, 1883.058759220408\n",
      "Epoch 163501, Training Loss: 95133, Validation Loss: 94221, 4119.5732203179205\n",
      "Epoch 163601, Training Loss: 100886, Validation Loss: 94276, 2680.162061796153\n",
      "Epoch 163701, Training Loss: 98883, Validation Loss: 94017, 1765.7440257528117\n",
      "Epoch 163801, Training Loss: 95005, Validation Loss: 94189, 2172.380156887052\n",
      "Epoch 163901, Training Loss: 97328, Validation Loss: 94218, 2462.736242710927\n",
      "Epoch 164001, Training Loss: 98856, Validation Loss: 94169, 5359.692335137385\n",
      "Epoch 164101, Training Loss: 96868, Validation Loss: 94403, 2014.1053817133536\n",
      "Epoch 164201, Training Loss: 97145, Validation Loss: 94390, 3594.401539235625\n",
      "Epoch 164301, Training Loss: 94573, Validation Loss: 93751, 4406.872465248746\n",
      "Epoch 164401, Training Loss: 98704, Validation Loss: 93893, 3184.743461395472\n",
      "Epoch 164501, Training Loss: 98473, Validation Loss: 94191, 3599.7417780874803\n",
      "Epoch 164601, Training Loss: 93729, Validation Loss: 93868, 4516.266374449811\n",
      "Epoch 164701, Training Loss: 78383, Validation Loss: 74280, 201637.78520050607\n",
      "Epoch 164801, Training Loss: 70001, Validation Loss: 79831, 433516.997046962\n",
      "Epoch 164901, Training Loss: 66543, Validation Loss: 72540, 324398.7582620886\n",
      "Epoch 165001, Training Loss: 66977, Validation Loss: 73055, 162165.95564914166\n",
      "Epoch 165101, Training Loss: 71963, Validation Loss: 69085, 306111.40363613947\n",
      "Epoch 165201, Training Loss: 72977, Validation Loss: 81745, 390607.191962465\n",
      "Epoch 165301, Training Loss: 68375, Validation Loss: 77561, 889246.3066165921\n",
      "Epoch 165401, Training Loss: 67428, Validation Loss: 70473, 221285.09117921462\n",
      "Epoch 165501, Training Loss: 68102, Validation Loss: 80362, 484560.4662906414\n",
      "Epoch 165601, Training Loss: 70784, Validation Loss: 71375, 688139.994268454\n",
      "Epoch 165701, Training Loss: 69661, Validation Loss: 80342, 635677.3354098351\n",
      "Epoch 165801, Training Loss: 72771, Validation Loss: 70212, 1190283.976547166\n",
      "Epoch 165901, Training Loss: 68470, Validation Loss: 81607, 586427.2721721418\n",
      "Epoch 166001, Training Loss: 64882, Validation Loss: 74850, 521387.9984176827\n",
      "Epoch 166101, Training Loss: 68658, Validation Loss: 71305, 1120138.0678712481\n",
      "Epoch 166201, Training Loss: 68695, Validation Loss: 81056, 742202.5008254438\n",
      "Epoch 166301, Training Loss: 66578, Validation Loss: 71361, 1418712.1560557901\n",
      "Epoch 166401, Training Loss: 64454, Validation Loss: 81179, 452028.2188563756\n",
      "Epoch 166501, Training Loss: 67914, Validation Loss: 67211, 560119.865205182\n",
      "Epoch 166601, Training Loss: 61394, Validation Loss: 84523, 443646.53239177517\n",
      "Epoch 166701, Training Loss: 66897, Validation Loss: 70943, 416616.83403629024\n",
      "Epoch 166801, Training Loss: 69202, Validation Loss: 73478, 466398.43549253675\n",
      "Epoch 166901, Training Loss: 66190, Validation Loss: 73581, 432956.7866394857\n",
      "Epoch 167001, Training Loss: 66705, Validation Loss: 75678, 877952.864595804\n",
      "Epoch 167101, Training Loss: 68690, Validation Loss: 78386, 659792.2173785833\n",
      "Epoch 167201, Training Loss: 66904, Validation Loss: 74387, 292325.91603517905\n",
      "Epoch 167301, Training Loss: 61145, Validation Loss: 72853, 330189.1978728984\n",
      "Epoch 167401, Training Loss: 62683, Validation Loss: 73803, 788846.7464551978\n",
      "Epoch 167501, Training Loss: 72329, Validation Loss: 68990, 1430088.1474150766\n",
      "Epoch 167601, Training Loss: 68355, Validation Loss: 78389, 823829.3666390477\n",
      "Epoch 167701, Training Loss: 68595, Validation Loss: 62015, 1518143.779248353\n",
      "Epoch 167801, Training Loss: 61454, Validation Loss: 68558, 425549.97919535247\n",
      "Epoch 167901, Training Loss: 68159, Validation Loss: 65856, 651681.1880873812\n",
      "Epoch 168001, Training Loss: 63238, Validation Loss: 72565, 280399.53369410045\n",
      "Epoch 168101, Training Loss: 67380, Validation Loss: 71557, 546138.5493678445\n",
      "Epoch 168201, Training Loss: 64177, Validation Loss: 71368, 691685.1986957152\n",
      "Epoch 168301, Training Loss: 65819, Validation Loss: 67325, 1237557.5101494754\n",
      "Epoch 168401, Training Loss: 61618, Validation Loss: 73887, 396393.8310210399\n",
      "Epoch 168501, Training Loss: 63583, Validation Loss: 74038, 188036.3215953384\n",
      "Epoch 168601, Training Loss: 65720, Validation Loss: 70090, 885393.9567396658\n",
      "Epoch 168701, Training Loss: 88338, Validation Loss: 61311, 2277920.257066613\n",
      "Epoch 168801, Training Loss: 65737, Validation Loss: 69952, 527168.3590834988\n",
      "Epoch 168901, Training Loss: 64779, Validation Loss: 73417, 1202310.0746244316\n",
      "Epoch 169001, Training Loss: 63282, Validation Loss: 70916, 537257.376412437\n",
      "Epoch 169101, Training Loss: 69002, Validation Loss: 87750, 672674.353955782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169201, Training Loss: 61081, Validation Loss: 79959, 277712.8462632758\n",
      "Epoch 169301, Training Loss: 67225, Validation Loss: 69256, 221548.82478340273\n",
      "Epoch 169401, Training Loss: 68895, Validation Loss: 71727, 476480.6967266742\n",
      "Epoch 169501, Training Loss: 63638, Validation Loss: 80925, 854892.0917004693\n",
      "Epoch 169601, Training Loss: 65261, Validation Loss: 60440, 374845.2335954532\n",
      "Epoch 169701, Training Loss: 67595, Validation Loss: 72458, 178188.461389218\n",
      "Epoch 169801, Training Loss: 63243, Validation Loss: 71694, 444865.1505868214\n",
      "Epoch 169901, Training Loss: 64296, Validation Loss: 73139, 337402.9590722719\n",
      "Epoch 170001, Training Loss: 68919, Validation Loss: 70585, 390425.85808031604\n",
      "Epoch 170101, Training Loss: 67463, Validation Loss: 72152, 381469.5806553036\n",
      "Epoch 170201, Training Loss: 63909, Validation Loss: 73476, 210845.98533744915\n",
      "Epoch 170301, Training Loss: 60345, Validation Loss: 77740, 269744.8144472368\n",
      "Epoch 170401, Training Loss: 65468, Validation Loss: 79405, 1180747.277338382\n",
      "Epoch 170501, Training Loss: 65422, Validation Loss: 78006, 276281.8457940921\n",
      "Epoch 170601, Training Loss: 69968, Validation Loss: 74568, 1027357.4942418272\n",
      "Epoch 170701, Training Loss: 70599, Validation Loss: 83693, 471625.2171696806\n",
      "Epoch 170801, Training Loss: 77805, Validation Loss: 84840, 552267.3567203456\n",
      "Epoch 170901, Training Loss: 65075, Validation Loss: 71229, 1045219.3704240131\n",
      "Epoch 171001, Training Loss: 67052, Validation Loss: 62214, 341722.62675135746\n",
      "Epoch 171101, Training Loss: 66464, Validation Loss: 66008, 585168.2589087834\n",
      "Epoch 171201, Training Loss: 66201, Validation Loss: 65091, 1806784.5288609266\n",
      "Epoch 171301, Training Loss: 63963, Validation Loss: 71307, 638813.9611896303\n",
      "Epoch 171401, Training Loss: 62308, Validation Loss: 68390, 390205.1214508529\n",
      "Epoch 171501, Training Loss: 64215, Validation Loss: 78780, 637503.3616536466\n",
      "Epoch 171601, Training Loss: 63551, Validation Loss: 79293, 334975.85788854485\n",
      "Epoch 171701, Training Loss: 64815, Validation Loss: 70429, 253958.54564454578\n",
      "Epoch 171801, Training Loss: 68752, Validation Loss: 67498, 821809.6066684452\n",
      "Epoch 171901, Training Loss: 67554, Validation Loss: 71398, 256893.8507885233\n",
      "Epoch 172001, Training Loss: 62785, Validation Loss: 69973, 283789.1393503979\n",
      "Epoch 172101, Training Loss: 71098, Validation Loss: 81283, 138016.0015077966\n",
      "Epoch 172201, Training Loss: 60072, Validation Loss: 74187, 382671.2514099325\n",
      "Epoch 172301, Training Loss: 65452, Validation Loss: 68281, 446611.0506493487\n",
      "Epoch 172401, Training Loss: 67264, Validation Loss: 65425, 362315.6740366893\n",
      "Epoch 172501, Training Loss: 63753, Validation Loss: 68044, 359649.6889263045\n",
      "Epoch 172601, Training Loss: 63400, Validation Loss: 65725, 788864.458133509\n",
      "Epoch 172701, Training Loss: 65576, Validation Loss: 69803, 791061.38648457\n",
      "Epoch 172801, Training Loss: 65412, Validation Loss: 75561, 537431.4700974821\n",
      "Epoch 172901, Training Loss: 63867, Validation Loss: 73331, 584204.5271885257\n",
      "Epoch 173001, Training Loss: 65244, Validation Loss: 69851, 705330.9873770932\n",
      "Epoch 173101, Training Loss: 64310, Validation Loss: 66710, 155733.29863381915\n",
      "Epoch 173201, Training Loss: 66228, Validation Loss: 71823, 569328.1360582908\n",
      "Epoch 173301, Training Loss: 65122, Validation Loss: 71852, 302881.59724856395\n",
      "Epoch 173401, Training Loss: 60103, Validation Loss: 69330, 194325.2935344275\n",
      "Epoch 173501, Training Loss: 64107, Validation Loss: 71661, 652590.3393548199\n",
      "Epoch 173601, Training Loss: 63310, Validation Loss: 76335, 187545.23766900695\n",
      "Epoch 173701, Training Loss: 59614, Validation Loss: 70512, 185983.78112907917\n",
      "Epoch 173801, Training Loss: 65104, Validation Loss: 72831, 344890.20517207374\n",
      "Epoch 173901, Training Loss: 66986, Validation Loss: 75962, 359820.8645304499\n",
      "Epoch 174001, Training Loss: 63735, Validation Loss: 64116, 1021642.3542295237\n",
      "Epoch 174101, Training Loss: 63833, Validation Loss: 68680, 451164.41593764257\n",
      "Epoch 174201, Training Loss: 99397, Validation Loss: 94093, 1706.128519799354\n",
      "Epoch 174301, Training Loss: 103413, Validation Loss: 93973, 3460.8823125043423\n",
      "Epoch 174401, Training Loss: 97177, Validation Loss: 93905, 1928.9408024584488\n",
      "Epoch 174501, Training Loss: 97096, Validation Loss: 94028, 2479.2200927269446\n",
      "Epoch 174601, Training Loss: 97189, Validation Loss: 93989, 3919.8914608907094\n",
      "Epoch 174701, Training Loss: 96743, Validation Loss: 94424, 6557.666856546265\n",
      "Epoch 174801, Training Loss: 96053, Validation Loss: 93814, 3166.188735896029\n",
      "Epoch 174901, Training Loss: 95842, Validation Loss: 94540, 3147.7832553206586\n",
      "Epoch 175001, Training Loss: 95898, Validation Loss: 93969, 2664.7347681974084\n",
      "Epoch 175101, Training Loss: 97801, Validation Loss: 93810, 2810.925341117921\n",
      "Epoch 175201, Training Loss: 101073, Validation Loss: 94028, 3401.4254702783023\n",
      "Epoch 175301, Training Loss: 92870, Validation Loss: 93907, 1790.900077752943\n",
      "Epoch 175401, Training Loss: 95354, Validation Loss: 94233, 2818.1977172333004\n",
      "Epoch 175501, Training Loss: 98596, Validation Loss: 94005, 2380.074700974127\n",
      "Epoch 175601, Training Loss: 97898, Validation Loss: 93817, 4010.5657562576057\n",
      "Epoch 175701, Training Loss: 99852, Validation Loss: 93909, 2940.8821784505503\n",
      "Epoch 175801, Training Loss: 99082, Validation Loss: 94393, 1534.5364361014574\n",
      "Epoch 175901, Training Loss: 95300, Validation Loss: 94111, 2082.860804645048\n",
      "Epoch 176001, Training Loss: 96846, Validation Loss: 94069, 3018.589523907615\n",
      "Epoch 176101, Training Loss: 97422, Validation Loss: 94018, 1809.3504006413707\n",
      "Epoch 176201, Training Loss: 96800, Validation Loss: 93839, 1516.341111964147\n",
      "Epoch 176301, Training Loss: 96407, Validation Loss: 93741, 2083.715252343994\n",
      "Epoch 176401, Training Loss: 100955, Validation Loss: 93901, 1869.6528224215535\n",
      "Epoch 176501, Training Loss: 100308, Validation Loss: 94264, 2223.3246510475924\n",
      "Epoch 176601, Training Loss: 100301, Validation Loss: 94188, 1884.6283578037758\n",
      "Epoch 176701, Training Loss: 96674, Validation Loss: 94097, 2366.5031101406585\n",
      "Epoch 176801, Training Loss: 100957, Validation Loss: 94011, 3668.785336193481\n",
      "Epoch 176901, Training Loss: 95108, Validation Loss: 94031, 2598.405343852559\n",
      "Epoch 177001, Training Loss: 102923, Validation Loss: 94103, 4808.00342385715\n",
      "Epoch 177101, Training Loss: 99033, Validation Loss: 94112, 2085.475903964068\n",
      "Epoch 177201, Training Loss: 100424, Validation Loss: 93907, 2128.3110803018785\n",
      "Epoch 177301, Training Loss: 100233, Validation Loss: 94104, 3135.6975537347025\n",
      "Epoch 177401, Training Loss: 99915, Validation Loss: 93912, 1794.8312099573443\n",
      "Epoch 177501, Training Loss: 98918, Validation Loss: 93985, 2505.2269478046032\n",
      "Epoch 177601, Training Loss: 94977, Validation Loss: 94259, 3882.4448171263066\n",
      "Epoch 177701, Training Loss: 95232, Validation Loss: 94055, 4505.38581157274\n",
      "Epoch 177801, Training Loss: 96162, Validation Loss: 94517, 3313.244601681654\n",
      "Epoch 177901, Training Loss: 99125, Validation Loss: 93829, 2536.7960312709197\n",
      "Epoch 178001, Training Loss: 95111, Validation Loss: 94086, 1707.517361866331\n",
      "Epoch 178101, Training Loss: 96942, Validation Loss: 93863, 4606.550810761108\n",
      "Epoch 178201, Training Loss: 100398, Validation Loss: 94055, 1451.0490179543913\n",
      "Epoch 178301, Training Loss: 97637, Validation Loss: 93999, 2781.965734362368\n",
      "Epoch 178401, Training Loss: 94308, Validation Loss: 94112, 2151.5974633700775\n",
      "Epoch 178501, Training Loss: 97204, Validation Loss: 94049, 1546.0973391073085\n",
      "Epoch 178601, Training Loss: 103273, Validation Loss: 94418, 2688.0891336252403\n",
      "Epoch 178701, Training Loss: 99352, Validation Loss: 94188, 2796.0290604627357\n",
      "Epoch 178801, Training Loss: 102295, Validation Loss: 93869, 1964.2764605680002\n",
      "Epoch 178901, Training Loss: 99020, Validation Loss: 94205, 3241.8806263612373\n",
      "Epoch 179001, Training Loss: 109093, Validation Loss: 94551, 622979.1147868169\n",
      "Epoch 179101, Training Loss: 95911, Validation Loss: 93765, 2807.7886654169724\n",
      "Epoch 179201, Training Loss: 98292, Validation Loss: 94349, 3056.082173698343\n",
      "Epoch 179301, Training Loss: 102301, Validation Loss: 93861, 3073.4818648844434\n",
      "Epoch 179401, Training Loss: 96318, Validation Loss: 93755, 2050.476374454301\n",
      "Epoch 179501, Training Loss: 95566, Validation Loss: 93883, 2452.2064673537257\n",
      "Epoch 179601, Training Loss: 98535, Validation Loss: 94156, 2771.0771085923593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179701, Training Loss: 93940, Validation Loss: 94425, 3222.145064819089\n",
      "Epoch 179801, Training Loss: 97106, Validation Loss: 93747, 5778.188817130965\n",
      "Epoch 179901, Training Loss: 94313, Validation Loss: 93826, 1476.7637191678148\n",
      "Epoch 180001, Training Loss: 101566, Validation Loss: 94109, 2719.7305687024477\n",
      "Epoch 180101, Training Loss: 96598, Validation Loss: 93746, 4189.75623450388\n",
      "Epoch 180201, Training Loss: 100479, Validation Loss: 94059, 2149.9870204879244\n",
      "Epoch 180301, Training Loss: 98054, Validation Loss: 94472, 1832.688849050788\n",
      "Epoch 180401, Training Loss: 96364, Validation Loss: 94143, 3339.8254931389943\n",
      "Epoch 180501, Training Loss: 95482, Validation Loss: 94402, 3565.3212888067733\n",
      "Epoch 180601, Training Loss: 102169, Validation Loss: 93798, 2209.9954431068923\n",
      "Epoch 180701, Training Loss: 117968, Validation Loss: 93845, 367509.1862474818\n",
      "Epoch 180801, Training Loss: 99442, Validation Loss: 93974, 3796.000395473664\n",
      "Epoch 180901, Training Loss: 95784, Validation Loss: 93754, 2761.8100192566485\n",
      "Epoch 181001, Training Loss: 99888, Validation Loss: 93960, 1927.920006527731\n",
      "Epoch 181101, Training Loss: 98668, Validation Loss: 93756, 2922.3007613309524\n",
      "Epoch 181201, Training Loss: 97531, Validation Loss: 94230, 2034.989966461362\n",
      "Epoch 181301, Training Loss: 97634, Validation Loss: 93743, 3125.9680144432546\n",
      "Epoch 181401, Training Loss: 100729, Validation Loss: 94008, 2453.437212661797\n",
      "Epoch 181501, Training Loss: 95279, Validation Loss: 94000, 2671.9306888150654\n",
      "Epoch 181601, Training Loss: 98545, Validation Loss: 94103, 1869.519203172698\n",
      "Epoch 181701, Training Loss: 100381, Validation Loss: 93947, 4043.9461423871467\n",
      "Epoch 181801, Training Loss: 98944, Validation Loss: 94097, 2028.208732938708\n",
      "Epoch 181901, Training Loss: 94377, Validation Loss: 94655, 2091.2317817663047\n",
      "Epoch 182001, Training Loss: 101012, Validation Loss: 93722, 1858.2589100720406\n",
      "Epoch 182101, Training Loss: 97639, Validation Loss: 93950, 2168.4969326142295\n",
      "Epoch 182201, Training Loss: 100835, Validation Loss: 93894, 3507.2268265531275\n",
      "Epoch 182301, Training Loss: 98340, Validation Loss: 93893, 3444.006987293657\n",
      "Epoch 182401, Training Loss: 98016, Validation Loss: 94322, 3572.573117843085\n",
      "Epoch 182501, Training Loss: 99209, Validation Loss: 93891, 1362.1724312933818\n",
      "Epoch 182601, Training Loss: 99815, Validation Loss: 93861, 3487.3826965925114\n",
      "Epoch 182701, Training Loss: 100547, Validation Loss: 93873, 2061.5433190733693\n",
      "Epoch 182801, Training Loss: 97011, Validation Loss: 93870, 3101.769573241483\n",
      "Epoch 182901, Training Loss: 96088, Validation Loss: 93758, 2830.421241812794\n",
      "Epoch 183001, Training Loss: 98717, Validation Loss: 94120, 1313.8207410390585\n",
      "Epoch 183101, Training Loss: 95813, Validation Loss: 94006, 2952.3199128393353\n",
      "Epoch 183201, Training Loss: 102452, Validation Loss: 93949, 3737.444450633449\n",
      "Epoch 183301, Training Loss: 95014, Validation Loss: 94561, 1879.721992659909\n",
      "Epoch 183401, Training Loss: 100251, Validation Loss: 94282, 3818.95499420462\n",
      "Epoch 183501, Training Loss: 103306, Validation Loss: 93903, 2062.9783897837\n",
      "Epoch 183601, Training Loss: 104799, Validation Loss: 93929, 1629.0748908435687\n",
      "Epoch 183701, Training Loss: 97216, Validation Loss: 94436, 2789.5004187129175\n",
      "Epoch 183801, Training Loss: 96691, Validation Loss: 93973, 2713.1091812816585\n",
      "Epoch 183901, Training Loss: 95151, Validation Loss: 93960, 1841.0554747384697\n",
      "Epoch 184001, Training Loss: 100192, Validation Loss: 93764, 1733.864674941334\n",
      "Epoch 184101, Training Loss: 96585, Validation Loss: 94033, 1373.8050742451378\n",
      "Epoch 184201, Training Loss: 95620, Validation Loss: 94050, 1684.3887877620457\n",
      "Epoch 184301, Training Loss: 91460, Validation Loss: 103190, 210445.6187445575\n",
      "Epoch 184401, Training Loss: 77283, Validation Loss: 82816, 430351.30945887597\n",
      "Epoch 184501, Training Loss: 71582, Validation Loss: 71824, 532648.8896926036\n",
      "Epoch 184601, Training Loss: 75129, Validation Loss: 73198, 354285.1677366076\n",
      "Epoch 184701, Training Loss: 70841, Validation Loss: 86279, 424469.1322949881\n",
      "Epoch 184801, Training Loss: 71716, Validation Loss: 75910, 896597.8775305079\n",
      "Epoch 184901, Training Loss: 66916, Validation Loss: 81525, 371427.2275522503\n",
      "Epoch 185001, Training Loss: 67285, Validation Loss: 79581, 451407.6863262206\n",
      "Epoch 185101, Training Loss: 74043, Validation Loss: 79620, 685362.9613899172\n",
      "Epoch 185201, Training Loss: 70199, Validation Loss: 66427, 584150.0733050172\n",
      "Epoch 185301, Training Loss: 67744, Validation Loss: 75054, 532224.855593914\n",
      "Epoch 185401, Training Loss: 80269, Validation Loss: 78619, 889028.2751886374\n",
      "Epoch 185501, Training Loss: 64604, Validation Loss: 77490, 315184.61642848596\n",
      "Epoch 185601, Training Loss: 94277, Validation Loss: 64806, 2001229.4141658263\n",
      "Epoch 185701, Training Loss: 65300, Validation Loss: 61239, 1385278.2350665352\n",
      "Epoch 185801, Training Loss: 67137, Validation Loss: 70729, 532510.9351928501\n",
      "Epoch 185901, Training Loss: 72843, Validation Loss: 84055, 899451.5169053567\n",
      "Epoch 186001, Training Loss: 70588, Validation Loss: 74642, 220024.62567998774\n",
      "Epoch 186101, Training Loss: 68428, Validation Loss: 69960, 116769.93036305676\n",
      "Epoch 186201, Training Loss: 62876, Validation Loss: 72315, 195732.68292433757\n",
      "Epoch 186301, Training Loss: 61825, Validation Loss: 80766, 232289.35978915275\n",
      "Epoch 186401, Training Loss: 68559, Validation Loss: 71543, 908079.0040080318\n",
      "Epoch 186501, Training Loss: 68241, Validation Loss: 70993, 215151.79800825295\n",
      "Epoch 186601, Training Loss: 68552, Validation Loss: 76121, 682477.5371073458\n",
      "Epoch 186701, Training Loss: 63786, Validation Loss: 76734, 563867.442048248\n",
      "Epoch 186801, Training Loss: 65245, Validation Loss: 67159, 183764.2034686346\n",
      "Epoch 186901, Training Loss: 70810, Validation Loss: 74813, 321369.2477233152\n",
      "Epoch 187001, Training Loss: 64738, Validation Loss: 65622, 769160.302060774\n",
      "Epoch 187101, Training Loss: 73285, Validation Loss: 68261, 788283.0265846584\n",
      "Epoch 187201, Training Loss: 70470, Validation Loss: 76246, 264338.32238621294\n",
      "Epoch 187301, Training Loss: 64860, Validation Loss: 68759, 576017.6786856459\n",
      "Epoch 187401, Training Loss: 71277, Validation Loss: 72071, 961119.9427051116\n",
      "Epoch 187501, Training Loss: 68010, Validation Loss: 68614, 585383.1635226888\n",
      "Epoch 187601, Training Loss: 64837, Validation Loss: 81872, 512423.4196848858\n",
      "Epoch 187701, Training Loss: 67286, Validation Loss: 62172, 334167.2747199778\n",
      "Epoch 187801, Training Loss: 69122, Validation Loss: 67604, 1037983.2648016475\n",
      "Epoch 187901, Training Loss: 70702, Validation Loss: 61178, 686228.3815691556\n",
      "Epoch 188001, Training Loss: 66578, Validation Loss: 76455, 743657.0222209838\n",
      "Epoch 188101, Training Loss: 71511, Validation Loss: 74618, 950225.407392034\n",
      "Epoch 188201, Training Loss: 72754, Validation Loss: 65254, 969412.2753297025\n",
      "Epoch 188301, Training Loss: 62142, Validation Loss: 69425, 232543.2561349692\n",
      "Epoch 188401, Training Loss: 98719, Validation Loss: 93823, 3733.893385263367\n",
      "Epoch 188501, Training Loss: 99436, Validation Loss: 93748, 4376.616634929272\n",
      "Epoch 188601, Training Loss: 99075, Validation Loss: 93692, 2193.1297188788317\n",
      "Epoch 188701, Training Loss: 99783, Validation Loss: 94049, 3109.3873694478666\n",
      "Epoch 188801, Training Loss: 99361, Validation Loss: 93770, 5846.389841991101\n",
      "Epoch 188901, Training Loss: 95618, Validation Loss: 94064, 2754.621943450215\n",
      "Epoch 189001, Training Loss: 100605, Validation Loss: 94060, 4791.0626324801815\n",
      "Epoch 189101, Training Loss: 101051, Validation Loss: 98494, 73955.89327028321\n",
      "Epoch 189201, Training Loss: 75566, Validation Loss: 80854, 366202.33000556333\n",
      "Epoch 189301, Training Loss: 77400, Validation Loss: 78772, 894038.1514649838\n",
      "Epoch 189401, Training Loss: 69009, Validation Loss: 75575, 773994.8160460037\n",
      "Epoch 189501, Training Loss: 73563, Validation Loss: 77755, 246948.61714968728\n",
      "Epoch 189601, Training Loss: 67896, Validation Loss: 65671, 690807.3359664247\n",
      "Epoch 189701, Training Loss: 65689, Validation Loss: 75699, 121336.24564749845\n",
      "Epoch 189801, Training Loss: 65275, Validation Loss: 69089, 225874.3649323075\n",
      "Epoch 189901, Training Loss: 65934, Validation Loss: 81314, 620966.681697078\n",
      "Epoch 190001, Training Loss: 68926, Validation Loss: 72054, 740737.1593863168\n",
      "Epoch 190101, Training Loss: 77346, Validation Loss: 70032, 1030418.5754996734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190201, Training Loss: 63716, Validation Loss: 76258, 644526.8476683454\n",
      "Epoch 190301, Training Loss: 68439, Validation Loss: 69594, 489160.02800204215\n",
      "Epoch 190401, Training Loss: 69706, Validation Loss: 70781, 665677.0455972129\n",
      "Epoch 190501, Training Loss: 65957, Validation Loss: 72974, 468026.68521482777\n",
      "Epoch 190601, Training Loss: 69766, Validation Loss: 72796, 267273.61360439606\n",
      "Epoch 190701, Training Loss: 71404, Validation Loss: 76455, 517799.98764765327\n",
      "Epoch 190801, Training Loss: 70606, Validation Loss: 69939, 343058.31994845165\n",
      "Epoch 190901, Training Loss: 87100, Validation Loss: 60021, 2623943.3062306317\n",
      "Epoch 191001, Training Loss: 64269, Validation Loss: 74084, 366583.2191454871\n",
      "Epoch 191101, Training Loss: 70548, Validation Loss: 63217, 715084.5936125814\n",
      "Epoch 191201, Training Loss: 72676, Validation Loss: 79894, 457970.55408549146\n",
      "Epoch 191301, Training Loss: 65390, Validation Loss: 74446, 340170.66752494936\n",
      "Epoch 191401, Training Loss: 70243, Validation Loss: 65612, 496109.5449278609\n",
      "Epoch 191501, Training Loss: 67505, Validation Loss: 74680, 412765.8594855975\n",
      "Epoch 191601, Training Loss: 66553, Validation Loss: 74840, 391263.0704019998\n",
      "Epoch 191701, Training Loss: 66135, Validation Loss: 74491, 257102.75288589916\n",
      "Epoch 191801, Training Loss: 67028, Validation Loss: 77486, 149222.32323116486\n",
      "Epoch 191901, Training Loss: 67979, Validation Loss: 71475, 122688.22012931673\n",
      "Epoch 192001, Training Loss: 64384, Validation Loss: 74207, 404732.158932477\n",
      "Epoch 192101, Training Loss: 68144, Validation Loss: 66480, 909096.3849459076\n",
      "Epoch 192201, Training Loss: 66617, Validation Loss: 68614, 680945.4005954553\n",
      "Epoch 192301, Training Loss: 83386, Validation Loss: 79265, 454064.3822562183\n",
      "Epoch 192401, Training Loss: 68868, Validation Loss: 69945, 1041241.2995285917\n",
      "Epoch 192501, Training Loss: 66732, Validation Loss: 68523, 279560.1476905528\n",
      "Epoch 192601, Training Loss: 69822, Validation Loss: 76804, 404328.11138845497\n",
      "Epoch 192701, Training Loss: 63324, Validation Loss: 71113, 173885.01927940117\n",
      "Epoch 192801, Training Loss: 68368, Validation Loss: 71858, 885939.3923277556\n",
      "Epoch 192901, Training Loss: 63647, Validation Loss: 70669, 669912.4938441501\n",
      "Epoch 193001, Training Loss: 66355, Validation Loss: 66152, 334115.1104568942\n",
      "Epoch 193101, Training Loss: 67082, Validation Loss: 74361, 581803.1079081451\n",
      "Epoch 193201, Training Loss: 78481, Validation Loss: 74148, 542613.3749718457\n",
      "Epoch 193301, Training Loss: 73567, Validation Loss: 70404, 220566.50518811375\n",
      "Epoch 193401, Training Loss: 66130, Validation Loss: 68940, 392728.03189602564\n",
      "Epoch 193501, Training Loss: 64362, Validation Loss: 79175, 307671.4866693867\n",
      "Epoch 193601, Training Loss: 67966, Validation Loss: 68889, 159524.4882295094\n",
      "Epoch 193701, Training Loss: 63360, Validation Loss: 74265, 268183.1153218009\n",
      "Epoch 193801, Training Loss: 65667, Validation Loss: 66256, 230745.60758481792\n",
      "Epoch 193901, Training Loss: 65354, Validation Loss: 75723, 313481.01292644476\n",
      "Epoch 194001, Training Loss: 68428, Validation Loss: 77238, 269785.28060280986\n",
      "Epoch 194101, Training Loss: 66560, Validation Loss: 85184, 610565.9412700258\n",
      "Epoch 194201, Training Loss: 101781, Validation Loss: 93816, 3979.609985989204\n",
      "Epoch 194301, Training Loss: 98550, Validation Loss: 93936, 2360.679550649436\n",
      "Epoch 194401, Training Loss: 99198, Validation Loss: 94269, 1910.1040823443839\n",
      "Epoch 194501, Training Loss: 93882, Validation Loss: 94006, 4147.857556696578\n",
      "Epoch 194601, Training Loss: 94915, Validation Loss: 94060, 2099.2395543012994\n",
      "Epoch 194701, Training Loss: 97899, Validation Loss: 94079, 1773.4209178947176\n",
      "Epoch 194801, Training Loss: 97201, Validation Loss: 94158, 3034.45457581721\n",
      "Epoch 194901, Training Loss: 98731, Validation Loss: 94020, 1561.1351429105525\n",
      "Epoch 195001, Training Loss: 95035, Validation Loss: 94254, 2699.828945644855\n",
      "Epoch 195101, Training Loss: 96763, Validation Loss: 94324, 1595.4580449419948\n",
      "Epoch 195201, Training Loss: 98672, Validation Loss: 94288, 2707.670139045927\n",
      "Epoch 195301, Training Loss: 93490, Validation Loss: 94196, 3051.886767128069\n",
      "Epoch 195401, Training Loss: 94622, Validation Loss: 94077, 2088.236700987239\n",
      "Epoch 195501, Training Loss: 99026, Validation Loss: 93746, 3795.006237336607\n",
      "Epoch 195601, Training Loss: 100744, Validation Loss: 93885, 2770.279233600982\n",
      "Epoch 195701, Training Loss: 101920, Validation Loss: 94171, 3305.2035504197443\n",
      "Epoch 195801, Training Loss: 97193, Validation Loss: 94001, 1908.8122844713491\n",
      "Epoch 195901, Training Loss: 94638, Validation Loss: 154378, 4674.836664997252\n",
      "Epoch 196001, Training Loss: 98229, Validation Loss: 94211, 3456.388104375603\n",
      "Epoch 196101, Training Loss: 100454, Validation Loss: 94301, 4372.389651920535\n",
      "Epoch 196201, Training Loss: 100838, Validation Loss: 94100, 1742.9787412355063\n",
      "Epoch 196301, Training Loss: 97377, Validation Loss: 94433, 3919.3203942586965\n",
      "Epoch 196401, Training Loss: 96884, Validation Loss: 94264, 3056.353768661187\n",
      "Epoch 196501, Training Loss: 96699, Validation Loss: 94158, 1599.3226092398372\n",
      "Epoch 196601, Training Loss: 98254, Validation Loss: 93806, 2727.5751760727544\n",
      "Epoch 196701, Training Loss: 97105, Validation Loss: 94147, 1708.0726000769118\n",
      "Epoch 196801, Training Loss: 101678, Validation Loss: 93704, 3019.2036966780956\n",
      "Epoch 196901, Training Loss: 95652, Validation Loss: 93894, 1977.3927057021167\n",
      "Epoch 197001, Training Loss: 96537, Validation Loss: 93921, 4221.873209262297\n",
      "Epoch 197101, Training Loss: 94271, Validation Loss: 93734, 2833.8159806675867\n",
      "Epoch 197201, Training Loss: 97849, Validation Loss: 94188, 1909.4547168164554\n",
      "Epoch 197301, Training Loss: 95220, Validation Loss: 94105, 2413.7581903181112\n",
      "Epoch 197401, Training Loss: 97343, Validation Loss: 93968, 3306.0393608512045\n",
      "Epoch 197501, Training Loss: 95022, Validation Loss: 93853, 2930.5058931583285\n",
      "Epoch 197601, Training Loss: 100904, Validation Loss: 93836, 3282.304617335793\n",
      "Epoch 197701, Training Loss: 96946, Validation Loss: 94115, 2336.3937042960183\n",
      "Epoch 197801, Training Loss: 94357, Validation Loss: 94587, 2916.329407100615\n",
      "Epoch 197901, Training Loss: 102165, Validation Loss: 93964, 4252.275769681045\n",
      "Epoch 198001, Training Loss: 98720, Validation Loss: 94222, 2935.709862324298\n",
      "Epoch 198101, Training Loss: 100484, Validation Loss: 93982, 1534.311822289358\n",
      "Epoch 198201, Training Loss: 101480, Validation Loss: 94184, 1943.679553371274\n",
      "Epoch 198301, Training Loss: 97409, Validation Loss: 93961, 2138.196289200662\n",
      "Epoch 198401, Training Loss: 96923, Validation Loss: 94011, 1772.66759961599\n",
      "Epoch 198501, Training Loss: 98767, Validation Loss: 94133, 1829.1051097392428\n",
      "Epoch 198601, Training Loss: 98150, Validation Loss: 94045, 2885.225101737771\n",
      "Epoch 198701, Training Loss: 97873, Validation Loss: 94166, 3587.078237867592\n",
      "Epoch 198801, Training Loss: 99956, Validation Loss: 93941, 1755.660336416934\n",
      "Epoch 198901, Training Loss: 96783, Validation Loss: 94133, 4337.0030904742825\n",
      "Epoch 199001, Training Loss: 99562, Validation Loss: 93752, 2860.230693122235\n",
      "Epoch 199101, Training Loss: 96612, Validation Loss: 93760, 1810.054846510165\n",
      "Epoch 199201, Training Loss: 100543, Validation Loss: 94570, 2059.803870035629\n",
      "Epoch 199301, Training Loss: 98707, Validation Loss: 93754, 2600.2348388990854\n",
      "Epoch 199401, Training Loss: 94841, Validation Loss: 93943, 3094.967944169959\n",
      "Epoch 199501, Training Loss: 97468, Validation Loss: 94031, 5151.294182440687\n",
      "Epoch 199601, Training Loss: 99183, Validation Loss: 93905, 2932.1477094548554\n",
      "Epoch 199701, Training Loss: 98374, Validation Loss: 93908, 2313.6114520307456\n",
      "Epoch 199801, Training Loss: 108727, Validation Loss: 93832, 397949.2236442281\n",
      "Epoch 199901, Training Loss: 100683, Validation Loss: 93985, 3240.3209316104494\n",
      "Epoch 200001, Training Loss: 100941, Validation Loss: 93962, 2387.562812995428\n",
      "Epoch 200101, Training Loss: 98851, Validation Loss: 93951, 2609.985295502133\n",
      "Epoch 200201, Training Loss: 99081, Validation Loss: 94058, 1755.3973461640633\n",
      "Epoch 200301, Training Loss: 100354, Validation Loss: 94128, 1901.523834349925\n",
      "Epoch 200401, Training Loss: 96719, Validation Loss: 94294, 2234.6026286270476\n",
      "Epoch 200501, Training Loss: 98564, Validation Loss: 94047, 3205.593597899571\n",
      "Epoch 200601, Training Loss: 99735, Validation Loss: 93764, 1992.2176199910075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200701, Training Loss: 97952, Validation Loss: 93744, 3805.470124708623\n",
      "Epoch 200801, Training Loss: 100650, Validation Loss: 93743, 3798.4269284157394\n",
      "Epoch 200901, Training Loss: 97273, Validation Loss: 93884, 2902.6751049780255\n",
      "Epoch 201001, Training Loss: 97834, Validation Loss: 94203, 2203.1431854162993\n",
      "Epoch 201101, Training Loss: 100913, Validation Loss: 94019, 1759.0949626171757\n",
      "Epoch 201201, Training Loss: 97005, Validation Loss: 93902, 4458.710087523732\n",
      "Epoch 201301, Training Loss: 97890, Validation Loss: 94181, 1635.0903164559434\n",
      "Epoch 201401, Training Loss: 92114, Validation Loss: 93874, 1612.6010147722802\n",
      "Epoch 201501, Training Loss: 99717, Validation Loss: 93833, 4326.9531507344755\n",
      "Epoch 201601, Training Loss: 96445, Validation Loss: 93932, 1462.167377868044\n",
      "Epoch 201701, Training Loss: 96848, Validation Loss: 93786, 2014.6527942209461\n",
      "Epoch 201801, Training Loss: 100030, Validation Loss: 94046, 1967.6982014520172\n",
      "Epoch 201901, Training Loss: 95687, Validation Loss: 93852, 1394.4844917541595\n",
      "Epoch 202001, Training Loss: 100799, Validation Loss: 93807, 2649.458493291196\n",
      "Epoch 202101, Training Loss: 100900, Validation Loss: 94036, 1733.504586767712\n",
      "Epoch 202201, Training Loss: 96955, Validation Loss: 93924, 2283.1781223374833\n",
      "Epoch 202301, Training Loss: 98886, Validation Loss: 93876, 2892.6362446750895\n",
      "Epoch 202401, Training Loss: 100133, Validation Loss: 93923, 1764.0950097550722\n",
      "Epoch 202501, Training Loss: 100489, Validation Loss: 93774, 2500.2440960768095\n",
      "Epoch 202601, Training Loss: 97513, Validation Loss: 93917, 2136.943520493316\n",
      "Epoch 202701, Training Loss: 100010, Validation Loss: 93929, 3364.018196564703\n",
      "Epoch 202801, Training Loss: 95506, Validation Loss: 93830, 1585.5626245222347\n",
      "Epoch 202901, Training Loss: 99161, Validation Loss: 93781, 2335.232056266401\n",
      "Epoch 203001, Training Loss: 98134, Validation Loss: 93824, 2559.2037577142896\n",
      "Epoch 203101, Training Loss: 99236, Validation Loss: 94048, 2078.0005523910445\n",
      "Epoch 203201, Training Loss: 93407, Validation Loss: 94090, 2425.4142118167206\n",
      "Epoch 203301, Training Loss: 99571, Validation Loss: 93803, 2157.91643047783\n",
      "Epoch 203401, Training Loss: 97416, Validation Loss: 93844, 2959.3752362790187\n",
      "Epoch 203501, Training Loss: 96571, Validation Loss: 94257, 2030.052695987136\n",
      "Epoch 203601, Training Loss: 102862, Validation Loss: 94404, 2358.3268351727843\n",
      "Epoch 203701, Training Loss: 101645, Validation Loss: 93783, 2034.5911687938471\n",
      "Epoch 203801, Training Loss: 94341, Validation Loss: 94176, 1506.192722542358\n",
      "Epoch 203901, Training Loss: 101167, Validation Loss: 94633, 3323.196011487436\n",
      "Epoch 204001, Training Loss: 97398, Validation Loss: 93818, 2910.387202098859\n",
      "Epoch 204101, Training Loss: 92852, Validation Loss: 94215, 1831.9974539954064\n",
      "Epoch 204201, Training Loss: 94540, Validation Loss: 94034, 2402.3039252664803\n",
      "Epoch 204301, Training Loss: 97409, Validation Loss: 93851, 2446.4954345306724\n",
      "Epoch 204401, Training Loss: 102799, Validation Loss: 94016, 2128.2474861989126\n",
      "Epoch 204501, Training Loss: 96893, Validation Loss: 93782, 1724.972679602555\n",
      "Epoch 204601, Training Loss: 98260, Validation Loss: 93987, 1983.9511898571798\n",
      "Epoch 204701, Training Loss: 101342, Validation Loss: 94063, 1784.6712911413917\n",
      "Epoch 204801, Training Loss: 99807, Validation Loss: 94114, 1781.0827741428059\n",
      "Epoch 204901, Training Loss: 97898, Validation Loss: 93997, 2486.476990484548\n",
      "Epoch 205001, Training Loss: 97338, Validation Loss: 94091, 2447.9355348127306\n",
      "Epoch 205101, Training Loss: 97340, Validation Loss: 93777, 2657.7824905583398\n",
      "Epoch 205201, Training Loss: 96737, Validation Loss: 94136, 2848.8115374652875\n",
      "Epoch 205301, Training Loss: 95704, Validation Loss: 93808, 1932.8413381288717\n",
      "Epoch 205401, Training Loss: 94546, Validation Loss: 94506, 2146.1111958306906\n",
      "Epoch 205501, Training Loss: 100625, Validation Loss: 94023, 1839.9917578851184\n",
      "Epoch 205601, Training Loss: 94511, Validation Loss: 93794, 2578.67536162528\n",
      "Epoch 205701, Training Loss: 94787, Validation Loss: 94124, 2857.863070866613\n",
      "Epoch 205801, Training Loss: 99064, Validation Loss: 93893, 2425.6077466335187\n",
      "Epoch 205901, Training Loss: 95794, Validation Loss: 93775, 1407.811804942574\n",
      "Epoch 206001, Training Loss: 97460, Validation Loss: 93743, 2114.833600887154\n",
      "Epoch 206101, Training Loss: 99017, Validation Loss: 93728, 2437.0461596489686\n",
      "Epoch 206201, Training Loss: 96344, Validation Loss: 94730, 2524.0292587212275\n",
      "Epoch 206301, Training Loss: 97653, Validation Loss: 94025, 1286.6334856880062\n",
      "Epoch 206401, Training Loss: 96724, Validation Loss: 94055, 2071.880133730828\n",
      "Epoch 206501, Training Loss: 101605, Validation Loss: 93818, 2463.359986936499\n",
      "Epoch 206601, Training Loss: 98744, Validation Loss: 93728, 3601.154995198816\n",
      "Epoch 206701, Training Loss: 97779, Validation Loss: 93788, 3877.4177112545603\n",
      "Epoch 206801, Training Loss: 95290, Validation Loss: 93779, 2373.442006024612\n",
      "Epoch 206901, Training Loss: 99294, Validation Loss: 93749, 1492.3074610117226\n",
      "Epoch 207001, Training Loss: 100028, Validation Loss: 93949, 1201.194685682096\n",
      "Epoch 207101, Training Loss: 100759, Validation Loss: 93926, 1519.514894631091\n",
      "Epoch 207201, Training Loss: 96728, Validation Loss: 93869, 2827.9603407169084\n",
      "Epoch 207301, Training Loss: 101018, Validation Loss: 94305, 2997.932019865811\n",
      "Epoch 207401, Training Loss: 95019, Validation Loss: 94077, 2399.0930608696367\n",
      "Epoch 207501, Training Loss: 98575, Validation Loss: 94134, 1485.750724724412\n",
      "Epoch 207601, Training Loss: 98365, Validation Loss: 94041, 2085.713663159358\n",
      "Epoch 207701, Training Loss: 100446, Validation Loss: 93786, 2322.850026102547\n",
      "Epoch 207801, Training Loss: 99557, Validation Loss: 93939, 1137.924967471666\n",
      "Epoch 207901, Training Loss: 102097, Validation Loss: 93923, 1268.8583234015457\n",
      "Epoch 208001, Training Loss: 95898, Validation Loss: 94010, 2086.7175713031093\n",
      "Epoch 208101, Training Loss: 99197, Validation Loss: 93966, 2351.495620360337\n",
      "Epoch 208201, Training Loss: 100135, Validation Loss: 93874, 2529.6786149757427\n",
      "Epoch 208301, Training Loss: 102045, Validation Loss: 94009, 2778.344743963478\n",
      "Epoch 208401, Training Loss: 98148, Validation Loss: 93986, 2426.657119159836\n",
      "Epoch 208501, Training Loss: 98086, Validation Loss: 93846, 2445.779846091966\n",
      "Epoch 208601, Training Loss: 97264, Validation Loss: 93880, 1917.4511890551112\n",
      "Epoch 208701, Training Loss: 93973, Validation Loss: 94283, 1224.061165165352\n",
      "Epoch 208801, Training Loss: 99211, Validation Loss: 94278, 3180.5794657664796\n",
      "Epoch 208901, Training Loss: 98700, Validation Loss: 94274, 1639.4783006375135\n",
      "Epoch 209001, Training Loss: 99178, Validation Loss: 93988, 3050.458898178422\n",
      "Epoch 209101, Training Loss: 97032, Validation Loss: 94064, 1582.255623388759\n",
      "Epoch 209201, Training Loss: 102406, Validation Loss: 94184, 2786.9864235222462\n",
      "Epoch 209301, Training Loss: 97469, Validation Loss: 94037, 2265.306607188314\n",
      "Epoch 209401, Training Loss: 96421, Validation Loss: 93983, 2183.06810939975\n",
      "Epoch 209501, Training Loss: 98401, Validation Loss: 94726, 2314.1716626848133\n",
      "Epoch 209601, Training Loss: 94232, Validation Loss: 93822, 4343.724126949176\n",
      "Epoch 209701, Training Loss: 97121, Validation Loss: 93841, 3262.2814577275553\n",
      "Epoch 209801, Training Loss: 101563, Validation Loss: 93792, 1441.2478519138356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m l1_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tuple_ \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     25\u001b[0m     datas, prices \u001b[38;5;241m=\u001b[39m tuple_\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=9e-3,\n",
    "    weight_decay=3e-4\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999999, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
