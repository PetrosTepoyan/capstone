{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 64\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.18),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 237618, Validation Loss: 244199, 4.901643965448241\n",
      "Epoch 101, Training Loss: 63822, Validation Loss: 75390, 115505.72718832786\n",
      "Epoch 201, Training Loss: 64750, Validation Loss: 58377, 157773.09242055262\n",
      "Epoch 301, Training Loss: 57438, Validation Loss: 57192, 131955.31950421073\n",
      "Epoch 401, Training Loss: 62794, Validation Loss: 82477, 105893.43425960625\n",
      "Epoch 501, Training Loss: 59906, Validation Loss: 67746, 154712.5234409844\n",
      "Epoch 601, Training Loss: 56398, Validation Loss: 62106, 143045.96713120272\n",
      "Epoch 701, Training Loss: 60372, Validation Loss: 168912, 102599.8403103687\n",
      "Epoch 801, Training Loss: 55904, Validation Loss: 57389, 169477.24510295154\n",
      "Epoch 901, Training Loss: 60868, Validation Loss: 58633, 148765.74806910063\n",
      "Epoch 00924: reducing learning rate of group 0 to 9.9900e-03.\n",
      "Epoch 1001, Training Loss: 57928, Validation Loss: 53580, 148000.65755869882\n",
      "Epoch 01025: reducing learning rate of group 0 to 9.9800e-03.\n",
      "Epoch 1101, Training Loss: 61289, Validation Loss: 56191, 137426.17627635243\n",
      "Epoch 01126: reducing learning rate of group 0 to 9.9700e-03.\n",
      "Epoch 1201, Training Loss: 58234, Validation Loss: 52270, 176971.93885847076\n",
      "Epoch 01279: reducing learning rate of group 0 to 9.9601e-03.\n",
      "Epoch 1301, Training Loss: 55828, Validation Loss: 60334, 133012.82865152563\n",
      "Epoch 01380: reducing learning rate of group 0 to 9.9501e-03.\n",
      "Epoch 1401, Training Loss: 59645, Validation Loss: 55181, 119028.56469970912\n",
      "Epoch 1501, Training Loss: 54587, Validation Loss: 57441, 128505.06104283268\n",
      "Epoch 01530: reducing learning rate of group 0 to 9.9401e-03.\n",
      "Epoch 1601, Training Loss: 54582, Validation Loss: 59648, 145568.52136254817\n",
      "Epoch 01631: reducing learning rate of group 0 to 9.9302e-03.\n",
      "Epoch 1701, Training Loss: 56839, Validation Loss: 57709, 116402.79128516931\n",
      "Epoch 01732: reducing learning rate of group 0 to 9.9203e-03.\n",
      "Epoch 1801, Training Loss: 54485, Validation Loss: 65825, 96601.76027434318\n",
      "Epoch 01833: reducing learning rate of group 0 to 9.9104e-03.\n",
      "Epoch 1901, Training Loss: 58561, Validation Loss: 61638, 139873.3557952469\n",
      "Epoch 01934: reducing learning rate of group 0 to 9.9004e-03.\n",
      "Epoch 2001, Training Loss: 57193, Validation Loss: 54670, 144191.15306824024\n",
      "Epoch 02099: reducing learning rate of group 0 to 9.8905e-03.\n",
      "Epoch 2101, Training Loss: 54420, Validation Loss: 56621, 160651.44851918277\n",
      "Epoch 02200: reducing learning rate of group 0 to 9.8807e-03.\n",
      "Epoch 2201, Training Loss: 55189, Validation Loss: 58049, 156828.2327151159\n",
      "Epoch 2301, Training Loss: 52972, Validation Loss: 54795, 118714.62767205975\n",
      "Epoch 02361: reducing learning rate of group 0 to 9.8708e-03.\n",
      "Epoch 2401, Training Loss: 53773, Validation Loss: 52578, 140063.49056871483\n",
      "Epoch 02462: reducing learning rate of group 0 to 9.8609e-03.\n",
      "Epoch 2501, Training Loss: 55059, Validation Loss: 56341, 135034.3508346992\n",
      "Epoch 02563: reducing learning rate of group 0 to 9.8510e-03.\n",
      "Epoch 2601, Training Loss: 53193, Validation Loss: 72820, 98243.88422618785\n",
      "Epoch 02664: reducing learning rate of group 0 to 9.8412e-03.\n",
      "Epoch 2701, Training Loss: 57050, Validation Loss: 54230, 183338.98275694423\n",
      "Epoch 02765: reducing learning rate of group 0 to 9.8314e-03.\n",
      "Epoch 2801, Training Loss: 54331, Validation Loss: 54107, 141828.88486265272\n",
      "Epoch 02866: reducing learning rate of group 0 to 9.8215e-03.\n",
      "Epoch 2901, Training Loss: 52694, Validation Loss: 58449, 113811.36517923402\n",
      "Epoch 02967: reducing learning rate of group 0 to 9.8117e-03.\n",
      "Epoch 3001, Training Loss: 54686, Validation Loss: 63177, 151086.94020224264\n",
      "Epoch 03068: reducing learning rate of group 0 to 9.8019e-03.\n",
      "Epoch 3101, Training Loss: 54225, Validation Loss: 55992, 97110.0646831288\n",
      "Epoch 03169: reducing learning rate of group 0 to 9.7921e-03.\n",
      "Epoch 3201, Training Loss: 52301, Validation Loss: 56574, 100903.94623463035\n",
      "Epoch 03270: reducing learning rate of group 0 to 9.7823e-03.\n",
      "Epoch 3301, Training Loss: 54413, Validation Loss: 61202, 125386.89220434504\n",
      "Epoch 03371: reducing learning rate of group 0 to 9.7725e-03.\n",
      "Epoch 3401, Training Loss: 53251, Validation Loss: 52756, 98228.16102170893\n",
      "Epoch 03472: reducing learning rate of group 0 to 9.7627e-03.\n",
      "Epoch 3501, Training Loss: 51810, Validation Loss: 54426, 131192.96530241834\n",
      "Epoch 03573: reducing learning rate of group 0 to 9.7530e-03.\n",
      "Epoch 3601, Training Loss: 51377, Validation Loss: 54692, 97029.99012383986\n",
      "Epoch 03674: reducing learning rate of group 0 to 9.7432e-03.\n",
      "Epoch 3701, Training Loss: 56210, Validation Loss: 52448, 165586.39614709807\n",
      "Epoch 03775: reducing learning rate of group 0 to 9.7335e-03.\n",
      "Epoch 3801, Training Loss: 52410, Validation Loss: 57858, 116832.41912596444\n",
      "Epoch 03876: reducing learning rate of group 0 to 9.7237e-03.\n",
      "Epoch 3901, Training Loss: 54546, Validation Loss: 57934, 168902.54766270472\n",
      "Epoch 03977: reducing learning rate of group 0 to 9.7140e-03.\n",
      "Epoch 4001, Training Loss: 51545, Validation Loss: 55411, 131960.36986093086\n",
      "Epoch 04078: reducing learning rate of group 0 to 9.7043e-03.\n",
      "Epoch 4101, Training Loss: 52644, Validation Loss: 60727, 153371.16480023516\n",
      "Epoch 04179: reducing learning rate of group 0 to 9.6946e-03.\n",
      "Epoch 4201, Training Loss: 54447, Validation Loss: 58476, 125292.39713382658\n",
      "Epoch 04280: reducing learning rate of group 0 to 9.6849e-03.\n",
      "Epoch 4301, Training Loss: 53553, Validation Loss: 57310, 172218.96546862338\n",
      "Epoch 04381: reducing learning rate of group 0 to 9.6752e-03.\n",
      "Epoch 4401, Training Loss: 52460, Validation Loss: 55839, 78954.92715553964\n",
      "Epoch 04482: reducing learning rate of group 0 to 9.6656e-03.\n",
      "Epoch 4501, Training Loss: 59064, Validation Loss: 60324, 159034.43367777378\n",
      "Epoch 04583: reducing learning rate of group 0 to 9.6559e-03.\n",
      "Epoch 4601, Training Loss: 51300, Validation Loss: 55232, 85092.48644469747\n",
      "Epoch 04684: reducing learning rate of group 0 to 9.6462e-03.\n",
      "Epoch 4701, Training Loss: 49924, Validation Loss: 58275, 106952.98453951285\n",
      "Epoch 04785: reducing learning rate of group 0 to 9.6366e-03.\n",
      "Epoch 4801, Training Loss: 53966, Validation Loss: 60687, 129489.4260932558\n",
      "Epoch 04886: reducing learning rate of group 0 to 9.6269e-03.\n",
      "Epoch 4901, Training Loss: 49125, Validation Loss: 54776, 111809.28336506976\n",
      "Epoch 04987: reducing learning rate of group 0 to 9.6173e-03.\n",
      "Epoch 5001, Training Loss: 52634, Validation Loss: 55755, 134885.58191916347\n",
      "Epoch 05088: reducing learning rate of group 0 to 9.6077e-03.\n",
      "Epoch 5101, Training Loss: 50792, Validation Loss: 57049, 126699.94674993558\n",
      "Epoch 05189: reducing learning rate of group 0 to 9.5981e-03.\n",
      "Epoch 5201, Training Loss: 50239, Validation Loss: 59383, 131609.22024408245\n",
      "Epoch 05290: reducing learning rate of group 0 to 9.5885e-03.\n",
      "Epoch 5301, Training Loss: 56262, Validation Loss: 52792, 122020.51610308129\n",
      "Epoch 05391: reducing learning rate of group 0 to 9.5789e-03.\n",
      "Epoch 5401, Training Loss: 51056, Validation Loss: 58392, 113753.87926446083\n",
      "Epoch 05492: reducing learning rate of group 0 to 9.5693e-03.\n",
      "Epoch 5501, Training Loss: 49127, Validation Loss: 52844, 113790.55170294503\n",
      "Epoch 05593: reducing learning rate of group 0 to 9.5598e-03.\n",
      "Epoch 5601, Training Loss: 56573, Validation Loss: 67185, 118955.20573853496\n",
      "Epoch 05694: reducing learning rate of group 0 to 9.5502e-03.\n",
      "Epoch 5701, Training Loss: 50285, Validation Loss: 54247, 100854.6912015014\n",
      "Epoch 05795: reducing learning rate of group 0 to 9.5406e-03.\n",
      "Epoch 5801, Training Loss: 52965, Validation Loss: 60247, 159580.02694218783\n",
      "Epoch 05896: reducing learning rate of group 0 to 9.5311e-03.\n",
      "Epoch 5901, Training Loss: 52964, Validation Loss: 58432, 151935.53454027782\n",
      "Epoch 05997: reducing learning rate of group 0 to 9.5216e-03.\n",
      "Epoch 6001, Training Loss: 54435, Validation Loss: 58462, 104500.16262384337\n",
      "Epoch 06098: reducing learning rate of group 0 to 9.5121e-03.\n",
      "Epoch 6101, Training Loss: 50563, Validation Loss: 55123, 114218.22962552456\n",
      "Epoch 06199: reducing learning rate of group 0 to 9.5025e-03.\n",
      "Epoch 6201, Training Loss: 50828, Validation Loss: 58326, 137340.42379829925\n",
      "Epoch 06300: reducing learning rate of group 0 to 9.4930e-03.\n",
      "Epoch 6301, Training Loss: 51207, Validation Loss: 58042, 133777.41625636845\n",
      "Epoch 06401: reducing learning rate of group 0 to 9.4835e-03.\n",
      "Epoch 6401, Training Loss: 48558, Validation Loss: 54006, 85905.89052362622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6501, Training Loss: 49778, Validation Loss: 55482, 90929.4095855466\n",
      "Epoch 06502: reducing learning rate of group 0 to 9.4741e-03.\n",
      "Epoch 6601, Training Loss: 51690, Validation Loss: 53204, 95933.37841312143\n",
      "Epoch 06603: reducing learning rate of group 0 to 9.4646e-03.\n",
      "Epoch 6701, Training Loss: 52297, Validation Loss: 66845, 137095.54305543765\n",
      "Epoch 06704: reducing learning rate of group 0 to 9.4551e-03.\n",
      "Epoch 6801, Training Loss: 50851, Validation Loss: 54155, 138843.5112023227\n",
      "Epoch 06805: reducing learning rate of group 0 to 9.4457e-03.\n",
      "Epoch 6901, Training Loss: 51059, Validation Loss: 52701, 131093.14645773373\n",
      "Epoch 06960: reducing learning rate of group 0 to 9.4362e-03.\n",
      "Epoch 7001, Training Loss: 47896, Validation Loss: 58193, 105503.8994500561\n",
      "Epoch 07061: reducing learning rate of group 0 to 9.4268e-03.\n",
      "Epoch 7101, Training Loss: 49386, Validation Loss: 54902, 147763.48371033216\n",
      "Epoch 07162: reducing learning rate of group 0 to 9.4174e-03.\n",
      "Epoch 7201, Training Loss: 50735, Validation Loss: 53709, 100410.26222198764\n",
      "Epoch 07263: reducing learning rate of group 0 to 9.4079e-03.\n",
      "Epoch 7301, Training Loss: 50730, Validation Loss: 55164, 100573.19881138807\n",
      "Epoch 07364: reducing learning rate of group 0 to 9.3985e-03.\n",
      "Epoch 7401, Training Loss: 49325, Validation Loss: 55453, 123015.27059205448\n",
      "Epoch 07465: reducing learning rate of group 0 to 9.3891e-03.\n",
      "Epoch 7501, Training Loss: 49993, Validation Loss: 61058, 116609.64030844702\n",
      "Epoch 07566: reducing learning rate of group 0 to 9.3797e-03.\n",
      "Epoch 7601, Training Loss: 49387, Validation Loss: 53350, 119505.03815053531\n",
      "Epoch 07667: reducing learning rate of group 0 to 9.3704e-03.\n",
      "Epoch 7701, Training Loss: 48647, Validation Loss: 58090, 87157.21745913742\n",
      "Epoch 7801, Training Loss: 50124, Validation Loss: 61877, 103954.8167508531\n",
      "Epoch 07867: reducing learning rate of group 0 to 9.3610e-03.\n",
      "Epoch 7901, Training Loss: 48479, Validation Loss: 54086, 109905.70094334299\n",
      "Epoch 07968: reducing learning rate of group 0 to 9.3516e-03.\n",
      "Epoch 8001, Training Loss: 45720, Validation Loss: 55325, 99107.67792595028\n",
      "Epoch 08069: reducing learning rate of group 0 to 9.3423e-03.\n",
      "Epoch 8101, Training Loss: 51922, Validation Loss: 57304, 113143.59567252027\n",
      "Epoch 08170: reducing learning rate of group 0 to 9.3329e-03.\n",
      "Epoch 8201, Training Loss: 49431, Validation Loss: 55827, 118762.06794882276\n",
      "Epoch 08271: reducing learning rate of group 0 to 9.3236e-03.\n",
      "Epoch 8301, Training Loss: 48458, Validation Loss: 58019, 125729.72414968301\n",
      "Epoch 08372: reducing learning rate of group 0 to 9.3143e-03.\n",
      "Epoch 8401, Training Loss: 49097, Validation Loss: 53972, 81884.96568104041\n",
      "Epoch 08473: reducing learning rate of group 0 to 9.3050e-03.\n",
      "Epoch 8501, Training Loss: 51068, Validation Loss: 56901, 106543.78881457048\n",
      "Epoch 08574: reducing learning rate of group 0 to 9.2957e-03.\n",
      "Epoch 8601, Training Loss: 48940, Validation Loss: 66268, 98823.29583948078\n",
      "Epoch 08675: reducing learning rate of group 0 to 9.2864e-03.\n",
      "Epoch 8701, Training Loss: 49499, Validation Loss: 55213, 73281.92297732642\n",
      "Epoch 08776: reducing learning rate of group 0 to 9.2771e-03.\n",
      "Epoch 8801, Training Loss: 48407, Validation Loss: 58079, 118415.96672918131\n",
      "Epoch 08877: reducing learning rate of group 0 to 9.2678e-03.\n",
      "Epoch 8901, Training Loss: 47014, Validation Loss: 56738, 102957.05149109148\n",
      "Epoch 08978: reducing learning rate of group 0 to 9.2585e-03.\n",
      "Epoch 9001, Training Loss: 55145, Validation Loss: 54731, 99772.58028918061\n",
      "Epoch 09079: reducing learning rate of group 0 to 9.2493e-03.\n",
      "Epoch 9101, Training Loss: 49978, Validation Loss: 52160, 105876.81599370552\n",
      "Epoch 09180: reducing learning rate of group 0 to 9.2400e-03.\n",
      "Epoch 9201, Training Loss: 46990, Validation Loss: 56816, 120426.73501207151\n",
      "Epoch 09295: reducing learning rate of group 0 to 9.2308e-03.\n",
      "Epoch 9301, Training Loss: 49132, Validation Loss: 50952, 84085.80183001008\n",
      "Epoch 09396: reducing learning rate of group 0 to 9.2216e-03.\n",
      "Epoch 9401, Training Loss: 47675, Validation Loss: 62949, 145881.5377696353\n",
      "Epoch 09497: reducing learning rate of group 0 to 9.2123e-03.\n",
      "Epoch 9501, Training Loss: 47284, Validation Loss: 53810, 117321.68538930232\n",
      "Epoch 9601, Training Loss: 48401, Validation Loss: 63334, 100365.67700824048\n",
      "Epoch 09648: reducing learning rate of group 0 to 9.2031e-03.\n",
      "Epoch 9701, Training Loss: 46070, Validation Loss: 54837, 74149.57110131514\n",
      "Epoch 09749: reducing learning rate of group 0 to 9.1939e-03.\n",
      "Epoch 9801, Training Loss: 52457, Validation Loss: 52313, 94690.23986377293\n",
      "Epoch 09850: reducing learning rate of group 0 to 9.1847e-03.\n",
      "Epoch 9901, Training Loss: 47790, Validation Loss: 52902, 101778.05309805942\n",
      "Epoch 09951: reducing learning rate of group 0 to 9.1755e-03.\n",
      "Epoch 10001, Training Loss: 48089, Validation Loss: 56391, 77517.30326400661\n",
      "Epoch 10052: reducing learning rate of group 0 to 9.1664e-03.\n",
      "Epoch 10101, Training Loss: 48055, Validation Loss: 54640, 93841.65297265637\n",
      "Epoch 10201, Training Loss: 47246, Validation Loss: 55886, 113293.40104616307\n",
      "Epoch 10217: reducing learning rate of group 0 to 9.1572e-03.\n",
      "Epoch 10301, Training Loss: 49221, Validation Loss: 55656, 93070.7813017326\n",
      "Epoch 10318: reducing learning rate of group 0 to 9.1480e-03.\n",
      "Epoch 10401, Training Loss: 48250, Validation Loss: 53469, 75326.55153266269\n",
      "Epoch 10419: reducing learning rate of group 0 to 9.1389e-03.\n",
      "Epoch 10501, Training Loss: 49049, Validation Loss: 53001, 62030.636677114795\n",
      "Epoch 10520: reducing learning rate of group 0 to 9.1298e-03.\n",
      "Epoch 10601, Training Loss: 50969, Validation Loss: 53547, 90016.07426299794\n",
      "Epoch 10621: reducing learning rate of group 0 to 9.1206e-03.\n",
      "Epoch 10701, Training Loss: 50293, Validation Loss: 53427, 101308.47822421168\n",
      "Epoch 10722: reducing learning rate of group 0 to 9.1115e-03.\n",
      "Epoch 10801, Training Loss: 51207, Validation Loss: 52937, 104274.03749654966\n",
      "Epoch 10823: reducing learning rate of group 0 to 9.1024e-03.\n",
      "Epoch 10901, Training Loss: 49323, Validation Loss: 54439, 87641.34413277988\n",
      "Epoch 10933: reducing learning rate of group 0 to 9.0933e-03.\n",
      "Epoch 11001, Training Loss: 49796, Validation Loss: 54720, 92392.88178671752\n",
      "Epoch 11034: reducing learning rate of group 0 to 9.0842e-03.\n",
      "Epoch 11101, Training Loss: 47259, Validation Loss: 56912, 108400.95954096166\n",
      "Epoch 11135: reducing learning rate of group 0 to 9.0751e-03.\n",
      "Epoch 11201, Training Loss: 46271, Validation Loss: 51033, 87609.06391345062\n",
      "Epoch 11236: reducing learning rate of group 0 to 9.0660e-03.\n",
      "Epoch 11301, Training Loss: 50838, Validation Loss: 52602, 151771.59628790113\n",
      "Epoch 11337: reducing learning rate of group 0 to 9.0570e-03.\n",
      "Epoch 11401, Training Loss: 49462, Validation Loss: 56002, 117005.56461175015\n",
      "Epoch 11438: reducing learning rate of group 0 to 9.0479e-03.\n",
      "Epoch 11501, Training Loss: 48469, Validation Loss: 52574, 67118.92745399207\n",
      "Epoch 11539: reducing learning rate of group 0 to 9.0389e-03.\n",
      "Epoch 11601, Training Loss: 50270, Validation Loss: 54626, 105692.4742073103\n",
      "Epoch 11640: reducing learning rate of group 0 to 9.0298e-03.\n",
      "Epoch 11701, Training Loss: 49124, Validation Loss: 59481, 117947.65794694079\n",
      "Epoch 11741: reducing learning rate of group 0 to 9.0208e-03.\n",
      "Epoch 11801, Training Loss: 48200, Validation Loss: 55489, 80937.28303112254\n",
      "Epoch 11842: reducing learning rate of group 0 to 9.0118e-03.\n",
      "Epoch 11901, Training Loss: 50726, Validation Loss: 52091, 127768.66542735223\n",
      "Epoch 12001, Training Loss: 47997, Validation Loss: 52653, 122793.4784051339\n",
      "Epoch 12022: reducing learning rate of group 0 to 9.0028e-03.\n",
      "Epoch 12101, Training Loss: 47061, Validation Loss: 51560, 65345.55566997249\n",
      "Epoch 12123: reducing learning rate of group 0 to 8.9938e-03.\n",
      "Epoch 12201, Training Loss: 50453, Validation Loss: 57576, 91840.23776244815\n",
      "Epoch 12301, Training Loss: 48961, Validation Loss: 57790, 109351.8956999792\n",
      "Epoch 12342: reducing learning rate of group 0 to 8.9848e-03.\n",
      "Epoch 12401, Training Loss: 48213, Validation Loss: 58546, 126815.75037386849\n",
      "Epoch 12443: reducing learning rate of group 0 to 8.9758e-03.\n",
      "Epoch 12501, Training Loss: 45911, Validation Loss: 56016, 85248.28168731144\n",
      "Epoch 12544: reducing learning rate of group 0 to 8.9668e-03.\n",
      "Epoch 12601, Training Loss: 49733, Validation Loss: 52873, 88113.40484526443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12645: reducing learning rate of group 0 to 8.9578e-03.\n",
      "Epoch 12701, Training Loss: 49561, Validation Loss: 51860, 107873.8696744588\n",
      "Epoch 12800: reducing learning rate of group 0 to 8.9489e-03.\n",
      "Epoch 12801, Training Loss: 51763, Validation Loss: 54715, 95622.51468398089\n",
      "Epoch 12901, Training Loss: 49329, Validation Loss: 60460, 94477.72792014561\n",
      "Epoch 12959: reducing learning rate of group 0 to 8.9399e-03.\n",
      "Epoch 13001, Training Loss: 46545, Validation Loss: 53403, 87409.93206160713\n",
      "Epoch 13060: reducing learning rate of group 0 to 8.9310e-03.\n",
      "Epoch 13101, Training Loss: 46151, Validation Loss: 59621, 84539.31168394558\n",
      "Epoch 13161: reducing learning rate of group 0 to 8.9221e-03.\n",
      "Epoch 13201, Training Loss: 45114, Validation Loss: 51706, 59563.38587538896\n",
      "Epoch 13262: reducing learning rate of group 0 to 8.9131e-03.\n",
      "Epoch 13301, Training Loss: 47152, Validation Loss: 50866, 91134.80842914048\n",
      "Epoch 13363: reducing learning rate of group 0 to 8.9042e-03.\n",
      "Epoch 13401, Training Loss: 46536, Validation Loss: 53825, 97950.79387831455\n",
      "Epoch 13464: reducing learning rate of group 0 to 8.8953e-03.\n",
      "Epoch 13501, Training Loss: 46558, Validation Loss: 56875, 79826.8450105603\n",
      "Epoch 13565: reducing learning rate of group 0 to 8.8864e-03.\n",
      "Epoch 13601, Training Loss: 43799, Validation Loss: 52079, 84089.00228876916\n",
      "Epoch 13666: reducing learning rate of group 0 to 8.8775e-03.\n",
      "Epoch 13701, Training Loss: 44801, Validation Loss: 52320, 74264.95371768624\n",
      "Epoch 13767: reducing learning rate of group 0 to 8.8687e-03.\n",
      "Epoch 13801, Training Loss: 47017, Validation Loss: 59441, 99706.12879087025\n",
      "Epoch 13868: reducing learning rate of group 0 to 8.8598e-03.\n",
      "Epoch 13901, Training Loss: 44285, Validation Loss: 53703, 97649.84310481422\n",
      "Epoch 13969: reducing learning rate of group 0 to 8.8509e-03.\n",
      "Epoch 14001, Training Loss: 43909, Validation Loss: 59926, 89597.96477132755\n",
      "Epoch 14070: reducing learning rate of group 0 to 8.8421e-03.\n",
      "Epoch 14101, Training Loss: 46978, Validation Loss: 59700, 91119.4678265638\n",
      "Epoch 14171: reducing learning rate of group 0 to 8.8333e-03.\n",
      "Epoch 14201, Training Loss: 47223, Validation Loss: 50711, 65557.92552004675\n",
      "Epoch 14301, Training Loss: 44189, Validation Loss: 59171, 101338.76060629345\n",
      "Epoch 14368: reducing learning rate of group 0 to 8.8244e-03.\n",
      "Epoch 14401, Training Loss: 46836, Validation Loss: 52537, 64767.08052445381\n",
      "Epoch 14469: reducing learning rate of group 0 to 8.8156e-03.\n",
      "Epoch 14501, Training Loss: 43894, Validation Loss: 54708, 91172.21179757947\n",
      "Epoch 14570: reducing learning rate of group 0 to 8.8068e-03.\n",
      "Epoch 14601, Training Loss: 45345, Validation Loss: 55867, 100956.68797641557\n",
      "Epoch 14671: reducing learning rate of group 0 to 8.7980e-03.\n",
      "Epoch 14701, Training Loss: 47369, Validation Loss: 59057, 87234.13951430045\n",
      "Epoch 14772: reducing learning rate of group 0 to 8.7892e-03.\n",
      "Epoch 14801, Training Loss: 44182, Validation Loss: 53665, 100929.65524658324\n",
      "Epoch 14873: reducing learning rate of group 0 to 8.7804e-03.\n",
      "Epoch 14901, Training Loss: 46914, Validation Loss: 59276, 98383.86195586233\n",
      "Epoch 14974: reducing learning rate of group 0 to 8.7716e-03.\n",
      "Epoch 15001, Training Loss: 47013, Validation Loss: 51002, 96116.38903686772\n",
      "Epoch 15075: reducing learning rate of group 0 to 8.7628e-03.\n",
      "Epoch 15101, Training Loss: 45792, Validation Loss: 59153, 74831.91345542524\n",
      "Epoch 15176: reducing learning rate of group 0 to 8.7541e-03.\n",
      "Epoch 15201, Training Loss: 48120, Validation Loss: 53558, 68835.35683798383\n",
      "Epoch 15277: reducing learning rate of group 0 to 8.7453e-03.\n",
      "Epoch 15301, Training Loss: 45335, Validation Loss: 60591, 91381.3297930975\n",
      "Epoch 15378: reducing learning rate of group 0 to 8.7366e-03.\n",
      "Epoch 15401, Training Loss: 44939, Validation Loss: 57132, 84223.88990644735\n",
      "Epoch 15479: reducing learning rate of group 0 to 8.7278e-03.\n",
      "Epoch 15501, Training Loss: 44334, Validation Loss: 78452, 105896.44175632628\n",
      "Epoch 15580: reducing learning rate of group 0 to 8.7191e-03.\n",
      "Epoch 15601, Training Loss: 44580, Validation Loss: 51242, 81077.38223522679\n",
      "Epoch 15681: reducing learning rate of group 0 to 8.7104e-03.\n",
      "Epoch 15701, Training Loss: 46756, Validation Loss: 56469, 84787.26257195882\n",
      "Epoch 15782: reducing learning rate of group 0 to 8.7017e-03.\n",
      "Epoch 15801, Training Loss: 45201, Validation Loss: 63229, 71296.9292196416\n",
      "Epoch 15883: reducing learning rate of group 0 to 8.6930e-03.\n",
      "Epoch 15901, Training Loss: 47343, Validation Loss: 50619, 86960.32720826638\n",
      "Epoch 15984: reducing learning rate of group 0 to 8.6843e-03.\n",
      "Epoch 16001, Training Loss: 43748, Validation Loss: 50128, 88841.92972789337\n",
      "Epoch 16085: reducing learning rate of group 0 to 8.6756e-03.\n",
      "Epoch 16101, Training Loss: 46280, Validation Loss: 52362, 60986.75670448982\n",
      "Epoch 16186: reducing learning rate of group 0 to 8.6669e-03.\n",
      "Epoch 16201, Training Loss: 45603, Validation Loss: 56544, 116010.88721588784\n",
      "Epoch 16287: reducing learning rate of group 0 to 8.6583e-03.\n",
      "Epoch 16301, Training Loss: 46022, Validation Loss: 56302, 108724.83885572567\n",
      "Epoch 16388: reducing learning rate of group 0 to 8.6496e-03.\n",
      "Epoch 16401, Training Loss: 45524, Validation Loss: 59608, 86425.01781246589\n",
      "Epoch 16489: reducing learning rate of group 0 to 8.6409e-03.\n",
      "Epoch 16501, Training Loss: 46061, Validation Loss: 63767, 92608.22012830153\n",
      "Epoch 16590: reducing learning rate of group 0 to 8.6323e-03.\n",
      "Epoch 16601, Training Loss: 42090, Validation Loss: 50656, 78663.92230340687\n",
      "Epoch 16691: reducing learning rate of group 0 to 8.6237e-03.\n",
      "Epoch 16701, Training Loss: 44976, Validation Loss: 59096, 68999.58021288413\n",
      "Epoch 16792: reducing learning rate of group 0 to 8.6150e-03.\n",
      "Epoch 16801, Training Loss: 44859, Validation Loss: 54282, 76953.46725723696\n",
      "Epoch 16893: reducing learning rate of group 0 to 8.6064e-03.\n",
      "Epoch 16901, Training Loss: 44076, Validation Loss: 51131, 79168.01024268038\n",
      "Epoch 16994: reducing learning rate of group 0 to 8.5978e-03.\n",
      "Epoch 17001, Training Loss: 43739, Validation Loss: 53512, 104526.99313850491\n",
      "Epoch 17095: reducing learning rate of group 0 to 8.5892e-03.\n",
      "Epoch 17101, Training Loss: 45413, Validation Loss: 63359, 100857.8576095936\n",
      "Epoch 17196: reducing learning rate of group 0 to 8.5806e-03.\n",
      "Epoch 17201, Training Loss: 44101, Validation Loss: 55257, 69838.13094084461\n",
      "Epoch 17297: reducing learning rate of group 0 to 8.5721e-03.\n",
      "Epoch 17301, Training Loss: 46311, Validation Loss: 50779, 108517.74370658\n",
      "Epoch 17398: reducing learning rate of group 0 to 8.5635e-03.\n",
      "Epoch 17401, Training Loss: 44426, Validation Loss: 50858, 84653.82401609904\n",
      "Epoch 17499: reducing learning rate of group 0 to 8.5549e-03.\n",
      "Epoch 17501, Training Loss: 43241, Validation Loss: 59143, 100287.50744024497\n",
      "Epoch 17600: reducing learning rate of group 0 to 8.5464e-03.\n",
      "Epoch 17601, Training Loss: 42947, Validation Loss: 52798, 75117.49686893733\n",
      "Epoch 17701: reducing learning rate of group 0 to 8.5378e-03.\n",
      "Epoch 17701, Training Loss: 42326, Validation Loss: 55657, 75372.55079632283\n",
      "Epoch 17801, Training Loss: 45320, Validation Loss: 57314, 132436.44730531637\n",
      "Epoch 17802: reducing learning rate of group 0 to 8.5293e-03.\n",
      "Epoch 17901, Training Loss: 41673, Validation Loss: 52949, 84237.38560647609\n",
      "Epoch 17903: reducing learning rate of group 0 to 8.5208e-03.\n",
      "Epoch 18001, Training Loss: 45755, Validation Loss: 53790, 100451.49581311447\n",
      "Epoch 18004: reducing learning rate of group 0 to 8.5122e-03.\n",
      "Epoch 18101, Training Loss: 43409, Validation Loss: 52223, 71666.09381197981\n",
      "Epoch 18105: reducing learning rate of group 0 to 8.5037e-03.\n",
      "Epoch 18201, Training Loss: 44073, Validation Loss: 65294, 79709.68776083745\n",
      "Epoch 18206: reducing learning rate of group 0 to 8.4952e-03.\n",
      "Epoch 18301, Training Loss: 43654, Validation Loss: 51266, 65858.66382069804\n",
      "Epoch 18307: reducing learning rate of group 0 to 8.4867e-03.\n",
      "Epoch 18401, Training Loss: 45009, Validation Loss: 60702, 85022.08682163777\n",
      "Epoch 18408: reducing learning rate of group 0 to 8.4782e-03.\n",
      "Epoch 18501, Training Loss: 44288, Validation Loss: 56044, 86935.12428442083\n",
      "Epoch 18509: reducing learning rate of group 0 to 8.4698e-03.\n",
      "Epoch 18601, Training Loss: 45633, Validation Loss: 55397, 62342.769410023815\n",
      "Epoch 18610: reducing learning rate of group 0 to 8.4613e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18701, Training Loss: 43037, Validation Loss: 56599, 82536.27153606045\n",
      "Epoch 18711: reducing learning rate of group 0 to 8.4528e-03.\n",
      "Epoch 18801, Training Loss: 43298, Validation Loss: 57732, 93173.59518425603\n",
      "Epoch 18812: reducing learning rate of group 0 to 8.4444e-03.\n",
      "Epoch 18901, Training Loss: 43567, Validation Loss: 56540, 65743.9606122034\n",
      "Epoch 18913: reducing learning rate of group 0 to 8.4359e-03.\n",
      "Epoch 19001, Training Loss: 42777, Validation Loss: 54452, 69766.40851174305\n",
      "Epoch 19014: reducing learning rate of group 0 to 8.4275e-03.\n",
      "Epoch 19101, Training Loss: 42403, Validation Loss: 52360, 88661.60953604062\n",
      "Epoch 19115: reducing learning rate of group 0 to 8.4191e-03.\n",
      "Epoch 19201, Training Loss: 45860, Validation Loss: 58323, 58193.174449187565\n",
      "Epoch 19216: reducing learning rate of group 0 to 8.4106e-03.\n",
      "Epoch 19301, Training Loss: 44380, Validation Loss: 52751, 63036.33019523054\n",
      "Epoch 19317: reducing learning rate of group 0 to 8.4022e-03.\n",
      "Epoch 19401, Training Loss: 42255, Validation Loss: 64620, 73604.83866080591\n",
      "Epoch 19418: reducing learning rate of group 0 to 8.3938e-03.\n",
      "Epoch 19501, Training Loss: 44859, Validation Loss: 51591, 73488.60432823775\n",
      "Epoch 19519: reducing learning rate of group 0 to 8.3854e-03.\n",
      "Epoch 19601, Training Loss: 43563, Validation Loss: 53375, 67996.72874799049\n",
      "Epoch 19620: reducing learning rate of group 0 to 8.3771e-03.\n",
      "Epoch 19701, Training Loss: 42845, Validation Loss: 53476, 96588.48311890905\n",
      "Epoch 19721: reducing learning rate of group 0 to 8.3687e-03.\n",
      "Epoch 19801, Training Loss: 42937, Validation Loss: 64860, 102140.78393572189\n",
      "Epoch 19822: reducing learning rate of group 0 to 8.3603e-03.\n",
      "Epoch 19901, Training Loss: 42950, Validation Loss: 60702, 83231.3534813584\n",
      "Epoch 19923: reducing learning rate of group 0 to 8.3519e-03.\n",
      "Epoch 20001, Training Loss: 45963, Validation Loss: 52075, 86723.07061269587\n",
      "Epoch 20024: reducing learning rate of group 0 to 8.3436e-03.\n",
      "Epoch 20101, Training Loss: 43204, Validation Loss: 51310, 58762.242897696306\n",
      "Epoch 20125: reducing learning rate of group 0 to 8.3353e-03.\n",
      "Epoch 20201, Training Loss: 41823, Validation Loss: 57809, 80864.75556971396\n",
      "Epoch 20226: reducing learning rate of group 0 to 8.3269e-03.\n",
      "Epoch 20301, Training Loss: 43562, Validation Loss: 57823, 74076.93440224737\n",
      "Epoch 20327: reducing learning rate of group 0 to 8.3186e-03.\n",
      "Epoch 20401, Training Loss: 43697, Validation Loss: 50855, 80339.13131503992\n",
      "Epoch 20428: reducing learning rate of group 0 to 8.3103e-03.\n",
      "Epoch 20501, Training Loss: 43070, Validation Loss: 54195, 73952.70633607963\n",
      "Epoch 20529: reducing learning rate of group 0 to 8.3020e-03.\n",
      "Epoch 20601, Training Loss: 42874, Validation Loss: 53958, 76233.25206563529\n",
      "Epoch 20630: reducing learning rate of group 0 to 8.2937e-03.\n",
      "Epoch 20701, Training Loss: 41043, Validation Loss: 53246, 79530.45003597329\n",
      "Epoch 20731: reducing learning rate of group 0 to 8.2854e-03.\n",
      "Epoch 20801, Training Loss: 42206, Validation Loss: 60728, 78263.07170415555\n",
      "Epoch 20832: reducing learning rate of group 0 to 8.2771e-03.\n",
      "Epoch 20901, Training Loss: 41959, Validation Loss: 56047, 80492.76775130986\n",
      "Epoch 20933: reducing learning rate of group 0 to 8.2688e-03.\n",
      "Epoch 21001, Training Loss: 41818, Validation Loss: 62240, 88668.22893395834\n",
      "Epoch 21034: reducing learning rate of group 0 to 8.2605e-03.\n",
      "Epoch 21101, Training Loss: 44944, Validation Loss: 56947, 71467.52400613723\n",
      "Epoch 21135: reducing learning rate of group 0 to 8.2523e-03.\n",
      "Epoch 21201, Training Loss: 41557, Validation Loss: 57012, 53787.56367990951\n",
      "Epoch 21236: reducing learning rate of group 0 to 8.2440e-03.\n",
      "Epoch 21301, Training Loss: 43311, Validation Loss: 50790, 77706.53426521648\n",
      "Epoch 21337: reducing learning rate of group 0 to 8.2358e-03.\n",
      "Epoch 21401, Training Loss: 43920, Validation Loss: 56910, 77566.16180130992\n",
      "Epoch 21438: reducing learning rate of group 0 to 8.2275e-03.\n",
      "Epoch 21501, Training Loss: 45451, Validation Loss: 56912, 83319.54231370268\n",
      "Epoch 21539: reducing learning rate of group 0 to 8.2193e-03.\n",
      "Epoch 21601, Training Loss: 42851, Validation Loss: 64266, 64699.11454516533\n",
      "Epoch 21640: reducing learning rate of group 0 to 8.2111e-03.\n",
      "Epoch 21701, Training Loss: 41761, Validation Loss: 52949, 95208.92591546243\n",
      "Epoch 21741: reducing learning rate of group 0 to 8.2029e-03.\n",
      "Epoch 21801, Training Loss: 42862, Validation Loss: 52853, 80955.60548409358\n",
      "Epoch 21842: reducing learning rate of group 0 to 8.1947e-03.\n",
      "Epoch 21901, Training Loss: 43570, Validation Loss: 58992, 95205.58028034538\n",
      "Epoch 21943: reducing learning rate of group 0 to 8.1865e-03.\n",
      "Epoch 22001, Training Loss: 42307, Validation Loss: 53880, 95294.33699073608\n",
      "Epoch 22044: reducing learning rate of group 0 to 8.1783e-03.\n",
      "Epoch 22101, Training Loss: 42263, Validation Loss: 51002, 93568.12890246573\n",
      "Epoch 22145: reducing learning rate of group 0 to 8.1701e-03.\n",
      "Epoch 22201, Training Loss: 43565, Validation Loss: 52704, 85750.53968550608\n",
      "Epoch 22301, Training Loss: 42739, Validation Loss: 58565, 68540.2663910252\n",
      "Epoch 22344: reducing learning rate of group 0 to 8.1620e-03.\n",
      "Epoch 22401, Training Loss: 41569, Validation Loss: 52500, 72742.76190308621\n",
      "Epoch 22445: reducing learning rate of group 0 to 8.1538e-03.\n",
      "Epoch 22501, Training Loss: 48443, Validation Loss: 60386, 87336.34459966379\n",
      "Epoch 22546: reducing learning rate of group 0 to 8.1456e-03.\n",
      "Epoch 22601, Training Loss: 42898, Validation Loss: 51751, 71494.00184722734\n",
      "Epoch 22647: reducing learning rate of group 0 to 8.1375e-03.\n",
      "Epoch 22701, Training Loss: 42526, Validation Loss: 51560, 96873.87722390669\n",
      "Epoch 22748: reducing learning rate of group 0 to 8.1294e-03.\n",
      "Epoch 22801, Training Loss: 42480, Validation Loss: 53696, 99240.75227478956\n",
      "Epoch 22849: reducing learning rate of group 0 to 8.1212e-03.\n",
      "Epoch 22901, Training Loss: 43227, Validation Loss: 53166, 73758.55820511658\n",
      "Epoch 22950: reducing learning rate of group 0 to 8.1131e-03.\n",
      "Epoch 23001, Training Loss: 41618, Validation Loss: 52748, 57507.888221135596\n",
      "Epoch 23051: reducing learning rate of group 0 to 8.1050e-03.\n",
      "Epoch 23101, Training Loss: 43687, Validation Loss: 53978, 55017.80166195146\n",
      "Epoch 23152: reducing learning rate of group 0 to 8.0969e-03.\n",
      "Epoch 23201, Training Loss: 44606, Validation Loss: 52652, 92510.75359113801\n",
      "Epoch 23253: reducing learning rate of group 0 to 8.0888e-03.\n",
      "Epoch 23301, Training Loss: 41557, Validation Loss: 60911, 66675.60263026143\n",
      "Epoch 23354: reducing learning rate of group 0 to 8.0807e-03.\n",
      "Epoch 23401, Training Loss: 43434, Validation Loss: 53485, 80614.44126660525\n",
      "Epoch 23455: reducing learning rate of group 0 to 8.0726e-03.\n",
      "Epoch 23501, Training Loss: 41802, Validation Loss: 51676, 71913.6250201942\n",
      "Epoch 23556: reducing learning rate of group 0 to 8.0645e-03.\n",
      "Epoch 23601, Training Loss: 41993, Validation Loss: 49955, 76248.52905505743\n",
      "Epoch 23657: reducing learning rate of group 0 to 8.0565e-03.\n",
      "Epoch 23701, Training Loss: 42308, Validation Loss: 52912, 80743.40188319785\n",
      "Epoch 23758: reducing learning rate of group 0 to 8.0484e-03.\n",
      "Epoch 23801, Training Loss: 40457, Validation Loss: 59196, 73685.88901935324\n",
      "Epoch 23859: reducing learning rate of group 0 to 8.0404e-03.\n",
      "Epoch 23901, Training Loss: 40435, Validation Loss: 54039, 63423.76469979018\n",
      "Epoch 23960: reducing learning rate of group 0 to 8.0323e-03.\n",
      "Epoch 24001, Training Loss: 41776, Validation Loss: 64006, 73325.01014892507\n",
      "Epoch 24061: reducing learning rate of group 0 to 8.0243e-03.\n",
      "Epoch 24101, Training Loss: 42780, Validation Loss: 53748, 80972.58945888391\n",
      "Epoch 24162: reducing learning rate of group 0 to 8.0163e-03.\n",
      "Epoch 24201, Training Loss: 43804, Validation Loss: 54542, 97163.15992991121\n",
      "Epoch 24263: reducing learning rate of group 0 to 8.0083e-03.\n",
      "Epoch 24301, Training Loss: 40226, Validation Loss: 54900, 70491.7179340822\n",
      "Epoch 24364: reducing learning rate of group 0 to 8.0003e-03.\n",
      "Epoch 24401, Training Loss: 39905, Validation Loss: 54891, 78510.53543249724\n",
      "Epoch 24465: reducing learning rate of group 0 to 7.9923e-03.\n",
      "Epoch 24501, Training Loss: 40189, Validation Loss: 63721, 62280.662504335116\n",
      "Epoch 24566: reducing learning rate of group 0 to 7.9843e-03.\n",
      "Epoch 24601, Training Loss: 43300, Validation Loss: 54325, 102433.94735471085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24667: reducing learning rate of group 0 to 7.9763e-03.\n",
      "Epoch 24701, Training Loss: 42095, Validation Loss: 54462, 72643.01521585522\n",
      "Epoch 24768: reducing learning rate of group 0 to 7.9683e-03.\n",
      "Epoch 24801, Training Loss: 44389, Validation Loss: 59984, 73319.50819990812\n",
      "Epoch 24869: reducing learning rate of group 0 to 7.9603e-03.\n",
      "Epoch 24901, Training Loss: 40050, Validation Loss: 54720, 66237.0583959132\n",
      "Epoch 24970: reducing learning rate of group 0 to 7.9524e-03.\n",
      "Epoch 25001, Training Loss: 41730, Validation Loss: 51555, 70620.66590926622\n",
      "Epoch 25071: reducing learning rate of group 0 to 7.9444e-03.\n",
      "Epoch 25101, Training Loss: 39993, Validation Loss: 55228, 74187.04472094741\n",
      "Epoch 25172: reducing learning rate of group 0 to 7.9365e-03.\n",
      "Epoch 25201, Training Loss: 42541, Validation Loss: 61211, 67553.9474084752\n",
      "Epoch 25273: reducing learning rate of group 0 to 7.9285e-03.\n",
      "Epoch 25301, Training Loss: 45147, Validation Loss: 52303, 57622.86752188695\n",
      "Epoch 25401, Training Loss: 40291, Validation Loss: 52857, 74223.99815191337\n",
      "Epoch 25424: reducing learning rate of group 0 to 7.9206e-03.\n",
      "Epoch 25501, Training Loss: 40638, Validation Loss: 53114, 72162.25658880066\n",
      "Epoch 25525: reducing learning rate of group 0 to 7.9127e-03.\n",
      "Epoch 25601, Training Loss: 42242, Validation Loss: 52237, 85247.09741630535\n",
      "Epoch 25626: reducing learning rate of group 0 to 7.9048e-03.\n",
      "Epoch 25701, Training Loss: 37415, Validation Loss: 61175, 66597.22488318007\n",
      "Epoch 25727: reducing learning rate of group 0 to 7.8969e-03.\n",
      "Epoch 25801, Training Loss: 41532, Validation Loss: 61385, 84034.47270344775\n",
      "Epoch 25828: reducing learning rate of group 0 to 7.8890e-03.\n",
      "Epoch 25901, Training Loss: 46721, Validation Loss: 57351, 115031.24084854322\n",
      "Epoch 25929: reducing learning rate of group 0 to 7.8811e-03.\n",
      "Epoch 26001, Training Loss: 39311, Validation Loss: 51122, 79557.54448705817\n",
      "Epoch 26030: reducing learning rate of group 0 to 7.8732e-03.\n",
      "Epoch 26101, Training Loss: 43325, Validation Loss: 53628, 69356.40937936556\n",
      "Epoch 26131: reducing learning rate of group 0 to 7.8653e-03.\n",
      "Epoch 26201, Training Loss: 40206, Validation Loss: 53844, 74399.80493105325\n",
      "Epoch 26232: reducing learning rate of group 0 to 7.8575e-03.\n",
      "Epoch 26301, Training Loss: 42727, Validation Loss: 54003, 70300.65089194753\n",
      "Epoch 26333: reducing learning rate of group 0 to 7.8496e-03.\n",
      "Epoch 26401, Training Loss: 42082, Validation Loss: 56949, 84366.41905040314\n",
      "Epoch 26434: reducing learning rate of group 0 to 7.8418e-03.\n",
      "Epoch 26501, Training Loss: 40140, Validation Loss: 50889, 66109.63150199462\n",
      "Epoch 26535: reducing learning rate of group 0 to 7.8339e-03.\n",
      "Epoch 26601, Training Loss: 42734, Validation Loss: 50612, 85016.64059348566\n",
      "Epoch 26636: reducing learning rate of group 0 to 7.8261e-03.\n",
      "Epoch 26701, Training Loss: 41102, Validation Loss: 50035, 73544.93413903347\n",
      "Epoch 26737: reducing learning rate of group 0 to 7.8183e-03.\n",
      "Epoch 26801, Training Loss: 40499, Validation Loss: 57626, 67146.3966844743\n",
      "Epoch 26838: reducing learning rate of group 0 to 7.8104e-03.\n",
      "Epoch 26901, Training Loss: 38720, Validation Loss: 55797, 74745.7301454036\n",
      "Epoch 26939: reducing learning rate of group 0 to 7.8026e-03.\n",
      "Epoch 27001, Training Loss: 45879, Validation Loss: 54274, 65261.62320515828\n",
      "Epoch 27040: reducing learning rate of group 0 to 7.7948e-03.\n",
      "Epoch 27101, Training Loss: 38677, Validation Loss: 53306, 64056.23745153247\n",
      "Epoch 27141: reducing learning rate of group 0 to 7.7870e-03.\n",
      "Epoch 27201, Training Loss: 39133, Validation Loss: 54707, 71379.99468097674\n",
      "Epoch 27242: reducing learning rate of group 0 to 7.7792e-03.\n",
      "Epoch 27301, Training Loss: 40037, Validation Loss: 53327, 73613.42425994824\n",
      "Epoch 27343: reducing learning rate of group 0 to 7.7715e-03.\n",
      "Epoch 27401, Training Loss: 42025, Validation Loss: 52651, 77448.04301949458\n",
      "Epoch 27444: reducing learning rate of group 0 to 7.7637e-03.\n",
      "Epoch 27501, Training Loss: 42107, Validation Loss: 55754, 64768.22006535906\n",
      "Epoch 27545: reducing learning rate of group 0 to 7.7559e-03.\n",
      "Epoch 27601, Training Loss: 38507, Validation Loss: 52811, 73346.22957633092\n",
      "Epoch 27646: reducing learning rate of group 0 to 7.7482e-03.\n",
      "Epoch 27701, Training Loss: 39745, Validation Loss: 52973, 65129.09746309065\n",
      "Epoch 27747: reducing learning rate of group 0 to 7.7404e-03.\n",
      "Epoch 27801, Training Loss: 41862, Validation Loss: 50756, 69512.30346434213\n",
      "Epoch 27848: reducing learning rate of group 0 to 7.7327e-03.\n",
      "Epoch 27901, Training Loss: 41185, Validation Loss: 51776, 66354.15954213905\n",
      "Epoch 27949: reducing learning rate of group 0 to 7.7250e-03.\n",
      "Epoch 28001, Training Loss: 41715, Validation Loss: 55748, 90451.07683607323\n",
      "Epoch 28050: reducing learning rate of group 0 to 7.7172e-03.\n",
      "Epoch 28101, Training Loss: 41054, Validation Loss: 54860, 79726.87513415382\n",
      "Epoch 28151: reducing learning rate of group 0 to 7.7095e-03.\n",
      "Epoch 28201, Training Loss: 39905, Validation Loss: 52842, 70090.16217865741\n",
      "Epoch 28252: reducing learning rate of group 0 to 7.7018e-03.\n",
      "Epoch 28301, Training Loss: 39127, Validation Loss: 58780, 76914.46387423441\n",
      "Epoch 28353: reducing learning rate of group 0 to 7.6941e-03.\n",
      "Epoch 28401, Training Loss: 39794, Validation Loss: 52821, 85246.92480126354\n",
      "Epoch 28454: reducing learning rate of group 0 to 7.6864e-03.\n",
      "Epoch 28501, Training Loss: 39837, Validation Loss: 55011, 97701.81921441318\n",
      "Epoch 28555: reducing learning rate of group 0 to 7.6787e-03.\n",
      "Epoch 28601, Training Loss: 39085, Validation Loss: 55641, 62493.04006704011\n",
      "Epoch 28656: reducing learning rate of group 0 to 7.6710e-03.\n",
      "Epoch 28701, Training Loss: 38689, Validation Loss: 52542, 114222.8262847633\n",
      "Epoch 28757: reducing learning rate of group 0 to 7.6634e-03.\n",
      "Epoch 28801, Training Loss: 40723, Validation Loss: 56816, 68652.88918537794\n",
      "Epoch 28858: reducing learning rate of group 0 to 7.6557e-03.\n",
      "Epoch 28901, Training Loss: 41517, Validation Loss: 55299, 60095.06310780682\n",
      "Epoch 28959: reducing learning rate of group 0 to 7.6481e-03.\n",
      "Epoch 29001, Training Loss: 42013, Validation Loss: 54699, 75161.99103983254\n",
      "Epoch 29060: reducing learning rate of group 0 to 7.6404e-03.\n",
      "Epoch 29101, Training Loss: 40097, Validation Loss: 53910, 68341.15709956702\n",
      "Epoch 29161: reducing learning rate of group 0 to 7.6328e-03.\n",
      "Epoch 29201, Training Loss: 38593, Validation Loss: 57341, 69634.2131823768\n",
      "Epoch 29262: reducing learning rate of group 0 to 7.6251e-03.\n",
      "Epoch 29301, Training Loss: 39593, Validation Loss: 52912, 99603.30191880814\n",
      "Epoch 29363: reducing learning rate of group 0 to 7.6175e-03.\n",
      "Epoch 29401, Training Loss: 42699, Validation Loss: 52050, 66742.37490938402\n",
      "Epoch 29464: reducing learning rate of group 0 to 7.6099e-03.\n",
      "Epoch 29501, Training Loss: 41287, Validation Loss: 54614, 54662.326019082044\n",
      "Epoch 29565: reducing learning rate of group 0 to 7.6023e-03.\n",
      "Epoch 29601, Training Loss: 39804, Validation Loss: 51977, 74480.9310622366\n",
      "Epoch 29666: reducing learning rate of group 0 to 7.5947e-03.\n",
      "Epoch 29701, Training Loss: 41330, Validation Loss: 54202, 82360.98607736504\n",
      "Epoch 29767: reducing learning rate of group 0 to 7.5871e-03.\n",
      "Epoch 29801, Training Loss: 38947, Validation Loss: 56006, 71806.20486260542\n",
      "Epoch 29868: reducing learning rate of group 0 to 7.5795e-03.\n",
      "Epoch 29901, Training Loss: 40490, Validation Loss: 52670, 65280.0531729658\n",
      "Epoch 29969: reducing learning rate of group 0 to 7.5719e-03.\n",
      "Epoch 30001, Training Loss: 38718, Validation Loss: 52408, 76385.18379642999\n",
      "Epoch 30070: reducing learning rate of group 0 to 7.5643e-03.\n",
      "Epoch 30101, Training Loss: 42248, Validation Loss: 54831, 73347.45140051785\n",
      "Epoch 30171: reducing learning rate of group 0 to 7.5568e-03.\n",
      "Epoch 30201, Training Loss: 41307, Validation Loss: 55820, 93546.33428929331\n",
      "Epoch 30272: reducing learning rate of group 0 to 7.5492e-03.\n",
      "Epoch 30301, Training Loss: 37731, Validation Loss: 53287, 74441.90464780173\n",
      "Epoch 30373: reducing learning rate of group 0 to 7.5417e-03.\n",
      "Epoch 30401, Training Loss: 39596, Validation Loss: 54878, 59591.92533789572\n",
      "Epoch 30474: reducing learning rate of group 0 to 7.5341e-03.\n",
      "Epoch 30501, Training Loss: 39734, Validation Loss: 50871, 79853.99291304732\n",
      "Epoch 30575: reducing learning rate of group 0 to 7.5266e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30601, Training Loss: 39042, Validation Loss: 65140, 73236.39172894268\n",
      "Epoch 30676: reducing learning rate of group 0 to 7.5191e-03.\n",
      "Epoch 30701, Training Loss: 41697, Validation Loss: 51063, 86333.23300106144\n",
      "Epoch 30777: reducing learning rate of group 0 to 7.5116e-03.\n",
      "Epoch 30801, Training Loss: 37160, Validation Loss: 51637, 59966.47711114135\n",
      "Epoch 30878: reducing learning rate of group 0 to 7.5040e-03.\n",
      "Epoch 30901, Training Loss: 40922, Validation Loss: 51374, 70687.27364701634\n",
      "Epoch 30979: reducing learning rate of group 0 to 7.4965e-03.\n",
      "Epoch 31001, Training Loss: 40216, Validation Loss: 55245, 62349.091923537024\n",
      "Epoch 31080: reducing learning rate of group 0 to 7.4890e-03.\n",
      "Epoch 31101, Training Loss: 38418, Validation Loss: 58201, 81251.23506829778\n",
      "Epoch 31181: reducing learning rate of group 0 to 7.4816e-03.\n",
      "Epoch 31201, Training Loss: 40000, Validation Loss: 53841, 61092.8271808126\n",
      "Epoch 31282: reducing learning rate of group 0 to 7.4741e-03.\n",
      "Epoch 31301, Training Loss: 38789, Validation Loss: 54526, 63425.49457350936\n",
      "Epoch 31383: reducing learning rate of group 0 to 7.4666e-03.\n",
      "Epoch 31401, Training Loss: 39667, Validation Loss: 58025, 60029.184959773054\n",
      "Epoch 31484: reducing learning rate of group 0 to 7.4591e-03.\n",
      "Epoch 31501, Training Loss: 38040, Validation Loss: 52712, 60101.79474933748\n",
      "Epoch 31585: reducing learning rate of group 0 to 7.4517e-03.\n",
      "Epoch 31601, Training Loss: 39378, Validation Loss: 52864, 89238.15122224814\n",
      "Epoch 31686: reducing learning rate of group 0 to 7.4442e-03.\n",
      "Epoch 31701, Training Loss: 37198, Validation Loss: 59388, 80637.48890359928\n",
      "Epoch 31787: reducing learning rate of group 0 to 7.4368e-03.\n",
      "Epoch 31801, Training Loss: 40383, Validation Loss: 51120, 89111.84734976286\n",
      "Epoch 31888: reducing learning rate of group 0 to 7.4293e-03.\n",
      "Epoch 31901, Training Loss: 38000, Validation Loss: 58117, 82977.79823586448\n",
      "Epoch 31989: reducing learning rate of group 0 to 7.4219e-03.\n",
      "Epoch 32001, Training Loss: 36490, Validation Loss: 55045, 73558.8327181381\n",
      "Epoch 32090: reducing learning rate of group 0 to 7.4145e-03.\n",
      "Epoch 32101, Training Loss: 38200, Validation Loss: 51826, 73431.33678073308\n",
      "Epoch 32191: reducing learning rate of group 0 to 7.4071e-03.\n",
      "Epoch 32201, Training Loss: 41095, Validation Loss: 52292, 87437.11599512705\n",
      "Epoch 32292: reducing learning rate of group 0 to 7.3997e-03.\n",
      "Epoch 32301, Training Loss: 39976, Validation Loss: 61776, 72355.17957474633\n",
      "Epoch 32393: reducing learning rate of group 0 to 7.3923e-03.\n",
      "Epoch 32401, Training Loss: 40188, Validation Loss: 56944, 60945.08420812783\n",
      "Epoch 32494: reducing learning rate of group 0 to 7.3849e-03.\n",
      "Epoch 32501, Training Loss: 38294, Validation Loss: 54359, 59363.91538793667\n",
      "Epoch 32595: reducing learning rate of group 0 to 7.3775e-03.\n",
      "Epoch 32601, Training Loss: 40164, Validation Loss: 50088, 66017.2330475856\n",
      "Epoch 32696: reducing learning rate of group 0 to 7.3701e-03.\n",
      "Epoch 32701, Training Loss: 39919, Validation Loss: 51760, 105618.0140705724\n",
      "Epoch 32797: reducing learning rate of group 0 to 7.3627e-03.\n",
      "Epoch 32801, Training Loss: 38766, Validation Loss: 54314, 61408.49723697664\n",
      "Epoch 32898: reducing learning rate of group 0 to 7.3554e-03.\n",
      "Epoch 32901, Training Loss: 40015, Validation Loss: 56288, 63712.85768262821\n",
      "Epoch 32999: reducing learning rate of group 0 to 7.3480e-03.\n",
      "Epoch 33001, Training Loss: 41503, Validation Loss: 51511, 75836.99099807684\n",
      "Epoch 33100: reducing learning rate of group 0 to 7.3407e-03.\n",
      "Epoch 33101, Training Loss: 41832, Validation Loss: 55181, 67659.6562640443\n",
      "Epoch 33201: reducing learning rate of group 0 to 7.3333e-03.\n",
      "Epoch 33201, Training Loss: 40153, Validation Loss: 57155, 74444.56610800086\n",
      "Epoch 33301, Training Loss: 38156, Validation Loss: 51321, 55324.14521240412\n",
      "Epoch 33302: reducing learning rate of group 0 to 7.3260e-03.\n",
      "Epoch 33401, Training Loss: 40505, Validation Loss: 54106, 101031.72093581645\n",
      "Epoch 33403: reducing learning rate of group 0 to 7.3187e-03.\n",
      "Epoch 33501, Training Loss: 40039, Validation Loss: 53734, 62580.07579653128\n",
      "Epoch 33504: reducing learning rate of group 0 to 7.3114e-03.\n",
      "Epoch 33601, Training Loss: 38329, Validation Loss: 50524, 66232.80765464465\n",
      "Epoch 33605: reducing learning rate of group 0 to 7.3040e-03.\n",
      "Epoch 33701, Training Loss: 41296, Validation Loss: 51055, 88798.41018787911\n",
      "Epoch 33706: reducing learning rate of group 0 to 7.2967e-03.\n",
      "Epoch 33801, Training Loss: 36304, Validation Loss: 52652, 71835.43604152568\n",
      "Epoch 33807: reducing learning rate of group 0 to 7.2894e-03.\n",
      "Epoch 33901, Training Loss: 37547, Validation Loss: 51134, 76146.71482764134\n",
      "Epoch 33908: reducing learning rate of group 0 to 7.2822e-03.\n",
      "Epoch 34001, Training Loss: 38479, Validation Loss: 53331, 81251.95672365319\n",
      "Epoch 34009: reducing learning rate of group 0 to 7.2749e-03.\n",
      "Epoch 34101, Training Loss: 38678, Validation Loss: 57169, 69335.30774733653\n",
      "Epoch 34110: reducing learning rate of group 0 to 7.2676e-03.\n",
      "Epoch 34201, Training Loss: 42113, Validation Loss: 53852, 88221.9018141062\n",
      "Epoch 34211: reducing learning rate of group 0 to 7.2603e-03.\n",
      "Epoch 34301, Training Loss: 39606, Validation Loss: 53857, 65083.52355082661\n",
      "Epoch 34312: reducing learning rate of group 0 to 7.2531e-03.\n",
      "Epoch 34401, Training Loss: 37546, Validation Loss: 56687, 75648.15296530953\n",
      "Epoch 34413: reducing learning rate of group 0 to 7.2458e-03.\n",
      "Epoch 34501, Training Loss: 40460, Validation Loss: 54247, 74090.17076692579\n",
      "Epoch 34514: reducing learning rate of group 0 to 7.2386e-03.\n",
      "Epoch 34601, Training Loss: 39083, Validation Loss: 53419, 62143.29666754202\n",
      "Epoch 34615: reducing learning rate of group 0 to 7.2313e-03.\n",
      "Epoch 34701, Training Loss: 37804, Validation Loss: 53525, 75654.7247414826\n",
      "Epoch 34716: reducing learning rate of group 0 to 7.2241e-03.\n",
      "Epoch 34801, Training Loss: 35977, Validation Loss: 54552, 80673.99548669567\n",
      "Epoch 34817: reducing learning rate of group 0 to 7.2169e-03.\n",
      "Epoch 34901, Training Loss: 39133, Validation Loss: 55721, 76154.00585448292\n",
      "Epoch 34918: reducing learning rate of group 0 to 7.2097e-03.\n",
      "Epoch 35001, Training Loss: 38760, Validation Loss: 57880, 102678.60336399368\n",
      "Epoch 35019: reducing learning rate of group 0 to 7.2024e-03.\n",
      "Epoch 35101, Training Loss: 38431, Validation Loss: 53392, 70468.51232330993\n",
      "Epoch 35120: reducing learning rate of group 0 to 7.1952e-03.\n",
      "Epoch 35201, Training Loss: 39918, Validation Loss: 53919, 77544.5084857027\n",
      "Epoch 35221: reducing learning rate of group 0 to 7.1881e-03.\n",
      "Epoch 35301, Training Loss: 37030, Validation Loss: 53524, 70610.71504620835\n",
      "Epoch 35322: reducing learning rate of group 0 to 7.1809e-03.\n",
      "Epoch 35401, Training Loss: 40199, Validation Loss: 54529, 74616.81942093352\n",
      "Epoch 35423: reducing learning rate of group 0 to 7.1737e-03.\n",
      "Epoch 35501, Training Loss: 41244, Validation Loss: 55724, 77034.72831453069\n",
      "Epoch 35524: reducing learning rate of group 0 to 7.1665e-03.\n",
      "Epoch 35601, Training Loss: 40078, Validation Loss: 58841, 82539.12321451989\n",
      "Epoch 35625: reducing learning rate of group 0 to 7.1593e-03.\n",
      "Epoch 35701, Training Loss: 37409, Validation Loss: 53752, 57838.42582621012\n",
      "Epoch 35726: reducing learning rate of group 0 to 7.1522e-03.\n",
      "Epoch 35801, Training Loss: 38944, Validation Loss: 52974, 65239.20637245963\n",
      "Epoch 35827: reducing learning rate of group 0 to 7.1450e-03.\n",
      "Epoch 35901, Training Loss: 37826, Validation Loss: 51133, 83623.25032120332\n",
      "Epoch 35928: reducing learning rate of group 0 to 7.1379e-03.\n",
      "Epoch 36001, Training Loss: 38160, Validation Loss: 54710, 79075.33001025698\n",
      "Epoch 36029: reducing learning rate of group 0 to 7.1307e-03.\n",
      "Epoch 36101, Training Loss: 36643, Validation Loss: 55210, 74567.28287760595\n",
      "Epoch 36130: reducing learning rate of group 0 to 7.1236e-03.\n",
      "Epoch 36201, Training Loss: 36615, Validation Loss: 56488, 73375.1060412763\n",
      "Epoch 36231: reducing learning rate of group 0 to 7.1165e-03.\n",
      "Epoch 36301, Training Loss: 39823, Validation Loss: 53551, 91509.55142934487\n",
      "Epoch 36332: reducing learning rate of group 0 to 7.1094e-03.\n",
      "Epoch 36401, Training Loss: 39203, Validation Loss: 55358, 60494.37944481479\n",
      "Epoch 36433: reducing learning rate of group 0 to 7.1023e-03.\n",
      "Epoch 36501, Training Loss: 36577, Validation Loss: 57148, 62830.46917566983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36534: reducing learning rate of group 0 to 7.0952e-03.\n",
      "Epoch 36601, Training Loss: 39814, Validation Loss: 58436, 82646.72197131149\n",
      "Epoch 36635: reducing learning rate of group 0 to 7.0881e-03.\n",
      "Epoch 36701, Training Loss: 40433, Validation Loss: 61890, 84065.3035492763\n",
      "Epoch 36736: reducing learning rate of group 0 to 7.0810e-03.\n",
      "Epoch 36801, Training Loss: 40118, Validation Loss: 50713, 80866.47974222731\n",
      "Epoch 36837: reducing learning rate of group 0 to 7.0739e-03.\n",
      "Epoch 36901, Training Loss: 37942, Validation Loss: 52836, 61680.32161599363\n",
      "Epoch 36938: reducing learning rate of group 0 to 7.0668e-03.\n",
      "Epoch 37001, Training Loss: 38985, Validation Loss: 56812, 67104.0615869971\n",
      "Epoch 37039: reducing learning rate of group 0 to 7.0598e-03.\n",
      "Epoch 37101, Training Loss: 37074, Validation Loss: 55729, 60953.57776337161\n",
      "Epoch 37140: reducing learning rate of group 0 to 7.0527e-03.\n",
      "Epoch 37201, Training Loss: 38412, Validation Loss: 54188, 71383.34783272764\n",
      "Epoch 37241: reducing learning rate of group 0 to 7.0456e-03.\n",
      "Epoch 37301, Training Loss: 38680, Validation Loss: 54539, 74508.44068555109\n",
      "Epoch 37342: reducing learning rate of group 0 to 7.0386e-03.\n",
      "Epoch 37401, Training Loss: 38603, Validation Loss: 55597, 64185.14612098052\n",
      "Epoch 37443: reducing learning rate of group 0 to 7.0316e-03.\n",
      "Epoch 37501, Training Loss: 43587, Validation Loss: 56274, 74870.29227967505\n",
      "Epoch 37544: reducing learning rate of group 0 to 7.0245e-03.\n",
      "Epoch 37601, Training Loss: 38688, Validation Loss: 54502, 59652.14838008582\n",
      "Epoch 37645: reducing learning rate of group 0 to 7.0175e-03.\n",
      "Epoch 37701, Training Loss: 37846, Validation Loss: 55380, 73163.1145606538\n",
      "Epoch 37746: reducing learning rate of group 0 to 7.0105e-03.\n",
      "Epoch 37801, Training Loss: 36858, Validation Loss: 54617, 79443.52641949912\n",
      "Epoch 37847: reducing learning rate of group 0 to 7.0035e-03.\n",
      "Epoch 37901, Training Loss: 38256, Validation Loss: 53358, 69450.54863840717\n",
      "Epoch 37948: reducing learning rate of group 0 to 6.9965e-03.\n",
      "Epoch 38001, Training Loss: 36120, Validation Loss: 52945, 60192.426817867\n",
      "Epoch 38049: reducing learning rate of group 0 to 6.9895e-03.\n",
      "Epoch 38101, Training Loss: 37508, Validation Loss: 59062, 94049.38255198703\n",
      "Epoch 38150: reducing learning rate of group 0 to 6.9825e-03.\n",
      "Epoch 38201, Training Loss: 39942, Validation Loss: 52812, 72199.72891941595\n",
      "Epoch 38251: reducing learning rate of group 0 to 6.9755e-03.\n",
      "Epoch 38301, Training Loss: 40403, Validation Loss: 55477, 96018.3746620611\n",
      "Epoch 38352: reducing learning rate of group 0 to 6.9685e-03.\n",
      "Epoch 38401, Training Loss: 35414, Validation Loss: 52944, 55253.13092109767\n",
      "Epoch 38453: reducing learning rate of group 0 to 6.9616e-03.\n",
      "Epoch 38501, Training Loss: 38175, Validation Loss: 54687, 79141.62029372297\n",
      "Epoch 38554: reducing learning rate of group 0 to 6.9546e-03.\n",
      "Epoch 38601, Training Loss: 36639, Validation Loss: 54523, 91606.7128851163\n",
      "Epoch 38655: reducing learning rate of group 0 to 6.9476e-03.\n",
      "Epoch 38701, Training Loss: 39094, Validation Loss: 52693, 89805.43318030475\n",
      "Epoch 38756: reducing learning rate of group 0 to 6.9407e-03.\n",
      "Epoch 38801, Training Loss: 38309, Validation Loss: 53240, 67582.40773121189\n",
      "Epoch 38857: reducing learning rate of group 0 to 6.9338e-03.\n",
      "Epoch 38901, Training Loss: 37235, Validation Loss: 55776, 80569.88567106253\n",
      "Epoch 38958: reducing learning rate of group 0 to 6.9268e-03.\n",
      "Epoch 39001, Training Loss: 37137, Validation Loss: 50696, 80101.41776502093\n",
      "Epoch 39059: reducing learning rate of group 0 to 6.9199e-03.\n",
      "Epoch 39101, Training Loss: 36818, Validation Loss: 51273, 64184.27123654494\n",
      "Epoch 39160: reducing learning rate of group 0 to 6.9130e-03.\n",
      "Epoch 39201, Training Loss: 37718, Validation Loss: 54094, 71340.72338286345\n",
      "Epoch 39261: reducing learning rate of group 0 to 6.9061e-03.\n",
      "Epoch 39301, Training Loss: 36505, Validation Loss: 53767, 72008.0803420923\n",
      "Epoch 39362: reducing learning rate of group 0 to 6.8992e-03.\n",
      "Epoch 39401, Training Loss: 38341, Validation Loss: 54306, 79278.30801793894\n",
      "Epoch 39463: reducing learning rate of group 0 to 6.8923e-03.\n",
      "Epoch 39501, Training Loss: 36940, Validation Loss: 55389, 69092.47692490194\n",
      "Epoch 39564: reducing learning rate of group 0 to 6.8854e-03.\n",
      "Epoch 39601, Training Loss: 38043, Validation Loss: 53155, 72357.44143735625\n",
      "Epoch 39665: reducing learning rate of group 0 to 6.8785e-03.\n",
      "Epoch 39701, Training Loss: 35817, Validation Loss: 54434, 76928.16733351532\n",
      "Epoch 39766: reducing learning rate of group 0 to 6.8716e-03.\n",
      "Epoch 39801, Training Loss: 36367, Validation Loss: 58844, 71545.21069202946\n",
      "Epoch 39867: reducing learning rate of group 0 to 6.8647e-03.\n",
      "Epoch 39901, Training Loss: 37808, Validation Loss: 55417, 64945.71413286318\n",
      "Epoch 39968: reducing learning rate of group 0 to 6.8579e-03.\n",
      "Epoch 40001, Training Loss: 36427, Validation Loss: 49770, 75433.64232881751\n",
      "Epoch 40069: reducing learning rate of group 0 to 6.8510e-03.\n",
      "Epoch 40101, Training Loss: 37795, Validation Loss: 52945, 62356.02765712895\n",
      "Epoch 40170: reducing learning rate of group 0 to 6.8442e-03.\n",
      "Epoch 40201, Training Loss: 38001, Validation Loss: 53958, 82360.23434246895\n",
      "Epoch 40271: reducing learning rate of group 0 to 6.8373e-03.\n",
      "Epoch 40301, Training Loss: 38522, Validation Loss: 54499, 65375.75069210713\n",
      "Epoch 40372: reducing learning rate of group 0 to 6.8305e-03.\n",
      "Epoch 40401, Training Loss: 35021, Validation Loss: 52500, 65687.40081157791\n",
      "Epoch 40473: reducing learning rate of group 0 to 6.8236e-03.\n",
      "Epoch 40501, Training Loss: 37381, Validation Loss: 54374, 72123.70348105353\n",
      "Epoch 40574: reducing learning rate of group 0 to 6.8168e-03.\n",
      "Epoch 40601, Training Loss: 36530, Validation Loss: 49919, 72323.13420334931\n",
      "Epoch 40675: reducing learning rate of group 0 to 6.8100e-03.\n",
      "Epoch 40701, Training Loss: 39629, Validation Loss: 54603, 54589.521464296085\n",
      "Epoch 40776: reducing learning rate of group 0 to 6.8032e-03.\n",
      "Epoch 40801, Training Loss: 36736, Validation Loss: 58034, 94408.00747461601\n",
      "Epoch 40877: reducing learning rate of group 0 to 6.7964e-03.\n",
      "Epoch 40901, Training Loss: 35064, Validation Loss: 53164, 71098.78649306968\n",
      "Epoch 40978: reducing learning rate of group 0 to 6.7896e-03.\n",
      "Epoch 41001, Training Loss: 36874, Validation Loss: 54508, 82931.97810422548\n",
      "Epoch 41079: reducing learning rate of group 0 to 6.7828e-03.\n",
      "Epoch 41101, Training Loss: 37831, Validation Loss: 56861, 75804.71728581525\n",
      "Epoch 41180: reducing learning rate of group 0 to 6.7760e-03.\n",
      "Epoch 41201, Training Loss: 35567, Validation Loss: 52380, 63544.22695382335\n",
      "Epoch 41281: reducing learning rate of group 0 to 6.7692e-03.\n",
      "Epoch 41301, Training Loss: 40089, Validation Loss: 53087, 68814.78789965011\n",
      "Epoch 41382: reducing learning rate of group 0 to 6.7625e-03.\n",
      "Epoch 41401, Training Loss: 37463, Validation Loss: 60085, 80884.12346220772\n",
      "Epoch 41483: reducing learning rate of group 0 to 6.7557e-03.\n",
      "Epoch 41501, Training Loss: 34917, Validation Loss: 54607, 66310.87385561276\n",
      "Epoch 41584: reducing learning rate of group 0 to 6.7490e-03.\n",
      "Epoch 41601, Training Loss: 38631, Validation Loss: 53342, 94132.9102438668\n",
      "Epoch 41685: reducing learning rate of group 0 to 6.7422e-03.\n",
      "Epoch 41701, Training Loss: 37450, Validation Loss: 51049, 71884.53741022576\n",
      "Epoch 41786: reducing learning rate of group 0 to 6.7355e-03.\n",
      "Epoch 41801, Training Loss: 36345, Validation Loss: 52558, 58265.68914175328\n",
      "Epoch 41887: reducing learning rate of group 0 to 6.7287e-03.\n",
      "Epoch 41901, Training Loss: 38755, Validation Loss: 59280, 73020.7058080743\n",
      "Epoch 41988: reducing learning rate of group 0 to 6.7220e-03.\n",
      "Epoch 42001, Training Loss: 37079, Validation Loss: 58807, 62031.87436376282\n",
      "Epoch 42089: reducing learning rate of group 0 to 6.7153e-03.\n",
      "Epoch 42101, Training Loss: 39574, Validation Loss: 50196, 64473.321494700656\n",
      "Epoch 42190: reducing learning rate of group 0 to 6.7086e-03.\n",
      "Epoch 42201, Training Loss: 35643, Validation Loss: 50440, 55454.2214262869\n",
      "Epoch 42291: reducing learning rate of group 0 to 6.7019e-03.\n",
      "Epoch 42301, Training Loss: 39183, Validation Loss: 51870, 71481.95503162978\n",
      "Epoch 42392: reducing learning rate of group 0 to 6.6952e-03.\n",
      "Epoch 42401, Training Loss: 37824, Validation Loss: 58292, 82216.71437316848\n",
      "Epoch 42493: reducing learning rate of group 0 to 6.6885e-03.\n",
      "Epoch 42501, Training Loss: 37793, Validation Loss: 53914, 82957.33941184058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42594: reducing learning rate of group 0 to 6.6818e-03.\n",
      "Epoch 42601, Training Loss: 35414, Validation Loss: 50587, 76091.27768300241\n",
      "Epoch 42695: reducing learning rate of group 0 to 6.6751e-03.\n",
      "Epoch 42701, Training Loss: 38269, Validation Loss: 50163, 72042.7185639383\n",
      "Epoch 42796: reducing learning rate of group 0 to 6.6684e-03.\n",
      "Epoch 42801, Training Loss: 36223, Validation Loss: 52424, 74080.99363306521\n",
      "Epoch 42897: reducing learning rate of group 0 to 6.6617e-03.\n",
      "Epoch 42901, Training Loss: 38257, Validation Loss: 51656, 89714.51073796715\n",
      "Epoch 42998: reducing learning rate of group 0 to 6.6551e-03.\n",
      "Epoch 43001, Training Loss: 36980, Validation Loss: 52442, 66140.8326538039\n",
      "Epoch 43099: reducing learning rate of group 0 to 6.6484e-03.\n",
      "Epoch 43101, Training Loss: 37870, Validation Loss: 53507, 71994.97660054134\n",
      "Epoch 43200: reducing learning rate of group 0 to 6.6418e-03.\n",
      "Epoch 43201, Training Loss: 39995, Validation Loss: 56003, 62457.4255507549\n",
      "Epoch 43301: reducing learning rate of group 0 to 6.6351e-03.\n",
      "Epoch 43301, Training Loss: 34628, Validation Loss: 54348, 64636.09362034098\n",
      "Epoch 43401, Training Loss: 35793, Validation Loss: 54540, 84241.42414057242\n",
      "Epoch 43402: reducing learning rate of group 0 to 6.6285e-03.\n",
      "Epoch 43501, Training Loss: 37495, Validation Loss: 56513, 64654.26380407019\n",
      "Epoch 43503: reducing learning rate of group 0 to 6.6219e-03.\n",
      "Epoch 43601, Training Loss: 37139, Validation Loss: 50763, 74547.67744706928\n",
      "Epoch 43604: reducing learning rate of group 0 to 6.6153e-03.\n",
      "Epoch 43701, Training Loss: 35499, Validation Loss: 55301, 70375.43719083605\n",
      "Epoch 43705: reducing learning rate of group 0 to 6.6086e-03.\n",
      "Epoch 43801, Training Loss: 36740, Validation Loss: 54037, 66091.31436128003\n",
      "Epoch 43806: reducing learning rate of group 0 to 6.6020e-03.\n",
      "Epoch 43901, Training Loss: 37146, Validation Loss: 54023, 70616.01212337216\n",
      "Epoch 43907: reducing learning rate of group 0 to 6.5954e-03.\n",
      "Epoch 44001, Training Loss: 36687, Validation Loss: 55249, 78111.68384783546\n",
      "Epoch 44008: reducing learning rate of group 0 to 6.5888e-03.\n",
      "Epoch 44101, Training Loss: 38369, Validation Loss: 53976, 77315.48857283528\n",
      "Epoch 44109: reducing learning rate of group 0 to 6.5822e-03.\n",
      "Epoch 44201, Training Loss: 35405, Validation Loss: 56507, 71491.75979659143\n",
      "Epoch 44210: reducing learning rate of group 0 to 6.5757e-03.\n",
      "Epoch 44301, Training Loss: 38641, Validation Loss: 55449, 64939.67597978883\n",
      "Epoch 44311: reducing learning rate of group 0 to 6.5691e-03.\n",
      "Epoch 44401, Training Loss: 38247, Validation Loss: 53836, 93482.22864624567\n",
      "Epoch 44412: reducing learning rate of group 0 to 6.5625e-03.\n",
      "Epoch 44501, Training Loss: 35476, Validation Loss: 53377, 76529.91180245018\n",
      "Epoch 44513: reducing learning rate of group 0 to 6.5560e-03.\n",
      "Epoch 44601, Training Loss: 35804, Validation Loss: 57981, 87187.43353107164\n",
      "Epoch 44614: reducing learning rate of group 0 to 6.5494e-03.\n",
      "Epoch 44701, Training Loss: 32785, Validation Loss: 53575, 73989.77672974086\n",
      "Epoch 44715: reducing learning rate of group 0 to 6.5429e-03.\n",
      "Epoch 44801, Training Loss: 38426, Validation Loss: 54825, 52423.496919562225\n",
      "Epoch 44816: reducing learning rate of group 0 to 6.5363e-03.\n",
      "Epoch 44901, Training Loss: 38292, Validation Loss: 51956, 82613.3514695227\n",
      "Epoch 44917: reducing learning rate of group 0 to 6.5298e-03.\n",
      "Epoch 45001, Training Loss: 35550, Validation Loss: 53125, 112968.40631617319\n",
      "Epoch 45018: reducing learning rate of group 0 to 6.5232e-03.\n",
      "Epoch 45101, Training Loss: 36725, Validation Loss: 53893, 67678.0538222367\n",
      "Epoch 45119: reducing learning rate of group 0 to 6.5167e-03.\n",
      "Epoch 45201, Training Loss: 34132, Validation Loss: 52105, 68175.08611798618\n",
      "Epoch 45220: reducing learning rate of group 0 to 6.5102e-03.\n",
      "Epoch 45301, Training Loss: 36121, Validation Loss: 53300, 75302.66814549339\n",
      "Epoch 45321: reducing learning rate of group 0 to 6.5037e-03.\n",
      "Epoch 45401, Training Loss: 37040, Validation Loss: 55523, 69303.36534145726\n",
      "Epoch 45422: reducing learning rate of group 0 to 6.4972e-03.\n",
      "Epoch 45501, Training Loss: 36849, Validation Loss: 55948, 83042.83686476729\n",
      "Epoch 45523: reducing learning rate of group 0 to 6.4907e-03.\n",
      "Epoch 45601, Training Loss: 41027, Validation Loss: 56337, 72863.08584103288\n",
      "Epoch 45624: reducing learning rate of group 0 to 6.4842e-03.\n",
      "Epoch 45701, Training Loss: 36595, Validation Loss: 53487, 78079.36136286064\n",
      "Epoch 45725: reducing learning rate of group 0 to 6.4777e-03.\n",
      "Epoch 45801, Training Loss: 33907, Validation Loss: 53492, 61953.646761128395\n",
      "Epoch 45826: reducing learning rate of group 0 to 6.4712e-03.\n",
      "Epoch 45901, Training Loss: 38317, Validation Loss: 56100, 73333.98330503005\n",
      "Epoch 45927: reducing learning rate of group 0 to 6.4648e-03.\n",
      "Epoch 46001, Training Loss: 36028, Validation Loss: 60225, 65592.71611987338\n",
      "Epoch 46028: reducing learning rate of group 0 to 6.4583e-03.\n",
      "Epoch 46101, Training Loss: 37174, Validation Loss: 53374, 64832.44672537628\n",
      "Epoch 46129: reducing learning rate of group 0 to 6.4518e-03.\n",
      "Epoch 46201, Training Loss: 37350, Validation Loss: 50544, 60653.04025020399\n",
      "Epoch 46230: reducing learning rate of group 0 to 6.4454e-03.\n",
      "Epoch 46301, Training Loss: 37485, Validation Loss: 55900, 74024.60046535137\n",
      "Epoch 46331: reducing learning rate of group 0 to 6.4389e-03.\n",
      "Epoch 46401, Training Loss: 35314, Validation Loss: 53937, 69104.69869116228\n",
      "Epoch 46432: reducing learning rate of group 0 to 6.4325e-03.\n",
      "Epoch 46501, Training Loss: 37640, Validation Loss: 52472, 77190.55183618686\n",
      "Epoch 46533: reducing learning rate of group 0 to 6.4261e-03.\n",
      "Epoch 46601, Training Loss: 36716, Validation Loss: 50797, 63680.43031408407\n",
      "Epoch 46634: reducing learning rate of group 0 to 6.4196e-03.\n",
      "Epoch 46701, Training Loss: 35573, Validation Loss: 52265, 70172.69833003201\n",
      "Epoch 46735: reducing learning rate of group 0 to 6.4132e-03.\n",
      "Epoch 46801, Training Loss: 36481, Validation Loss: 52776, 92817.64845901726\n",
      "Epoch 46836: reducing learning rate of group 0 to 6.4068e-03.\n",
      "Epoch 46901, Training Loss: 36029, Validation Loss: 52265, 69005.25078307504\n",
      "Epoch 46937: reducing learning rate of group 0 to 6.4004e-03.\n",
      "Epoch 47001, Training Loss: 33740, Validation Loss: 52331, 56900.0412999833\n",
      "Epoch 47038: reducing learning rate of group 0 to 6.3940e-03.\n",
      "Epoch 47101, Training Loss: 35404, Validation Loss: 50590, 78046.78385116899\n",
      "Epoch 47139: reducing learning rate of group 0 to 6.3876e-03.\n",
      "Epoch 47201, Training Loss: 38968, Validation Loss: 52327, 71445.10978893797\n",
      "Epoch 47240: reducing learning rate of group 0 to 6.3812e-03.\n",
      "Epoch 47301, Training Loss: 34483, Validation Loss: 51690, 65753.20838057603\n",
      "Epoch 47341: reducing learning rate of group 0 to 6.3748e-03.\n",
      "Epoch 47401, Training Loss: 35859, Validation Loss: 52246, 60943.054787530295\n",
      "Epoch 47442: reducing learning rate of group 0 to 6.3685e-03.\n",
      "Epoch 47501, Training Loss: 39249, Validation Loss: 56166, 83753.81202853478\n",
      "Epoch 47543: reducing learning rate of group 0 to 6.3621e-03.\n",
      "Epoch 47601, Training Loss: 37870, Validation Loss: 51708, 77045.63931312169\n",
      "Epoch 47644: reducing learning rate of group 0 to 6.3557e-03.\n",
      "Epoch 47701, Training Loss: 36304, Validation Loss: 55248, 83686.46930708892\n",
      "Epoch 47745: reducing learning rate of group 0 to 6.3494e-03.\n",
      "Epoch 47801, Training Loss: 35805, Validation Loss: 53493, 72906.12790511046\n",
      "Epoch 47846: reducing learning rate of group 0 to 6.3430e-03.\n",
      "Epoch 47901, Training Loss: 38716, Validation Loss: 50513, 69779.02053797303\n",
      "Epoch 47947: reducing learning rate of group 0 to 6.3367e-03.\n",
      "Epoch 48001, Training Loss: 38174, Validation Loss: 53087, 67885.8335233837\n",
      "Epoch 48048: reducing learning rate of group 0 to 6.3304e-03.\n",
      "Epoch 48101, Training Loss: 35125, Validation Loss: 53722, 84987.019389349\n",
      "Epoch 48149: reducing learning rate of group 0 to 6.3240e-03.\n",
      "Epoch 48201, Training Loss: 35730, Validation Loss: 54291, 72788.5208236502\n",
      "Epoch 48250: reducing learning rate of group 0 to 6.3177e-03.\n",
      "Epoch 48301, Training Loss: 37845, Validation Loss: 53978, 78474.20558365807\n",
      "Epoch 48351: reducing learning rate of group 0 to 6.3114e-03.\n",
      "Epoch 48401, Training Loss: 40917, Validation Loss: 63058, 80434.20993829511\n",
      "Epoch 48452: reducing learning rate of group 0 to 6.3051e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48501, Training Loss: 36320, Validation Loss: 53556, 60781.77746595943\n",
      "Epoch 48553: reducing learning rate of group 0 to 6.2988e-03.\n",
      "Epoch 48601, Training Loss: 36385, Validation Loss: 56897, 71505.5519354909\n",
      "Epoch 48654: reducing learning rate of group 0 to 6.2925e-03.\n",
      "Epoch 48701, Training Loss: 34111, Validation Loss: 56649, 78165.74063992522\n",
      "Epoch 48755: reducing learning rate of group 0 to 6.2862e-03.\n",
      "Epoch 48801, Training Loss: 36361, Validation Loss: 58149, 78598.40296818729\n",
      "Epoch 48856: reducing learning rate of group 0 to 6.2799e-03.\n",
      "Epoch 48901, Training Loss: 35084, Validation Loss: 53951, 68716.69253269274\n",
      "Epoch 48957: reducing learning rate of group 0 to 6.2736e-03.\n",
      "Epoch 49001, Training Loss: 34930, Validation Loss: 52394, 56861.90241490663\n",
      "Epoch 49058: reducing learning rate of group 0 to 6.2673e-03.\n",
      "Epoch 49101, Training Loss: 36333, Validation Loss: 55313, 74525.15040172475\n",
      "Epoch 49159: reducing learning rate of group 0 to 6.2611e-03.\n",
      "Epoch 49201, Training Loss: 34314, Validation Loss: 53034, 65383.595725228915\n",
      "Epoch 49260: reducing learning rate of group 0 to 6.2548e-03.\n",
      "Epoch 49301, Training Loss: 34873, Validation Loss: 50722, 64486.693317565834\n",
      "Epoch 49361: reducing learning rate of group 0 to 6.2486e-03.\n",
      "Epoch 49401, Training Loss: 34673, Validation Loss: 52216, 61461.260347337695\n",
      "Epoch 49462: reducing learning rate of group 0 to 6.2423e-03.\n",
      "Epoch 49501, Training Loss: 39309, Validation Loss: 54236, 69257.32918079443\n",
      "Epoch 49563: reducing learning rate of group 0 to 6.2361e-03.\n",
      "Epoch 49601, Training Loss: 36150, Validation Loss: 52148, 81202.55680959077\n",
      "Epoch 49664: reducing learning rate of group 0 to 6.2298e-03.\n",
      "Epoch 49701, Training Loss: 33688, Validation Loss: 53890, 77337.19579534908\n",
      "Epoch 49765: reducing learning rate of group 0 to 6.2236e-03.\n",
      "Epoch 49801, Training Loss: 34591, Validation Loss: 52949, 71866.67089443405\n",
      "Epoch 49866: reducing learning rate of group 0 to 6.2174e-03.\n",
      "Epoch 49901, Training Loss: 37848, Validation Loss: 55952, 89130.01821432619\n",
      "Epoch 49967: reducing learning rate of group 0 to 6.2112e-03.\n",
      "Epoch 50001, Training Loss: 35713, Validation Loss: 53370, 71273.3966236707\n",
      "Epoch 50068: reducing learning rate of group 0 to 6.2049e-03.\n",
      "Epoch 50101, Training Loss: 35600, Validation Loss: 57950, 73103.83170672996\n",
      "Epoch 50169: reducing learning rate of group 0 to 6.1987e-03.\n",
      "Epoch 50201, Training Loss: 36670, Validation Loss: 50417, 65265.14752228587\n",
      "Epoch 50270: reducing learning rate of group 0 to 6.1925e-03.\n",
      "Epoch 50301, Training Loss: 36855, Validation Loss: 54827, 75103.28708266081\n",
      "Epoch 50371: reducing learning rate of group 0 to 6.1863e-03.\n",
      "Epoch 50401, Training Loss: 37199, Validation Loss: 53636, 72179.59457969418\n",
      "Epoch 50472: reducing learning rate of group 0 to 6.1802e-03.\n",
      "Epoch 50501, Training Loss: 34150, Validation Loss: 56599, 83407.93224700411\n",
      "Epoch 50573: reducing learning rate of group 0 to 6.1740e-03.\n",
      "Epoch 50601, Training Loss: 36036, Validation Loss: 52293, 69087.12732013693\n",
      "Epoch 50674: reducing learning rate of group 0 to 6.1678e-03.\n",
      "Epoch 50701, Training Loss: 36717, Validation Loss: 52304, 89164.15719565637\n",
      "Epoch 50775: reducing learning rate of group 0 to 6.1616e-03.\n",
      "Epoch 50801, Training Loss: 36708, Validation Loss: 55792, 74416.00592732154\n",
      "Epoch 50876: reducing learning rate of group 0 to 6.1555e-03.\n",
      "Epoch 50901, Training Loss: 34236, Validation Loss: 57870, 74127.5897491818\n",
      "Epoch 50977: reducing learning rate of group 0 to 6.1493e-03.\n",
      "Epoch 51001, Training Loss: 34913, Validation Loss: 52632, 78296.55357070318\n",
      "Epoch 51078: reducing learning rate of group 0 to 6.1432e-03.\n",
      "Epoch 51101, Training Loss: 35655, Validation Loss: 51706, 59923.908794093266\n",
      "Epoch 51179: reducing learning rate of group 0 to 6.1370e-03.\n",
      "Epoch 51201, Training Loss: 33916, Validation Loss: 49962, 69411.51637879228\n",
      "Epoch 51280: reducing learning rate of group 0 to 6.1309e-03.\n",
      "Epoch 51301, Training Loss: 34885, Validation Loss: 52697, 73930.02898854231\n",
      "Epoch 51381: reducing learning rate of group 0 to 6.1248e-03.\n",
      "Epoch 51401, Training Loss: 35419, Validation Loss: 51985, 76376.5433571392\n",
      "Epoch 51482: reducing learning rate of group 0 to 6.1186e-03.\n",
      "Epoch 51501, Training Loss: 37048, Validation Loss: 55732, 64680.888411749685\n",
      "Epoch 51583: reducing learning rate of group 0 to 6.1125e-03.\n",
      "Epoch 51601, Training Loss: 32726, Validation Loss: 57246, 68911.53406181915\n",
      "Epoch 51684: reducing learning rate of group 0 to 6.1064e-03.\n",
      "Epoch 51701, Training Loss: 34194, Validation Loss: 57758, 88717.56736470602\n",
      "Epoch 51785: reducing learning rate of group 0 to 6.1003e-03.\n",
      "Epoch 51801, Training Loss: 36021, Validation Loss: 53499, 68704.63873628373\n",
      "Epoch 51886: reducing learning rate of group 0 to 6.0942e-03.\n",
      "Epoch 51901, Training Loss: 36142, Validation Loss: 52641, 85826.3929143905\n",
      "Epoch 51987: reducing learning rate of group 0 to 6.0881e-03.\n",
      "Epoch 52001, Training Loss: 35495, Validation Loss: 55942, 74771.13816549155\n",
      "Epoch 52088: reducing learning rate of group 0 to 6.0820e-03.\n",
      "Epoch 52101, Training Loss: 34544, Validation Loss: 56618, 65817.88917171238\n",
      "Epoch 52189: reducing learning rate of group 0 to 6.0759e-03.\n",
      "Epoch 52201, Training Loss: 35689, Validation Loss: 52746, 83963.96308674845\n",
      "Epoch 52290: reducing learning rate of group 0 to 6.0699e-03.\n",
      "Epoch 52301, Training Loss: 37166, Validation Loss: 51239, 85286.46138260321\n",
      "Epoch 52391: reducing learning rate of group 0 to 6.0638e-03.\n",
      "Epoch 52401, Training Loss: 34983, Validation Loss: 53574, 85796.75736346956\n",
      "Epoch 52492: reducing learning rate of group 0 to 6.0577e-03.\n",
      "Epoch 52501, Training Loss: 34479, Validation Loss: 57226, 65349.09713375891\n",
      "Epoch 52593: reducing learning rate of group 0 to 6.0517e-03.\n",
      "Epoch 52601, Training Loss: 32884, Validation Loss: 53468, 69578.63199898043\n",
      "Epoch 52694: reducing learning rate of group 0 to 6.0456e-03.\n",
      "Epoch 52701, Training Loss: 33519, Validation Loss: 52419, 80331.12370830674\n",
      "Epoch 52795: reducing learning rate of group 0 to 6.0396e-03.\n",
      "Epoch 52801, Training Loss: 36299, Validation Loss: 53418, 80961.31237077618\n",
      "Epoch 52896: reducing learning rate of group 0 to 6.0335e-03.\n",
      "Epoch 52901, Training Loss: 36551, Validation Loss: 53736, 79039.3993163776\n",
      "Epoch 52997: reducing learning rate of group 0 to 6.0275e-03.\n",
      "Epoch 53001, Training Loss: 32287, Validation Loss: 59219, 81273.9089136388\n",
      "Epoch 53098: reducing learning rate of group 0 to 6.0215e-03.\n",
      "Epoch 53101, Training Loss: 34993, Validation Loss: 52092, 76761.67708659319\n",
      "Epoch 53199: reducing learning rate of group 0 to 6.0154e-03.\n",
      "Epoch 53201, Training Loss: 38252, Validation Loss: 52546, 77336.46673178276\n",
      "Epoch 53300: reducing learning rate of group 0 to 6.0094e-03.\n",
      "Epoch 53301, Training Loss: 36537, Validation Loss: 56713, 62223.63022520817\n",
      "Epoch 53401: reducing learning rate of group 0 to 6.0034e-03.\n",
      "Epoch 53401, Training Loss: 35593, Validation Loss: 51256, 80444.81784566776\n",
      "Epoch 53501, Training Loss: 35876, Validation Loss: 53894, 75728.44525148954\n",
      "Epoch 53502: reducing learning rate of group 0 to 5.9974e-03.\n",
      "Epoch 53601, Training Loss: 36483, Validation Loss: 53912, 61774.96756638325\n",
      "Epoch 53603: reducing learning rate of group 0 to 5.9914e-03.\n",
      "Epoch 53701, Training Loss: 32954, Validation Loss: 51357, 63171.36797891002\n",
      "Epoch 53704: reducing learning rate of group 0 to 5.9854e-03.\n",
      "Epoch 53801, Training Loss: 36322, Validation Loss: 55482, 82433.96077881793\n",
      "Epoch 53805: reducing learning rate of group 0 to 5.9794e-03.\n",
      "Epoch 53901, Training Loss: 35022, Validation Loss: 54917, 81184.85642435723\n",
      "Epoch 53906: reducing learning rate of group 0 to 5.9735e-03.\n",
      "Epoch 54001, Training Loss: 40270, Validation Loss: 52173, 66229.9986182819\n",
      "Epoch 54007: reducing learning rate of group 0 to 5.9675e-03.\n",
      "Epoch 54101, Training Loss: 36140, Validation Loss: 56206, 63158.494557681515\n",
      "Epoch 54108: reducing learning rate of group 0 to 5.9615e-03.\n",
      "Epoch 54201, Training Loss: 36717, Validation Loss: 57156, 75063.48674614252\n",
      "Epoch 54209: reducing learning rate of group 0 to 5.9556e-03.\n",
      "Epoch 54301, Training Loss: 36195, Validation Loss: 54268, 77910.48787016103\n",
      "Epoch 54310: reducing learning rate of group 0 to 5.9496e-03.\n",
      "Epoch 54401, Training Loss: 34792, Validation Loss: 54347, 75897.87148668437\n",
      "Epoch 54411: reducing learning rate of group 0 to 5.9437e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54501, Training Loss: 36577, Validation Loss: 54163, 68000.56173291408\n",
      "Epoch 54512: reducing learning rate of group 0 to 5.9377e-03.\n",
      "Epoch 54601, Training Loss: 37434, Validation Loss: 54179, 82709.58601982292\n",
      "Epoch 54613: reducing learning rate of group 0 to 5.9318e-03.\n",
      "Epoch 54701, Training Loss: 35943, Validation Loss: 53115, 66093.28196591813\n",
      "Epoch 54714: reducing learning rate of group 0 to 5.9258e-03.\n",
      "Epoch 54801, Training Loss: 35070, Validation Loss: 56117, 68814.53079623311\n",
      "Epoch 54815: reducing learning rate of group 0 to 5.9199e-03.\n",
      "Epoch 54901, Training Loss: 32036, Validation Loss: 54373, 70707.00149234787\n",
      "Epoch 54916: reducing learning rate of group 0 to 5.9140e-03.\n",
      "Epoch 55001, Training Loss: 35421, Validation Loss: 54090, 60464.071614690714\n",
      "Epoch 55017: reducing learning rate of group 0 to 5.9081e-03.\n",
      "Epoch 55101, Training Loss: 39047, Validation Loss: 50646, 73934.88590211056\n",
      "Epoch 55118: reducing learning rate of group 0 to 5.9022e-03.\n",
      "Epoch 55201, Training Loss: 35247, Validation Loss: 53863, 55464.03105062072\n",
      "Epoch 55219: reducing learning rate of group 0 to 5.8963e-03.\n",
      "Epoch 55301, Training Loss: 36827, Validation Loss: 54127, 66844.48705730042\n",
      "Epoch 55320: reducing learning rate of group 0 to 5.8904e-03.\n",
      "Epoch 55401, Training Loss: 33113, Validation Loss: 57840, 110225.40060740414\n",
      "Epoch 55421: reducing learning rate of group 0 to 5.8845e-03.\n",
      "Epoch 55501, Training Loss: 33698, Validation Loss: 56358, 87854.90365698155\n",
      "Epoch 55522: reducing learning rate of group 0 to 5.8786e-03.\n",
      "Epoch 55601, Training Loss: 34777, Validation Loss: 54319, 62049.40948206798\n",
      "Epoch 55623: reducing learning rate of group 0 to 5.8727e-03.\n",
      "Epoch 55701, Training Loss: 36034, Validation Loss: 52726, 78260.8823996097\n",
      "Epoch 55724: reducing learning rate of group 0 to 5.8669e-03.\n",
      "Epoch 55801, Training Loss: 33381, Validation Loss: 52840, 67114.34929709234\n",
      "Epoch 55825: reducing learning rate of group 0 to 5.8610e-03.\n",
      "Epoch 55901, Training Loss: 35106, Validation Loss: 53057, 80659.88013412298\n",
      "Epoch 55926: reducing learning rate of group 0 to 5.8551e-03.\n",
      "Epoch 56001, Training Loss: 33703, Validation Loss: 52989, 76568.70268307057\n",
      "Epoch 56027: reducing learning rate of group 0 to 5.8493e-03.\n",
      "Epoch 56101, Training Loss: 37146, Validation Loss: 50854, 69919.14375957473\n",
      "Epoch 56128: reducing learning rate of group 0 to 5.8434e-03.\n",
      "Epoch 56201, Training Loss: 36236, Validation Loss: 52998, 78699.41840407987\n",
      "Epoch 56229: reducing learning rate of group 0 to 5.8376e-03.\n",
      "Epoch 56301, Training Loss: 34595, Validation Loss: 53274, 71867.21739443572\n",
      "Epoch 56330: reducing learning rate of group 0 to 5.8317e-03.\n",
      "Epoch 56401, Training Loss: 33245, Validation Loss: 57378, 66925.48010796019\n",
      "Epoch 56431: reducing learning rate of group 0 to 5.8259e-03.\n",
      "Epoch 56501, Training Loss: 34040, Validation Loss: 53495, 88345.10628002441\n",
      "Epoch 56532: reducing learning rate of group 0 to 5.8201e-03.\n",
      "Epoch 56601, Training Loss: 35730, Validation Loss: 53555, 79755.06333667527\n",
      "Epoch 56633: reducing learning rate of group 0 to 5.8143e-03.\n",
      "Epoch 56701, Training Loss: 37375, Validation Loss: 55156, 65485.04599368599\n",
      "Epoch 56734: reducing learning rate of group 0 to 5.8084e-03.\n",
      "Epoch 56801, Training Loss: 36084, Validation Loss: 57442, 73406.19238657094\n",
      "Epoch 56835: reducing learning rate of group 0 to 5.8026e-03.\n",
      "Epoch 56901, Training Loss: 34811, Validation Loss: 57710, 59381.33111948835\n",
      "Epoch 56936: reducing learning rate of group 0 to 5.7968e-03.\n",
      "Epoch 57001, Training Loss: 36623, Validation Loss: 56264, 71972.97480015624\n",
      "Epoch 57037: reducing learning rate of group 0 to 5.7910e-03.\n",
      "Epoch 57101, Training Loss: 34907, Validation Loss: 55171, 82443.38828051786\n",
      "Epoch 57138: reducing learning rate of group 0 to 5.7852e-03.\n",
      "Epoch 57201, Training Loss: 36520, Validation Loss: 55640, 60492.594272516464\n",
      "Epoch 57239: reducing learning rate of group 0 to 5.7795e-03.\n",
      "Epoch 57301, Training Loss: 34320, Validation Loss: 56241, 68358.10504432651\n",
      "Epoch 57340: reducing learning rate of group 0 to 5.7737e-03.\n",
      "Epoch 57401, Training Loss: 34950, Validation Loss: 51720, 62420.44148562963\n",
      "Epoch 57441: reducing learning rate of group 0 to 5.7679e-03.\n",
      "Epoch 57501, Training Loss: 33516, Validation Loss: 57100, 72096.4240803732\n",
      "Epoch 57542: reducing learning rate of group 0 to 5.7621e-03.\n",
      "Epoch 57601, Training Loss: 33133, Validation Loss: 52406, 74660.74138754467\n",
      "Epoch 57643: reducing learning rate of group 0 to 5.7564e-03.\n",
      "Epoch 57701, Training Loss: 34259, Validation Loss: 55067, 77646.8791552689\n",
      "Epoch 57744: reducing learning rate of group 0 to 5.7506e-03.\n",
      "Epoch 57801, Training Loss: 34630, Validation Loss: 53890, 70097.62207966995\n",
      "Epoch 57845: reducing learning rate of group 0 to 5.7449e-03.\n",
      "Epoch 57901, Training Loss: 36565, Validation Loss: 51891, 83287.75149336968\n",
      "Epoch 57946: reducing learning rate of group 0 to 5.7391e-03.\n",
      "Epoch 58001, Training Loss: 33951, Validation Loss: 53941, 68490.19216533286\n",
      "Epoch 58047: reducing learning rate of group 0 to 5.7334e-03.\n",
      "Epoch 58101, Training Loss: 37463, Validation Loss: 52232, 75755.91128256272\n",
      "Epoch 58148: reducing learning rate of group 0 to 5.7277e-03.\n",
      "Epoch 58201, Training Loss: 34048, Validation Loss: 52240, 66821.67076144867\n",
      "Epoch 58249: reducing learning rate of group 0 to 5.7219e-03.\n",
      "Epoch 58301, Training Loss: 38697, Validation Loss: 54077, 72017.1648035215\n",
      "Epoch 58350: reducing learning rate of group 0 to 5.7162e-03.\n",
      "Epoch 58401, Training Loss: 37004, Validation Loss: 54540, 72673.81104755537\n",
      "Epoch 58451: reducing learning rate of group 0 to 5.7105e-03.\n",
      "Epoch 58501, Training Loss: 33579, Validation Loss: 52591, 74666.45856968935\n",
      "Epoch 58552: reducing learning rate of group 0 to 5.7048e-03.\n",
      "Epoch 58601, Training Loss: 36478, Validation Loss: 52667, 62470.42570050049\n",
      "Epoch 58653: reducing learning rate of group 0 to 5.6991e-03.\n",
      "Epoch 58701, Training Loss: 31477, Validation Loss: 53550, 79340.15592177147\n",
      "Epoch 58754: reducing learning rate of group 0 to 5.6934e-03.\n",
      "Epoch 58801, Training Loss: 35511, Validation Loss: 52957, 76616.85665151481\n",
      "Epoch 58855: reducing learning rate of group 0 to 5.6877e-03.\n",
      "Epoch 58901, Training Loss: 35299, Validation Loss: 57340, 86409.0881860783\n",
      "Epoch 58956: reducing learning rate of group 0 to 5.6820e-03.\n",
      "Epoch 59001, Training Loss: 35029, Validation Loss: 53068, 70448.55538530769\n",
      "Epoch 59057: reducing learning rate of group 0 to 5.6763e-03.\n",
      "Epoch 59101, Training Loss: 35286, Validation Loss: 51346, 62476.259943824545\n",
      "Epoch 59158: reducing learning rate of group 0 to 5.6706e-03.\n",
      "Epoch 59201, Training Loss: 35262, Validation Loss: 53159, 80637.07125292147\n",
      "Epoch 59259: reducing learning rate of group 0 to 5.6650e-03.\n",
      "Epoch 59301, Training Loss: 35173, Validation Loss: 55519, 93962.12151241132\n",
      "Epoch 59360: reducing learning rate of group 0 to 5.6593e-03.\n",
      "Epoch 59401, Training Loss: 36365, Validation Loss: 56192, 82850.04070146418\n",
      "Epoch 59461: reducing learning rate of group 0 to 5.6536e-03.\n",
      "Epoch 59501, Training Loss: 33291, Validation Loss: 61422, 71737.90891687036\n",
      "Epoch 59562: reducing learning rate of group 0 to 5.6480e-03.\n",
      "Epoch 59601, Training Loss: 34154, Validation Loss: 52983, 84879.04393747033\n",
      "Epoch 59663: reducing learning rate of group 0 to 5.6423e-03.\n",
      "Epoch 59701, Training Loss: 32341, Validation Loss: 52155, 69938.72062794522\n",
      "Epoch 59764: reducing learning rate of group 0 to 5.6367e-03.\n",
      "Epoch 59801, Training Loss: 35183, Validation Loss: 56807, 67305.43032802526\n",
      "Epoch 59865: reducing learning rate of group 0 to 5.6311e-03.\n",
      "Epoch 59901, Training Loss: 34395, Validation Loss: 51268, 79489.83875361257\n",
      "Epoch 59966: reducing learning rate of group 0 to 5.6254e-03.\n",
      "Epoch 60001, Training Loss: 32490, Validation Loss: 55739, 76526.86080548358\n",
      "Epoch 60067: reducing learning rate of group 0 to 5.6198e-03.\n",
      "Epoch 60101, Training Loss: 32603, Validation Loss: 53403, 67661.09081487509\n",
      "Epoch 60168: reducing learning rate of group 0 to 5.6142e-03.\n",
      "Epoch 60201, Training Loss: 34210, Validation Loss: 51295, 63775.830491712644\n",
      "Epoch 60269: reducing learning rate of group 0 to 5.6086e-03.\n",
      "Epoch 60301, Training Loss: 36804, Validation Loss: 55828, 81072.04026725255\n",
      "Epoch 60370: reducing learning rate of group 0 to 5.6030e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60401, Training Loss: 32582, Validation Loss: 52283, 65606.95380549692\n",
      "Epoch 60471: reducing learning rate of group 0 to 5.5974e-03.\n",
      "Epoch 60501, Training Loss: 36000, Validation Loss: 55183, 56153.93620651838\n",
      "Epoch 60572: reducing learning rate of group 0 to 5.5918e-03.\n",
      "Epoch 60601, Training Loss: 34436, Validation Loss: 51338, 100323.60037983886\n",
      "Epoch 60673: reducing learning rate of group 0 to 5.5862e-03.\n",
      "Epoch 60701, Training Loss: 33965, Validation Loss: 53091, 72419.0418107506\n",
      "Epoch 60774: reducing learning rate of group 0 to 5.5806e-03.\n",
      "Epoch 60801, Training Loss: 35945, Validation Loss: 53906, 70480.73381636072\n",
      "Epoch 60875: reducing learning rate of group 0 to 5.5750e-03.\n",
      "Epoch 60901, Training Loss: 33668, Validation Loss: 54205, 79410.99757098219\n",
      "Epoch 60976: reducing learning rate of group 0 to 5.5694e-03.\n",
      "Epoch 61001, Training Loss: 36610, Validation Loss: 56163, 74861.92337913146\n",
      "Epoch 61077: reducing learning rate of group 0 to 5.5639e-03.\n",
      "Epoch 61101, Training Loss: 33749, Validation Loss: 55552, 74337.4920110588\n",
      "Epoch 61178: reducing learning rate of group 0 to 5.5583e-03.\n",
      "Epoch 61201, Training Loss: 34109, Validation Loss: 52721, 84536.10981240311\n",
      "Epoch 61279: reducing learning rate of group 0 to 5.5527e-03.\n",
      "Epoch 61301, Training Loss: 34241, Validation Loss: 52752, 58123.610037523555\n",
      "Epoch 61380: reducing learning rate of group 0 to 5.5472e-03.\n",
      "Epoch 61401, Training Loss: 32794, Validation Loss: 53009, 83796.0266353954\n",
      "Epoch 61481: reducing learning rate of group 0 to 5.5416e-03.\n",
      "Epoch 61501, Training Loss: 33724, Validation Loss: 53169, 67664.92128218495\n",
      "Epoch 61582: reducing learning rate of group 0 to 5.5361e-03.\n",
      "Epoch 61601, Training Loss: 32418, Validation Loss: 53565, 81389.57709163026\n",
      "Epoch 61683: reducing learning rate of group 0 to 5.5306e-03.\n",
      "Epoch 61701, Training Loss: 33226, Validation Loss: 52735, 75010.16827795008\n",
      "Epoch 61784: reducing learning rate of group 0 to 5.5250e-03.\n",
      "Epoch 61801, Training Loss: 31381, Validation Loss: 55148, 71818.15484273098\n",
      "Epoch 61885: reducing learning rate of group 0 to 5.5195e-03.\n",
      "Epoch 61901, Training Loss: 33205, Validation Loss: 55736, 88184.24293351879\n",
      "Epoch 61986: reducing learning rate of group 0 to 5.5140e-03.\n",
      "Epoch 62001, Training Loss: 35362, Validation Loss: 52448, 76435.13485702788\n",
      "Epoch 62087: reducing learning rate of group 0 to 5.5085e-03.\n",
      "Epoch 62101, Training Loss: 36036, Validation Loss: 52232, 98004.9023960904\n",
      "Epoch 62188: reducing learning rate of group 0 to 5.5030e-03.\n",
      "Epoch 62201, Training Loss: 32091, Validation Loss: 57140, 84051.2614635525\n",
      "Epoch 62289: reducing learning rate of group 0 to 5.4975e-03.\n",
      "Epoch 62301, Training Loss: 33310, Validation Loss: 55040, 79927.7079376747\n",
      "Epoch 62390: reducing learning rate of group 0 to 5.4920e-03.\n",
      "Epoch 62401, Training Loss: 32671, Validation Loss: 54278, 76674.95767952004\n",
      "Epoch 62491: reducing learning rate of group 0 to 5.4865e-03.\n",
      "Epoch 62501, Training Loss: 35113, Validation Loss: 53508, 72992.53014641143\n",
      "Epoch 62592: reducing learning rate of group 0 to 5.4810e-03.\n",
      "Epoch 62601, Training Loss: 32529, Validation Loss: 53979, 69438.64926628512\n",
      "Epoch 62693: reducing learning rate of group 0 to 5.4755e-03.\n",
      "Epoch 62701, Training Loss: 32968, Validation Loss: 57369, 65870.57436912319\n",
      "Epoch 62794: reducing learning rate of group 0 to 5.4700e-03.\n",
      "Epoch 62801, Training Loss: 32609, Validation Loss: 55128, 73935.7766792462\n",
      "Epoch 62895: reducing learning rate of group 0 to 5.4646e-03.\n",
      "Epoch 62901, Training Loss: 30984, Validation Loss: 55298, 76616.36503662932\n",
      "Epoch 62996: reducing learning rate of group 0 to 5.4591e-03.\n",
      "Epoch 63001, Training Loss: 33144, Validation Loss: 52262, 78249.66409757301\n",
      "Epoch 63097: reducing learning rate of group 0 to 5.4536e-03.\n",
      "Epoch 63101, Training Loss: 33983, Validation Loss: 52272, 77208.72579355481\n",
      "Epoch 63198: reducing learning rate of group 0 to 5.4482e-03.\n",
      "Epoch 63201, Training Loss: 35792, Validation Loss: 51493, 74966.81532323225\n",
      "Epoch 63299: reducing learning rate of group 0 to 5.4427e-03.\n",
      "Epoch 63301, Training Loss: 33180, Validation Loss: 53004, 76142.78375282265\n",
      "Epoch 63400: reducing learning rate of group 0 to 5.4373e-03.\n",
      "Epoch 63401, Training Loss: 31965, Validation Loss: 58920, 75099.51250257778\n",
      "Epoch 63501: reducing learning rate of group 0 to 5.4319e-03.\n",
      "Epoch 63501, Training Loss: 31931, Validation Loss: 52144, 75120.6734304676\n",
      "Epoch 63601, Training Loss: 32782, Validation Loss: 57371, 69687.85888801096\n",
      "Epoch 63602: reducing learning rate of group 0 to 5.4264e-03.\n",
      "Epoch 63701, Training Loss: 33116, Validation Loss: 54579, 66905.8188817649\n",
      "Epoch 63703: reducing learning rate of group 0 to 5.4210e-03.\n",
      "Epoch 63801, Training Loss: 34868, Validation Loss: 58309, 86271.98797705052\n",
      "Epoch 63804: reducing learning rate of group 0 to 5.4156e-03.\n",
      "Epoch 63901, Training Loss: 32625, Validation Loss: 53139, 78520.21765107721\n",
      "Epoch 63905: reducing learning rate of group 0 to 5.4102e-03.\n",
      "Epoch 64001, Training Loss: 34756, Validation Loss: 54501, 69603.59304448204\n",
      "Epoch 64006: reducing learning rate of group 0 to 5.4047e-03.\n",
      "Epoch 64101, Training Loss: 30851, Validation Loss: 54706, 69340.73569530634\n",
      "Epoch 64107: reducing learning rate of group 0 to 5.3993e-03.\n",
      "Epoch 64201, Training Loss: 33788, Validation Loss: 55149, 64967.17980704868\n",
      "Epoch 64208: reducing learning rate of group 0 to 5.3939e-03.\n",
      "Epoch 64301, Training Loss: 36148, Validation Loss: 51929, 62779.117319785815\n",
      "Epoch 64309: reducing learning rate of group 0 to 5.3885e-03.\n",
      "Epoch 64401, Training Loss: 33018, Validation Loss: 55384, 74494.89536847519\n",
      "Epoch 64410: reducing learning rate of group 0 to 5.3832e-03.\n",
      "Epoch 64501, Training Loss: 33954, Validation Loss: 53458, 80651.29049079593\n",
      "Epoch 64511: reducing learning rate of group 0 to 5.3778e-03.\n",
      "Epoch 64601, Training Loss: 33694, Validation Loss: 52435, 78412.30028126122\n",
      "Epoch 64612: reducing learning rate of group 0 to 5.3724e-03.\n",
      "Epoch 64701, Training Loss: 31166, Validation Loss: 53848, 74662.3649861685\n",
      "Epoch 64713: reducing learning rate of group 0 to 5.3670e-03.\n",
      "Epoch 64801, Training Loss: 35504, Validation Loss: 54117, 81595.573013859\n",
      "Epoch 64814: reducing learning rate of group 0 to 5.3617e-03.\n",
      "Epoch 64901, Training Loss: 36372, Validation Loss: 57947, 77942.54410044379\n",
      "Epoch 64915: reducing learning rate of group 0 to 5.3563e-03.\n",
      "Epoch 65001, Training Loss: 30792, Validation Loss: 51217, 73172.08522048096\n",
      "Epoch 65016: reducing learning rate of group 0 to 5.3509e-03.\n",
      "Epoch 65101, Training Loss: 33589, Validation Loss: 57269, 68788.2922248481\n",
      "Epoch 65117: reducing learning rate of group 0 to 5.3456e-03.\n",
      "Epoch 65201, Training Loss: 39052, Validation Loss: 51523, 66956.67894997438\n",
      "Epoch 65218: reducing learning rate of group 0 to 5.3402e-03.\n",
      "Epoch 65301, Training Loss: 33556, Validation Loss: 54563, 101602.62798941626\n",
      "Epoch 65319: reducing learning rate of group 0 to 5.3349e-03.\n",
      "Epoch 65401, Training Loss: 32829, Validation Loss: 52473, 88455.99590304086\n",
      "Epoch 65420: reducing learning rate of group 0 to 5.3296e-03.\n",
      "Epoch 65501, Training Loss: 34558, Validation Loss: 51939, 62123.43964737797\n",
      "Epoch 65521: reducing learning rate of group 0 to 5.3242e-03.\n",
      "Epoch 65601, Training Loss: 33005, Validation Loss: 54200, 82983.43738422808\n",
      "Epoch 65622: reducing learning rate of group 0 to 5.3189e-03.\n",
      "Epoch 65701, Training Loss: 34226, Validation Loss: 54290, 69854.97414383215\n",
      "Epoch 65723: reducing learning rate of group 0 to 5.3136e-03.\n",
      "Epoch 65801, Training Loss: 36545, Validation Loss: 54163, 79163.58314861743\n",
      "Epoch 65824: reducing learning rate of group 0 to 5.3083e-03.\n",
      "Epoch 65901, Training Loss: 34645, Validation Loss: 53794, 82288.37696804071\n",
      "Epoch 65925: reducing learning rate of group 0 to 5.3030e-03.\n",
      "Epoch 66001, Training Loss: 38540, Validation Loss: 54853, 83111.74168787268\n",
      "Epoch 66026: reducing learning rate of group 0 to 5.2977e-03.\n",
      "Epoch 66101, Training Loss: 31644, Validation Loss: 54064, 93730.08658523615\n",
      "Epoch 66127: reducing learning rate of group 0 to 5.2924e-03.\n",
      "Epoch 66201, Training Loss: 39853, Validation Loss: 54189, 77932.67972132946\n",
      "Epoch 66228: reducing learning rate of group 0 to 5.2871e-03.\n",
      "Epoch 66301, Training Loss: 34031, Validation Loss: 54492, 62591.64089959345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66329: reducing learning rate of group 0 to 5.2818e-03.\n",
      "Epoch 66401, Training Loss: 31668, Validation Loss: 54936, 78949.8565020673\n",
      "Epoch 66430: reducing learning rate of group 0 to 5.2765e-03.\n",
      "Epoch 66501, Training Loss: 34466, Validation Loss: 52820, 76522.15154150118\n",
      "Epoch 66531: reducing learning rate of group 0 to 5.2712e-03.\n",
      "Epoch 66601, Training Loss: 36701, Validation Loss: 54916, 75032.70204232018\n",
      "Epoch 66632: reducing learning rate of group 0 to 5.2660e-03.\n",
      "Epoch 66701, Training Loss: 33351, Validation Loss: 51115, 83083.98718077813\n",
      "Epoch 66733: reducing learning rate of group 0 to 5.2607e-03.\n",
      "Epoch 66801, Training Loss: 32664, Validation Loss: 53624, 76436.77344530523\n",
      "Epoch 66834: reducing learning rate of group 0 to 5.2554e-03.\n",
      "Epoch 66901, Training Loss: 33514, Validation Loss: 53700, 78878.56174495337\n",
      "Epoch 66935: reducing learning rate of group 0 to 5.2502e-03.\n",
      "Epoch 67001, Training Loss: 29917, Validation Loss: 51708, 78028.5143738089\n",
      "Epoch 67036: reducing learning rate of group 0 to 5.2449e-03.\n",
      "Epoch 67101, Training Loss: 31352, Validation Loss: 52804, 68988.06369940631\n",
      "Epoch 67137: reducing learning rate of group 0 to 5.2397e-03.\n",
      "Epoch 67201, Training Loss: 31654, Validation Loss: 53770, 72419.85293009559\n",
      "Epoch 67238: reducing learning rate of group 0 to 5.2344e-03.\n",
      "Epoch 67301, Training Loss: 33540, Validation Loss: 56123, 67715.84879533226\n",
      "Epoch 67339: reducing learning rate of group 0 to 5.2292e-03.\n",
      "Epoch 67401, Training Loss: 35557, Validation Loss: 52469, 83516.42240949624\n",
      "Epoch 67440: reducing learning rate of group 0 to 5.2240e-03.\n",
      "Epoch 67501, Training Loss: 36383, Validation Loss: 53492, 78048.38149537283\n",
      "Epoch 67541: reducing learning rate of group 0 to 5.2188e-03.\n",
      "Epoch 67601, Training Loss: 33982, Validation Loss: 54541, 75721.4929474874\n",
      "Epoch 67642: reducing learning rate of group 0 to 5.2135e-03.\n",
      "Epoch 67701, Training Loss: 33303, Validation Loss: 53881, 76702.28571892616\n",
      "Epoch 67743: reducing learning rate of group 0 to 5.2083e-03.\n",
      "Epoch 67801, Training Loss: 34742, Validation Loss: 53455, 75855.0103589954\n",
      "Epoch 67844: reducing learning rate of group 0 to 5.2031e-03.\n",
      "Epoch 67901, Training Loss: 33086, Validation Loss: 54923, 82827.87404088429\n",
      "Epoch 67945: reducing learning rate of group 0 to 5.1979e-03.\n",
      "Epoch 68001, Training Loss: 33518, Validation Loss: 53440, 67584.6484634715\n",
      "Epoch 68046: reducing learning rate of group 0 to 5.1927e-03.\n",
      "Epoch 68101, Training Loss: 35240, Validation Loss: 57343, 64881.18736557235\n",
      "Epoch 68147: reducing learning rate of group 0 to 5.1875e-03.\n",
      "Epoch 68201, Training Loss: 34557, Validation Loss: 52336, 69036.17267938699\n",
      "Epoch 68248: reducing learning rate of group 0 to 5.1823e-03.\n",
      "Epoch 68301, Training Loss: 30423, Validation Loss: 55860, 72967.87115953925\n",
      "Epoch 68349: reducing learning rate of group 0 to 5.1772e-03.\n",
      "Epoch 68401, Training Loss: 31758, Validation Loss: 55977, 73080.19623490347\n",
      "Epoch 68450: reducing learning rate of group 0 to 5.1720e-03.\n",
      "Epoch 68501, Training Loss: 35219, Validation Loss: 52930, 74716.41426567861\n",
      "Epoch 68551: reducing learning rate of group 0 to 5.1668e-03.\n",
      "Epoch 68601, Training Loss: 32400, Validation Loss: 57570, 67774.72659565601\n",
      "Epoch 68652: reducing learning rate of group 0 to 5.1616e-03.\n",
      "Epoch 68701, Training Loss: 31544, Validation Loss: 51048, 78107.0501087054\n",
      "Epoch 68753: reducing learning rate of group 0 to 5.1565e-03.\n",
      "Epoch 68801, Training Loss: 34587, Validation Loss: 55357, 62785.737967267225\n",
      "Epoch 68854: reducing learning rate of group 0 to 5.1513e-03.\n",
      "Epoch 68901, Training Loss: 34135, Validation Loss: 54299, 68637.702151474\n",
      "Epoch 68955: reducing learning rate of group 0 to 5.1462e-03.\n",
      "Epoch 69001, Training Loss: 34034, Validation Loss: 56424, 80013.3947517381\n",
      "Epoch 69056: reducing learning rate of group 0 to 5.1410e-03.\n",
      "Epoch 69101, Training Loss: 32239, Validation Loss: 52724, 83811.90223641628\n",
      "Epoch 69157: reducing learning rate of group 0 to 5.1359e-03.\n",
      "Epoch 69201, Training Loss: 34934, Validation Loss: 52123, 77006.73812376366\n",
      "Epoch 69258: reducing learning rate of group 0 to 5.1307e-03.\n",
      "Epoch 69301, Training Loss: 32698, Validation Loss: 57314, 74940.22807643384\n",
      "Epoch 69359: reducing learning rate of group 0 to 5.1256e-03.\n",
      "Epoch 69401, Training Loss: 36563, Validation Loss: 54076, 72294.74333276507\n",
      "Epoch 69460: reducing learning rate of group 0 to 5.1205e-03.\n",
      "Epoch 69501, Training Loss: 34964, Validation Loss: 53651, 75408.27868563526\n",
      "Epoch 69561: reducing learning rate of group 0 to 5.1154e-03.\n",
      "Epoch 69601, Training Loss: 32093, Validation Loss: 53673, 89769.38344046679\n",
      "Epoch 69662: reducing learning rate of group 0 to 5.1103e-03.\n",
      "Epoch 69701, Training Loss: 33890, Validation Loss: 52661, 65741.19274195036\n",
      "Epoch 69763: reducing learning rate of group 0 to 5.1051e-03.\n",
      "Epoch 69801, Training Loss: 32845, Validation Loss: 53406, 95618.90868055087\n",
      "Epoch 69864: reducing learning rate of group 0 to 5.1000e-03.\n",
      "Epoch 69901, Training Loss: 35068, Validation Loss: 55241, 96131.35242177488\n",
      "Epoch 69965: reducing learning rate of group 0 to 5.0949e-03.\n",
      "Epoch 70001, Training Loss: 32680, Validation Loss: 52632, 74980.7665909227\n",
      "Epoch 70066: reducing learning rate of group 0 to 5.0898e-03.\n",
      "Epoch 70101, Training Loss: 32087, Validation Loss: 54364, 75296.14074364428\n",
      "Epoch 70167: reducing learning rate of group 0 to 5.0848e-03.\n",
      "Epoch 70201, Training Loss: 32322, Validation Loss: 53553, 79208.93881122208\n",
      "Epoch 70268: reducing learning rate of group 0 to 5.0797e-03.\n",
      "Epoch 70301, Training Loss: 31089, Validation Loss: 54379, 84962.41187268324\n",
      "Epoch 70369: reducing learning rate of group 0 to 5.0746e-03.\n",
      "Epoch 70401, Training Loss: 30466, Validation Loss: 54172, 70653.56496868249\n",
      "Epoch 70470: reducing learning rate of group 0 to 5.0695e-03.\n",
      "Epoch 70501, Training Loss: 33313, Validation Loss: 54521, 75418.11440551722\n",
      "Epoch 70571: reducing learning rate of group 0 to 5.0644e-03.\n",
      "Epoch 70601, Training Loss: 31246, Validation Loss: 57245, 62982.842035412534\n",
      "Epoch 70672: reducing learning rate of group 0 to 5.0594e-03.\n",
      "Epoch 70701, Training Loss: 36750, Validation Loss: 54207, 64716.513436658504\n",
      "Epoch 70773: reducing learning rate of group 0 to 5.0543e-03.\n",
      "Epoch 70801, Training Loss: 33826, Validation Loss: 54174, 76169.68139587164\n",
      "Epoch 70874: reducing learning rate of group 0 to 5.0493e-03.\n",
      "Epoch 70901, Training Loss: 32586, Validation Loss: 53021, 68295.13508885894\n",
      "Epoch 70975: reducing learning rate of group 0 to 5.0442e-03.\n",
      "Epoch 71001, Training Loss: 34956, Validation Loss: 53375, 79136.89050322093\n",
      "Epoch 71076: reducing learning rate of group 0 to 5.0392e-03.\n",
      "Epoch 71101, Training Loss: 34397, Validation Loss: 56684, 77738.98100758891\n",
      "Epoch 71177: reducing learning rate of group 0 to 5.0341e-03.\n",
      "Epoch 71201, Training Loss: 33312, Validation Loss: 52907, 75621.66938105828\n",
      "Epoch 71278: reducing learning rate of group 0 to 5.0291e-03.\n",
      "Epoch 71301, Training Loss: 32606, Validation Loss: 56526, 71794.18938241967\n",
      "Epoch 71379: reducing learning rate of group 0 to 5.0241e-03.\n",
      "Epoch 71401, Training Loss: 33185, Validation Loss: 54706, 70556.24563546774\n",
      "Epoch 71480: reducing learning rate of group 0 to 5.0190e-03.\n",
      "Epoch 71501, Training Loss: 32907, Validation Loss: 55895, 71208.63027516716\n",
      "Epoch 71581: reducing learning rate of group 0 to 5.0140e-03.\n",
      "Epoch 71601, Training Loss: 32434, Validation Loss: 54228, 74890.45623940032\n",
      "Epoch 71682: reducing learning rate of group 0 to 5.0090e-03.\n",
      "Epoch 71701, Training Loss: 32507, Validation Loss: 53163, 58168.53799569348\n",
      "Epoch 71783: reducing learning rate of group 0 to 5.0040e-03.\n",
      "Epoch 71801, Training Loss: 32555, Validation Loss: 55370, 64210.86650391176\n",
      "Epoch 71884: reducing learning rate of group 0 to 4.9990e-03.\n",
      "Epoch 71901, Training Loss: 33283, Validation Loss: 56072, 95621.04075251064\n",
      "Epoch 71985: reducing learning rate of group 0 to 4.9940e-03.\n",
      "Epoch 72001, Training Loss: 33269, Validation Loss: 56024, 73194.95624713314\n",
      "Epoch 72086: reducing learning rate of group 0 to 4.9890e-03.\n",
      "Epoch 72101, Training Loss: 32626, Validation Loss: 53050, 73322.93482187256\n",
      "Epoch 72187: reducing learning rate of group 0 to 4.9840e-03.\n",
      "Epoch 72201, Training Loss: 31852, Validation Loss: 55590, 73790.91733952967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72288: reducing learning rate of group 0 to 4.9790e-03.\n",
      "Epoch 72301, Training Loss: 32448, Validation Loss: 55842, 75382.77898045773\n",
      "Epoch 72389: reducing learning rate of group 0 to 4.9741e-03.\n",
      "Epoch 72401, Training Loss: 33942, Validation Loss: 52129, 66898.65018481125\n",
      "Epoch 72490: reducing learning rate of group 0 to 4.9691e-03.\n",
      "Epoch 72501, Training Loss: 34744, Validation Loss: 52194, 75177.41861740455\n",
      "Epoch 72591: reducing learning rate of group 0 to 4.9641e-03.\n",
      "Epoch 72601, Training Loss: 31973, Validation Loss: 50463, 83695.58660877064\n",
      "Epoch 72692: reducing learning rate of group 0 to 4.9592e-03.\n",
      "Epoch 72701, Training Loss: 32895, Validation Loss: 55723, 77823.8829116401\n",
      "Epoch 72793: reducing learning rate of group 0 to 4.9542e-03.\n",
      "Epoch 72801, Training Loss: 33120, Validation Loss: 52337, 67726.1779463428\n",
      "Epoch 72894: reducing learning rate of group 0 to 4.9492e-03.\n",
      "Epoch 72901, Training Loss: 31776, Validation Loss: 57323, 79219.2091614129\n",
      "Epoch 72995: reducing learning rate of group 0 to 4.9443e-03.\n",
      "Epoch 73001, Training Loss: 31811, Validation Loss: 60809, 66669.3652496232\n",
      "Epoch 73096: reducing learning rate of group 0 to 4.9393e-03.\n",
      "Epoch 73101, Training Loss: 35467, Validation Loss: 53887, 60446.14905444746\n",
      "Epoch 73197: reducing learning rate of group 0 to 4.9344e-03.\n",
      "Epoch 73201, Training Loss: 33487, Validation Loss: 53930, 69592.17087356643\n",
      "Epoch 73298: reducing learning rate of group 0 to 4.9295e-03.\n",
      "Epoch 73301, Training Loss: 35152, Validation Loss: 53283, 88507.48129250649\n",
      "Epoch 73399: reducing learning rate of group 0 to 4.9245e-03.\n",
      "Epoch 73401, Training Loss: 33405, Validation Loss: 52747, 74713.36981297134\n",
      "Epoch 73500: reducing learning rate of group 0 to 4.9196e-03.\n",
      "Epoch 73501, Training Loss: 30839, Validation Loss: 52884, 82243.94429895842\n",
      "Epoch 73601: reducing learning rate of group 0 to 4.9147e-03.\n",
      "Epoch 73601, Training Loss: 31937, Validation Loss: 56606, 68875.73723013571\n",
      "Epoch 73701, Training Loss: 30279, Validation Loss: 56844, 77783.80274159933\n",
      "Epoch 73702: reducing learning rate of group 0 to 4.9098e-03.\n",
      "Epoch 73801, Training Loss: 33078, Validation Loss: 54176, 71561.14576027247\n",
      "Epoch 73803: reducing learning rate of group 0 to 4.9049e-03.\n",
      "Epoch 73901, Training Loss: 30818, Validation Loss: 53310, 75614.68368527322\n",
      "Epoch 73904: reducing learning rate of group 0 to 4.9000e-03.\n",
      "Epoch 74001, Training Loss: 34162, Validation Loss: 53256, 66998.47224096073\n",
      "Epoch 74005: reducing learning rate of group 0 to 4.8951e-03.\n",
      "Epoch 74101, Training Loss: 32620, Validation Loss: 53302, 70107.95836668166\n",
      "Epoch 74106: reducing learning rate of group 0 to 4.8902e-03.\n",
      "Epoch 74201, Training Loss: 33701, Validation Loss: 53211, 75631.62303398893\n",
      "Epoch 74207: reducing learning rate of group 0 to 4.8853e-03.\n",
      "Epoch 74301, Training Loss: 33899, Validation Loss: 56029, 63851.6395904709\n",
      "Epoch 74308: reducing learning rate of group 0 to 4.8804e-03.\n",
      "Epoch 74401, Training Loss: 32163, Validation Loss: 56811, 72986.44348847622\n",
      "Epoch 74409: reducing learning rate of group 0 to 4.8755e-03.\n",
      "Epoch 74501, Training Loss: 33195, Validation Loss: 51920, 74894.82123123882\n",
      "Epoch 74510: reducing learning rate of group 0 to 4.8706e-03.\n",
      "Epoch 74601, Training Loss: 31062, Validation Loss: 58832, 88151.0210845931\n",
      "Epoch 74611: reducing learning rate of group 0 to 4.8658e-03.\n",
      "Epoch 74701, Training Loss: 31909, Validation Loss: 54339, 71064.79821589467\n",
      "Epoch 74712: reducing learning rate of group 0 to 4.8609e-03.\n",
      "Epoch 74801, Training Loss: 31105, Validation Loss: 53992, 70392.92560573002\n",
      "Epoch 74813: reducing learning rate of group 0 to 4.8560e-03.\n",
      "Epoch 74901, Training Loss: 32385, Validation Loss: 55310, 70500.23605661457\n",
      "Epoch 74914: reducing learning rate of group 0 to 4.8512e-03.\n",
      "Epoch 75001, Training Loss: 33703, Validation Loss: 53237, 68113.85397034667\n",
      "Epoch 75015: reducing learning rate of group 0 to 4.8463e-03.\n",
      "Epoch 75101, Training Loss: 32505, Validation Loss: 54213, 74313.85679881198\n",
      "Epoch 75116: reducing learning rate of group 0 to 4.8415e-03.\n",
      "Epoch 75201, Training Loss: 34151, Validation Loss: 53860, 68703.09210188378\n",
      "Epoch 75217: reducing learning rate of group 0 to 4.8366e-03.\n",
      "Epoch 75301, Training Loss: 31517, Validation Loss: 57191, 75395.93796603267\n",
      "Epoch 75318: reducing learning rate of group 0 to 4.8318e-03.\n",
      "Epoch 75401, Training Loss: 32084, Validation Loss: 53968, 62084.20463561596\n",
      "Epoch 75419: reducing learning rate of group 0 to 4.8270e-03.\n",
      "Epoch 75501, Training Loss: 34049, Validation Loss: 55605, 71097.06638407613\n",
      "Epoch 75520: reducing learning rate of group 0 to 4.8222e-03.\n",
      "Epoch 75601, Training Loss: 31281, Validation Loss: 55441, 80024.51664799084\n",
      "Epoch 75621: reducing learning rate of group 0 to 4.8173e-03.\n",
      "Epoch 75701, Training Loss: 36123, Validation Loss: 54889, 75584.43705645778\n",
      "Epoch 75722: reducing learning rate of group 0 to 4.8125e-03.\n",
      "Epoch 75801, Training Loss: 33220, Validation Loss: 50833, 67115.47309565007\n",
      "Epoch 75823: reducing learning rate of group 0 to 4.8077e-03.\n",
      "Epoch 75901, Training Loss: 31729, Validation Loss: 53235, 78771.93616841092\n",
      "Epoch 75924: reducing learning rate of group 0 to 4.8029e-03.\n",
      "Epoch 76001, Training Loss: 33682, Validation Loss: 56840, 88836.9301618884\n",
      "Epoch 76025: reducing learning rate of group 0 to 4.7981e-03.\n",
      "Epoch 76101, Training Loss: 32113, Validation Loss: 56159, 76702.65284315935\n",
      "Epoch 76126: reducing learning rate of group 0 to 4.7933e-03.\n",
      "Epoch 76201, Training Loss: 32435, Validation Loss: 51832, 85297.65978937547\n",
      "Epoch 76227: reducing learning rate of group 0 to 4.7885e-03.\n",
      "Epoch 76301, Training Loss: 31977, Validation Loss: 54734, 82444.94136528626\n",
      "Epoch 76328: reducing learning rate of group 0 to 4.7837e-03.\n",
      "Epoch 76401, Training Loss: 33272, Validation Loss: 54753, 71369.9257080313\n",
      "Epoch 76429: reducing learning rate of group 0 to 4.7789e-03.\n",
      "Epoch 76501, Training Loss: 35147, Validation Loss: 56860, 73666.8694827913\n",
      "Epoch 76530: reducing learning rate of group 0 to 4.7741e-03.\n",
      "Epoch 76601, Training Loss: 31000, Validation Loss: 52659, 86463.94349833346\n",
      "Epoch 76631: reducing learning rate of group 0 to 4.7694e-03.\n",
      "Epoch 76701, Training Loss: 32876, Validation Loss: 57574, 69183.14161186574\n",
      "Epoch 76732: reducing learning rate of group 0 to 4.7646e-03.\n",
      "Epoch 76801, Training Loss: 31155, Validation Loss: 52585, 66316.7361436446\n",
      "Epoch 76833: reducing learning rate of group 0 to 4.7598e-03.\n",
      "Epoch 76901, Training Loss: 32137, Validation Loss: 52028, 86858.96117934743\n",
      "Epoch 76934: reducing learning rate of group 0 to 4.7551e-03.\n",
      "Epoch 77001, Training Loss: 33634, Validation Loss: 53860, 79165.27006641657\n",
      "Epoch 77035: reducing learning rate of group 0 to 4.7503e-03.\n",
      "Epoch 77101, Training Loss: 30932, Validation Loss: 56861, 72944.542441096\n",
      "Epoch 77136: reducing learning rate of group 0 to 4.7456e-03.\n",
      "Epoch 77201, Training Loss: 31837, Validation Loss: 52952, 84944.3450636448\n",
      "Epoch 77237: reducing learning rate of group 0 to 4.7408e-03.\n",
      "Epoch 77301, Training Loss: 32040, Validation Loss: 52872, 72188.96084382372\n",
      "Epoch 77338: reducing learning rate of group 0 to 4.7361e-03.\n",
      "Epoch 77401, Training Loss: 30035, Validation Loss: 53925, 71395.61718590053\n",
      "Epoch 77439: reducing learning rate of group 0 to 4.7314e-03.\n",
      "Epoch 77501, Training Loss: 35235, Validation Loss: 55032, 79177.01832219218\n",
      "Epoch 77540: reducing learning rate of group 0 to 4.7266e-03.\n",
      "Epoch 77601, Training Loss: 31701, Validation Loss: 55804, 68826.09601929887\n",
      "Epoch 77641: reducing learning rate of group 0 to 4.7219e-03.\n",
      "Epoch 77701, Training Loss: 35414, Validation Loss: 52602, 71570.23576150411\n",
      "Epoch 77742: reducing learning rate of group 0 to 4.7172e-03.\n",
      "Epoch 77801, Training Loss: 34212, Validation Loss: 51985, 79511.19458761695\n",
      "Epoch 77843: reducing learning rate of group 0 to 4.7125e-03.\n",
      "Epoch 77901, Training Loss: 33456, Validation Loss: 52861, 68041.91247916156\n",
      "Epoch 77944: reducing learning rate of group 0 to 4.7077e-03.\n",
      "Epoch 78001, Training Loss: 31739, Validation Loss: 53640, 79516.42996252893\n",
      "Epoch 78045: reducing learning rate of group 0 to 4.7030e-03.\n",
      "Epoch 78101, Training Loss: 31217, Validation Loss: 52816, 77691.49381312716\n",
      "Epoch 78146: reducing learning rate of group 0 to 4.6983e-03.\n",
      "Epoch 78201, Training Loss: 33422, Validation Loss: 55048, 69714.80901429603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78247: reducing learning rate of group 0 to 4.6936e-03.\n",
      "Epoch 78301, Training Loss: 30385, Validation Loss: 52237, 81320.00256680645\n",
      "Epoch 78348: reducing learning rate of group 0 to 4.6889e-03.\n",
      "Epoch 78401, Training Loss: 34503, Validation Loss: 56260, 76812.9307981403\n",
      "Epoch 78449: reducing learning rate of group 0 to 4.6843e-03.\n",
      "Epoch 78501, Training Loss: 31492, Validation Loss: 56294, 67805.5325616305\n",
      "Epoch 78550: reducing learning rate of group 0 to 4.6796e-03.\n",
      "Epoch 78601, Training Loss: 31734, Validation Loss: 53082, 70947.7373047715\n",
      "Epoch 78651: reducing learning rate of group 0 to 4.6749e-03.\n",
      "Epoch 78701, Training Loss: 33421, Validation Loss: 52044, 64567.18730769379\n",
      "Epoch 78752: reducing learning rate of group 0 to 4.6702e-03.\n",
      "Epoch 78801, Training Loss: 34230, Validation Loss: 54224, 70217.79566391675\n",
      "Epoch 78853: reducing learning rate of group 0 to 4.6655e-03.\n",
      "Epoch 78901, Training Loss: 32793, Validation Loss: 55096, 74290.91229650246\n",
      "Epoch 78954: reducing learning rate of group 0 to 4.6609e-03.\n",
      "Epoch 79001, Training Loss: 31935, Validation Loss: 55832, 69817.99072111314\n",
      "Epoch 79055: reducing learning rate of group 0 to 4.6562e-03.\n",
      "Epoch 79101, Training Loss: 31867, Validation Loss: 56298, 71441.43120238655\n",
      "Epoch 79156: reducing learning rate of group 0 to 4.6516e-03.\n",
      "Epoch 79201, Training Loss: 32811, Validation Loss: 54508, 84630.87745263397\n",
      "Epoch 79257: reducing learning rate of group 0 to 4.6469e-03.\n",
      "Epoch 79301, Training Loss: 32387, Validation Loss: 55525, 82974.19302866842\n",
      "Epoch 79358: reducing learning rate of group 0 to 4.6423e-03.\n",
      "Epoch 79401, Training Loss: 29068, Validation Loss: 54600, 75457.75895288611\n",
      "Epoch 79459: reducing learning rate of group 0 to 4.6376e-03.\n",
      "Epoch 79501, Training Loss: 32390, Validation Loss: 54273, 107622.74475947589\n",
      "Epoch 79560: reducing learning rate of group 0 to 4.6330e-03.\n",
      "Epoch 79601, Training Loss: 31636, Validation Loss: 56560, 96649.29167004465\n",
      "Epoch 79661: reducing learning rate of group 0 to 4.6283e-03.\n",
      "Epoch 79701, Training Loss: 34134, Validation Loss: 54572, 87072.64002345515\n",
      "Epoch 79762: reducing learning rate of group 0 to 4.6237e-03.\n",
      "Epoch 79801, Training Loss: 36477, Validation Loss: 54559, 71981.73432842789\n",
      "Epoch 79863: reducing learning rate of group 0 to 4.6191e-03.\n",
      "Epoch 79901, Training Loss: 31598, Validation Loss: 54293, 80738.86231365161\n",
      "Epoch 79964: reducing learning rate of group 0 to 4.6145e-03.\n",
      "Epoch 80001, Training Loss: 30111, Validation Loss: 54146, 68891.07085688552\n",
      "Epoch 80065: reducing learning rate of group 0 to 4.6099e-03.\n",
      "Epoch 80101, Training Loss: 35265, Validation Loss: 55213, 79530.85984685564\n",
      "Epoch 80166: reducing learning rate of group 0 to 4.6053e-03.\n",
      "Epoch 80201, Training Loss: 32381, Validation Loss: 54347, 57181.99744984982\n",
      "Epoch 80267: reducing learning rate of group 0 to 4.6006e-03.\n",
      "Epoch 80301, Training Loss: 35289, Validation Loss: 54097, 70267.26731374393\n",
      "Epoch 80368: reducing learning rate of group 0 to 4.5960e-03.\n",
      "Epoch 80401, Training Loss: 33154, Validation Loss: 54752, 70100.30891723705\n",
      "Epoch 80469: reducing learning rate of group 0 to 4.5914e-03.\n",
      "Epoch 80501, Training Loss: 29354, Validation Loss: 54913, 79971.58326838606\n",
      "Epoch 80570: reducing learning rate of group 0 to 4.5869e-03.\n",
      "Epoch 80601, Training Loss: 32058, Validation Loss: 54627, 86089.77023347122\n",
      "Epoch 80671: reducing learning rate of group 0 to 4.5823e-03.\n",
      "Epoch 80701, Training Loss: 31661, Validation Loss: 53317, 72618.01383520669\n",
      "Epoch 80772: reducing learning rate of group 0 to 4.5777e-03.\n",
      "Epoch 80801, Training Loss: 31638, Validation Loss: 52860, 75781.79042362921\n",
      "Epoch 80873: reducing learning rate of group 0 to 4.5731e-03.\n",
      "Epoch 80901, Training Loss: 30513, Validation Loss: 53536, 73146.80795114873\n",
      "Epoch 80974: reducing learning rate of group 0 to 4.5685e-03.\n",
      "Epoch 81001, Training Loss: 33252, Validation Loss: 52260, 70915.13943278608\n",
      "Epoch 81075: reducing learning rate of group 0 to 4.5640e-03.\n",
      "Epoch 81101, Training Loss: 30930, Validation Loss: 52809, 80147.79597625755\n",
      "Epoch 81176: reducing learning rate of group 0 to 4.5594e-03.\n",
      "Epoch 81201, Training Loss: 32551, Validation Loss: 51272, 62692.517168654296\n",
      "Epoch 81277: reducing learning rate of group 0 to 4.5548e-03.\n",
      "Epoch 81301, Training Loss: 31657, Validation Loss: 53173, 78462.75478948762\n",
      "Epoch 81378: reducing learning rate of group 0 to 4.5503e-03.\n",
      "Epoch 81401, Training Loss: 32610, Validation Loss: 54634, 69470.23193010206\n",
      "Epoch 81479: reducing learning rate of group 0 to 4.5457e-03.\n",
      "Epoch 81501, Training Loss: 31007, Validation Loss: 54438, 55138.51605929361\n",
      "Epoch 81580: reducing learning rate of group 0 to 4.5412e-03.\n",
      "Epoch 81601, Training Loss: 31497, Validation Loss: 57939, 87324.825803764\n",
      "Epoch 81681: reducing learning rate of group 0 to 4.5367e-03.\n",
      "Epoch 81701, Training Loss: 30774, Validation Loss: 56694, 99917.99564928345\n",
      "Epoch 81782: reducing learning rate of group 0 to 4.5321e-03.\n",
      "Epoch 81801, Training Loss: 35841, Validation Loss: 52524, 86559.39422862117\n",
      "Epoch 81883: reducing learning rate of group 0 to 4.5276e-03.\n",
      "Epoch 81901, Training Loss: 32883, Validation Loss: 53098, 68929.39463474115\n",
      "Epoch 81984: reducing learning rate of group 0 to 4.5231e-03.\n",
      "Epoch 82001, Training Loss: 29766, Validation Loss: 53787, 64132.54702541063\n",
      "Epoch 82085: reducing learning rate of group 0 to 4.5185e-03.\n",
      "Epoch 82101, Training Loss: 30933, Validation Loss: 52503, 74814.04684414918\n",
      "Epoch 82186: reducing learning rate of group 0 to 4.5140e-03.\n",
      "Epoch 82201, Training Loss: 32773, Validation Loss: 52755, 68996.69570357364\n",
      "Epoch 82287: reducing learning rate of group 0 to 4.5095e-03.\n",
      "Epoch 82301, Training Loss: 30078, Validation Loss: 52251, 75711.6052912625\n",
      "Epoch 82388: reducing learning rate of group 0 to 4.5050e-03.\n",
      "Epoch 82401, Training Loss: 30247, Validation Loss: 53593, 67633.24409626634\n",
      "Epoch 82489: reducing learning rate of group 0 to 4.5005e-03.\n",
      "Epoch 82501, Training Loss: 33596, Validation Loss: 53932, 71680.937247528\n",
      "Epoch 82590: reducing learning rate of group 0 to 4.4960e-03.\n",
      "Epoch 82601, Training Loss: 31321, Validation Loss: 54162, 72360.82543489111\n",
      "Epoch 82691: reducing learning rate of group 0 to 4.4915e-03.\n",
      "Epoch 82701, Training Loss: 33006, Validation Loss: 51844, 72402.46705130176\n",
      "Epoch 82792: reducing learning rate of group 0 to 4.4870e-03.\n",
      "Epoch 82801, Training Loss: 30091, Validation Loss: 56941, 67557.58887054125\n",
      "Epoch 82893: reducing learning rate of group 0 to 4.4825e-03.\n",
      "Epoch 82901, Training Loss: 31188, Validation Loss: 55014, 79317.31361860842\n",
      "Epoch 82994: reducing learning rate of group 0 to 4.4780e-03.\n",
      "Epoch 83001, Training Loss: 35740, Validation Loss: 56706, 68907.94143280831\n",
      "Epoch 83095: reducing learning rate of group 0 to 4.4736e-03.\n",
      "Epoch 83101, Training Loss: 30882, Validation Loss: 56029, 74640.78822512568\n",
      "Epoch 83196: reducing learning rate of group 0 to 4.4691e-03.\n",
      "Epoch 83201, Training Loss: 31988, Validation Loss: 55133, 78543.08318975774\n",
      "Epoch 83297: reducing learning rate of group 0 to 4.4646e-03.\n",
      "Epoch 83301, Training Loss: 32282, Validation Loss: 52546, 79361.60201867229\n",
      "Epoch 83398: reducing learning rate of group 0 to 4.4601e-03.\n",
      "Epoch 83401, Training Loss: 33618, Validation Loss: 54190, 108984.61714065522\n",
      "Epoch 83499: reducing learning rate of group 0 to 4.4557e-03.\n",
      "Epoch 83501, Training Loss: 31393, Validation Loss: 51976, 72402.1184909494\n",
      "Epoch 83600: reducing learning rate of group 0 to 4.4512e-03.\n",
      "Epoch 83601, Training Loss: 32609, Validation Loss: 56167, 81256.42244338815\n",
      "Epoch 83701: reducing learning rate of group 0 to 4.4468e-03.\n",
      "Epoch 83701, Training Loss: 29251, Validation Loss: 54287, 76151.61997521362\n",
      "Epoch 83801, Training Loss: 34048, Validation Loss: 54709, 69954.44585514131\n",
      "Epoch 83802: reducing learning rate of group 0 to 4.4423e-03.\n",
      "Epoch 83901, Training Loss: 30844, Validation Loss: 58466, 78750.06039233926\n",
      "Epoch 83903: reducing learning rate of group 0 to 4.4379e-03.\n",
      "Epoch 84001, Training Loss: 31204, Validation Loss: 53815, 78026.21218793101\n",
      "Epoch 84004: reducing learning rate of group 0 to 4.4335e-03.\n",
      "Epoch 84101, Training Loss: 30859, Validation Loss: 54552, 75127.64193354984\n",
      "Epoch 84105: reducing learning rate of group 0 to 4.4290e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84201, Training Loss: 31646, Validation Loss: 55290, 79688.77330217404\n",
      "Epoch 84206: reducing learning rate of group 0 to 4.4246e-03.\n",
      "Epoch 84301, Training Loss: 36511, Validation Loss: 53233, 83123.62976062471\n",
      "Epoch 84307: reducing learning rate of group 0 to 4.4202e-03.\n",
      "Epoch 84401, Training Loss: 30621, Validation Loss: 57943, 77086.5754634336\n",
      "Epoch 84408: reducing learning rate of group 0 to 4.4157e-03.\n",
      "Epoch 84501, Training Loss: 35115, Validation Loss: 54136, 71193.72023889176\n",
      "Epoch 84509: reducing learning rate of group 0 to 4.4113e-03.\n",
      "Epoch 84601, Training Loss: 33510, Validation Loss: 54412, 70849.09368067935\n",
      "Epoch 84610: reducing learning rate of group 0 to 4.4069e-03.\n",
      "Epoch 84701, Training Loss: 29225, Validation Loss: 54518, 76862.31476852445\n",
      "Epoch 84711: reducing learning rate of group 0 to 4.4025e-03.\n",
      "Epoch 84801, Training Loss: 32178, Validation Loss: 52645, 66509.75339753012\n",
      "Epoch 84812: reducing learning rate of group 0 to 4.3981e-03.\n",
      "Epoch 84901, Training Loss: 32918, Validation Loss: 55562, 86003.99643023912\n",
      "Epoch 84913: reducing learning rate of group 0 to 4.3937e-03.\n",
      "Epoch 85001, Training Loss: 30843, Validation Loss: 53658, 60603.02161468352\n",
      "Epoch 85014: reducing learning rate of group 0 to 4.3893e-03.\n",
      "Epoch 85101, Training Loss: 32493, Validation Loss: 53269, 82295.54954100894\n",
      "Epoch 85115: reducing learning rate of group 0 to 4.3849e-03.\n",
      "Epoch 85201, Training Loss: 29697, Validation Loss: 53295, 68769.5899234489\n",
      "Epoch 85216: reducing learning rate of group 0 to 4.3805e-03.\n",
      "Epoch 85301, Training Loss: 30604, Validation Loss: 54536, 76057.63023115107\n",
      "Epoch 85317: reducing learning rate of group 0 to 4.3762e-03.\n",
      "Epoch 85401, Training Loss: 30679, Validation Loss: 54182, 80174.04286596466\n",
      "Epoch 85418: reducing learning rate of group 0 to 4.3718e-03.\n",
      "Epoch 85501, Training Loss: 33140, Validation Loss: 55842, 77660.91920182265\n",
      "Epoch 85519: reducing learning rate of group 0 to 4.3674e-03.\n",
      "Epoch 85601, Training Loss: 33175, Validation Loss: 57218, 74047.59247006626\n",
      "Epoch 85620: reducing learning rate of group 0 to 4.3630e-03.\n",
      "Epoch 85701, Training Loss: 36687, Validation Loss: 53094, 82372.1053497731\n",
      "Epoch 85721: reducing learning rate of group 0 to 4.3587e-03.\n",
      "Epoch 85801, Training Loss: 30967, Validation Loss: 52588, 70895.3328301573\n",
      "Epoch 85822: reducing learning rate of group 0 to 4.3543e-03.\n",
      "Epoch 85901, Training Loss: 32270, Validation Loss: 52991, 77732.2918608389\n",
      "Epoch 85923: reducing learning rate of group 0 to 4.3500e-03.\n",
      "Epoch 86001, Training Loss: 33107, Validation Loss: 53303, 68732.24308229063\n",
      "Epoch 86024: reducing learning rate of group 0 to 4.3456e-03.\n",
      "Epoch 86101, Training Loss: 33155, Validation Loss: 54621, 78007.98146932323\n",
      "Epoch 86125: reducing learning rate of group 0 to 4.3413e-03.\n",
      "Epoch 86201, Training Loss: 31160, Validation Loss: 52256, 63915.05606190619\n",
      "Epoch 86226: reducing learning rate of group 0 to 4.3369e-03.\n",
      "Epoch 86301, Training Loss: 32160, Validation Loss: 56171, 64841.346843451174\n",
      "Epoch 86327: reducing learning rate of group 0 to 4.3326e-03.\n",
      "Epoch 86401, Training Loss: 32340, Validation Loss: 51650, 68306.61603769183\n",
      "Epoch 86428: reducing learning rate of group 0 to 4.3283e-03.\n",
      "Epoch 86501, Training Loss: 30727, Validation Loss: 54994, 60892.27827040036\n",
      "Epoch 86529: reducing learning rate of group 0 to 4.3239e-03.\n",
      "Epoch 86601, Training Loss: 31511, Validation Loss: 56750, 63445.23112688257\n",
      "Epoch 86630: reducing learning rate of group 0 to 4.3196e-03.\n",
      "Epoch 86701, Training Loss: 29944, Validation Loss: 53884, 87500.19863041044\n",
      "Epoch 86731: reducing learning rate of group 0 to 4.3153e-03.\n",
      "Epoch 86801, Training Loss: 31165, Validation Loss: 54858, 80495.59402608026\n",
      "Epoch 86832: reducing learning rate of group 0 to 4.3110e-03.\n",
      "Epoch 86901, Training Loss: 33103, Validation Loss: 55485, 65710.49955175817\n",
      "Epoch 86933: reducing learning rate of group 0 to 4.3067e-03.\n",
      "Epoch 87001, Training Loss: 29367, Validation Loss: 53208, 75311.6921589379\n",
      "Epoch 87034: reducing learning rate of group 0 to 4.3024e-03.\n",
      "Epoch 87101, Training Loss: 30436, Validation Loss: 54390, 71355.5289043277\n",
      "Epoch 87135: reducing learning rate of group 0 to 4.2981e-03.\n",
      "Epoch 87201, Training Loss: 33252, Validation Loss: 55048, 76840.34480588044\n",
      "Epoch 87236: reducing learning rate of group 0 to 4.2938e-03.\n",
      "Epoch 87301, Training Loss: 31685, Validation Loss: 52679, 73855.81554711377\n",
      "Epoch 87337: reducing learning rate of group 0 to 4.2895e-03.\n",
      "Epoch 87401, Training Loss: 29794, Validation Loss: 55731, 69473.82421023153\n",
      "Epoch 87438: reducing learning rate of group 0 to 4.2852e-03.\n",
      "Epoch 87501, Training Loss: 30898, Validation Loss: 52722, 83556.34141743342\n",
      "Epoch 87539: reducing learning rate of group 0 to 4.2809e-03.\n",
      "Epoch 87601, Training Loss: 32715, Validation Loss: 51037, 99148.76036620313\n",
      "Epoch 87640: reducing learning rate of group 0 to 4.2766e-03.\n",
      "Epoch 87701, Training Loss: 31236, Validation Loss: 51802, 74736.23093279538\n",
      "Epoch 87741: reducing learning rate of group 0 to 4.2723e-03.\n",
      "Epoch 87801, Training Loss: 31319, Validation Loss: 52129, 71237.17100651366\n",
      "Epoch 87842: reducing learning rate of group 0 to 4.2681e-03.\n",
      "Epoch 87901, Training Loss: 32879, Validation Loss: 52733, 77850.81114685173\n",
      "Epoch 87943: reducing learning rate of group 0 to 4.2638e-03.\n",
      "Epoch 88001, Training Loss: 34140, Validation Loss: 55151, 73668.29662022706\n",
      "Epoch 88044: reducing learning rate of group 0 to 4.2595e-03.\n",
      "Epoch 88101, Training Loss: 32732, Validation Loss: 54355, 66277.39471440906\n",
      "Epoch 88145: reducing learning rate of group 0 to 4.2553e-03.\n",
      "Epoch 88201, Training Loss: 33497, Validation Loss: 52861, 72053.60261868201\n",
      "Epoch 88246: reducing learning rate of group 0 to 4.2510e-03.\n",
      "Epoch 88301, Training Loss: 30706, Validation Loss: 55207, 66334.93767309016\n",
      "Epoch 88347: reducing learning rate of group 0 to 4.2468e-03.\n",
      "Epoch 88401, Training Loss: 29829, Validation Loss: 57186, 79615.91157202552\n",
      "Epoch 88448: reducing learning rate of group 0 to 4.2425e-03.\n",
      "Epoch 88501, Training Loss: 30971, Validation Loss: 53101, 75330.18388569281\n",
      "Epoch 88549: reducing learning rate of group 0 to 4.2383e-03.\n",
      "Epoch 88601, Training Loss: 32952, Validation Loss: 56223, 74731.35223228979\n",
      "Epoch 88650: reducing learning rate of group 0 to 4.2340e-03.\n",
      "Epoch 88701, Training Loss: 31056, Validation Loss: 53947, 78526.96458256907\n",
      "Epoch 88751: reducing learning rate of group 0 to 4.2298e-03.\n",
      "Epoch 88801, Training Loss: 29018, Validation Loss: 55381, 74797.24989695921\n",
      "Epoch 88852: reducing learning rate of group 0 to 4.2256e-03.\n",
      "Epoch 88901, Training Loss: 32432, Validation Loss: 52880, 81675.66422637063\n",
      "Epoch 88953: reducing learning rate of group 0 to 4.2213e-03.\n",
      "Epoch 89001, Training Loss: 33109, Validation Loss: 55338, 78504.14542572387\n",
      "Epoch 89054: reducing learning rate of group 0 to 4.2171e-03.\n",
      "Epoch 89101, Training Loss: 32526, Validation Loss: 54079, 79482.89720791848\n",
      "Epoch 89155: reducing learning rate of group 0 to 4.2129e-03.\n",
      "Epoch 89201, Training Loss: 30480, Validation Loss: 55731, 84854.82562930428\n",
      "Epoch 89256: reducing learning rate of group 0 to 4.2087e-03.\n",
      "Epoch 89301, Training Loss: 31164, Validation Loss: 52823, 117055.3204056186\n",
      "Epoch 89357: reducing learning rate of group 0 to 4.2045e-03.\n",
      "Epoch 89401, Training Loss: 31172, Validation Loss: 56367, 102246.46539378115\n",
      "Epoch 89458: reducing learning rate of group 0 to 4.2003e-03.\n",
      "Epoch 89501, Training Loss: 29252, Validation Loss: 57916, 71532.26963557988\n",
      "Epoch 89559: reducing learning rate of group 0 to 4.1961e-03.\n",
      "Epoch 89601, Training Loss: 32662, Validation Loss: 57923, 84807.6146563553\n",
      "Epoch 89660: reducing learning rate of group 0 to 4.1919e-03.\n",
      "Epoch 89701, Training Loss: 32458, Validation Loss: 53910, 80756.3779014347\n",
      "Epoch 89761: reducing learning rate of group 0 to 4.1877e-03.\n",
      "Epoch 89801, Training Loss: 30229, Validation Loss: 54506, 88306.58296052193\n",
      "Epoch 89862: reducing learning rate of group 0 to 4.1835e-03.\n",
      "Epoch 89901, Training Loss: 31896, Validation Loss: 54553, 80057.3278195423\n",
      "Epoch 89963: reducing learning rate of group 0 to 4.1793e-03.\n",
      "Epoch 90001, Training Loss: 31747, Validation Loss: 54477, 79366.8068766581\n",
      "Epoch 90064: reducing learning rate of group 0 to 4.1751e-03.\n",
      "Epoch 90101, Training Loss: 32831, Validation Loss: 53243, 69714.556445394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90165: reducing learning rate of group 0 to 4.1710e-03.\n",
      "Epoch 90201, Training Loss: 29192, Validation Loss: 55548, 72225.35283814033\n",
      "Epoch 90266: reducing learning rate of group 0 to 4.1668e-03.\n",
      "Epoch 90301, Training Loss: 32469, Validation Loss: 58224, 87257.18426713771\n",
      "Epoch 90367: reducing learning rate of group 0 to 4.1626e-03.\n",
      "Epoch 90401, Training Loss: 31483, Validation Loss: 57924, 76619.16752972633\n",
      "Epoch 90468: reducing learning rate of group 0 to 4.1585e-03.\n",
      "Epoch 90501, Training Loss: 30011, Validation Loss: 53242, 77899.07648855665\n",
      "Epoch 90569: reducing learning rate of group 0 to 4.1543e-03.\n",
      "Epoch 90601, Training Loss: 31080, Validation Loss: 54115, 67472.00305439625\n",
      "Epoch 90670: reducing learning rate of group 0 to 4.1502e-03.\n",
      "Epoch 90701, Training Loss: 29803, Validation Loss: 56609, 78983.90092481201\n",
      "Epoch 90771: reducing learning rate of group 0 to 4.1460e-03.\n",
      "Epoch 90801, Training Loss: 29354, Validation Loss: 57518, 70849.97986905226\n",
      "Epoch 90872: reducing learning rate of group 0 to 4.1419e-03.\n",
      "Epoch 90901, Training Loss: 32743, Validation Loss: 54135, 66506.36886955817\n",
      "Epoch 90973: reducing learning rate of group 0 to 4.1377e-03.\n",
      "Epoch 91001, Training Loss: 29423, Validation Loss: 53908, 91827.67459517573\n",
      "Epoch 91074: reducing learning rate of group 0 to 4.1336e-03.\n",
      "Epoch 91101, Training Loss: 34272, Validation Loss: 55028, 71918.89512536282\n",
      "Epoch 91175: reducing learning rate of group 0 to 4.1294e-03.\n",
      "Epoch 91201, Training Loss: 33206, Validation Loss: 53330, 72538.78071589723\n",
      "Epoch 91276: reducing learning rate of group 0 to 4.1253e-03.\n",
      "Epoch 91301, Training Loss: 29642, Validation Loss: 53500, 64160.523013326514\n",
      "Epoch 91377: reducing learning rate of group 0 to 4.1212e-03.\n",
      "Epoch 91401, Training Loss: 31923, Validation Loss: 54366, 83986.91029971017\n",
      "Epoch 91478: reducing learning rate of group 0 to 4.1171e-03.\n",
      "Epoch 91501, Training Loss: 32254, Validation Loss: 54279, 76774.52880790223\n",
      "Epoch 91579: reducing learning rate of group 0 to 4.1130e-03.\n",
      "Epoch 91601, Training Loss: 30337, Validation Loss: 53755, 79835.52447410765\n",
      "Epoch 91680: reducing learning rate of group 0 to 4.1088e-03.\n",
      "Epoch 91701, Training Loss: 31510, Validation Loss: 52314, 69638.56628902398\n",
      "Epoch 91781: reducing learning rate of group 0 to 4.1047e-03.\n",
      "Epoch 91801, Training Loss: 31218, Validation Loss: 56105, 88876.40773957287\n",
      "Epoch 91882: reducing learning rate of group 0 to 4.1006e-03.\n",
      "Epoch 91901, Training Loss: 31471, Validation Loss: 52823, 68102.35982734307\n",
      "Epoch 91983: reducing learning rate of group 0 to 4.0965e-03.\n",
      "Epoch 92001, Training Loss: 31927, Validation Loss: 52575, 123493.62198607204\n",
      "Epoch 92084: reducing learning rate of group 0 to 4.0924e-03.\n",
      "Epoch 92101, Training Loss: 32770, Validation Loss: 56385, 69123.6824446227\n",
      "Epoch 92185: reducing learning rate of group 0 to 4.0883e-03.\n",
      "Epoch 92201, Training Loss: 34702, Validation Loss: 53924, 104963.3100538274\n",
      "Epoch 92286: reducing learning rate of group 0 to 4.0842e-03.\n",
      "Epoch 92301, Training Loss: 32614, Validation Loss: 55825, 73014.959978782\n",
      "Epoch 92387: reducing learning rate of group 0 to 4.0802e-03.\n",
      "Epoch 92401, Training Loss: 29688, Validation Loss: 56357, 90142.95230145637\n",
      "Epoch 92488: reducing learning rate of group 0 to 4.0761e-03.\n",
      "Epoch 92501, Training Loss: 32427, Validation Loss: 53650, 79679.18283957733\n",
      "Epoch 92589: reducing learning rate of group 0 to 4.0720e-03.\n",
      "Epoch 92601, Training Loss: 32908, Validation Loss: 56160, 82501.90271230445\n",
      "Epoch 92690: reducing learning rate of group 0 to 4.0679e-03.\n",
      "Epoch 92701, Training Loss: 30745, Validation Loss: 55720, 78304.08096918085\n",
      "Epoch 92791: reducing learning rate of group 0 to 4.0639e-03.\n",
      "Epoch 92801, Training Loss: 30791, Validation Loss: 56897, 74300.00708085591\n",
      "Epoch 92892: reducing learning rate of group 0 to 4.0598e-03.\n",
      "Epoch 92901, Training Loss: 30212, Validation Loss: 54328, 64963.0048161213\n",
      "Epoch 92993: reducing learning rate of group 0 to 4.0557e-03.\n",
      "Epoch 93001, Training Loss: 31264, Validation Loss: 53931, 88115.68515657626\n",
      "Epoch 93094: reducing learning rate of group 0 to 4.0517e-03.\n",
      "Epoch 93101, Training Loss: 31951, Validation Loss: 52100, 96642.1217053025\n",
      "Epoch 93195: reducing learning rate of group 0 to 4.0476e-03.\n",
      "Epoch 93201, Training Loss: 30343, Validation Loss: 54072, 102464.32279867318\n",
      "Epoch 93296: reducing learning rate of group 0 to 4.0436e-03.\n",
      "Epoch 93301, Training Loss: 30834, Validation Loss: 50974, 166581.1101158748\n",
      "Epoch 93397: reducing learning rate of group 0 to 4.0395e-03.\n",
      "Epoch 93401, Training Loss: 32029, Validation Loss: 57315, 78180.8084228786\n",
      "Epoch 93498: reducing learning rate of group 0 to 4.0355e-03.\n",
      "Epoch 93501, Training Loss: 29782, Validation Loss: 53840, 87797.53707629064\n",
      "Epoch 93599: reducing learning rate of group 0 to 4.0315e-03.\n",
      "Epoch 93601, Training Loss: 30894, Validation Loss: 55372, 77041.3781462814\n",
      "Epoch 93700: reducing learning rate of group 0 to 4.0274e-03.\n",
      "Epoch 93701, Training Loss: 31318, Validation Loss: 55839, 75740.62439014188\n",
      "Epoch 93801: reducing learning rate of group 0 to 4.0234e-03.\n",
      "Epoch 93801, Training Loss: 32024, Validation Loss: 56398, 69786.32468493271\n",
      "Epoch 93901, Training Loss: 30780, Validation Loss: 54959, 79435.76886184512\n",
      "Epoch 93902: reducing learning rate of group 0 to 4.0194e-03.\n",
      "Epoch 94001, Training Loss: 29538, Validation Loss: 54945, 88272.43014202926\n",
      "Epoch 94003: reducing learning rate of group 0 to 4.0154e-03.\n",
      "Epoch 94101, Training Loss: 31162, Validation Loss: 54278, 95795.98848549735\n",
      "Epoch 94104: reducing learning rate of group 0 to 4.0114e-03.\n",
      "Epoch 94201, Training Loss: 31530, Validation Loss: 54962, 74376.94094808941\n",
      "Epoch 94205: reducing learning rate of group 0 to 4.0073e-03.\n",
      "Epoch 94301, Training Loss: 32511, Validation Loss: 56557, 73182.66286562414\n",
      "Epoch 94306: reducing learning rate of group 0 to 4.0033e-03.\n",
      "Epoch 94401, Training Loss: 31239, Validation Loss: 52863, 79220.76566979167\n",
      "Epoch 94407: reducing learning rate of group 0 to 3.9993e-03.\n",
      "Epoch 94501, Training Loss: 31386, Validation Loss: 57446, 68446.63243946829\n",
      "Epoch 94508: reducing learning rate of group 0 to 3.9953e-03.\n",
      "Epoch 94601, Training Loss: 28729, Validation Loss: 55453, 92616.67562903736\n",
      "Epoch 94609: reducing learning rate of group 0 to 3.9913e-03.\n",
      "Epoch 94701, Training Loss: 29553, Validation Loss: 55652, 82849.995242625\n",
      "Epoch 94710: reducing learning rate of group 0 to 3.9873e-03.\n",
      "Epoch 94801, Training Loss: 29791, Validation Loss: 54241, 68929.42182722532\n",
      "Epoch 94811: reducing learning rate of group 0 to 3.9834e-03.\n",
      "Epoch 94901, Training Loss: 29522, Validation Loss: 53582, 83712.16278052388\n",
      "Epoch 94912: reducing learning rate of group 0 to 3.9794e-03.\n",
      "Epoch 95001, Training Loss: 33633, Validation Loss: 55757, 99778.17897688842\n",
      "Epoch 95013: reducing learning rate of group 0 to 3.9754e-03.\n",
      "Epoch 95101, Training Loss: 31773, Validation Loss: 52728, 78597.322619497\n",
      "Epoch 95114: reducing learning rate of group 0 to 3.9714e-03.\n",
      "Epoch 95201, Training Loss: 30657, Validation Loss: 59786, 73563.44924368335\n",
      "Epoch 95215: reducing learning rate of group 0 to 3.9674e-03.\n",
      "Epoch 95301, Training Loss: 32352, Validation Loss: 57839, 69230.69388575359\n",
      "Epoch 95316: reducing learning rate of group 0 to 3.9635e-03.\n",
      "Epoch 95401, Training Loss: 30207, Validation Loss: 52870, 70519.58300370966\n",
      "Epoch 95417: reducing learning rate of group 0 to 3.9595e-03.\n",
      "Epoch 95501, Training Loss: 31197, Validation Loss: 53166, 75930.27012158492\n",
      "Epoch 95518: reducing learning rate of group 0 to 3.9556e-03.\n",
      "Epoch 95601, Training Loss: 33272, Validation Loss: 57071, 78939.64871489939\n",
      "Epoch 95619: reducing learning rate of group 0 to 3.9516e-03.\n",
      "Epoch 95701, Training Loss: 31283, Validation Loss: 55293, 82930.58944323477\n",
      "Epoch 95720: reducing learning rate of group 0 to 3.9476e-03.\n",
      "Epoch 95801, Training Loss: 32290, Validation Loss: 55586, 82623.7889044283\n",
      "Epoch 95821: reducing learning rate of group 0 to 3.9437e-03.\n",
      "Epoch 95901, Training Loss: 31684, Validation Loss: 56897, 84039.81109707283\n",
      "Epoch 95922: reducing learning rate of group 0 to 3.9398e-03.\n",
      "Epoch 96001, Training Loss: 29698, Validation Loss: 57286, 81080.86846859635\n",
      "Epoch 96023: reducing learning rate of group 0 to 3.9358e-03.\n",
      "Epoch 96101, Training Loss: 29524, Validation Loss: 54960, 75083.70081655156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96124: reducing learning rate of group 0 to 3.9319e-03.\n",
      "Epoch 96201, Training Loss: 30962, Validation Loss: 53141, 78188.36754663913\n",
      "Epoch 96225: reducing learning rate of group 0 to 3.9280e-03.\n",
      "Epoch 96301, Training Loss: 30051, Validation Loss: 53238, 81005.94994572276\n",
      "Epoch 96326: reducing learning rate of group 0 to 3.9240e-03.\n",
      "Epoch 96401, Training Loss: 29603, Validation Loss: 53258, 73715.10637944234\n",
      "Epoch 96427: reducing learning rate of group 0 to 3.9201e-03.\n",
      "Epoch 96501, Training Loss: 32166, Validation Loss: 53593, 87249.30125825519\n",
      "Epoch 96528: reducing learning rate of group 0 to 3.9162e-03.\n",
      "Epoch 96601, Training Loss: 31997, Validation Loss: 55755, 93779.48656487298\n",
      "Epoch 96629: reducing learning rate of group 0 to 3.9123e-03.\n",
      "Epoch 96701, Training Loss: 30782, Validation Loss: 56520, 74535.55672502374\n",
      "Epoch 96730: reducing learning rate of group 0 to 3.9083e-03.\n",
      "Epoch 96801, Training Loss: 30928, Validation Loss: 51610, 83503.7377977036\n",
      "Epoch 96831: reducing learning rate of group 0 to 3.9044e-03.\n",
      "Epoch 96901, Training Loss: 30955, Validation Loss: 52503, 82010.98028248355\n",
      "Epoch 96932: reducing learning rate of group 0 to 3.9005e-03.\n",
      "Epoch 97001, Training Loss: 30367, Validation Loss: 53921, 77333.84180638522\n",
      "Epoch 97033: reducing learning rate of group 0 to 3.8966e-03.\n",
      "Epoch 97101, Training Loss: 33896, Validation Loss: 54325, 74255.65899853838\n",
      "Epoch 97134: reducing learning rate of group 0 to 3.8927e-03.\n",
      "Epoch 97201, Training Loss: 30205, Validation Loss: 53214, 100103.38933886646\n",
      "Epoch 97235: reducing learning rate of group 0 to 3.8888e-03.\n",
      "Epoch 97301, Training Loss: 32270, Validation Loss: 53559, 66509.99871742025\n",
      "Epoch 97336: reducing learning rate of group 0 to 3.8850e-03.\n",
      "Epoch 97401, Training Loss: 34652, Validation Loss: 57478, 88433.20899097231\n",
      "Epoch 97437: reducing learning rate of group 0 to 3.8811e-03.\n",
      "Epoch 97501, Training Loss: 31495, Validation Loss: 54305, 100765.66876935831\n",
      "Epoch 97538: reducing learning rate of group 0 to 3.8772e-03.\n",
      "Epoch 97601, Training Loss: 30751, Validation Loss: 61185, 64640.38358910151\n",
      "Epoch 97639: reducing learning rate of group 0 to 3.8733e-03.\n",
      "Epoch 97701, Training Loss: 35630, Validation Loss: 51978, 81652.76694812135\n",
      "Epoch 97740: reducing learning rate of group 0 to 3.8694e-03.\n",
      "Epoch 97801, Training Loss: 35636, Validation Loss: 55799, 76607.85989795164\n",
      "Epoch 97841: reducing learning rate of group 0 to 3.8656e-03.\n",
      "Epoch 97901, Training Loss: 33122, Validation Loss: 56471, 77843.51768779905\n",
      "Epoch 97942: reducing learning rate of group 0 to 3.8617e-03.\n",
      "Epoch 98001, Training Loss: 33493, Validation Loss: 55749, 72570.17332524086\n",
      "Epoch 98043: reducing learning rate of group 0 to 3.8578e-03.\n",
      "Epoch 98101, Training Loss: 29794, Validation Loss: 56408, 74666.33353380089\n",
      "Epoch 98144: reducing learning rate of group 0 to 3.8540e-03.\n",
      "Epoch 98201, Training Loss: 29644, Validation Loss: 55578, 72417.87165225863\n",
      "Epoch 98245: reducing learning rate of group 0 to 3.8501e-03.\n",
      "Epoch 98301, Training Loss: 31590, Validation Loss: 56502, 81120.15227118066\n",
      "Epoch 98346: reducing learning rate of group 0 to 3.8463e-03.\n",
      "Epoch 98401, Training Loss: 30987, Validation Loss: 54791, 80998.99535239355\n",
      "Epoch 98447: reducing learning rate of group 0 to 3.8424e-03.\n",
      "Epoch 98501, Training Loss: 30987, Validation Loss: 56731, 65729.33559775037\n",
      "Epoch 98548: reducing learning rate of group 0 to 3.8386e-03.\n",
      "Epoch 98601, Training Loss: 31601, Validation Loss: 53969, 76088.78950416997\n",
      "Epoch 98649: reducing learning rate of group 0 to 3.8348e-03.\n",
      "Epoch 98701, Training Loss: 30122, Validation Loss: 56899, 76843.07160299778\n",
      "Epoch 98750: reducing learning rate of group 0 to 3.8309e-03.\n",
      "Epoch 98801, Training Loss: 31366, Validation Loss: 53622, 73217.67076755707\n",
      "Epoch 98851: reducing learning rate of group 0 to 3.8271e-03.\n",
      "Epoch 98901, Training Loss: 30978, Validation Loss: 57245, 74725.92840570219\n",
      "Epoch 98952: reducing learning rate of group 0 to 3.8233e-03.\n",
      "Epoch 99001, Training Loss: 31300, Validation Loss: 55657, 72985.94223856954\n",
      "Epoch 99053: reducing learning rate of group 0 to 3.8194e-03.\n",
      "Epoch 99101, Training Loss: 29256, Validation Loss: 55567, 80296.73827611243\n",
      "Epoch 99154: reducing learning rate of group 0 to 3.8156e-03.\n",
      "Epoch 99201, Training Loss: 29585, Validation Loss: 53349, 69585.88061963348\n",
      "Epoch 99255: reducing learning rate of group 0 to 3.8118e-03.\n",
      "Epoch 99301, Training Loss: 31856, Validation Loss: 52355, 86710.12385710521\n",
      "Epoch 99356: reducing learning rate of group 0 to 3.8080e-03.\n",
      "Epoch 99401, Training Loss: 31227, Validation Loss: 54420, 82257.55372579035\n",
      "Epoch 99457: reducing learning rate of group 0 to 3.8042e-03.\n",
      "Epoch 99501, Training Loss: 29942, Validation Loss: 55943, 78721.58359404556\n",
      "Epoch 99558: reducing learning rate of group 0 to 3.8004e-03.\n",
      "Epoch 99601, Training Loss: 29969, Validation Loss: 54973, 85851.5521869501\n",
      "Epoch 99659: reducing learning rate of group 0 to 3.7966e-03.\n",
      "Epoch 99701, Training Loss: 31185, Validation Loss: 52850, 78943.21082814118\n",
      "Epoch 99760: reducing learning rate of group 0 to 3.7928e-03.\n",
      "Epoch 99801, Training Loss: 30208, Validation Loss: 58836, 68196.04843314888\n",
      "Epoch 99861: reducing learning rate of group 0 to 3.7890e-03.\n",
      "Epoch 99901, Training Loss: 30108, Validation Loss: 53893, 86065.52853444884\n",
      "Epoch 99962: reducing learning rate of group 0 to 3.7852e-03.\n",
      "Epoch 100001, Training Loss: 31803, Validation Loss: 53205, 68165.15276164298\n",
      "Epoch 100063: reducing learning rate of group 0 to 3.7814e-03.\n",
      "Epoch 100101, Training Loss: 29948, Validation Loss: 53552, 78754.1182269643\n",
      "Epoch 100164: reducing learning rate of group 0 to 3.7776e-03.\n",
      "Epoch 100201, Training Loss: 30261, Validation Loss: 55167, 82119.0695167654\n",
      "Epoch 100265: reducing learning rate of group 0 to 3.7739e-03.\n",
      "Epoch 100301, Training Loss: 29547, Validation Loss: 53171, 85940.98144061767\n",
      "Epoch 100366: reducing learning rate of group 0 to 3.7701e-03.\n",
      "Epoch 100401, Training Loss: 31727, Validation Loss: 53532, 74054.1643189729\n",
      "Epoch 100467: reducing learning rate of group 0 to 3.7663e-03.\n",
      "Epoch 100501, Training Loss: 30520, Validation Loss: 53133, 79399.83677079123\n",
      "Epoch 100568: reducing learning rate of group 0 to 3.7625e-03.\n",
      "Epoch 100601, Training Loss: 30124, Validation Loss: 53499, 99960.16612492815\n",
      "Epoch 100669: reducing learning rate of group 0 to 3.7588e-03.\n",
      "Epoch 100701, Training Loss: 29787, Validation Loss: 56152, 79090.38271486064\n",
      "Epoch 100770: reducing learning rate of group 0 to 3.7550e-03.\n",
      "Epoch 100801, Training Loss: 29254, Validation Loss: 53827, 74453.4632698844\n",
      "Epoch 100871: reducing learning rate of group 0 to 3.7513e-03.\n",
      "Epoch 100901, Training Loss: 31423, Validation Loss: 54709, 76263.11152717688\n",
      "Epoch 100972: reducing learning rate of group 0 to 3.7475e-03.\n",
      "Epoch 101001, Training Loss: 32636, Validation Loss: 52636, 71886.43447924271\n",
      "Epoch 101073: reducing learning rate of group 0 to 3.7438e-03.\n",
      "Epoch 101101, Training Loss: 30088, Validation Loss: 53041, 88081.3042221488\n",
      "Epoch 101174: reducing learning rate of group 0 to 3.7400e-03.\n",
      "Epoch 101201, Training Loss: 28626, Validation Loss: 54116, 65593.69229170932\n",
      "Epoch 101275: reducing learning rate of group 0 to 3.7363e-03.\n",
      "Epoch 101301, Training Loss: 31654, Validation Loss: 53370, 81076.1964779402\n",
      "Epoch 101376: reducing learning rate of group 0 to 3.7326e-03.\n",
      "Epoch 101401, Training Loss: 30455, Validation Loss: 55489, 83817.96702755966\n",
      "Epoch 101477: reducing learning rate of group 0 to 3.7288e-03.\n",
      "Epoch 101501, Training Loss: 28773, Validation Loss: 55093, 70949.29596779222\n",
      "Epoch 101578: reducing learning rate of group 0 to 3.7251e-03.\n",
      "Epoch 101601, Training Loss: 32970, Validation Loss: 54832, 69064.25915653679\n",
      "Epoch 101679: reducing learning rate of group 0 to 3.7214e-03.\n",
      "Epoch 101701, Training Loss: 30187, Validation Loss: 56381, 63081.039264418265\n",
      "Epoch 101780: reducing learning rate of group 0 to 3.7176e-03.\n",
      "Epoch 101801, Training Loss: 31706, Validation Loss: 53034, 77669.76888169277\n",
      "Epoch 101881: reducing learning rate of group 0 to 3.7139e-03.\n",
      "Epoch 101901, Training Loss: 29968, Validation Loss: 53810, 69022.20668502616\n",
      "Epoch 101982: reducing learning rate of group 0 to 3.7102e-03.\n",
      "Epoch 102001, Training Loss: 28942, Validation Loss: 53490, 75812.1377533568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102083: reducing learning rate of group 0 to 3.7065e-03.\n",
      "Epoch 102101, Training Loss: 29554, Validation Loss: 55734, 77777.84880839473\n",
      "Epoch 102184: reducing learning rate of group 0 to 3.7028e-03.\n",
      "Epoch 102201, Training Loss: 30164, Validation Loss: 53193, 69685.55659145978\n",
      "Epoch 102285: reducing learning rate of group 0 to 3.6991e-03.\n",
      "Epoch 102301, Training Loss: 31618, Validation Loss: 53789, 99015.89473739993\n",
      "Epoch 102386: reducing learning rate of group 0 to 3.6954e-03.\n",
      "Epoch 102401, Training Loss: 28598, Validation Loss: 53466, 76457.02216260976\n",
      "Epoch 102487: reducing learning rate of group 0 to 3.6917e-03.\n",
      "Epoch 102501, Training Loss: 27836, Validation Loss: 53624, 74765.94972702696\n",
      "Epoch 102588: reducing learning rate of group 0 to 3.6880e-03.\n",
      "Epoch 102601, Training Loss: 31593, Validation Loss: 54437, 85068.5733787671\n",
      "Epoch 102689: reducing learning rate of group 0 to 3.6843e-03.\n",
      "Epoch 102701, Training Loss: 34165, Validation Loss: 58057, 101110.66075281102\n",
      "Epoch 102790: reducing learning rate of group 0 to 3.6806e-03.\n",
      "Epoch 102801, Training Loss: 29855, Validation Loss: 54007, 84958.53866973601\n",
      "Epoch 102891: reducing learning rate of group 0 to 3.6770e-03.\n",
      "Epoch 102901, Training Loss: 30820, Validation Loss: 55620, 84837.64914670336\n",
      "Epoch 102992: reducing learning rate of group 0 to 3.6733e-03.\n",
      "Epoch 103001, Training Loss: 30607, Validation Loss: 55699, 71389.11885609092\n",
      "Epoch 103093: reducing learning rate of group 0 to 3.6696e-03.\n",
      "Epoch 103101, Training Loss: 29258, Validation Loss: 57419, 93410.38612579787\n",
      "Epoch 103194: reducing learning rate of group 0 to 3.6659e-03.\n",
      "Epoch 103201, Training Loss: 30119, Validation Loss: 57711, 73025.48951931484\n",
      "Epoch 103295: reducing learning rate of group 0 to 3.6623e-03.\n",
      "Epoch 103301, Training Loss: 31880, Validation Loss: 53432, 91128.28254069168\n",
      "Epoch 103396: reducing learning rate of group 0 to 3.6586e-03.\n",
      "Epoch 103401, Training Loss: 32986, Validation Loss: 55349, 77272.64250238916\n",
      "Epoch 103497: reducing learning rate of group 0 to 3.6549e-03.\n",
      "Epoch 103501, Training Loss: 33745, Validation Loss: 53163, 76192.49053227917\n",
      "Epoch 103598: reducing learning rate of group 0 to 3.6513e-03.\n",
      "Epoch 103601, Training Loss: 30376, Validation Loss: 56716, 101857.07490615397\n",
      "Epoch 103699: reducing learning rate of group 0 to 3.6476e-03.\n",
      "Epoch 103701, Training Loss: 30837, Validation Loss: 54077, 79743.85085222597\n",
      "Epoch 103800: reducing learning rate of group 0 to 3.6440e-03.\n",
      "Epoch 103801, Training Loss: 30542, Validation Loss: 52210, 72374.55577759472\n",
      "Epoch 103901: reducing learning rate of group 0 to 3.6403e-03.\n",
      "Epoch 103901, Training Loss: 30093, Validation Loss: 56456, 76070.5427325275\n",
      "Epoch 104001, Training Loss: 28708, Validation Loss: 53409, 74571.57046934207\n",
      "Epoch 104002: reducing learning rate of group 0 to 3.6367e-03.\n",
      "Epoch 104101, Training Loss: 29130, Validation Loss: 54026, 78374.75469917292\n",
      "Epoch 104103: reducing learning rate of group 0 to 3.6331e-03.\n",
      "Epoch 104201, Training Loss: 29986, Validation Loss: 53687, 76037.59335515798\n",
      "Epoch 104204: reducing learning rate of group 0 to 3.6294e-03.\n",
      "Epoch 104301, Training Loss: 30081, Validation Loss: 53674, 79892.35059217286\n",
      "Epoch 104305: reducing learning rate of group 0 to 3.6258e-03.\n",
      "Epoch 104401, Training Loss: 30736, Validation Loss: 52536, 80423.5185267057\n",
      "Epoch 104406: reducing learning rate of group 0 to 3.6222e-03.\n",
      "Epoch 104501, Training Loss: 28948, Validation Loss: 56473, 89863.30675005859\n",
      "Epoch 104507: reducing learning rate of group 0 to 3.6186e-03.\n",
      "Epoch 104601, Training Loss: 29024, Validation Loss: 53307, 79799.60086381335\n",
      "Epoch 104608: reducing learning rate of group 0 to 3.6149e-03.\n",
      "Epoch 104701, Training Loss: 29822, Validation Loss: 54697, 78300.12084013292\n",
      "Epoch 104709: reducing learning rate of group 0 to 3.6113e-03.\n",
      "Epoch 104801, Training Loss: 30136, Validation Loss: 57871, 72367.76470335269\n",
      "Epoch 104810: reducing learning rate of group 0 to 3.6077e-03.\n",
      "Epoch 104901, Training Loss: 30689, Validation Loss: 54476, 94771.68205309066\n",
      "Epoch 104911: reducing learning rate of group 0 to 3.6041e-03.\n",
      "Epoch 105001, Training Loss: 28450, Validation Loss: 53051, 87690.40022779016\n",
      "Epoch 105012: reducing learning rate of group 0 to 3.6005e-03.\n",
      "Epoch 105101, Training Loss: 29003, Validation Loss: 52815, 79306.58788626663\n",
      "Epoch 105113: reducing learning rate of group 0 to 3.5969e-03.\n",
      "Epoch 105201, Training Loss: 28527, Validation Loss: 52805, 61760.63092687431\n",
      "Epoch 105214: reducing learning rate of group 0 to 3.5933e-03.\n",
      "Epoch 105301, Training Loss: 28648, Validation Loss: 56423, 78720.67133305944\n",
      "Epoch 105315: reducing learning rate of group 0 to 3.5897e-03.\n",
      "Epoch 105401, Training Loss: 28892, Validation Loss: 51905, 74877.18119969581\n",
      "Epoch 105416: reducing learning rate of group 0 to 3.5861e-03.\n",
      "Epoch 105501, Training Loss: 31825, Validation Loss: 54903, 74581.64829228603\n",
      "Epoch 105517: reducing learning rate of group 0 to 3.5825e-03.\n",
      "Epoch 105601, Training Loss: 28277, Validation Loss: 52933, 74251.35171811657\n",
      "Epoch 105618: reducing learning rate of group 0 to 3.5790e-03.\n",
      "Epoch 105701, Training Loss: 27734, Validation Loss: 53141, 84657.77920398202\n",
      "Epoch 105719: reducing learning rate of group 0 to 3.5754e-03.\n",
      "Epoch 105801, Training Loss: 33548, Validation Loss: 53784, 74610.0937220606\n",
      "Epoch 105820: reducing learning rate of group 0 to 3.5718e-03.\n",
      "Epoch 105901, Training Loss: 30039, Validation Loss: 54718, 105363.96761536604\n",
      "Epoch 105921: reducing learning rate of group 0 to 3.5682e-03.\n",
      "Epoch 106001, Training Loss: 31379, Validation Loss: 53355, 74775.40258417247\n",
      "Epoch 106022: reducing learning rate of group 0 to 3.5647e-03.\n",
      "Epoch 106101, Training Loss: 29422, Validation Loss: 58490, 80313.55956160922\n",
      "Epoch 106123: reducing learning rate of group 0 to 3.5611e-03.\n",
      "Epoch 106201, Training Loss: 28594, Validation Loss: 52851, 112266.29182356998\n",
      "Epoch 106224: reducing learning rate of group 0 to 3.5575e-03.\n",
      "Epoch 106301, Training Loss: 32478, Validation Loss: 54020, 76312.50741554356\n",
      "Epoch 106325: reducing learning rate of group 0 to 3.5540e-03.\n",
      "Epoch 106401, Training Loss: 34540, Validation Loss: 55445, 65681.47763788329\n",
      "Epoch 106426: reducing learning rate of group 0 to 3.5504e-03.\n",
      "Epoch 106501, Training Loss: 29545, Validation Loss: 54136, 87451.7946718613\n",
      "Epoch 106527: reducing learning rate of group 0 to 3.5469e-03.\n",
      "Epoch 106601, Training Loss: 30965, Validation Loss: 55487, 72055.32071210671\n",
      "Epoch 106628: reducing learning rate of group 0 to 3.5433e-03.\n",
      "Epoch 106701, Training Loss: 28612, Validation Loss: 55083, 77286.26544322153\n",
      "Epoch 106729: reducing learning rate of group 0 to 3.5398e-03.\n",
      "Epoch 106801, Training Loss: 29805, Validation Loss: 55419, 82215.76949527576\n",
      "Epoch 106830: reducing learning rate of group 0 to 3.5362e-03.\n",
      "Epoch 106901, Training Loss: 29689, Validation Loss: 53542, 85213.3328725268\n",
      "Epoch 106931: reducing learning rate of group 0 to 3.5327e-03.\n",
      "Epoch 107001, Training Loss: 29729, Validation Loss: 54047, 77855.86832900446\n",
      "Epoch 107032: reducing learning rate of group 0 to 3.5292e-03.\n",
      "Epoch 107101, Training Loss: 31510, Validation Loss: 56330, 74119.08677698059\n",
      "Epoch 107133: reducing learning rate of group 0 to 3.5256e-03.\n",
      "Epoch 107201, Training Loss: 30760, Validation Loss: 53723, 67739.55842171221\n",
      "Epoch 107234: reducing learning rate of group 0 to 3.5221e-03.\n",
      "Epoch 107301, Training Loss: 29741, Validation Loss: 54271, 87934.15996543395\n",
      "Epoch 107335: reducing learning rate of group 0 to 3.5186e-03.\n",
      "Epoch 107401, Training Loss: 28458, Validation Loss: 53331, 74255.78191183817\n",
      "Epoch 107436: reducing learning rate of group 0 to 3.5151e-03.\n",
      "Epoch 107501, Training Loss: 31627, Validation Loss: 53799, 80843.08214671646\n",
      "Epoch 107537: reducing learning rate of group 0 to 3.5116e-03.\n",
      "Epoch 107601, Training Loss: 29517, Validation Loss: 54591, 76924.40738253175\n",
      "Epoch 107638: reducing learning rate of group 0 to 3.5081e-03.\n",
      "Epoch 107701, Training Loss: 29352, Validation Loss: 52678, 70683.63775817081\n",
      "Epoch 107739: reducing learning rate of group 0 to 3.5045e-03.\n",
      "Epoch 107801, Training Loss: 30692, Validation Loss: 53851, 92416.72191754554\n",
      "Epoch 107840: reducing learning rate of group 0 to 3.5010e-03.\n",
      "Epoch 107901, Training Loss: 32571, Validation Loss: 55103, 74127.36527772648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107941: reducing learning rate of group 0 to 3.4975e-03.\n",
      "Epoch 108001, Training Loss: 31256, Validation Loss: 52811, 65733.25268857641\n",
      "Epoch 108042: reducing learning rate of group 0 to 3.4940e-03.\n",
      "Epoch 108101, Training Loss: 30384, Validation Loss: 57071, 81814.04256072284\n",
      "Epoch 108143: reducing learning rate of group 0 to 3.4905e-03.\n",
      "Epoch 108201, Training Loss: 30977, Validation Loss: 55068, 91202.0637826867\n",
      "Epoch 108244: reducing learning rate of group 0 to 3.4871e-03.\n",
      "Epoch 108301, Training Loss: 30354, Validation Loss: 55198, 83076.99765686771\n",
      "Epoch 108345: reducing learning rate of group 0 to 3.4836e-03.\n",
      "Epoch 108401, Training Loss: 29830, Validation Loss: 54386, 96540.15596838172\n",
      "Epoch 108446: reducing learning rate of group 0 to 3.4801e-03.\n",
      "Epoch 108501, Training Loss: 29542, Validation Loss: 56422, 90990.40106756415\n",
      "Epoch 108547: reducing learning rate of group 0 to 3.4766e-03.\n",
      "Epoch 108601, Training Loss: 33272, Validation Loss: 54383, 76484.16680898365\n",
      "Epoch 108648: reducing learning rate of group 0 to 3.4731e-03.\n",
      "Epoch 108701, Training Loss: 31363, Validation Loss: 55696, 90291.95801718596\n",
      "Epoch 108749: reducing learning rate of group 0 to 3.4697e-03.\n",
      "Epoch 108801, Training Loss: 29366, Validation Loss: 53281, 87853.08997591812\n",
      "Epoch 108850: reducing learning rate of group 0 to 3.4662e-03.\n",
      "Epoch 108901, Training Loss: 28630, Validation Loss: 53190, 84397.193334883\n",
      "Epoch 108951: reducing learning rate of group 0 to 3.4627e-03.\n",
      "Epoch 109001, Training Loss: 29910, Validation Loss: 55942, 76800.10339025759\n",
      "Epoch 109052: reducing learning rate of group 0 to 3.4593e-03.\n",
      "Epoch 109101, Training Loss: 30593, Validation Loss: 56129, 82475.76358867534\n",
      "Epoch 109153: reducing learning rate of group 0 to 3.4558e-03.\n",
      "Epoch 109201, Training Loss: 29145, Validation Loss: 55462, 77012.50355267209\n",
      "Epoch 109254: reducing learning rate of group 0 to 3.4523e-03.\n",
      "Epoch 109301, Training Loss: 29538, Validation Loss: 54831, 104279.83679308987\n",
      "Epoch 109355: reducing learning rate of group 0 to 3.4489e-03.\n",
      "Epoch 109401, Training Loss: 31546, Validation Loss: 55880, 111086.11348820305\n",
      "Epoch 109456: reducing learning rate of group 0 to 3.4454e-03.\n",
      "Epoch 109501, Training Loss: 29481, Validation Loss: 53251, 70094.76103911642\n",
      "Epoch 109557: reducing learning rate of group 0 to 3.4420e-03.\n",
      "Epoch 109601, Training Loss: 31825, Validation Loss: 53130, 82599.4461958361\n",
      "Epoch 109658: reducing learning rate of group 0 to 3.4386e-03.\n",
      "Epoch 109701, Training Loss: 29661, Validation Loss: 55094, 71565.23736584875\n",
      "Epoch 109759: reducing learning rate of group 0 to 3.4351e-03.\n",
      "Epoch 109801, Training Loss: 28703, Validation Loss: 53117, 76620.34687983438\n",
      "Epoch 109860: reducing learning rate of group 0 to 3.4317e-03.\n",
      "Epoch 109901, Training Loss: 32093, Validation Loss: 56584, 78425.45956431497\n",
      "Epoch 109961: reducing learning rate of group 0 to 3.4282e-03.\n",
      "Epoch 110001, Training Loss: 30750, Validation Loss: 55023, 75607.65902091043\n",
      "Epoch 110062: reducing learning rate of group 0 to 3.4248e-03.\n",
      "Epoch 110101, Training Loss: 30030, Validation Loss: 55675, 60558.284121636556\n",
      "Epoch 110163: reducing learning rate of group 0 to 3.4214e-03.\n",
      "Epoch 110201, Training Loss: 30939, Validation Loss: 57918, 77963.90991065883\n",
      "Epoch 110264: reducing learning rate of group 0 to 3.4180e-03.\n",
      "Epoch 110301, Training Loss: 31939, Validation Loss: 55430, 96363.94548708905\n",
      "Epoch 110365: reducing learning rate of group 0 to 3.4146e-03.\n",
      "Epoch 110401, Training Loss: 28809, Validation Loss: 54008, 62744.41626698882\n",
      "Epoch 110466: reducing learning rate of group 0 to 3.4111e-03.\n",
      "Epoch 110501, Training Loss: 28606, Validation Loss: 54864, 81559.54560870225\n",
      "Epoch 110567: reducing learning rate of group 0 to 3.4077e-03.\n",
      "Epoch 110601, Training Loss: 29788, Validation Loss: 53522, 68611.49029872371\n",
      "Epoch 110668: reducing learning rate of group 0 to 3.4043e-03.\n",
      "Epoch 110701, Training Loss: 30079, Validation Loss: 53784, 73895.01177898228\n",
      "Epoch 110769: reducing learning rate of group 0 to 3.4009e-03.\n",
      "Epoch 110801, Training Loss: 32283, Validation Loss: 58091, 68273.62913336275\n",
      "Epoch 110870: reducing learning rate of group 0 to 3.3975e-03.\n",
      "Epoch 110901, Training Loss: 29349, Validation Loss: 53289, 90472.17964625056\n",
      "Epoch 110971: reducing learning rate of group 0 to 3.3941e-03.\n",
      "Epoch 111001, Training Loss: 30431, Validation Loss: 55428, 74150.0720365127\n",
      "Epoch 111072: reducing learning rate of group 0 to 3.3907e-03.\n",
      "Epoch 111101, Training Loss: 30621, Validation Loss: 55625, 74297.58421175274\n",
      "Epoch 111173: reducing learning rate of group 0 to 3.3873e-03.\n",
      "Epoch 111201, Training Loss: 31627, Validation Loss: 59174, 74169.59025941185\n",
      "Epoch 111274: reducing learning rate of group 0 to 3.3839e-03.\n",
      "Epoch 111301, Training Loss: 31070, Validation Loss: 53680, 90447.66564168117\n",
      "Epoch 111375: reducing learning rate of group 0 to 3.3806e-03.\n",
      "Epoch 111401, Training Loss: 26858, Validation Loss: 54385, 71845.16306911434\n",
      "Epoch 111476: reducing learning rate of group 0 to 3.3772e-03.\n",
      "Epoch 111501, Training Loss: 29882, Validation Loss: 55912, 95200.38357087446\n",
      "Epoch 111577: reducing learning rate of group 0 to 3.3738e-03.\n",
      "Epoch 111601, Training Loss: 31600, Validation Loss: 54107, 71819.92289188213\n",
      "Epoch 111678: reducing learning rate of group 0 to 3.3704e-03.\n",
      "Epoch 111701, Training Loss: 27417, Validation Loss: 53610, 73553.57108609968\n",
      "Epoch 111779: reducing learning rate of group 0 to 3.3671e-03.\n",
      "Epoch 111801, Training Loss: 30869, Validation Loss: 52638, 82913.81588357397\n",
      "Epoch 111880: reducing learning rate of group 0 to 3.3637e-03.\n",
      "Epoch 111901, Training Loss: 29636, Validation Loss: 55855, 78351.0003253026\n",
      "Epoch 111981: reducing learning rate of group 0 to 3.3603e-03.\n",
      "Epoch 112001, Training Loss: 27866, Validation Loss: 55461, 69068.05743015108\n",
      "Epoch 112082: reducing learning rate of group 0 to 3.3570e-03.\n",
      "Epoch 112101, Training Loss: 33009, Validation Loss: 53088, 64998.901176780615\n",
      "Epoch 112183: reducing learning rate of group 0 to 3.3536e-03.\n",
      "Epoch 112201, Training Loss: 34406, Validation Loss: 56227, 75852.34300838289\n",
      "Epoch 112284: reducing learning rate of group 0 to 3.3503e-03.\n",
      "Epoch 112301, Training Loss: 30298, Validation Loss: 54773, 87987.31512819203\n",
      "Epoch 112385: reducing learning rate of group 0 to 3.3469e-03.\n",
      "Epoch 112401, Training Loss: 31733, Validation Loss: 53112, 85351.38479622755\n",
      "Epoch 112486: reducing learning rate of group 0 to 3.3436e-03.\n",
      "Epoch 112501, Training Loss: 31811, Validation Loss: 55558, 98052.78722283638\n",
      "Epoch 112587: reducing learning rate of group 0 to 3.3402e-03.\n",
      "Epoch 112601, Training Loss: 31609, Validation Loss: 55340, 87224.31859433558\n",
      "Epoch 112688: reducing learning rate of group 0 to 3.3369e-03.\n",
      "Epoch 112701, Training Loss: 26911, Validation Loss: 54906, 88593.73491987925\n",
      "Epoch 112789: reducing learning rate of group 0 to 3.3335e-03.\n",
      "Epoch 112801, Training Loss: 29832, Validation Loss: 56629, 94961.92668252409\n",
      "Epoch 112890: reducing learning rate of group 0 to 3.3302e-03.\n",
      "Epoch 112901, Training Loss: 30528, Validation Loss: 54874, 76718.4669666\n",
      "Epoch 112991: reducing learning rate of group 0 to 3.3269e-03.\n",
      "Epoch 113001, Training Loss: 28270, Validation Loss: 54052, 87278.38558602233\n",
      "Epoch 113092: reducing learning rate of group 0 to 3.3236e-03.\n",
      "Epoch 113101, Training Loss: 30990, Validation Loss: 54441, 81354.39459604595\n",
      "Epoch 113193: reducing learning rate of group 0 to 3.3202e-03.\n",
      "Epoch 113201, Training Loss: 28436, Validation Loss: 55225, 79780.41492355474\n",
      "Epoch 113294: reducing learning rate of group 0 to 3.3169e-03.\n",
      "Epoch 113301, Training Loss: 29664, Validation Loss: 56451, 80771.49425258768\n",
      "Epoch 113395: reducing learning rate of group 0 to 3.3136e-03.\n",
      "Epoch 113401, Training Loss: 30162, Validation Loss: 54642, 76149.09308565148\n",
      "Epoch 113496: reducing learning rate of group 0 to 3.3103e-03.\n",
      "Epoch 113501, Training Loss: 27817, Validation Loss: 53604, 69575.94012656967\n",
      "Epoch 113597: reducing learning rate of group 0 to 3.3070e-03.\n",
      "Epoch 113601, Training Loss: 33702, Validation Loss: 56318, 70913.81260558114\n",
      "Epoch 113698: reducing learning rate of group 0 to 3.3037e-03.\n",
      "Epoch 113701, Training Loss: 29804, Validation Loss: 54525, 79808.94856637427\n",
      "Epoch 113799: reducing learning rate of group 0 to 3.3004e-03.\n",
      "Epoch 113801, Training Loss: 29023, Validation Loss: 54065, 80330.96870105271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113900: reducing learning rate of group 0 to 3.2971e-03.\n",
      "Epoch 113901, Training Loss: 28916, Validation Loss: 55183, 82474.17165479601\n",
      "Epoch 114001: reducing learning rate of group 0 to 3.2938e-03.\n",
      "Epoch 114001, Training Loss: 30777, Validation Loss: 54530, 84387.71064960536\n",
      "Epoch 114101, Training Loss: 30232, Validation Loss: 53169, 81364.26407724193\n",
      "Epoch 114102: reducing learning rate of group 0 to 3.2905e-03.\n",
      "Epoch 114201, Training Loss: 31601, Validation Loss: 53635, 76904.84887786371\n",
      "Epoch 114203: reducing learning rate of group 0 to 3.2872e-03.\n",
      "Epoch 114301, Training Loss: 28785, Validation Loss: 53427, 82640.78540513005\n",
      "Epoch 114304: reducing learning rate of group 0 to 3.2839e-03.\n",
      "Epoch 114401, Training Loss: 28380, Validation Loss: 55090, 85556.75259117364\n",
      "Epoch 114405: reducing learning rate of group 0 to 3.2806e-03.\n",
      "Epoch 114501, Training Loss: 29650, Validation Loss: 53722, 82661.24273434361\n",
      "Epoch 114506: reducing learning rate of group 0 to 3.2773e-03.\n",
      "Epoch 114601, Training Loss: 29608, Validation Loss: 53924, 66045.97409827175\n",
      "Epoch 114607: reducing learning rate of group 0 to 3.2740e-03.\n",
      "Epoch 114701, Training Loss: 29124, Validation Loss: 54035, 79285.5263951592\n",
      "Epoch 114708: reducing learning rate of group 0 to 3.2708e-03.\n",
      "Epoch 114801, Training Loss: 28565, Validation Loss: 55972, 94612.6859868929\n",
      "Epoch 114809: reducing learning rate of group 0 to 3.2675e-03.\n",
      "Epoch 114901, Training Loss: 29199, Validation Loss: 57034, 87083.20521574035\n",
      "Epoch 114910: reducing learning rate of group 0 to 3.2642e-03.\n",
      "Epoch 115001, Training Loss: 31665, Validation Loss: 57336, 80446.78094844335\n",
      "Epoch 115011: reducing learning rate of group 0 to 3.2610e-03.\n",
      "Epoch 115101, Training Loss: 29848, Validation Loss: 57797, 88849.27274504742\n",
      "Epoch 115112: reducing learning rate of group 0 to 3.2577e-03.\n",
      "Epoch 115201, Training Loss: 30403, Validation Loss: 55909, 83101.0592311012\n",
      "Epoch 115213: reducing learning rate of group 0 to 3.2545e-03.\n",
      "Epoch 115301, Training Loss: 29059, Validation Loss: 54656, 84093.2392759396\n",
      "Epoch 115314: reducing learning rate of group 0 to 3.2512e-03.\n",
      "Epoch 115401, Training Loss: 29628, Validation Loss: 56758, 88762.71999670278\n",
      "Epoch 115415: reducing learning rate of group 0 to 3.2479e-03.\n",
      "Epoch 115501, Training Loss: 28438, Validation Loss: 53751, 86780.04816199394\n",
      "Epoch 115516: reducing learning rate of group 0 to 3.2447e-03.\n",
      "Epoch 115601, Training Loss: 27603, Validation Loss: 55538, 71056.05275545157\n",
      "Epoch 115617: reducing learning rate of group 0 to 3.2415e-03.\n",
      "Epoch 115701, Training Loss: 30018, Validation Loss: 56175, 81707.64458397804\n",
      "Epoch 115718: reducing learning rate of group 0 to 3.2382e-03.\n",
      "Epoch 115801, Training Loss: 28823, Validation Loss: 54970, 76621.37655601102\n",
      "Epoch 115819: reducing learning rate of group 0 to 3.2350e-03.\n",
      "Epoch 115901, Training Loss: 29777, Validation Loss: 54524, 87386.73363220393\n",
      "Epoch 115920: reducing learning rate of group 0 to 3.2317e-03.\n",
      "Epoch 116001, Training Loss: 30342, Validation Loss: 58066, 62522.42805724213\n",
      "Epoch 116021: reducing learning rate of group 0 to 3.2285e-03.\n",
      "Epoch 116101, Training Loss: 30458, Validation Loss: 56711, 75384.3695279254\n",
      "Epoch 116122: reducing learning rate of group 0 to 3.2253e-03.\n",
      "Epoch 116201, Training Loss: 31869, Validation Loss: 52491, 74822.67823208314\n",
      "Epoch 116223: reducing learning rate of group 0 to 3.2221e-03.\n",
      "Epoch 116301, Training Loss: 31664, Validation Loss: 57044, 82380.9362048891\n",
      "Epoch 116324: reducing learning rate of group 0 to 3.2188e-03.\n",
      "Epoch 116401, Training Loss: 30973, Validation Loss: 57060, 105930.58959787355\n",
      "Epoch 116425: reducing learning rate of group 0 to 3.2156e-03.\n",
      "Epoch 116501, Training Loss: 29155, Validation Loss: 53716, 76411.16278542705\n",
      "Epoch 116526: reducing learning rate of group 0 to 3.2124e-03.\n",
      "Epoch 116601, Training Loss: 28913, Validation Loss: 53626, 69073.59340321989\n",
      "Epoch 116627: reducing learning rate of group 0 to 3.2092e-03.\n",
      "Epoch 116701, Training Loss: 30518, Validation Loss: 56304, 90527.75412214412\n",
      "Epoch 116728: reducing learning rate of group 0 to 3.2060e-03.\n",
      "Epoch 116801, Training Loss: 28807, Validation Loss: 56187, 102427.14143473869\n",
      "Epoch 116829: reducing learning rate of group 0 to 3.2028e-03.\n",
      "Epoch 116901, Training Loss: 29395, Validation Loss: 53149, 83880.35177024624\n",
      "Epoch 116930: reducing learning rate of group 0 to 3.1996e-03.\n",
      "Epoch 117001, Training Loss: 31171, Validation Loss: 55271, 82963.39807021467\n",
      "Epoch 117031: reducing learning rate of group 0 to 3.1964e-03.\n",
      "Epoch 117101, Training Loss: 28154, Validation Loss: 56595, 78607.68445467959\n",
      "Epoch 117132: reducing learning rate of group 0 to 3.1932e-03.\n",
      "Epoch 117201, Training Loss: 27503, Validation Loss: 54371, 66235.60052031618\n",
      "Epoch 117233: reducing learning rate of group 0 to 3.1900e-03.\n",
      "Epoch 117301, Training Loss: 29330, Validation Loss: 54930, 89233.29260870599\n",
      "Epoch 117334: reducing learning rate of group 0 to 3.1868e-03.\n",
      "Epoch 117401, Training Loss: 29124, Validation Loss: 54865, 96959.02930999086\n",
      "Epoch 117435: reducing learning rate of group 0 to 3.1836e-03.\n",
      "Epoch 117501, Training Loss: 28496, Validation Loss: 53479, 90489.44708145624\n",
      "Epoch 117536: reducing learning rate of group 0 to 3.1804e-03.\n",
      "Epoch 117601, Training Loss: 29659, Validation Loss: 55770, 69481.3438293287\n",
      "Epoch 117637: reducing learning rate of group 0 to 3.1772e-03.\n",
      "Epoch 117701, Training Loss: 31200, Validation Loss: 57597, 78311.19791768111\n",
      "Epoch 117738: reducing learning rate of group 0 to 3.1741e-03.\n",
      "Epoch 117801, Training Loss: 29897, Validation Loss: 54348, 67120.4410444766\n",
      "Epoch 117839: reducing learning rate of group 0 to 3.1709e-03.\n",
      "Epoch 117901, Training Loss: 29180, Validation Loss: 53731, 82036.5996124859\n",
      "Epoch 117940: reducing learning rate of group 0 to 3.1677e-03.\n",
      "Epoch 118001, Training Loss: 28354, Validation Loss: 55183, 73036.64155210736\n",
      "Epoch 118041: reducing learning rate of group 0 to 3.1645e-03.\n",
      "Epoch 118101, Training Loss: 29821, Validation Loss: 54401, 80092.52524241654\n",
      "Epoch 118142: reducing learning rate of group 0 to 3.1614e-03.\n",
      "Epoch 118201, Training Loss: 30876, Validation Loss: 54155, 93147.17824210961\n",
      "Epoch 118243: reducing learning rate of group 0 to 3.1582e-03.\n",
      "Epoch 118301, Training Loss: 30451, Validation Loss: 54564, 77697.77649243684\n",
      "Epoch 118344: reducing learning rate of group 0 to 3.1551e-03.\n",
      "Epoch 118401, Training Loss: 31764, Validation Loss: 54577, 71962.64725917345\n",
      "Epoch 118445: reducing learning rate of group 0 to 3.1519e-03.\n",
      "Epoch 118501, Training Loss: 29917, Validation Loss: 55811, 82192.15973596805\n",
      "Epoch 118546: reducing learning rate of group 0 to 3.1488e-03.\n",
      "Epoch 118601, Training Loss: 31628, Validation Loss: 55775, 72542.84624068746\n",
      "Epoch 118647: reducing learning rate of group 0 to 3.1456e-03.\n",
      "Epoch 118701, Training Loss: 31118, Validation Loss: 56000, 78518.75309426642\n",
      "Epoch 118748: reducing learning rate of group 0 to 3.1425e-03.\n",
      "Epoch 118801, Training Loss: 30013, Validation Loss: 53500, 83415.7580965285\n",
      "Epoch 118849: reducing learning rate of group 0 to 3.1393e-03.\n",
      "Epoch 118901, Training Loss: 29488, Validation Loss: 53777, 80915.78108397707\n",
      "Epoch 118950: reducing learning rate of group 0 to 3.1362e-03.\n",
      "Epoch 119001, Training Loss: 28893, Validation Loss: 56065, 69533.66308523148\n",
      "Epoch 119051: reducing learning rate of group 0 to 3.1330e-03.\n",
      "Epoch 119101, Training Loss: 29088, Validation Loss: 53324, 90720.24227982703\n",
      "Epoch 119152: reducing learning rate of group 0 to 3.1299e-03.\n",
      "Epoch 119201, Training Loss: 29725, Validation Loss: 54083, 88529.96375410508\n",
      "Epoch 119253: reducing learning rate of group 0 to 3.1268e-03.\n",
      "Epoch 119301, Training Loss: 29355, Validation Loss: 55147, 79044.49090253715\n",
      "Epoch 119354: reducing learning rate of group 0 to 3.1237e-03.\n",
      "Epoch 119401, Training Loss: 29212, Validation Loss: 55627, 86938.87936511824\n",
      "Epoch 119455: reducing learning rate of group 0 to 3.1205e-03.\n",
      "Epoch 119501, Training Loss: 28670, Validation Loss: 59944, 80479.5503159981\n",
      "Epoch 119556: reducing learning rate of group 0 to 3.1174e-03.\n",
      "Epoch 119601, Training Loss: 29044, Validation Loss: 80605, 77736.19700301754\n",
      "Epoch 119657: reducing learning rate of group 0 to 3.1143e-03.\n",
      "Epoch 119701, Training Loss: 32082, Validation Loss: 92781, 89819.36701869692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119758: reducing learning rate of group 0 to 3.1112e-03.\n",
      "Epoch 119801, Training Loss: 31221, Validation Loss: 111974, 94815.61064406372\n",
      "Epoch 119859: reducing learning rate of group 0 to 3.1081e-03.\n",
      "Epoch 119901, Training Loss: 28904, Validation Loss: 125316, 79645.89482151976\n",
      "Epoch 119960: reducing learning rate of group 0 to 3.1050e-03.\n",
      "Epoch 120001, Training Loss: 28849, Validation Loss: 53771, 89432.20958943221\n",
      "Epoch 120061: reducing learning rate of group 0 to 3.1019e-03.\n",
      "Epoch 120101, Training Loss: 28694, Validation Loss: 53049, 83648.49483346328\n",
      "Epoch 120162: reducing learning rate of group 0 to 3.0988e-03.\n",
      "Epoch 120201, Training Loss: 28047, Validation Loss: 57388, 83173.47984100856\n",
      "Epoch 120263: reducing learning rate of group 0 to 3.0957e-03.\n",
      "Epoch 120301, Training Loss: 30308, Validation Loss: 54892, 85513.9370088795\n",
      "Epoch 120364: reducing learning rate of group 0 to 3.0926e-03.\n",
      "Epoch 120401, Training Loss: 29864, Validation Loss: 56292, 78571.81656870608\n",
      "Epoch 120465: reducing learning rate of group 0 to 3.0895e-03.\n",
      "Epoch 120501, Training Loss: 28664, Validation Loss: 55012, 89421.11689222207\n",
      "Epoch 120566: reducing learning rate of group 0 to 3.0864e-03.\n",
      "Epoch 120601, Training Loss: 28632, Validation Loss: 54999, 73375.03316622839\n",
      "Epoch 120667: reducing learning rate of group 0 to 3.0833e-03.\n",
      "Epoch 120701, Training Loss: 27311, Validation Loss: 53309, 71490.08777087806\n",
      "Epoch 120768: reducing learning rate of group 0 to 3.0802e-03.\n",
      "Epoch 120801, Training Loss: 33805, Validation Loss: 55316, 95738.32546833386\n",
      "Epoch 120869: reducing learning rate of group 0 to 3.0771e-03.\n",
      "Epoch 120901, Training Loss: 29062, Validation Loss: 54409, 74624.4758488763\n",
      "Epoch 120970: reducing learning rate of group 0 to 3.0740e-03.\n",
      "Epoch 121001, Training Loss: 29721, Validation Loss: 53366, 81966.63505041593\n",
      "Epoch 121071: reducing learning rate of group 0 to 3.0710e-03.\n",
      "Epoch 121101, Training Loss: 30239, Validation Loss: 53359, 82798.10742898339\n",
      "Epoch 121172: reducing learning rate of group 0 to 3.0679e-03.\n",
      "Epoch 121201, Training Loss: 31511, Validation Loss: 55059, 78423.09428133833\n",
      "Epoch 121273: reducing learning rate of group 0 to 3.0648e-03.\n",
      "Epoch 121301, Training Loss: 32391, Validation Loss: 55782, 88278.81200554303\n",
      "Epoch 121374: reducing learning rate of group 0 to 3.0618e-03.\n",
      "Epoch 121401, Training Loss: 29395, Validation Loss: 54402, 90068.10956717217\n",
      "Epoch 121475: reducing learning rate of group 0 to 3.0587e-03.\n",
      "Epoch 121501, Training Loss: 30252, Validation Loss: 54624, 86470.63442059071\n",
      "Epoch 121576: reducing learning rate of group 0 to 3.0556e-03.\n",
      "Epoch 121601, Training Loss: 30371, Validation Loss: 56352, 94023.80369389647\n",
      "Epoch 121677: reducing learning rate of group 0 to 3.0526e-03.\n",
      "Epoch 121701, Training Loss: 30183, Validation Loss: 54844, 74104.74426600109\n",
      "Epoch 121778: reducing learning rate of group 0 to 3.0495e-03.\n",
      "Epoch 121801, Training Loss: 30069, Validation Loss: 55552, 83413.50108324624\n",
      "Epoch 121879: reducing learning rate of group 0 to 3.0465e-03.\n",
      "Epoch 121901, Training Loss: 29488, Validation Loss: 55557, 76754.47349010785\n",
      "Epoch 121980: reducing learning rate of group 0 to 3.0434e-03.\n",
      "Epoch 122001, Training Loss: 28940, Validation Loss: 54282, 96518.44930458396\n",
      "Epoch 122081: reducing learning rate of group 0 to 3.0404e-03.\n",
      "Epoch 122101, Training Loss: 29759, Validation Loss: 58510, 64383.26879320908\n",
      "Epoch 122182: reducing learning rate of group 0 to 3.0374e-03.\n",
      "Epoch 122201, Training Loss: 31486, Validation Loss: 55313, 81976.6891505207\n",
      "Epoch 122283: reducing learning rate of group 0 to 3.0343e-03.\n",
      "Epoch 122301, Training Loss: 30775, Validation Loss: 53321, 70384.58832807183\n",
      "Epoch 122384: reducing learning rate of group 0 to 3.0313e-03.\n",
      "Epoch 122401, Training Loss: 29476, Validation Loss: 56274, 65715.94431433175\n",
      "Epoch 122485: reducing learning rate of group 0 to 3.0283e-03.\n",
      "Epoch 122501, Training Loss: 31618, Validation Loss: 56956, 76750.34799441932\n",
      "Epoch 122586: reducing learning rate of group 0 to 3.0252e-03.\n",
      "Epoch 122601, Training Loss: 33395, Validation Loss: 54050, 89030.54511432043\n",
      "Epoch 122687: reducing learning rate of group 0 to 3.0222e-03.\n",
      "Epoch 122701, Training Loss: 30293, Validation Loss: 54751, 92850.15946615765\n",
      "Epoch 122788: reducing learning rate of group 0 to 3.0192e-03.\n",
      "Epoch 122801, Training Loss: 31134, Validation Loss: 54289, 75778.33607361425\n",
      "Epoch 122889: reducing learning rate of group 0 to 3.0162e-03.\n",
      "Epoch 122901, Training Loss: 29344, Validation Loss: 53714, 69530.09749565698\n",
      "Epoch 122990: reducing learning rate of group 0 to 3.0131e-03.\n",
      "Epoch 123001, Training Loss: 30603, Validation Loss: 52661, 97012.99267223601\n",
      "Epoch 123091: reducing learning rate of group 0 to 3.0101e-03.\n",
      "Epoch 123101, Training Loss: 28907, Validation Loss: 52904, 88737.47600705452\n",
      "Epoch 123192: reducing learning rate of group 0 to 3.0071e-03.\n",
      "Epoch 123201, Training Loss: 29830, Validation Loss: 55132, 87428.22647413773\n",
      "Epoch 123293: reducing learning rate of group 0 to 3.0041e-03.\n",
      "Epoch 123301, Training Loss: 25769, Validation Loss: 53701, 90457.22415548522\n",
      "Epoch 123394: reducing learning rate of group 0 to 3.0011e-03.\n",
      "Epoch 123401, Training Loss: 28271, Validation Loss: 54701, 94216.48252519376\n",
      "Epoch 123495: reducing learning rate of group 0 to 2.9981e-03.\n",
      "Epoch 123501, Training Loss: 27982, Validation Loss: 53974, 83898.38844717342\n",
      "Epoch 123596: reducing learning rate of group 0 to 2.9951e-03.\n",
      "Epoch 123601, Training Loss: 34296, Validation Loss: 55562, 85907.89329561572\n",
      "Epoch 123697: reducing learning rate of group 0 to 2.9921e-03.\n",
      "Epoch 123701, Training Loss: 31223, Validation Loss: 54769, 72148.59000500829\n",
      "Epoch 123798: reducing learning rate of group 0 to 2.9891e-03.\n",
      "Epoch 123801, Training Loss: 29774, Validation Loss: 54177, 73035.38422916824\n",
      "Epoch 123899: reducing learning rate of group 0 to 2.9861e-03.\n",
      "Epoch 123901, Training Loss: 29330, Validation Loss: 54519, 92222.45733414835\n",
      "Epoch 124000: reducing learning rate of group 0 to 2.9832e-03.\n",
      "Epoch 124001, Training Loss: 28511, Validation Loss: 56174, 75142.35889577593\n",
      "Epoch 124101: reducing learning rate of group 0 to 2.9802e-03.\n",
      "Epoch 124101, Training Loss: 29992, Validation Loss: 55795, 75116.31966698599\n",
      "Epoch 124201, Training Loss: 29366, Validation Loss: 54255, 79836.222401442\n",
      "Epoch 124202: reducing learning rate of group 0 to 2.9772e-03.\n",
      "Epoch 124301, Training Loss: 29390, Validation Loss: 55961, 73820.04188021291\n",
      "Epoch 124303: reducing learning rate of group 0 to 2.9742e-03.\n",
      "Epoch 124401, Training Loss: 29732, Validation Loss: 53238, 66713.6833232012\n",
      "Epoch 124404: reducing learning rate of group 0 to 2.9712e-03.\n",
      "Epoch 124501, Training Loss: 28080, Validation Loss: 55996, 95347.7999808869\n",
      "Epoch 124505: reducing learning rate of group 0 to 2.9683e-03.\n",
      "Epoch 124601, Training Loss: 33579, Validation Loss: 55371, 96917.09312086641\n",
      "Epoch 124606: reducing learning rate of group 0 to 2.9653e-03.\n",
      "Epoch 124701, Training Loss: 29206, Validation Loss: 55552, 71758.6980559414\n",
      "Epoch 124707: reducing learning rate of group 0 to 2.9623e-03.\n",
      "Epoch 124801, Training Loss: 31417, Validation Loss: 53053, 109236.01734765318\n",
      "Epoch 124808: reducing learning rate of group 0 to 2.9594e-03.\n",
      "Epoch 124901, Training Loss: 26987, Validation Loss: 55591, 80327.71454796578\n",
      "Epoch 124909: reducing learning rate of group 0 to 2.9564e-03.\n",
      "Epoch 125001, Training Loss: 25991, Validation Loss: 56594, 94540.78893715382\n",
      "Epoch 125010: reducing learning rate of group 0 to 2.9535e-03.\n",
      "Epoch 125101, Training Loss: 33994, Validation Loss: 55597, 70061.07931651436\n",
      "Epoch 125111: reducing learning rate of group 0 to 2.9505e-03.\n",
      "Epoch 125201, Training Loss: 26152, Validation Loss: 55069, 80420.22808972634\n",
      "Epoch 125212: reducing learning rate of group 0 to 2.9475e-03.\n",
      "Epoch 125301, Training Loss: 31109, Validation Loss: 56263, 93625.988800101\n",
      "Epoch 125313: reducing learning rate of group 0 to 2.9446e-03.\n",
      "Epoch 125401, Training Loss: 30458, Validation Loss: 55052, 73523.97976581211\n",
      "Epoch 125414: reducing learning rate of group 0 to 2.9417e-03.\n",
      "Epoch 125501, Training Loss: 27775, Validation Loss: 56473, 66282.94898293774\n",
      "Epoch 125515: reducing learning rate of group 0 to 2.9387e-03.\n",
      "Epoch 125601, Training Loss: 32237, Validation Loss: 55152, 74740.61242374881\n",
      "Epoch 125616: reducing learning rate of group 0 to 2.9358e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125701, Training Loss: 29854, Validation Loss: 55403, 100662.60885859503\n",
      "Epoch 125717: reducing learning rate of group 0 to 2.9328e-03.\n",
      "Epoch 125801, Training Loss: 26433, Validation Loss: 52919, 78743.41670371236\n",
      "Epoch 125818: reducing learning rate of group 0 to 2.9299e-03.\n",
      "Epoch 125901, Training Loss: 29751, Validation Loss: 53080, 88888.92965979148\n",
      "Epoch 125919: reducing learning rate of group 0 to 2.9270e-03.\n",
      "Epoch 126001, Training Loss: 28829, Validation Loss: 54129, 71745.90379815653\n",
      "Epoch 126020: reducing learning rate of group 0 to 2.9241e-03.\n",
      "Epoch 126101, Training Loss: 32756, Validation Loss: 54732, 92624.62727598035\n",
      "Epoch 126121: reducing learning rate of group 0 to 2.9211e-03.\n",
      "Epoch 126201, Training Loss: 29128, Validation Loss: 55030, 87393.78553364771\n",
      "Epoch 126222: reducing learning rate of group 0 to 2.9182e-03.\n",
      "Epoch 126301, Training Loss: 28657, Validation Loss: 57386, 103472.1777533181\n",
      "Epoch 126323: reducing learning rate of group 0 to 2.9153e-03.\n",
      "Epoch 126401, Training Loss: 29163, Validation Loss: 55281, 99377.025971197\n",
      "Epoch 126424: reducing learning rate of group 0 to 2.9124e-03.\n",
      "Epoch 126501, Training Loss: 29560, Validation Loss: 55331, 91019.38151111413\n",
      "Epoch 126525: reducing learning rate of group 0 to 2.9095e-03.\n",
      "Epoch 126601, Training Loss: 26793, Validation Loss: 56277, 85463.10587115264\n",
      "Epoch 126626: reducing learning rate of group 0 to 2.9066e-03.\n",
      "Epoch 126701, Training Loss: 30688, Validation Loss: 55371, 71481.16480559132\n",
      "Epoch 126727: reducing learning rate of group 0 to 2.9036e-03.\n",
      "Epoch 126801, Training Loss: 28854, Validation Loss: 54471, 77144.43626665279\n",
      "Epoch 126828: reducing learning rate of group 0 to 2.9007e-03.\n",
      "Epoch 126901, Training Loss: 27978, Validation Loss: 55424, 68029.31979495451\n",
      "Epoch 126929: reducing learning rate of group 0 to 2.8978e-03.\n",
      "Epoch 127001, Training Loss: 29123, Validation Loss: 53700, 67373.59465323434\n",
      "Epoch 127030: reducing learning rate of group 0 to 2.8949e-03.\n",
      "Epoch 127101, Training Loss: 31201, Validation Loss: 54688, 87558.01114386595\n",
      "Epoch 127131: reducing learning rate of group 0 to 2.8920e-03.\n",
      "Epoch 127201, Training Loss: 28153, Validation Loss: 55168, 78651.23361434508\n",
      "Epoch 127232: reducing learning rate of group 0 to 2.8892e-03.\n",
      "Epoch 127301, Training Loss: 32115, Validation Loss: 54250, 90061.41981922822\n",
      "Epoch 127333: reducing learning rate of group 0 to 2.8863e-03.\n",
      "Epoch 127401, Training Loss: 30044, Validation Loss: 56129, 73510.44406070405\n",
      "Epoch 127434: reducing learning rate of group 0 to 2.8834e-03.\n",
      "Epoch 127501, Training Loss: 29443, Validation Loss: 52491, 76054.14391001033\n",
      "Epoch 127535: reducing learning rate of group 0 to 2.8805e-03.\n",
      "Epoch 127601, Training Loss: 31117, Validation Loss: 54176, 80099.34953527717\n",
      "Epoch 127636: reducing learning rate of group 0 to 2.8776e-03.\n",
      "Epoch 127701, Training Loss: 28444, Validation Loss: 55924, 83420.43133214606\n",
      "Epoch 127737: reducing learning rate of group 0 to 2.8747e-03.\n",
      "Epoch 127801, Training Loss: 28909, Validation Loss: 55246, 78346.54042717048\n",
      "Epoch 127838: reducing learning rate of group 0 to 2.8719e-03.\n",
      "Epoch 127901, Training Loss: 30463, Validation Loss: 56170, 81804.9564762537\n",
      "Epoch 127939: reducing learning rate of group 0 to 2.8690e-03.\n",
      "Epoch 128001, Training Loss: 28217, Validation Loss: 53989, 78030.58373155365\n",
      "Epoch 128040: reducing learning rate of group 0 to 2.8661e-03.\n",
      "Epoch 128101, Training Loss: 30153, Validation Loss: 53159, 93577.81228912952\n",
      "Epoch 128141: reducing learning rate of group 0 to 2.8633e-03.\n",
      "Epoch 128201, Training Loss: 30578, Validation Loss: 56097, 89828.14750077481\n",
      "Epoch 128242: reducing learning rate of group 0 to 2.8604e-03.\n",
      "Epoch 128301, Training Loss: 29635, Validation Loss: 53915, 75213.71680745516\n",
      "Epoch 128343: reducing learning rate of group 0 to 2.8575e-03.\n",
      "Epoch 128401, Training Loss: 29742, Validation Loss: 53860, 67164.5787353499\n",
      "Epoch 128444: reducing learning rate of group 0 to 2.8547e-03.\n",
      "Epoch 128501, Training Loss: 31661, Validation Loss: 55397, 81464.8258554942\n",
      "Epoch 128545: reducing learning rate of group 0 to 2.8518e-03.\n",
      "Epoch 128601, Training Loss: 27715, Validation Loss: 53165, 82637.57879827092\n",
      "Epoch 128646: reducing learning rate of group 0 to 2.8490e-03.\n",
      "Epoch 128701, Training Loss: 30726, Validation Loss: 53663, 76880.9956082857\n",
      "Epoch 128747: reducing learning rate of group 0 to 2.8461e-03.\n",
      "Epoch 128801, Training Loss: 28225, Validation Loss: 56297, 93159.07896448381\n",
      "Epoch 128848: reducing learning rate of group 0 to 2.8433e-03.\n",
      "Epoch 128901, Training Loss: 29699, Validation Loss: 53502, 98599.06105823784\n",
      "Epoch 128949: reducing learning rate of group 0 to 2.8404e-03.\n",
      "Epoch 129001, Training Loss: 28523, Validation Loss: 55848, 69706.65401576957\n",
      "Epoch 129050: reducing learning rate of group 0 to 2.8376e-03.\n",
      "Epoch 129101, Training Loss: 29526, Validation Loss: 53354, 71495.93872177847\n",
      "Epoch 129151: reducing learning rate of group 0 to 2.8348e-03.\n",
      "Epoch 129201, Training Loss: 28204, Validation Loss: 54361, 87618.22076701389\n",
      "Epoch 129252: reducing learning rate of group 0 to 2.8319e-03.\n",
      "Epoch 129301, Training Loss: 30481, Validation Loss: 56092, 62424.84781897469\n",
      "Epoch 129353: reducing learning rate of group 0 to 2.8291e-03.\n",
      "Epoch 129401, Training Loss: 31235, Validation Loss: 54752, 88732.9118351973\n",
      "Epoch 129454: reducing learning rate of group 0 to 2.8263e-03.\n",
      "Epoch 129501, Training Loss: 29250, Validation Loss: 53777, 92171.09761657966\n",
      "Epoch 129555: reducing learning rate of group 0 to 2.8234e-03.\n",
      "Epoch 129601, Training Loss: 29010, Validation Loss: 55982, 72078.58355937946\n",
      "Epoch 129656: reducing learning rate of group 0 to 2.8206e-03.\n",
      "Epoch 129701, Training Loss: 28069, Validation Loss: 54360, 82142.44143040142\n",
      "Epoch 129757: reducing learning rate of group 0 to 2.8178e-03.\n",
      "Epoch 129801, Training Loss: 31048, Validation Loss: 55278, 78515.25365548897\n",
      "Epoch 129858: reducing learning rate of group 0 to 2.8150e-03.\n",
      "Epoch 129901, Training Loss: 29464, Validation Loss: 54199, 93855.64538919507\n",
      "Epoch 129959: reducing learning rate of group 0 to 2.8122e-03.\n",
      "Epoch 130001, Training Loss: 30279, Validation Loss: 53472, 94578.45835578523\n",
      "Epoch 130060: reducing learning rate of group 0 to 2.8093e-03.\n",
      "Epoch 130101, Training Loss: 28892, Validation Loss: 53915, 66322.18927963877\n",
      "Epoch 130161: reducing learning rate of group 0 to 2.8065e-03.\n",
      "Epoch 130201, Training Loss: 29884, Validation Loss: 54236, 86216.71721360365\n",
      "Epoch 130262: reducing learning rate of group 0 to 2.8037e-03.\n",
      "Epoch 130301, Training Loss: 31016, Validation Loss: 54338, 93805.76502578135\n",
      "Epoch 130363: reducing learning rate of group 0 to 2.8009e-03.\n",
      "Epoch 130401, Training Loss: 28649, Validation Loss: 55490, 90515.30938960459\n",
      "Epoch 130464: reducing learning rate of group 0 to 2.7981e-03.\n",
      "Epoch 130501, Training Loss: 29893, Validation Loss: 54040, 87544.34962833436\n",
      "Epoch 130565: reducing learning rate of group 0 to 2.7953e-03.\n",
      "Epoch 130601, Training Loss: 30699, Validation Loss: 53713, 76580.49353139546\n",
      "Epoch 130666: reducing learning rate of group 0 to 2.7925e-03.\n",
      "Epoch 130701, Training Loss: 28466, Validation Loss: 53908, 87832.35675612203\n",
      "Epoch 130767: reducing learning rate of group 0 to 2.7897e-03.\n",
      "Epoch 130801, Training Loss: 29254, Validation Loss: 56627, 81916.79838944816\n",
      "Epoch 130868: reducing learning rate of group 0 to 2.7869e-03.\n",
      "Epoch 130901, Training Loss: 29260, Validation Loss: 57772, 77178.3345095113\n",
      "Epoch 130969: reducing learning rate of group 0 to 2.7842e-03.\n",
      "Epoch 131001, Training Loss: 30626, Validation Loss: 54680, 86960.13111108524\n",
      "Epoch 131070: reducing learning rate of group 0 to 2.7814e-03.\n",
      "Epoch 131101, Training Loss: 28684, Validation Loss: 54320, 99934.90803774486\n",
      "Epoch 131171: reducing learning rate of group 0 to 2.7786e-03.\n",
      "Epoch 131201, Training Loss: 29421, Validation Loss: 57048, 83044.2580824985\n",
      "Epoch 131272: reducing learning rate of group 0 to 2.7758e-03.\n",
      "Epoch 131301, Training Loss: 30638, Validation Loss: 53975, 95236.92660956678\n",
      "Epoch 131373: reducing learning rate of group 0 to 2.7730e-03.\n",
      "Epoch 131401, Training Loss: 30356, Validation Loss: 56239, 69869.61071029071\n",
      "Epoch 131474: reducing learning rate of group 0 to 2.7703e-03.\n",
      "Epoch 131501, Training Loss: 29057, Validation Loss: 53788, 93524.70311432867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131575: reducing learning rate of group 0 to 2.7675e-03.\n",
      "Epoch 131601, Training Loss: 31463, Validation Loss: 56372, 82991.71691506558\n",
      "Epoch 131676: reducing learning rate of group 0 to 2.7647e-03.\n",
      "Epoch 131701, Training Loss: 28638, Validation Loss: 56077, 77237.96608629373\n",
      "Epoch 131777: reducing learning rate of group 0 to 2.7620e-03.\n",
      "Epoch 131801, Training Loss: 32173, Validation Loss: 58951, 95393.9114833229\n",
      "Epoch 131878: reducing learning rate of group 0 to 2.7592e-03.\n",
      "Epoch 131901, Training Loss: 33403, Validation Loss: 55626, 95558.60549732768\n",
      "Epoch 131979: reducing learning rate of group 0 to 2.7564e-03.\n",
      "Epoch 132001, Training Loss: 27865, Validation Loss: 55567, 2621975.7277731425\n",
      "Epoch 132080: reducing learning rate of group 0 to 2.7537e-03.\n",
      "Epoch 132101, Training Loss: 28279, Validation Loss: 52357, 64165.81326956752\n",
      "Epoch 132181: reducing learning rate of group 0 to 2.7509e-03.\n",
      "Epoch 132201, Training Loss: 28219, Validation Loss: 56993, 85432.20883220543\n",
      "Epoch 132282: reducing learning rate of group 0 to 2.7482e-03.\n",
      "Epoch 132301, Training Loss: 28668, Validation Loss: 54147, 82837.50543842457\n",
      "Epoch 132383: reducing learning rate of group 0 to 2.7454e-03.\n",
      "Epoch 132401, Training Loss: 27741, Validation Loss: 52645, 79639.40458957506\n",
      "Epoch 132484: reducing learning rate of group 0 to 2.7427e-03.\n",
      "Epoch 132501, Training Loss: 30417, Validation Loss: 54131, 86579.330125722\n",
      "Epoch 132585: reducing learning rate of group 0 to 2.7399e-03.\n",
      "Epoch 132601, Training Loss: 30180, Validation Loss: 55392, 80398.56968137984\n",
      "Epoch 132686: reducing learning rate of group 0 to 2.7372e-03.\n",
      "Epoch 132701, Training Loss: 30690, Validation Loss: 55093, 103755.69137496734\n",
      "Epoch 132787: reducing learning rate of group 0 to 2.7345e-03.\n",
      "Epoch 132801, Training Loss: 29058, Validation Loss: 57665, 85038.37012839042\n",
      "Epoch 132888: reducing learning rate of group 0 to 2.7317e-03.\n",
      "Epoch 132901, Training Loss: 30349, Validation Loss: 54899, 94191.08489035345\n",
      "Epoch 132989: reducing learning rate of group 0 to 2.7290e-03.\n",
      "Epoch 133001, Training Loss: 28101, Validation Loss: 55133, 71132.19736182087\n",
      "Epoch 133090: reducing learning rate of group 0 to 2.7263e-03.\n",
      "Epoch 133101, Training Loss: 29354, Validation Loss: 93507, 77166.50959334063\n",
      "Epoch 133191: reducing learning rate of group 0 to 2.7235e-03.\n",
      "Epoch 133201, Training Loss: 28136, Validation Loss: 94343, 77046.41671884531\n",
      "Epoch 133292: reducing learning rate of group 0 to 2.7208e-03.\n",
      "Epoch 133301, Training Loss: 27197, Validation Loss: 92359, 71365.355093168\n",
      "Epoch 133393: reducing learning rate of group 0 to 2.7181e-03.\n",
      "Epoch 133401, Training Loss: 28032, Validation Loss: 86627, 85831.08887842965\n",
      "Epoch 133494: reducing learning rate of group 0 to 2.7154e-03.\n",
      "Epoch 133501, Training Loss: 28593, Validation Loss: 85365, 85288.61757242355\n",
      "Epoch 133595: reducing learning rate of group 0 to 2.7127e-03.\n",
      "Epoch 133601, Training Loss: 28453, Validation Loss: 89762, 70920.74641209294\n",
      "Epoch 133696: reducing learning rate of group 0 to 2.7100e-03.\n",
      "Epoch 133701, Training Loss: 29525, Validation Loss: 87177, 81657.49269322831\n",
      "Epoch 133797: reducing learning rate of group 0 to 2.7072e-03.\n",
      "Epoch 133801, Training Loss: 29597, Validation Loss: 85205, 79510.2026691811\n",
      "Epoch 133898: reducing learning rate of group 0 to 2.7045e-03.\n",
      "Epoch 133901, Training Loss: 29263, Validation Loss: 94510, 81267.81922745495\n",
      "Epoch 133999: reducing learning rate of group 0 to 2.7018e-03.\n",
      "Epoch 134001, Training Loss: 28591, Validation Loss: 92614, 85212.14026390381\n",
      "Epoch 134100: reducing learning rate of group 0 to 2.6991e-03.\n",
      "Epoch 134101, Training Loss: 28093, Validation Loss: 91451, 84709.32689590106\n",
      "Epoch 134201: reducing learning rate of group 0 to 2.6964e-03.\n",
      "Epoch 134201, Training Loss: 28708, Validation Loss: 86743, 67028.21113707109\n",
      "Epoch 134301, Training Loss: 31689, Validation Loss: 90421, 99602.03475895093\n",
      "Epoch 134302: reducing learning rate of group 0 to 2.6937e-03.\n",
      "Epoch 134401, Training Loss: 31559, Validation Loss: 86489, 82089.77825654404\n",
      "Epoch 134403: reducing learning rate of group 0 to 2.6910e-03.\n",
      "Epoch 134501, Training Loss: 27296, Validation Loss: 92786, 75767.96713136153\n",
      "Epoch 134504: reducing learning rate of group 0 to 2.6884e-03.\n",
      "Epoch 134601, Training Loss: 28372, Validation Loss: 90589, 77575.47528541066\n",
      "Epoch 134605: reducing learning rate of group 0 to 2.6857e-03.\n",
      "Epoch 134701, Training Loss: 29704, Validation Loss: 87859, 89540.13969327744\n",
      "Epoch 134706: reducing learning rate of group 0 to 2.6830e-03.\n",
      "Epoch 134801, Training Loss: 28929, Validation Loss: 95043, 97740.04730812267\n",
      "Epoch 134807: reducing learning rate of group 0 to 2.6803e-03.\n",
      "Epoch 134901, Training Loss: 28026, Validation Loss: 96967, 91800.45155676552\n",
      "Epoch 134908: reducing learning rate of group 0 to 2.6776e-03.\n",
      "Epoch 135001, Training Loss: 28299, Validation Loss: 87946, 78315.53765914727\n",
      "Epoch 135009: reducing learning rate of group 0 to 2.6749e-03.\n",
      "Epoch 135101, Training Loss: 30580, Validation Loss: 90256, 77480.69737456343\n",
      "Epoch 135110: reducing learning rate of group 0 to 2.6723e-03.\n",
      "Epoch 135201, Training Loss: 30476, Validation Loss: 87176, 87670.52597703437\n",
      "Epoch 135211: reducing learning rate of group 0 to 2.6696e-03.\n",
      "Epoch 135301, Training Loss: 33048, Validation Loss: 87868, 105220.19841088406\n",
      "Epoch 135312: reducing learning rate of group 0 to 2.6669e-03.\n",
      "Epoch 135401, Training Loss: 28722, Validation Loss: 77974, 75029.74711481227\n",
      "Epoch 135413: reducing learning rate of group 0 to 2.6643e-03.\n",
      "Epoch 135501, Training Loss: 28069, Validation Loss: 78940, 86618.71603235573\n",
      "Epoch 135514: reducing learning rate of group 0 to 2.6616e-03.\n",
      "Epoch 135601, Training Loss: 29391, Validation Loss: 78662, 94512.53559787624\n",
      "Epoch 135615: reducing learning rate of group 0 to 2.6589e-03.\n",
      "Epoch 135701, Training Loss: 28663, Validation Loss: 78365, 77514.2589388228\n",
      "Epoch 135716: reducing learning rate of group 0 to 2.6563e-03.\n",
      "Epoch 135801, Training Loss: 30331, Validation Loss: 76443, 87377.54922774892\n",
      "Epoch 135817: reducing learning rate of group 0 to 2.6536e-03.\n",
      "Epoch 135901, Training Loss: 29391, Validation Loss: 72162, 80013.21809982753\n",
      "Epoch 135918: reducing learning rate of group 0 to 2.6510e-03.\n",
      "Epoch 136001, Training Loss: 29335, Validation Loss: 74104, 79606.09046715929\n",
      "Epoch 136019: reducing learning rate of group 0 to 2.6483e-03.\n",
      "Epoch 136101, Training Loss: 27068, Validation Loss: 77448, 98337.16576159203\n",
      "Epoch 136120: reducing learning rate of group 0 to 2.6457e-03.\n",
      "Epoch 136201, Training Loss: 30088, Validation Loss: 80117, 84499.64898404083\n",
      "Epoch 136221: reducing learning rate of group 0 to 2.6430e-03.\n",
      "Epoch 136301, Training Loss: 29003, Validation Loss: 81203, 91614.5819437992\n",
      "Epoch 136322: reducing learning rate of group 0 to 2.6404e-03.\n",
      "Epoch 136401, Training Loss: 29072, Validation Loss: 84924, 76557.85638029367\n",
      "Epoch 136423: reducing learning rate of group 0 to 2.6377e-03.\n",
      "Epoch 136501, Training Loss: 29369, Validation Loss: 86108, 90125.21653456817\n",
      "Epoch 136524: reducing learning rate of group 0 to 2.6351e-03.\n",
      "Epoch 136601, Training Loss: 30158, Validation Loss: 87091, 66586.21587714333\n",
      "Epoch 136625: reducing learning rate of group 0 to 2.6325e-03.\n",
      "Epoch 136701, Training Loss: 28424, Validation Loss: 85553, 84217.40242769198\n",
      "Epoch 136726: reducing learning rate of group 0 to 2.6298e-03.\n",
      "Epoch 136801, Training Loss: 29418, Validation Loss: 87162, 106221.29299368402\n",
      "Epoch 136827: reducing learning rate of group 0 to 2.6272e-03.\n",
      "Epoch 136901, Training Loss: 28964, Validation Loss: 88687, 74324.48616558495\n",
      "Epoch 136928: reducing learning rate of group 0 to 2.6246e-03.\n",
      "Epoch 137001, Training Loss: 29831, Validation Loss: 87059, 77237.01761480734\n",
      "Epoch 137029: reducing learning rate of group 0 to 2.6219e-03.\n",
      "Epoch 137101, Training Loss: 30807, Validation Loss: 79678, 82406.05349335412\n",
      "Epoch 137130: reducing learning rate of group 0 to 2.6193e-03.\n",
      "Epoch 137201, Training Loss: 29908, Validation Loss: 88708, 89324.92107876895\n",
      "Epoch 137231: reducing learning rate of group 0 to 2.6167e-03.\n",
      "Epoch 137301, Training Loss: 27466, Validation Loss: 80982, 77473.00787267549\n",
      "Epoch 137332: reducing learning rate of group 0 to 2.6141e-03.\n",
      "Epoch 137401, Training Loss: 28549, Validation Loss: 83074, 83809.24624068163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137433: reducing learning rate of group 0 to 2.6115e-03.\n",
      "Epoch 137501, Training Loss: 27840, Validation Loss: 85139, 82957.2067248427\n",
      "Epoch 137534: reducing learning rate of group 0 to 2.6089e-03.\n",
      "Epoch 137601, Training Loss: 28934, Validation Loss: 84687, 84368.03011334575\n",
      "Epoch 137635: reducing learning rate of group 0 to 2.6063e-03.\n",
      "Epoch 137701, Training Loss: 28994, Validation Loss: 88157, 90907.78266898806\n",
      "Epoch 137736: reducing learning rate of group 0 to 2.6036e-03.\n",
      "Epoch 137801, Training Loss: 29134, Validation Loss: 90792, 70950.19770310547\n",
      "Epoch 137837: reducing learning rate of group 0 to 2.6010e-03.\n",
      "Epoch 137901, Training Loss: 30974, Validation Loss: 89082, 100000.72772055653\n",
      "Epoch 137938: reducing learning rate of group 0 to 2.5984e-03.\n",
      "Epoch 138001, Training Loss: 28576, Validation Loss: 91467, 92688.21025582065\n",
      "Epoch 138039: reducing learning rate of group 0 to 2.5958e-03.\n",
      "Epoch 138101, Training Loss: 28877, Validation Loss: 89094, 77331.92852648627\n",
      "Epoch 138140: reducing learning rate of group 0 to 2.5932e-03.\n",
      "Epoch 138201, Training Loss: 28291, Validation Loss: 88469, 78430.87000522873\n",
      "Epoch 138241: reducing learning rate of group 0 to 2.5907e-03.\n",
      "Epoch 138301, Training Loss: 28844, Validation Loss: 89471, 99157.05029879195\n",
      "Epoch 138342: reducing learning rate of group 0 to 2.5881e-03.\n",
      "Epoch 138401, Training Loss: 27442, Validation Loss: 92522, 71637.30568744139\n",
      "Epoch 138443: reducing learning rate of group 0 to 2.5855e-03.\n",
      "Epoch 138501, Training Loss: 30819, Validation Loss: 92989, 82106.52558845573\n",
      "Epoch 138544: reducing learning rate of group 0 to 2.5829e-03.\n",
      "Epoch 138601, Training Loss: 28882, Validation Loss: 87672, 81679.32536475721\n",
      "Epoch 138645: reducing learning rate of group 0 to 2.5803e-03.\n",
      "Epoch 138701, Training Loss: 29339, Validation Loss: 89425, 110466.72451999539\n",
      "Epoch 138746: reducing learning rate of group 0 to 2.5777e-03.\n",
      "Epoch 138801, Training Loss: 28485, Validation Loss: 90292, 80580.25299028085\n",
      "Epoch 138847: reducing learning rate of group 0 to 2.5751e-03.\n",
      "Epoch 138901, Training Loss: 29678, Validation Loss: 90836, 73518.73809262636\n",
      "Epoch 138948: reducing learning rate of group 0 to 2.5726e-03.\n",
      "Epoch 139001, Training Loss: 29600, Validation Loss: 91081, 81347.06904865928\n",
      "Epoch 139049: reducing learning rate of group 0 to 2.5700e-03.\n",
      "Epoch 139101, Training Loss: 28079, Validation Loss: 89383, 85494.5207065814\n",
      "Epoch 139150: reducing learning rate of group 0 to 2.5674e-03.\n",
      "Epoch 139201, Training Loss: 34722, Validation Loss: 86646, 100145.9297606831\n",
      "Epoch 139251: reducing learning rate of group 0 to 2.5649e-03.\n",
      "Epoch 139301, Training Loss: 30721, Validation Loss: 84580, 83026.68724738607\n",
      "Epoch 139352: reducing learning rate of group 0 to 2.5623e-03.\n",
      "Epoch 139401, Training Loss: 27514, Validation Loss: 82339, 85055.45495221279\n",
      "Epoch 139453: reducing learning rate of group 0 to 2.5597e-03.\n",
      "Epoch 139501, Training Loss: 29187, Validation Loss: 80073, 87998.86656343949\n",
      "Epoch 139554: reducing learning rate of group 0 to 2.5572e-03.\n",
      "Epoch 139601, Training Loss: 27510, Validation Loss: 76352, 67838.2754153722\n",
      "Epoch 139655: reducing learning rate of group 0 to 2.5546e-03.\n",
      "Epoch 139701, Training Loss: 30359, Validation Loss: 75424, 79815.38880646462\n",
      "Epoch 139756: reducing learning rate of group 0 to 2.5521e-03.\n",
      "Epoch 139801, Training Loss: 29008, Validation Loss: 69529, 81101.5512789873\n",
      "Epoch 139857: reducing learning rate of group 0 to 2.5495e-03.\n",
      "Epoch 139901, Training Loss: 28082, Validation Loss: 67917, 80357.84070782458\n",
      "Epoch 139958: reducing learning rate of group 0 to 2.5470e-03.\n",
      "Epoch 140001, Training Loss: 27510, Validation Loss: 66755, 68717.21330409475\n",
      "Epoch 140059: reducing learning rate of group 0 to 2.5444e-03.\n",
      "Epoch 140101, Training Loss: 28854, Validation Loss: 66211, 152448.8320756587\n",
      "Epoch 140160: reducing learning rate of group 0 to 2.5419e-03.\n",
      "Epoch 140201, Training Loss: 28510, Validation Loss: 60872, 83102.09382914651\n",
      "Epoch 140261: reducing learning rate of group 0 to 2.5393e-03.\n",
      "Epoch 140301, Training Loss: 29921, Validation Loss: 61648, 106078.51413499909\n",
      "Epoch 140362: reducing learning rate of group 0 to 2.5368e-03.\n",
      "Epoch 140401, Training Loss: 27148, Validation Loss: 58294, 82267.88760677625\n",
      "Epoch 140463: reducing learning rate of group 0 to 2.5343e-03.\n",
      "Epoch 140501, Training Loss: 30335, Validation Loss: 56768, 87993.94291772286\n",
      "Epoch 140564: reducing learning rate of group 0 to 2.5317e-03.\n",
      "Epoch 140601, Training Loss: 27680, Validation Loss: 55312, 71588.38948197292\n",
      "Epoch 140665: reducing learning rate of group 0 to 2.5292e-03.\n",
      "Epoch 140701, Training Loss: 32328, Validation Loss: 57881, 115657.80246730159\n",
      "Epoch 140766: reducing learning rate of group 0 to 2.5267e-03.\n",
      "Epoch 140801, Training Loss: 27992, Validation Loss: 56628, 97155.00312545912\n",
      "Epoch 140867: reducing learning rate of group 0 to 2.5241e-03.\n",
      "Epoch 140901, Training Loss: 28679, Validation Loss: 52891, 99777.88366666409\n",
      "Epoch 140968: reducing learning rate of group 0 to 2.5216e-03.\n",
      "Epoch 141001, Training Loss: 30124, Validation Loss: 57074, 82784.22996339192\n",
      "Epoch 141069: reducing learning rate of group 0 to 2.5191e-03.\n",
      "Epoch 141101, Training Loss: 27789, Validation Loss: 53968, 67838.19528351964\n",
      "Epoch 141170: reducing learning rate of group 0 to 2.5166e-03.\n",
      "Epoch 141201, Training Loss: 28512, Validation Loss: 54795, 82106.09942175688\n",
      "Epoch 141271: reducing learning rate of group 0 to 2.5140e-03.\n",
      "Epoch 141301, Training Loss: 31646, Validation Loss: 54849, 110643.45714877006\n",
      "Epoch 141372: reducing learning rate of group 0 to 2.5115e-03.\n",
      "Epoch 141401, Training Loss: 27026, Validation Loss: 55716, 73258.01121634181\n",
      "Epoch 141473: reducing learning rate of group 0 to 2.5090e-03.\n",
      "Epoch 141501, Training Loss: 31310, Validation Loss: 54087, 97384.12841006064\n",
      "Epoch 141574: reducing learning rate of group 0 to 2.5065e-03.\n",
      "Epoch 141601, Training Loss: 31112, Validation Loss: 53996, 83707.3446936801\n",
      "Epoch 141675: reducing learning rate of group 0 to 2.5040e-03.\n",
      "Epoch 141701, Training Loss: 27728, Validation Loss: 57107, 94127.39324271352\n",
      "Epoch 141776: reducing learning rate of group 0 to 2.5015e-03.\n",
      "Epoch 141801, Training Loss: 30678, Validation Loss: 55013, 87404.31593863557\n",
      "Epoch 141877: reducing learning rate of group 0 to 2.4990e-03.\n",
      "Epoch 141901, Training Loss: 29949, Validation Loss: 55066, 86465.15654544657\n",
      "Epoch 141978: reducing learning rate of group 0 to 2.4965e-03.\n",
      "Epoch 142001, Training Loss: 29796, Validation Loss: 53795, 117393.21185332995\n",
      "Epoch 142079: reducing learning rate of group 0 to 2.4940e-03.\n",
      "Epoch 142101, Training Loss: 31616, Validation Loss: 54638, 108393.08893946675\n",
      "Epoch 142180: reducing learning rate of group 0 to 2.4915e-03.\n",
      "Epoch 142201, Training Loss: 27421, Validation Loss: 55678, 77162.80301872289\n",
      "Epoch 142281: reducing learning rate of group 0 to 2.4890e-03.\n",
      "Epoch 142301, Training Loss: 30682, Validation Loss: 55880, 91370.09559086822\n",
      "Epoch 142382: reducing learning rate of group 0 to 2.4865e-03.\n",
      "Epoch 142401, Training Loss: 28049, Validation Loss: 54341, 89134.2642607739\n",
      "Epoch 142483: reducing learning rate of group 0 to 2.4840e-03.\n",
      "Epoch 142501, Training Loss: 29848, Validation Loss: 56203, 92517.60617427113\n",
      "Epoch 142584: reducing learning rate of group 0 to 2.4816e-03.\n",
      "Epoch 142601, Training Loss: 27678, Validation Loss: 55073, 103892.5042767588\n",
      "Epoch 142685: reducing learning rate of group 0 to 2.4791e-03.\n",
      "Epoch 142701, Training Loss: 27373, Validation Loss: 54246, 81741.85688603316\n",
      "Epoch 142786: reducing learning rate of group 0 to 2.4766e-03.\n",
      "Epoch 142801, Training Loss: 29171, Validation Loss: 56823, 74591.98561223218\n",
      "Epoch 142887: reducing learning rate of group 0 to 2.4741e-03.\n",
      "Epoch 142901, Training Loss: 28560, Validation Loss: 54628, 87409.96106513205\n",
      "Epoch 142988: reducing learning rate of group 0 to 2.4717e-03.\n",
      "Epoch 143001, Training Loss: 29569, Validation Loss: 55234, 90808.9611469774\n",
      "Epoch 143089: reducing learning rate of group 0 to 2.4692e-03.\n",
      "Epoch 143101, Training Loss: 27633, Validation Loss: 54970, 88513.0776162246\n",
      "Epoch 143190: reducing learning rate of group 0 to 2.4667e-03.\n",
      "Epoch 143201, Training Loss: 29152, Validation Loss: 54828, 102564.68129695405\n",
      "Epoch 143291: reducing learning rate of group 0 to 2.4642e-03.\n",
      "Epoch 143301, Training Loss: 28984, Validation Loss: 54395, 79733.3317560898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143392: reducing learning rate of group 0 to 2.4618e-03.\n",
      "Epoch 143401, Training Loss: 32492, Validation Loss: 56373, 79098.14577925987\n",
      "Epoch 143493: reducing learning rate of group 0 to 2.4593e-03.\n",
      "Epoch 143501, Training Loss: 30264, Validation Loss: 56744, 83983.6884380908\n",
      "Epoch 143594: reducing learning rate of group 0 to 2.4569e-03.\n",
      "Epoch 143601, Training Loss: 28647, Validation Loss: 54871, 91995.16832768127\n",
      "Epoch 143695: reducing learning rate of group 0 to 2.4544e-03.\n",
      "Epoch 143701, Training Loss: 29332, Validation Loss: 55231, 81815.50463088803\n",
      "Epoch 143796: reducing learning rate of group 0 to 2.4519e-03.\n",
      "Epoch 143801, Training Loss: 31225, Validation Loss: 57107, 74435.93612978321\n",
      "Epoch 143897: reducing learning rate of group 0 to 2.4495e-03.\n",
      "Epoch 143901, Training Loss: 28093, Validation Loss: 58043, 80680.39143868403\n",
      "Epoch 143998: reducing learning rate of group 0 to 2.4470e-03.\n",
      "Epoch 144001, Training Loss: 28025, Validation Loss: 54124, 118650.8289861426\n",
      "Epoch 144099: reducing learning rate of group 0 to 2.4446e-03.\n",
      "Epoch 144101, Training Loss: 27566, Validation Loss: 55629, 93908.0751584352\n",
      "Epoch 144200: reducing learning rate of group 0 to 2.4422e-03.\n",
      "Epoch 144201, Training Loss: 31156, Validation Loss: 53480, 103270.61733146691\n",
      "Epoch 144301: reducing learning rate of group 0 to 2.4397e-03.\n",
      "Epoch 144301, Training Loss: 28287, Validation Loss: 53547, 76921.64848831271\n",
      "Epoch 144401, Training Loss: 27567, Validation Loss: 55404, 72206.1978846618\n",
      "Epoch 144402: reducing learning rate of group 0 to 2.4373e-03.\n",
      "Epoch 144501, Training Loss: 27426, Validation Loss: 57230, 71681.34898460982\n",
      "Epoch 144503: reducing learning rate of group 0 to 2.4348e-03.\n",
      "Epoch 144601, Training Loss: 26119, Validation Loss: 55351, 76609.70416522828\n",
      "Epoch 144604: reducing learning rate of group 0 to 2.4324e-03.\n",
      "Epoch 144701, Training Loss: 27131, Validation Loss: 54360, 74014.88599622255\n",
      "Epoch 144705: reducing learning rate of group 0 to 2.4300e-03.\n",
      "Epoch 144801, Training Loss: 28500, Validation Loss: 56249, 85240.06702615647\n",
      "Epoch 144806: reducing learning rate of group 0 to 2.4275e-03.\n",
      "Epoch 144901, Training Loss: 30128, Validation Loss: 55173, 92955.4446988177\n",
      "Epoch 144907: reducing learning rate of group 0 to 2.4251e-03.\n",
      "Epoch 145001, Training Loss: 26274, Validation Loss: 54545, 81996.81620807045\n",
      "Epoch 145008: reducing learning rate of group 0 to 2.4227e-03.\n",
      "Epoch 145101, Training Loss: 27973, Validation Loss: 56194, 79523.0194568631\n",
      "Epoch 145109: reducing learning rate of group 0 to 2.4203e-03.\n",
      "Epoch 145201, Training Loss: 28649, Validation Loss: 54973, 87794.44868088327\n",
      "Epoch 145210: reducing learning rate of group 0 to 2.4178e-03.\n",
      "Epoch 145301, Training Loss: 30621, Validation Loss: 57355, 83880.24934242415\n",
      "Epoch 145311: reducing learning rate of group 0 to 2.4154e-03.\n",
      "Epoch 145401, Training Loss: 27343, Validation Loss: 56898, 70043.40419820708\n",
      "Epoch 145412: reducing learning rate of group 0 to 2.4130e-03.\n",
      "Epoch 145501, Training Loss: 29316, Validation Loss: 55554, 85783.64762788433\n",
      "Epoch 145513: reducing learning rate of group 0 to 2.4106e-03.\n",
      "Epoch 145601, Training Loss: 26725, Validation Loss: 55963, 72975.7597793311\n",
      "Epoch 145614: reducing learning rate of group 0 to 2.4082e-03.\n",
      "Epoch 145701, Training Loss: 27891, Validation Loss: 55028, 94611.0091009316\n",
      "Epoch 145715: reducing learning rate of group 0 to 2.4058e-03.\n",
      "Epoch 145801, Training Loss: 29151, Validation Loss: 55974, 86862.38649147977\n",
      "Epoch 145816: reducing learning rate of group 0 to 2.4034e-03.\n",
      "Epoch 145901, Training Loss: 28265, Validation Loss: 55146, 97948.78059190647\n",
      "Epoch 145917: reducing learning rate of group 0 to 2.4010e-03.\n",
      "Epoch 146001, Training Loss: 28329, Validation Loss: 55296, 82989.74409222328\n",
      "Epoch 146018: reducing learning rate of group 0 to 2.3986e-03.\n",
      "Epoch 146101, Training Loss: 28777, Validation Loss: 55933, 85908.58275168146\n",
      "Epoch 146119: reducing learning rate of group 0 to 2.3962e-03.\n",
      "Epoch 146201, Training Loss: 29249, Validation Loss: 54428, 97081.51551819981\n",
      "Epoch 146220: reducing learning rate of group 0 to 2.3938e-03.\n",
      "Epoch 146301, Training Loss: 29389, Validation Loss: 55345, 98459.098035547\n",
      "Epoch 146321: reducing learning rate of group 0 to 2.3914e-03.\n",
      "Epoch 146401, Training Loss: 30229, Validation Loss: 54332, 74708.00581744697\n",
      "Epoch 146422: reducing learning rate of group 0 to 2.3890e-03.\n",
      "Epoch 146501, Training Loss: 28057, Validation Loss: 55952, 71092.02440373266\n",
      "Epoch 146523: reducing learning rate of group 0 to 2.3866e-03.\n",
      "Epoch 146601, Training Loss: 28298, Validation Loss: 55627, 84191.62163646946\n",
      "Epoch 146624: reducing learning rate of group 0 to 2.3842e-03.\n",
      "Epoch 146701, Training Loss: 28732, Validation Loss: 53358, 71648.90362552949\n",
      "Epoch 146725: reducing learning rate of group 0 to 2.3818e-03.\n",
      "Epoch 146801, Training Loss: 31444, Validation Loss: 56134, 88309.93174859411\n",
      "Epoch 146826: reducing learning rate of group 0 to 2.3794e-03.\n",
      "Epoch 146901, Training Loss: 26605, Validation Loss: 55523, 91698.63318113527\n",
      "Epoch 146927: reducing learning rate of group 0 to 2.3771e-03.\n",
      "Epoch 147001, Training Loss: 31689, Validation Loss: 52962, 102658.91626444133\n",
      "Epoch 147028: reducing learning rate of group 0 to 2.3747e-03.\n",
      "Epoch 147101, Training Loss: 27074, Validation Loss: 55144, 75699.26335932642\n",
      "Epoch 147129: reducing learning rate of group 0 to 2.3723e-03.\n",
      "Epoch 147201, Training Loss: 27992, Validation Loss: 55258, 84416.51459300132\n",
      "Epoch 147230: reducing learning rate of group 0 to 2.3699e-03.\n",
      "Epoch 147301, Training Loss: 30727, Validation Loss: 54736, 85258.96581267059\n",
      "Epoch 147331: reducing learning rate of group 0 to 2.3676e-03.\n",
      "Epoch 147401, Training Loss: 30034, Validation Loss: 55362, 94814.10568113416\n",
      "Epoch 147432: reducing learning rate of group 0 to 2.3652e-03.\n",
      "Epoch 147501, Training Loss: 29690, Validation Loss: 56206, 82643.33991483778\n",
      "Epoch 147533: reducing learning rate of group 0 to 2.3628e-03.\n",
      "Epoch 147601, Training Loss: 29768, Validation Loss: 56799, 83761.40744780836\n",
      "Epoch 147634: reducing learning rate of group 0 to 2.3605e-03.\n",
      "Epoch 147701, Training Loss: 28120, Validation Loss: 55086, 82684.16436486204\n",
      "Epoch 147735: reducing learning rate of group 0 to 2.3581e-03.\n",
      "Epoch 147801, Training Loss: 28367, Validation Loss: 55028, 71698.25825925129\n",
      "Epoch 147836: reducing learning rate of group 0 to 2.3558e-03.\n",
      "Epoch 147901, Training Loss: 27496, Validation Loss: 58001, 81456.17137459591\n",
      "Epoch 147937: reducing learning rate of group 0 to 2.3534e-03.\n",
      "Epoch 148001, Training Loss: 25536, Validation Loss: 52976, 77166.31088549279\n",
      "Epoch 148038: reducing learning rate of group 0 to 2.3510e-03.\n",
      "Epoch 148101, Training Loss: 26224, Validation Loss: 54652, 75979.19805090113\n",
      "Epoch 148139: reducing learning rate of group 0 to 2.3487e-03.\n",
      "Epoch 148201, Training Loss: 27706, Validation Loss: 56587, 81097.76004819306\n",
      "Epoch 148240: reducing learning rate of group 0 to 2.3463e-03.\n",
      "Epoch 148301, Training Loss: 27180, Validation Loss: 56115, 76934.43319973191\n",
      "Epoch 148341: reducing learning rate of group 0 to 2.3440e-03.\n",
      "Epoch 148401, Training Loss: 29234, Validation Loss: 57284, 83336.06546103605\n",
      "Epoch 148442: reducing learning rate of group 0 to 2.3417e-03.\n",
      "Epoch 148501, Training Loss: 27321, Validation Loss: 55556, 91163.23320209188\n",
      "Epoch 148543: reducing learning rate of group 0 to 2.3393e-03.\n",
      "Epoch 148601, Training Loss: 27126, Validation Loss: 55438, 97231.69435627548\n",
      "Epoch 148644: reducing learning rate of group 0 to 2.3370e-03.\n",
      "Epoch 148701, Training Loss: 26933, Validation Loss: 55725, 83229.75617573621\n",
      "Epoch 148745: reducing learning rate of group 0 to 2.3346e-03.\n",
      "Epoch 148801, Training Loss: 28721, Validation Loss: 55437, 78153.35260222481\n",
      "Epoch 148846: reducing learning rate of group 0 to 2.3323e-03.\n",
      "Epoch 148901, Training Loss: 28404, Validation Loss: 55014, 92320.89681075735\n",
      "Epoch 148947: reducing learning rate of group 0 to 2.3300e-03.\n",
      "Epoch 149001, Training Loss: 27584, Validation Loss: 55094, 81363.86043055366\n",
      "Epoch 149048: reducing learning rate of group 0 to 2.3276e-03.\n",
      "Epoch 149101, Training Loss: 31286, Validation Loss: 57002, 80038.82692902627\n",
      "Epoch 149149: reducing learning rate of group 0 to 2.3253e-03.\n",
      "Epoch 149201, Training Loss: 28472, Validation Loss: 56589, 84479.74037454555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149250: reducing learning rate of group 0 to 2.3230e-03.\n",
      "Epoch 149301, Training Loss: 28954, Validation Loss: 55542, 78497.060991479\n",
      "Epoch 149351: reducing learning rate of group 0 to 2.3207e-03.\n",
      "Epoch 149401, Training Loss: 27526, Validation Loss: 55196, 77501.14219468541\n",
      "Epoch 149452: reducing learning rate of group 0 to 2.3183e-03.\n",
      "Epoch 149501, Training Loss: 27616, Validation Loss: 55169, 72396.76879649387\n",
      "Epoch 149553: reducing learning rate of group 0 to 2.3160e-03.\n",
      "Epoch 149601, Training Loss: 27388, Validation Loss: 54645, 85212.62574185517\n",
      "Epoch 149654: reducing learning rate of group 0 to 2.3137e-03.\n",
      "Epoch 149701, Training Loss: 29217, Validation Loss: 57500, 70489.2510163071\n",
      "Epoch 149755: reducing learning rate of group 0 to 2.3114e-03.\n",
      "Epoch 149801, Training Loss: 28945, Validation Loss: 53550, 83848.62406497473\n",
      "Epoch 149856: reducing learning rate of group 0 to 2.3091e-03.\n",
      "Epoch 149901, Training Loss: 29158, Validation Loss: 56000, 91838.50292651712\n",
      "Epoch 149957: reducing learning rate of group 0 to 2.3068e-03.\n",
      "Epoch 150001, Training Loss: 26993, Validation Loss: 55287, 74155.45107067999\n",
      "Epoch 150058: reducing learning rate of group 0 to 2.3045e-03.\n",
      "Epoch 150101, Training Loss: 28351, Validation Loss: 55302, 90870.82262653952\n",
      "Epoch 150159: reducing learning rate of group 0 to 2.3022e-03.\n",
      "Epoch 150201, Training Loss: 28587, Validation Loss: 55094, 101563.50516070901\n",
      "Epoch 150260: reducing learning rate of group 0 to 2.2999e-03.\n",
      "Epoch 150301, Training Loss: 29295, Validation Loss: 56488, 91824.83661708725\n",
      "Epoch 150361: reducing learning rate of group 0 to 2.2976e-03.\n",
      "Epoch 150401, Training Loss: 26233, Validation Loss: 53528, 76851.52214432143\n",
      "Epoch 150462: reducing learning rate of group 0 to 2.2953e-03.\n",
      "Epoch 150501, Training Loss: 29823, Validation Loss: 55806, 82071.61794575806\n",
      "Epoch 150563: reducing learning rate of group 0 to 2.2930e-03.\n",
      "Epoch 150601, Training Loss: 29611, Validation Loss: 54153, 78666.03253747779\n",
      "Epoch 150664: reducing learning rate of group 0 to 2.2907e-03.\n",
      "Epoch 150701, Training Loss: 28106, Validation Loss: 54884, 89723.54420934984\n",
      "Epoch 150765: reducing learning rate of group 0 to 2.2884e-03.\n",
      "Epoch 150801, Training Loss: 27937, Validation Loss: 56841, 70223.1227410966\n",
      "Epoch 150866: reducing learning rate of group 0 to 2.2861e-03.\n",
      "Epoch 150901, Training Loss: 31055, Validation Loss: 55313, 77110.5419414956\n",
      "Epoch 150967: reducing learning rate of group 0 to 2.2838e-03.\n",
      "Epoch 151001, Training Loss: 28832, Validation Loss: 55462, 72033.17979471364\n",
      "Epoch 151068: reducing learning rate of group 0 to 2.2815e-03.\n",
      "Epoch 151101, Training Loss: 28465, Validation Loss: 53844, 86417.9525296337\n",
      "Epoch 151169: reducing learning rate of group 0 to 2.2792e-03.\n",
      "Epoch 151201, Training Loss: 29106, Validation Loss: 54274, 80873.46005858108\n",
      "Epoch 151270: reducing learning rate of group 0 to 2.2770e-03.\n",
      "Epoch 151301, Training Loss: 30412, Validation Loss: 54024, 85620.54062670104\n",
      "Epoch 151371: reducing learning rate of group 0 to 2.2747e-03.\n",
      "Epoch 151401, Training Loss: 29842, Validation Loss: 54086, 98323.61881886527\n",
      "Epoch 151472: reducing learning rate of group 0 to 2.2724e-03.\n",
      "Epoch 151501, Training Loss: 27204, Validation Loss: 55654, 73353.01870612253\n",
      "Epoch 151573: reducing learning rate of group 0 to 2.2701e-03.\n",
      "Epoch 151601, Training Loss: 27487, Validation Loss: 56417, 93859.04763667767\n",
      "Epoch 151674: reducing learning rate of group 0 to 2.2679e-03.\n",
      "Epoch 151701, Training Loss: 28310, Validation Loss: 54843, 82142.72245416084\n",
      "Epoch 151775: reducing learning rate of group 0 to 2.2656e-03.\n",
      "Epoch 151801, Training Loss: 27493, Validation Loss: 55297, 78162.6968045047\n",
      "Epoch 151876: reducing learning rate of group 0 to 2.2633e-03.\n",
      "Epoch 151901, Training Loss: 28462, Validation Loss: 54993, 119740.889930371\n",
      "Epoch 151977: reducing learning rate of group 0 to 2.2611e-03.\n",
      "Epoch 152001, Training Loss: 27242, Validation Loss: 56060, 96149.47410156722\n",
      "Epoch 152078: reducing learning rate of group 0 to 2.2588e-03.\n",
      "Epoch 152101, Training Loss: 29015, Validation Loss: 56021, 67777.01829575008\n",
      "Epoch 152179: reducing learning rate of group 0 to 2.2566e-03.\n",
      "Epoch 152201, Training Loss: 30278, Validation Loss: 55649, 99164.57778342727\n",
      "Epoch 152280: reducing learning rate of group 0 to 2.2543e-03.\n",
      "Epoch 152301, Training Loss: 29450, Validation Loss: 54912, 94284.79967617022\n",
      "Epoch 152381: reducing learning rate of group 0 to 2.2520e-03.\n",
      "Epoch 152401, Training Loss: 27391, Validation Loss: 55620, 75490.3104511169\n",
      "Epoch 152482: reducing learning rate of group 0 to 2.2498e-03.\n",
      "Epoch 152501, Training Loss: 29009, Validation Loss: 54861, 84919.22314624266\n",
      "Epoch 152583: reducing learning rate of group 0 to 2.2475e-03.\n",
      "Epoch 152601, Training Loss: 29567, Validation Loss: 56278, 98839.42676697492\n",
      "Epoch 152684: reducing learning rate of group 0 to 2.2453e-03.\n",
      "Epoch 152701, Training Loss: 29065, Validation Loss: 54404, 75257.63206749491\n",
      "Epoch 152785: reducing learning rate of group 0 to 2.2431e-03.\n",
      "Epoch 152801, Training Loss: 29526, Validation Loss: 53834, 93371.8209037232\n",
      "Epoch 152886: reducing learning rate of group 0 to 2.2408e-03.\n",
      "Epoch 152901, Training Loss: 26707, Validation Loss: 53384, 74148.65712603525\n",
      "Epoch 152987: reducing learning rate of group 0 to 2.2386e-03.\n",
      "Epoch 153001, Training Loss: 27240, Validation Loss: 55111, 102366.25330708502\n",
      "Epoch 153088: reducing learning rate of group 0 to 2.2363e-03.\n",
      "Epoch 153101, Training Loss: 28097, Validation Loss: 54794, 86580.52195964544\n",
      "Epoch 153189: reducing learning rate of group 0 to 2.2341e-03.\n",
      "Epoch 153201, Training Loss: 30352, Validation Loss: 56674, 92000.30930937838\n",
      "Epoch 153290: reducing learning rate of group 0 to 2.2319e-03.\n",
      "Epoch 153301, Training Loss: 28698, Validation Loss: 54154, 91977.63918432298\n",
      "Epoch 153391: reducing learning rate of group 0 to 2.2296e-03.\n",
      "Epoch 153401, Training Loss: 29343, Validation Loss: 56598, 76671.46249201639\n",
      "Epoch 153492: reducing learning rate of group 0 to 2.2274e-03.\n",
      "Epoch 153501, Training Loss: 31626, Validation Loss: 55262, 83723.61680952965\n",
      "Epoch 153593: reducing learning rate of group 0 to 2.2252e-03.\n",
      "Epoch 153601, Training Loss: 28267, Validation Loss: 55485, 80086.89643283411\n",
      "Epoch 153694: reducing learning rate of group 0 to 2.2229e-03.\n",
      "Epoch 153701, Training Loss: 29116, Validation Loss: 56454, 71714.98291720402\n",
      "Epoch 153795: reducing learning rate of group 0 to 2.2207e-03.\n",
      "Epoch 153801, Training Loss: 26585, Validation Loss: 56723, 98324.16351620806\n",
      "Epoch 153896: reducing learning rate of group 0 to 2.2185e-03.\n",
      "Epoch 153901, Training Loss: 29899, Validation Loss: 56698, 107028.01904203703\n",
      "Epoch 153997: reducing learning rate of group 0 to 2.2163e-03.\n",
      "Epoch 154001, Training Loss: 27810, Validation Loss: 55989, 77122.71931509816\n",
      "Epoch 154098: reducing learning rate of group 0 to 2.2141e-03.\n",
      "Epoch 154101, Training Loss: 30412, Validation Loss: 52895, 93058.96216019854\n",
      "Epoch 154199: reducing learning rate of group 0 to 2.2119e-03.\n",
      "Epoch 154201, Training Loss: 27874, Validation Loss: 54585, 76503.95342897408\n",
      "Epoch 154300: reducing learning rate of group 0 to 2.2096e-03.\n",
      "Epoch 154301, Training Loss: 28089, Validation Loss: 54790, 75280.2367517351\n",
      "Epoch 154401: reducing learning rate of group 0 to 2.2074e-03.\n",
      "Epoch 154401, Training Loss: 28436, Validation Loss: 55460, 76634.59717951129\n",
      "Epoch 154501, Training Loss: 26183, Validation Loss: 53445, 86920.81815360441\n",
      "Epoch 154502: reducing learning rate of group 0 to 2.2052e-03.\n",
      "Epoch 154601, Training Loss: 29081, Validation Loss: 55219, 86901.68238069233\n",
      "Epoch 154603: reducing learning rate of group 0 to 2.2030e-03.\n",
      "Epoch 154701, Training Loss: 29095, Validation Loss: 53835, 86296.15402086945\n",
      "Epoch 154704: reducing learning rate of group 0 to 2.2008e-03.\n",
      "Epoch 154801, Training Loss: 30127, Validation Loss: 55059, 80913.10675476691\n",
      "Epoch 154805: reducing learning rate of group 0 to 2.1986e-03.\n",
      "Epoch 154901, Training Loss: 28065, Validation Loss: 55134, 94217.04720171861\n",
      "Epoch 154906: reducing learning rate of group 0 to 2.1964e-03.\n",
      "Epoch 155001, Training Loss: 30206, Validation Loss: 56120, 73914.7573182209\n",
      "Epoch 155007: reducing learning rate of group 0 to 2.1942e-03.\n",
      "Epoch 155101, Training Loss: 27551, Validation Loss: 54347, 90522.0303317989\n",
      "Epoch 155108: reducing learning rate of group 0 to 2.1920e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155201, Training Loss: 27634, Validation Loss: 56173, 81300.07723061663\n",
      "Epoch 155209: reducing learning rate of group 0 to 2.1898e-03.\n",
      "Epoch 155301, Training Loss: 28855, Validation Loss: 56241, 84241.90534427334\n",
      "Epoch 155310: reducing learning rate of group 0 to 2.1876e-03.\n",
      "Epoch 155401, Training Loss: 27961, Validation Loss: 54899, 89433.44712604203\n",
      "Epoch 155411: reducing learning rate of group 0 to 2.1855e-03.\n",
      "Epoch 155501, Training Loss: 28596, Validation Loss: 53799, 72015.50396300408\n",
      "Epoch 155512: reducing learning rate of group 0 to 2.1833e-03.\n",
      "Epoch 155601, Training Loss: 28262, Validation Loss: 55990, 83509.90625616357\n",
      "Epoch 155613: reducing learning rate of group 0 to 2.1811e-03.\n",
      "Epoch 155701, Training Loss: 28684, Validation Loss: 54137, 93454.27643345977\n",
      "Epoch 155714: reducing learning rate of group 0 to 2.1789e-03.\n",
      "Epoch 155801, Training Loss: 26866, Validation Loss: 53885, 73267.93599552097\n",
      "Epoch 155815: reducing learning rate of group 0 to 2.1767e-03.\n",
      "Epoch 155901, Training Loss: 27470, Validation Loss: 54053, 90623.79646087218\n",
      "Epoch 155916: reducing learning rate of group 0 to 2.1746e-03.\n",
      "Epoch 156001, Training Loss: 29881, Validation Loss: 57041, 91271.21691020862\n",
      "Epoch 156017: reducing learning rate of group 0 to 2.1724e-03.\n",
      "Epoch 156101, Training Loss: 28153, Validation Loss: 56140, 92789.7361829386\n",
      "Epoch 156118: reducing learning rate of group 0 to 2.1702e-03.\n",
      "Epoch 156201, Training Loss: 29630, Validation Loss: 55710, 83484.44719189839\n",
      "Epoch 156219: reducing learning rate of group 0 to 2.1680e-03.\n",
      "Epoch 156301, Training Loss: 27338, Validation Loss: 55546, 97144.94795552403\n",
      "Epoch 156320: reducing learning rate of group 0 to 2.1659e-03.\n",
      "Epoch 156401, Training Loss: 28663, Validation Loss: 55336, 78840.34638324122\n",
      "Epoch 156421: reducing learning rate of group 0 to 2.1637e-03.\n",
      "Epoch 156501, Training Loss: 28249, Validation Loss: 55141, 95094.14261664147\n",
      "Epoch 156522: reducing learning rate of group 0 to 2.1615e-03.\n",
      "Epoch 156601, Training Loss: 28428, Validation Loss: 106330, 94771.96253618873\n",
      "Epoch 156623: reducing learning rate of group 0 to 2.1594e-03.\n",
      "Epoch 156701, Training Loss: 28688, Validation Loss: 54458, 89120.73251586869\n",
      "Epoch 156724: reducing learning rate of group 0 to 2.1572e-03.\n",
      "Epoch 156801, Training Loss: 30269, Validation Loss: 54245, 86045.92767239263\n",
      "Epoch 156825: reducing learning rate of group 0 to 2.1551e-03.\n",
      "Epoch 156901, Training Loss: 27625, Validation Loss: 56872, 95689.43557046882\n",
      "Epoch 156926: reducing learning rate of group 0 to 2.1529e-03.\n",
      "Epoch 157001, Training Loss: 29061, Validation Loss: 54905, 85191.62479543699\n",
      "Epoch 157027: reducing learning rate of group 0 to 2.1507e-03.\n",
      "Epoch 157101, Training Loss: 28461, Validation Loss: 54274, 80057.50447913818\n",
      "Epoch 157128: reducing learning rate of group 0 to 2.1486e-03.\n",
      "Epoch 157201, Training Loss: 30263, Validation Loss: 55248, 92710.93463821539\n",
      "Epoch 157229: reducing learning rate of group 0 to 2.1465e-03.\n",
      "Epoch 157301, Training Loss: 28223, Validation Loss: 55098, 110862.88265852374\n",
      "Epoch 157330: reducing learning rate of group 0 to 2.1443e-03.\n",
      "Epoch 157401, Training Loss: 27744, Validation Loss: 55470, 72107.17185611626\n",
      "Epoch 157431: reducing learning rate of group 0 to 2.1422e-03.\n",
      "Epoch 157501, Training Loss: 29880, Validation Loss: 54486, 94262.70182507888\n",
      "Epoch 157532: reducing learning rate of group 0 to 2.1400e-03.\n",
      "Epoch 157601, Training Loss: 28200, Validation Loss: 55125, 81789.39448001357\n",
      "Epoch 157633: reducing learning rate of group 0 to 2.1379e-03.\n",
      "Epoch 157701, Training Loss: 27860, Validation Loss: 54860, 78277.37058105323\n",
      "Epoch 157734: reducing learning rate of group 0 to 2.1357e-03.\n",
      "Epoch 157801, Training Loss: 27846, Validation Loss: 55156, 98256.84394422318\n",
      "Epoch 157835: reducing learning rate of group 0 to 2.1336e-03.\n",
      "Epoch 157901, Training Loss: 29659, Validation Loss: 54419, 85957.50064636122\n",
      "Epoch 157936: reducing learning rate of group 0 to 2.1315e-03.\n",
      "Epoch 158001, Training Loss: 26372, Validation Loss: 54286, 90165.41772873816\n",
      "Epoch 158037: reducing learning rate of group 0 to 2.1293e-03.\n",
      "Epoch 158101, Training Loss: 28016, Validation Loss: 54537, 77950.3008708096\n",
      "Epoch 158138: reducing learning rate of group 0 to 2.1272e-03.\n",
      "Epoch 158201, Training Loss: 26891, Validation Loss: 55315, 86194.12813424942\n",
      "Epoch 158239: reducing learning rate of group 0 to 2.1251e-03.\n",
      "Epoch 158301, Training Loss: 28427, Validation Loss: 54985, 97854.87662337837\n",
      "Epoch 158340: reducing learning rate of group 0 to 2.1230e-03.\n",
      "Epoch 158401, Training Loss: 28477, Validation Loss: 54302, 75490.23849275043\n",
      "Epoch 158441: reducing learning rate of group 0 to 2.1208e-03.\n",
      "Epoch 158501, Training Loss: 35224, Validation Loss: 56071, 91828.5772709673\n",
      "Epoch 158542: reducing learning rate of group 0 to 2.1187e-03.\n",
      "Epoch 158601, Training Loss: 27443, Validation Loss: 54145, 91587.34270561796\n",
      "Epoch 158643: reducing learning rate of group 0 to 2.1166e-03.\n",
      "Epoch 158701, Training Loss: 31919, Validation Loss: 55311, 95395.73137434851\n",
      "Epoch 158744: reducing learning rate of group 0 to 2.1145e-03.\n",
      "Epoch 158801, Training Loss: 30223, Validation Loss: 57371, 98054.77987486015\n",
      "Epoch 158845: reducing learning rate of group 0 to 2.1124e-03.\n",
      "Epoch 158901, Training Loss: 28390, Validation Loss: 55046, 85549.02206200093\n",
      "Epoch 158946: reducing learning rate of group 0 to 2.1103e-03.\n",
      "Epoch 159001, Training Loss: 28113, Validation Loss: 56310, 80842.71698339982\n",
      "Epoch 159047: reducing learning rate of group 0 to 2.1081e-03.\n",
      "Epoch 159101, Training Loss: 29695, Validation Loss: 54832, 76165.72598275082\n",
      "Epoch 159148: reducing learning rate of group 0 to 2.1060e-03.\n",
      "Epoch 159201, Training Loss: 30238, Validation Loss: 55945, 91630.92703299945\n",
      "Epoch 159249: reducing learning rate of group 0 to 2.1039e-03.\n",
      "Epoch 159301, Training Loss: 28194, Validation Loss: 54922, 109273.57607472049\n",
      "Epoch 159350: reducing learning rate of group 0 to 2.1018e-03.\n",
      "Epoch 159401, Training Loss: 28107, Validation Loss: 55626, 78055.88998829246\n",
      "Epoch 159451: reducing learning rate of group 0 to 2.0997e-03.\n",
      "Epoch 159501, Training Loss: 28680, Validation Loss: 53346, 86712.57382056372\n",
      "Epoch 159552: reducing learning rate of group 0 to 2.0976e-03.\n",
      "Epoch 159601, Training Loss: 29061, Validation Loss: 57238, 88803.79923713517\n",
      "Epoch 159653: reducing learning rate of group 0 to 2.0955e-03.\n",
      "Epoch 159701, Training Loss: 29157, Validation Loss: 56794, 95140.86727208969\n",
      "Epoch 159754: reducing learning rate of group 0 to 2.0934e-03.\n",
      "Epoch 159801, Training Loss: 27473, Validation Loss: 55278, 85823.50196657081\n",
      "Epoch 159855: reducing learning rate of group 0 to 2.0913e-03.\n",
      "Epoch 159901, Training Loss: 26821, Validation Loss: 55711, 80867.12920618978\n",
      "Epoch 159956: reducing learning rate of group 0 to 2.0892e-03.\n",
      "Epoch 160001, Training Loss: 28090, Validation Loss: 54490, 78530.89116209344\n",
      "Epoch 160057: reducing learning rate of group 0 to 2.0872e-03.\n",
      "Epoch 160101, Training Loss: 27738, Validation Loss: 54937, 90446.0605734068\n",
      "Epoch 160158: reducing learning rate of group 0 to 2.0851e-03.\n",
      "Epoch 160201, Training Loss: 27259, Validation Loss: 55574, 77248.14693651797\n",
      "Epoch 160259: reducing learning rate of group 0 to 2.0830e-03.\n",
      "Epoch 160301, Training Loss: 31766, Validation Loss: 57057, 94471.76923978695\n",
      "Epoch 160360: reducing learning rate of group 0 to 2.0809e-03.\n",
      "Epoch 160401, Training Loss: 26449, Validation Loss: 55279, 81645.14939287864\n",
      "Epoch 160461: reducing learning rate of group 0 to 2.0788e-03.\n",
      "Epoch 160501, Training Loss: 27593, Validation Loss: 55035, 100732.12934231311\n",
      "Epoch 160562: reducing learning rate of group 0 to 2.0767e-03.\n",
      "Epoch 160601, Training Loss: 27463, Validation Loss: 54955, 79642.41688664738\n",
      "Epoch 160663: reducing learning rate of group 0 to 2.0747e-03.\n",
      "Epoch 160701, Training Loss: 29662, Validation Loss: 54638, 91161.13668672094\n",
      "Epoch 160764: reducing learning rate of group 0 to 2.0726e-03.\n",
      "Epoch 160801, Training Loss: 30462, Validation Loss: 55619, 112650.27708463855\n",
      "Epoch 160865: reducing learning rate of group 0 to 2.0705e-03.\n",
      "Epoch 160901, Training Loss: 28382, Validation Loss: 54433, 92953.49196875076\n",
      "Epoch 160966: reducing learning rate of group 0 to 2.0684e-03.\n",
      "Epoch 161001, Training Loss: 29044, Validation Loss: 56198, 86939.57254327816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161067: reducing learning rate of group 0 to 2.0664e-03.\n",
      "Epoch 161101, Training Loss: 28618, Validation Loss: 54778, 89742.93608873589\n",
      "Epoch 161168: reducing learning rate of group 0 to 2.0643e-03.\n",
      "Epoch 161201, Training Loss: 28348, Validation Loss: 53490, 81600.51210590889\n",
      "Epoch 161269: reducing learning rate of group 0 to 2.0622e-03.\n",
      "Epoch 161301, Training Loss: 27898, Validation Loss: 55242, 86609.98105054814\n",
      "Epoch 161370: reducing learning rate of group 0 to 2.0602e-03.\n",
      "Epoch 161401, Training Loss: 29057, Validation Loss: 54365, 78764.4353709584\n",
      "Epoch 161471: reducing learning rate of group 0 to 2.0581e-03.\n",
      "Epoch 161501, Training Loss: 29235, Validation Loss: 55610, 103627.34358062537\n",
      "Epoch 161572: reducing learning rate of group 0 to 2.0561e-03.\n",
      "Epoch 161601, Training Loss: 28417, Validation Loss: 55187, 71021.22549874494\n",
      "Epoch 161673: reducing learning rate of group 0 to 2.0540e-03.\n",
      "Epoch 161701, Training Loss: 29796, Validation Loss: 54096, 82639.90169566472\n",
      "Epoch 161774: reducing learning rate of group 0 to 2.0520e-03.\n",
      "Epoch 161801, Training Loss: 29063, Validation Loss: 53900, 112624.57840493531\n",
      "Epoch 161875: reducing learning rate of group 0 to 2.0499e-03.\n",
      "Epoch 161901, Training Loss: 25821, Validation Loss: 54123, 75537.9125941034\n",
      "Epoch 161976: reducing learning rate of group 0 to 2.0479e-03.\n",
      "Epoch 162001, Training Loss: 28837, Validation Loss: 55953, 84813.09858841714\n",
      "Epoch 162077: reducing learning rate of group 0 to 2.0458e-03.\n",
      "Epoch 162101, Training Loss: 30112, Validation Loss: 56522, 84189.22974829144\n",
      "Epoch 162178: reducing learning rate of group 0 to 2.0438e-03.\n",
      "Epoch 162201, Training Loss: 29661, Validation Loss: 54421, 93125.87987839761\n",
      "Epoch 162279: reducing learning rate of group 0 to 2.0417e-03.\n",
      "Epoch 162301, Training Loss: 28928, Validation Loss: 53870, 73931.52019010471\n",
      "Epoch 162380: reducing learning rate of group 0 to 2.0397e-03.\n",
      "Epoch 162401, Training Loss: 29034, Validation Loss: 54240, 87603.3167498955\n",
      "Epoch 162481: reducing learning rate of group 0 to 2.0376e-03.\n",
      "Epoch 162501, Training Loss: 29673, Validation Loss: 58218, 94697.6481001563\n",
      "Epoch 162582: reducing learning rate of group 0 to 2.0356e-03.\n",
      "Epoch 162601, Training Loss: 29434, Validation Loss: 54762, 96214.91774496001\n",
      "Epoch 162683: reducing learning rate of group 0 to 2.0336e-03.\n",
      "Epoch 162701, Training Loss: 26646, Validation Loss: 53977, 87651.09946439463\n",
      "Epoch 162784: reducing learning rate of group 0 to 2.0315e-03.\n",
      "Epoch 162801, Training Loss: 26329, Validation Loss: 54772, 74475.30731515402\n",
      "Epoch 162885: reducing learning rate of group 0 to 2.0295e-03.\n",
      "Epoch 162901, Training Loss: 26230, Validation Loss: 55273, 90776.85253724812\n",
      "Epoch 162986: reducing learning rate of group 0 to 2.0275e-03.\n",
      "Epoch 163001, Training Loss: 27754, Validation Loss: 54342, 89225.0962845356\n",
      "Epoch 163087: reducing learning rate of group 0 to 2.0254e-03.\n",
      "Epoch 163101, Training Loss: 29623, Validation Loss: 53775, 91138.64334120606\n",
      "Epoch 163188: reducing learning rate of group 0 to 2.0234e-03.\n",
      "Epoch 163201, Training Loss: 27856, Validation Loss: 54778, 78896.94632807634\n",
      "Epoch 163289: reducing learning rate of group 0 to 2.0214e-03.\n",
      "Epoch 163301, Training Loss: 28516, Validation Loss: 54478, 86927.7762093966\n",
      "Epoch 163390: reducing learning rate of group 0 to 2.0194e-03.\n",
      "Epoch 163401, Training Loss: 25898, Validation Loss: 53569, 92516.69001673789\n",
      "Epoch 163491: reducing learning rate of group 0 to 2.0173e-03.\n",
      "Epoch 163501, Training Loss: 28880, Validation Loss: 58142, 81200.1938345078\n",
      "Epoch 163592: reducing learning rate of group 0 to 2.0153e-03.\n",
      "Epoch 163601, Training Loss: 29203, Validation Loss: 54397, 98799.86122200971\n",
      "Epoch 163693: reducing learning rate of group 0 to 2.0133e-03.\n",
      "Epoch 163701, Training Loss: 28803, Validation Loss: 56764, 89020.08400020031\n",
      "Epoch 163794: reducing learning rate of group 0 to 2.0113e-03.\n",
      "Epoch 163801, Training Loss: 27942, Validation Loss: 54652, 94041.83755757874\n",
      "Epoch 163895: reducing learning rate of group 0 to 2.0093e-03.\n",
      "Epoch 163901, Training Loss: 27211, Validation Loss: 55879, 75763.12513371995\n",
      "Epoch 163996: reducing learning rate of group 0 to 2.0073e-03.\n",
      "Epoch 164001, Training Loss: 27495, Validation Loss: 54197, 75091.90841527138\n",
      "Epoch 164097: reducing learning rate of group 0 to 2.0053e-03.\n",
      "Epoch 164101, Training Loss: 29385, Validation Loss: 54326, 78846.7682975421\n",
      "Epoch 164198: reducing learning rate of group 0 to 2.0033e-03.\n",
      "Epoch 164201, Training Loss: 27978, Validation Loss: 55503, 98672.41665151618\n",
      "Epoch 164299: reducing learning rate of group 0 to 2.0013e-03.\n",
      "Epoch 164301, Training Loss: 27499, Validation Loss: 56132, 85571.22111352651\n",
      "Epoch 164400: reducing learning rate of group 0 to 1.9993e-03.\n",
      "Epoch 164401, Training Loss: 27817, Validation Loss: 56002, 79977.79849484087\n",
      "Epoch 164501: reducing learning rate of group 0 to 1.9973e-03.\n",
      "Epoch 164501, Training Loss: 31528, Validation Loss: 53497, 86551.75447515829\n",
      "Epoch 164601, Training Loss: 26623, Validation Loss: 54547, 75543.58356360474\n",
      "Epoch 164602: reducing learning rate of group 0 to 1.9953e-03.\n",
      "Epoch 164701, Training Loss: 30001, Validation Loss: 53932, 80282.80236717426\n",
      "Epoch 164703: reducing learning rate of group 0 to 1.9933e-03.\n",
      "Epoch 164801, Training Loss: 27938, Validation Loss: 57240, 94465.79373100463\n",
      "Epoch 164804: reducing learning rate of group 0 to 1.9913e-03.\n",
      "Epoch 164901, Training Loss: 25744, Validation Loss: 54978, 85793.65252600257\n",
      "Epoch 164905: reducing learning rate of group 0 to 1.9893e-03.\n",
      "Epoch 165001, Training Loss: 28816, Validation Loss: 55040, 93886.49697897247\n",
      "Epoch 165006: reducing learning rate of group 0 to 1.9873e-03.\n",
      "Epoch 165101, Training Loss: 29250, Validation Loss: 56225, 100908.08765185403\n",
      "Epoch 165107: reducing learning rate of group 0 to 1.9853e-03.\n",
      "Epoch 165201, Training Loss: 28368, Validation Loss: 55895, 95807.65608073899\n",
      "Epoch 165208: reducing learning rate of group 0 to 1.9833e-03.\n",
      "Epoch 165301, Training Loss: 28451, Validation Loss: 56846, 75901.22869556338\n",
      "Epoch 165309: reducing learning rate of group 0 to 1.9813e-03.\n",
      "Epoch 165401, Training Loss: 27187, Validation Loss: 56943, 79034.45731432045\n",
      "Epoch 165410: reducing learning rate of group 0 to 1.9794e-03.\n",
      "Epoch 165501, Training Loss: 28776, Validation Loss: 55798, 114183.94124955118\n",
      "Epoch 165511: reducing learning rate of group 0 to 1.9774e-03.\n",
      "Epoch 165601, Training Loss: 28413, Validation Loss: 54222, 107347.91467336654\n",
      "Epoch 165612: reducing learning rate of group 0 to 1.9754e-03.\n",
      "Epoch 165701, Training Loss: 27181, Validation Loss: 54208, 79957.53275906465\n",
      "Epoch 165713: reducing learning rate of group 0 to 1.9734e-03.\n",
      "Epoch 165801, Training Loss: 27635, Validation Loss: 54164, 79278.65536875326\n",
      "Epoch 165814: reducing learning rate of group 0 to 1.9715e-03.\n",
      "Epoch 165901, Training Loss: 25692, Validation Loss: 55161, 87576.17749990158\n",
      "Epoch 165915: reducing learning rate of group 0 to 1.9695e-03.\n",
      "Epoch 166001, Training Loss: 29410, Validation Loss: 55424, 107660.17865886858\n",
      "Epoch 166016: reducing learning rate of group 0 to 1.9675e-03.\n",
      "Epoch 166101, Training Loss: 29430, Validation Loss: 55624, 74645.42843408821\n",
      "Epoch 166117: reducing learning rate of group 0 to 1.9655e-03.\n",
      "Epoch 166201, Training Loss: 28727, Validation Loss: 55841, 95479.37699386808\n",
      "Epoch 166218: reducing learning rate of group 0 to 1.9636e-03.\n",
      "Epoch 166301, Training Loss: 28748, Validation Loss: 55723, 84477.72052208088\n",
      "Epoch 166319: reducing learning rate of group 0 to 1.9616e-03.\n",
      "Epoch 166401, Training Loss: 27996, Validation Loss: 56472, 108861.47218495207\n",
      "Epoch 166420: reducing learning rate of group 0 to 1.9597e-03.\n",
      "Epoch 166501, Training Loss: 32809, Validation Loss: 56150, 97207.45316193411\n",
      "Epoch 166521: reducing learning rate of group 0 to 1.9577e-03.\n",
      "Epoch 166601, Training Loss: 27645, Validation Loss: 54650, 89027.67382418743\n",
      "Epoch 166622: reducing learning rate of group 0 to 1.9557e-03.\n",
      "Epoch 166701, Training Loss: 29761, Validation Loss: 54412, 112977.42299213097\n",
      "Epoch 166723: reducing learning rate of group 0 to 1.9538e-03.\n",
      "Epoch 166801, Training Loss: 27328, Validation Loss: 55053, 102015.63346524806\n",
      "Epoch 166824: reducing learning rate of group 0 to 1.9518e-03.\n",
      "Epoch 166901, Training Loss: 28386, Validation Loss: 55749, 84733.47113217205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166925: reducing learning rate of group 0 to 1.9499e-03.\n",
      "Epoch 167001, Training Loss: 27692, Validation Loss: 56176, 105023.01635243504\n",
      "Epoch 167026: reducing learning rate of group 0 to 1.9479e-03.\n",
      "Epoch 167101, Training Loss: 27734, Validation Loss: 55750, 98820.65487524215\n",
      "Epoch 167127: reducing learning rate of group 0 to 1.9460e-03.\n",
      "Epoch 167201, Training Loss: 27575, Validation Loss: 52924, 83482.68113199237\n",
      "Epoch 167228: reducing learning rate of group 0 to 1.9440e-03.\n",
      "Epoch 167301, Training Loss: 28185, Validation Loss: 56181, 81198.3748158553\n",
      "Epoch 167329: reducing learning rate of group 0 to 1.9421e-03.\n",
      "Epoch 167401, Training Loss: 27433, Validation Loss: 55151, 82842.36248263935\n",
      "Epoch 167430: reducing learning rate of group 0 to 1.9401e-03.\n",
      "Epoch 167501, Training Loss: 28732, Validation Loss: 53630, 89610.67876415445\n",
      "Epoch 167531: reducing learning rate of group 0 to 1.9382e-03.\n",
      "Epoch 167601, Training Loss: 30208, Validation Loss: 53915, 104260.19471847377\n",
      "Epoch 167632: reducing learning rate of group 0 to 1.9363e-03.\n",
      "Epoch 167701, Training Loss: 27377, Validation Loss: 54671, 94690.8006696349\n",
      "Epoch 167733: reducing learning rate of group 0 to 1.9343e-03.\n",
      "Epoch 167801, Training Loss: 27441, Validation Loss: 55969, 110076.18455895588\n",
      "Epoch 167834: reducing learning rate of group 0 to 1.9324e-03.\n",
      "Epoch 167901, Training Loss: 27927, Validation Loss: 56491, 97821.01079762298\n",
      "Epoch 167935: reducing learning rate of group 0 to 1.9305e-03.\n",
      "Epoch 168001, Training Loss: 26090, Validation Loss: 55289, 93130.1977799498\n",
      "Epoch 168036: reducing learning rate of group 0 to 1.9285e-03.\n",
      "Epoch 168101, Training Loss: 26771, Validation Loss: 55298, 85478.04585679925\n",
      "Epoch 168137: reducing learning rate of group 0 to 1.9266e-03.\n",
      "Epoch 168201, Training Loss: 28876, Validation Loss: 55221, 91661.02041794277\n",
      "Epoch 168238: reducing learning rate of group 0 to 1.9247e-03.\n",
      "Epoch 168301, Training Loss: 28874, Validation Loss: 55778, 93997.79833585052\n",
      "Epoch 168339: reducing learning rate of group 0 to 1.9228e-03.\n",
      "Epoch 168401, Training Loss: 25930, Validation Loss: 54110, 87476.60970432371\n",
      "Epoch 168440: reducing learning rate of group 0 to 1.9208e-03.\n",
      "Epoch 168501, Training Loss: 30541, Validation Loss: 55862, 81918.47130327363\n",
      "Epoch 168541: reducing learning rate of group 0 to 1.9189e-03.\n",
      "Epoch 168601, Training Loss: 27508, Validation Loss: 54259, 100712.19964239601\n",
      "Epoch 168642: reducing learning rate of group 0 to 1.9170e-03.\n",
      "Epoch 168701, Training Loss: 29152, Validation Loss: 55978, 103965.5935463402\n",
      "Epoch 168743: reducing learning rate of group 0 to 1.9151e-03.\n",
      "Epoch 168801, Training Loss: 29903, Validation Loss: 55064, 106085.21891762379\n",
      "Epoch 168844: reducing learning rate of group 0 to 1.9132e-03.\n",
      "Epoch 168901, Training Loss: 28064, Validation Loss: 56735, 89018.42472149641\n",
      "Epoch 168945: reducing learning rate of group 0 to 1.9113e-03.\n",
      "Epoch 169001, Training Loss: 27496, Validation Loss: 54354, 68632.94184384322\n",
      "Epoch 169046: reducing learning rate of group 0 to 1.9093e-03.\n",
      "Epoch 169101, Training Loss: 28648, Validation Loss: 56184, 89608.06395470379\n",
      "Epoch 169147: reducing learning rate of group 0 to 1.9074e-03.\n",
      "Epoch 169201, Training Loss: 28027, Validation Loss: 56557, 94261.70164453896\n",
      "Epoch 169248: reducing learning rate of group 0 to 1.9055e-03.\n",
      "Epoch 169301, Training Loss: 25006, Validation Loss: 54576, 91275.10049706422\n",
      "Epoch 169349: reducing learning rate of group 0 to 1.9036e-03.\n",
      "Epoch 169401, Training Loss: 28176, Validation Loss: 55578, 94047.97670140327\n",
      "Epoch 169450: reducing learning rate of group 0 to 1.9017e-03.\n",
      "Epoch 169501, Training Loss: 28403, Validation Loss: 55355, 81985.46525752767\n",
      "Epoch 169551: reducing learning rate of group 0 to 1.8998e-03.\n",
      "Epoch 169601, Training Loss: 27166, Validation Loss: 54170, 98033.27116953449\n",
      "Epoch 169652: reducing learning rate of group 0 to 1.8979e-03.\n",
      "Epoch 169701, Training Loss: 25683, Validation Loss: 55169, 78001.46470918275\n",
      "Epoch 169753: reducing learning rate of group 0 to 1.8960e-03.\n",
      "Epoch 169801, Training Loss: 27799, Validation Loss: 54164, 91478.93290345476\n",
      "Epoch 169854: reducing learning rate of group 0 to 1.8941e-03.\n",
      "Epoch 169901, Training Loss: 26631, Validation Loss: 56190, 97080.71419646293\n",
      "Epoch 169955: reducing learning rate of group 0 to 1.8922e-03.\n",
      "Epoch 170001, Training Loss: 27087, Validation Loss: 55742, 109287.55492982583\n",
      "Epoch 170056: reducing learning rate of group 0 to 1.8903e-03.\n",
      "Epoch 170101, Training Loss: 29445, Validation Loss: 55622, 85269.90831411978\n",
      "Epoch 170157: reducing learning rate of group 0 to 1.8884e-03.\n",
      "Epoch 170201, Training Loss: 28003, Validation Loss: 55380, 109509.01490430348\n",
      "Epoch 170258: reducing learning rate of group 0 to 1.8866e-03.\n",
      "Epoch 170301, Training Loss: 30748, Validation Loss: 53316, 91286.67648712144\n",
      "Epoch 170359: reducing learning rate of group 0 to 1.8847e-03.\n",
      "Epoch 170401, Training Loss: 27363, Validation Loss: 55387, 73924.66259972636\n",
      "Epoch 170460: reducing learning rate of group 0 to 1.8828e-03.\n",
      "Epoch 170501, Training Loss: 29611, Validation Loss: 55079, 108181.7232484774\n",
      "Epoch 170561: reducing learning rate of group 0 to 1.8809e-03.\n",
      "Epoch 170601, Training Loss: 27392, Validation Loss: 55906, 80157.69449674261\n",
      "Epoch 170662: reducing learning rate of group 0 to 1.8790e-03.\n",
      "Epoch 170701, Training Loss: 26421, Validation Loss: 55248, 91174.08675132511\n",
      "Epoch 170763: reducing learning rate of group 0 to 1.8771e-03.\n",
      "Epoch 170801, Training Loss: 26638, Validation Loss: 54880, 89838.88499245956\n",
      "Epoch 170864: reducing learning rate of group 0 to 1.8753e-03.\n",
      "Epoch 170901, Training Loss: 26450, Validation Loss: 54659, 84310.31933867618\n",
      "Epoch 170965: reducing learning rate of group 0 to 1.8734e-03.\n",
      "Epoch 171001, Training Loss: 26872, Validation Loss: 55595, 91881.92627026186\n",
      "Epoch 171066: reducing learning rate of group 0 to 1.8715e-03.\n",
      "Epoch 171101, Training Loss: 27823, Validation Loss: 55447, 85033.83658033119\n",
      "Epoch 171167: reducing learning rate of group 0 to 1.8696e-03.\n",
      "Epoch 171201, Training Loss: 29947, Validation Loss: 54161, 76855.46997779862\n",
      "Epoch 171268: reducing learning rate of group 0 to 1.8678e-03.\n",
      "Epoch 171301, Training Loss: 26529, Validation Loss: 55119, 84296.03974468897\n",
      "Epoch 171369: reducing learning rate of group 0 to 1.8659e-03.\n",
      "Epoch 171401, Training Loss: 29283, Validation Loss: 55697, 104281.09864094888\n",
      "Epoch 171470: reducing learning rate of group 0 to 1.8640e-03.\n",
      "Epoch 171501, Training Loss: 29104, Validation Loss: 55550, 97889.91425256636\n",
      "Epoch 171571: reducing learning rate of group 0 to 1.8622e-03.\n",
      "Epoch 171601, Training Loss: 28357, Validation Loss: 54645, 89179.5219318157\n",
      "Epoch 171672: reducing learning rate of group 0 to 1.8603e-03.\n",
      "Epoch 171701, Training Loss: 27056, Validation Loss: 56618, 95434.51767134642\n",
      "Epoch 171773: reducing learning rate of group 0 to 1.8585e-03.\n",
      "Epoch 171801, Training Loss: 26423, Validation Loss: 57113, 80046.69607846219\n",
      "Epoch 171874: reducing learning rate of group 0 to 1.8566e-03.\n",
      "Epoch 171901, Training Loss: 27825, Validation Loss: 56809, 96555.87262959378\n",
      "Epoch 171975: reducing learning rate of group 0 to 1.8547e-03.\n",
      "Epoch 172001, Training Loss: 26920, Validation Loss: 55457, 84317.1173185102\n",
      "Epoch 172076: reducing learning rate of group 0 to 1.8529e-03.\n",
      "Epoch 172101, Training Loss: 27294, Validation Loss: 55852, 86400.90074864472\n",
      "Epoch 172177: reducing learning rate of group 0 to 1.8510e-03.\n",
      "Epoch 172201, Training Loss: 26036, Validation Loss: 56702, 89867.13243211964\n",
      "Epoch 172278: reducing learning rate of group 0 to 1.8492e-03.\n",
      "Epoch 172301, Training Loss: 25395, Validation Loss: 57115, 88912.30989101942\n",
      "Epoch 172379: reducing learning rate of group 0 to 1.8473e-03.\n",
      "Epoch 172401, Training Loss: 30778, Validation Loss: 55338, 105687.94357384254\n",
      "Epoch 172480: reducing learning rate of group 0 to 1.8455e-03.\n",
      "Epoch 172501, Training Loss: 31194, Validation Loss: 54665, 107455.09920328483\n",
      "Epoch 172581: reducing learning rate of group 0 to 1.8436e-03.\n",
      "Epoch 172601, Training Loss: 27233, Validation Loss: 55291, 90646.41083658622\n",
      "Epoch 172682: reducing learning rate of group 0 to 1.8418e-03.\n",
      "Epoch 172701, Training Loss: 29741, Validation Loss: 56185, 97446.05477600441\n",
      "Epoch 172783: reducing learning rate of group 0 to 1.8400e-03.\n",
      "Epoch 172801, Training Loss: 28566, Validation Loss: 55670, 87507.88862760692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172884: reducing learning rate of group 0 to 1.8381e-03.\n",
      "Epoch 172901, Training Loss: 27459, Validation Loss: 55432, 112207.20081511175\n",
      "Epoch 172985: reducing learning rate of group 0 to 1.8363e-03.\n",
      "Epoch 173001, Training Loss: 25662, Validation Loss: 57587, 88741.31568987941\n",
      "Epoch 173086: reducing learning rate of group 0 to 1.8344e-03.\n",
      "Epoch 173101, Training Loss: 28103, Validation Loss: 57761, 83811.34234915965\n",
      "Epoch 173187: reducing learning rate of group 0 to 1.8326e-03.\n",
      "Epoch 173201, Training Loss: 26789, Validation Loss: 54815, 92955.42916805879\n",
      "Epoch 173288: reducing learning rate of group 0 to 1.8308e-03.\n",
      "Epoch 173301, Training Loss: 29851, Validation Loss: 56006, 81571.55088451804\n",
      "Epoch 173389: reducing learning rate of group 0 to 1.8289e-03.\n",
      "Epoch 173401, Training Loss: 29356, Validation Loss: 55611, 98876.48084266581\n",
      "Epoch 173490: reducing learning rate of group 0 to 1.8271e-03.\n",
      "Epoch 173501, Training Loss: 27415, Validation Loss: 56005, 84016.65537479882\n",
      "Epoch 173591: reducing learning rate of group 0 to 1.8253e-03.\n",
      "Epoch 173601, Training Loss: 28824, Validation Loss: 54630, 92017.8423828561\n",
      "Epoch 173692: reducing learning rate of group 0 to 1.8235e-03.\n",
      "Epoch 173701, Training Loss: 25188, Validation Loss: 55296, 81116.82951745104\n",
      "Epoch 173793: reducing learning rate of group 0 to 1.8216e-03.\n",
      "Epoch 173801, Training Loss: 28888, Validation Loss: 55266, 87237.65234419923\n",
      "Epoch 173894: reducing learning rate of group 0 to 1.8198e-03.\n",
      "Epoch 173901, Training Loss: 27912, Validation Loss: 53499, 89424.42911204808\n",
      "Epoch 173995: reducing learning rate of group 0 to 1.8180e-03.\n",
      "Epoch 174001, Training Loss: 27139, Validation Loss: 54410, 88692.25839971635\n",
      "Epoch 174096: reducing learning rate of group 0 to 1.8162e-03.\n",
      "Epoch 174101, Training Loss: 26168, Validation Loss: 56382, 82843.31114395116\n",
      "Epoch 174197: reducing learning rate of group 0 to 1.8144e-03.\n",
      "Epoch 174201, Training Loss: 27513, Validation Loss: 56613, 85420.6719894909\n",
      "Epoch 174298: reducing learning rate of group 0 to 1.8125e-03.\n",
      "Epoch 174301, Training Loss: 29326, Validation Loss: 53803, 88206.56573393948\n",
      "Epoch 174399: reducing learning rate of group 0 to 1.8107e-03.\n",
      "Epoch 174401, Training Loss: 27966, Validation Loss: 55225, 106371.36263355846\n",
      "Epoch 174500: reducing learning rate of group 0 to 1.8089e-03.\n",
      "Epoch 174501, Training Loss: 27759, Validation Loss: 55169, 87434.25397358979\n",
      "Epoch 174601: reducing learning rate of group 0 to 1.8071e-03.\n",
      "Epoch 174601, Training Loss: 25909, Validation Loss: 54584, 91182.92614016752\n",
      "Epoch 174701, Training Loss: 27848, Validation Loss: 56439, 90042.26907607098\n",
      "Epoch 174702: reducing learning rate of group 0 to 1.8053e-03.\n",
      "Epoch 174801, Training Loss: 27923, Validation Loss: 55391, 84644.09191811335\n",
      "Epoch 174803: reducing learning rate of group 0 to 1.8035e-03.\n",
      "Epoch 174901, Training Loss: 28124, Validation Loss: 55562, 80872.75817477086\n",
      "Epoch 174904: reducing learning rate of group 0 to 1.8017e-03.\n",
      "Epoch 175001, Training Loss: 26788, Validation Loss: 56056, 90999.91575854951\n",
      "Epoch 175005: reducing learning rate of group 0 to 1.7999e-03.\n",
      "Epoch 175101, Training Loss: 30374, Validation Loss: 56117, 88068.57534752926\n",
      "Epoch 175106: reducing learning rate of group 0 to 1.7981e-03.\n",
      "Epoch 175201, Training Loss: 26848, Validation Loss: 55968, 64967.149418538706\n",
      "Epoch 175207: reducing learning rate of group 0 to 1.7963e-03.\n",
      "Epoch 175301, Training Loss: 29205, Validation Loss: 55534, 100840.00713239843\n",
      "Epoch 175308: reducing learning rate of group 0 to 1.7945e-03.\n",
      "Epoch 175401, Training Loss: 27829, Validation Loss: 55603, 83995.41212159941\n",
      "Epoch 175409: reducing learning rate of group 0 to 1.7927e-03.\n",
      "Epoch 175501, Training Loss: 29592, Validation Loss: 54556, 102006.2544175912\n",
      "Epoch 175510: reducing learning rate of group 0 to 1.7909e-03.\n",
      "Epoch 175601, Training Loss: 31427, Validation Loss: 55524, 86015.15332942574\n",
      "Epoch 175611: reducing learning rate of group 0 to 1.7891e-03.\n",
      "Epoch 175701, Training Loss: 28181, Validation Loss: 55874, 79173.51899159927\n",
      "Epoch 175712: reducing learning rate of group 0 to 1.7873e-03.\n",
      "Epoch 175801, Training Loss: 25379, Validation Loss: 55035, 73342.64846705126\n",
      "Epoch 175813: reducing learning rate of group 0 to 1.7855e-03.\n",
      "Epoch 175901, Training Loss: 28792, Validation Loss: 56051, 95262.93147645264\n",
      "Epoch 175914: reducing learning rate of group 0 to 1.7838e-03.\n",
      "Epoch 176001, Training Loss: 27912, Validation Loss: 54748, 88260.86559125474\n",
      "Epoch 176015: reducing learning rate of group 0 to 1.7820e-03.\n",
      "Epoch 176101, Training Loss: 27280, Validation Loss: 54233, 69248.99695868057\n",
      "Epoch 176116: reducing learning rate of group 0 to 1.7802e-03.\n",
      "Epoch 176201, Training Loss: 26804, Validation Loss: 55693, 91730.92994100363\n",
      "Epoch 176217: reducing learning rate of group 0 to 1.7784e-03.\n",
      "Epoch 176301, Training Loss: 26454, Validation Loss: 55863, 77515.41886481195\n",
      "Epoch 176318: reducing learning rate of group 0 to 1.7766e-03.\n",
      "Epoch 176401, Training Loss: 28501, Validation Loss: 57471, 77254.35533294335\n",
      "Epoch 176419: reducing learning rate of group 0 to 1.7749e-03.\n",
      "Epoch 176501, Training Loss: 26923, Validation Loss: 56054, 92298.2870556342\n",
      "Epoch 176520: reducing learning rate of group 0 to 1.7731e-03.\n",
      "Epoch 176601, Training Loss: 27907, Validation Loss: 55380, 100293.9119258076\n",
      "Epoch 176621: reducing learning rate of group 0 to 1.7713e-03.\n",
      "Epoch 176701, Training Loss: 28767, Validation Loss: 54890, 75857.28349968056\n",
      "Epoch 176722: reducing learning rate of group 0 to 1.7695e-03.\n",
      "Epoch 176801, Training Loss: 27718, Validation Loss: 56023, 86545.72699087707\n",
      "Epoch 176823: reducing learning rate of group 0 to 1.7678e-03.\n",
      "Epoch 176901, Training Loss: 27512, Validation Loss: 55865, 71241.38683977326\n",
      "Epoch 176924: reducing learning rate of group 0 to 1.7660e-03.\n",
      "Epoch 177001, Training Loss: 28487, Validation Loss: 56296, 83827.31328753837\n",
      "Epoch 177025: reducing learning rate of group 0 to 1.7642e-03.\n",
      "Epoch 177101, Training Loss: 28045, Validation Loss: 56264, 96920.59650288204\n",
      "Epoch 177126: reducing learning rate of group 0 to 1.7625e-03.\n",
      "Epoch 177201, Training Loss: 28190, Validation Loss: 54181, 85665.9972386073\n",
      "Epoch 177227: reducing learning rate of group 0 to 1.7607e-03.\n",
      "Epoch 177301, Training Loss: 27310, Validation Loss: 54705, 97889.9452397308\n",
      "Epoch 177328: reducing learning rate of group 0 to 1.7589e-03.\n",
      "Epoch 177401, Training Loss: 27306, Validation Loss: 55706, 93882.16129080187\n",
      "Epoch 177429: reducing learning rate of group 0 to 1.7572e-03.\n",
      "Epoch 177501, Training Loss: 28052, Validation Loss: 55314, 86618.85808325118\n",
      "Epoch 177530: reducing learning rate of group 0 to 1.7554e-03.\n",
      "Epoch 177601, Training Loss: 26627, Validation Loss: 55738, 92875.65866883146\n",
      "Epoch 177631: reducing learning rate of group 0 to 1.7537e-03.\n",
      "Epoch 177701, Training Loss: 25597, Validation Loss: 55313, 101045.39158531837\n",
      "Epoch 177732: reducing learning rate of group 0 to 1.7519e-03.\n",
      "Epoch 177801, Training Loss: 28254, Validation Loss: 54958, 82849.84250177503\n",
      "Epoch 177833: reducing learning rate of group 0 to 1.7502e-03.\n",
      "Epoch 177901, Training Loss: 29958, Validation Loss: 54912, 115420.56744743937\n",
      "Epoch 177934: reducing learning rate of group 0 to 1.7484e-03.\n",
      "Epoch 178001, Training Loss: 27778, Validation Loss: 53966, 85645.51846215218\n",
      "Epoch 178035: reducing learning rate of group 0 to 1.7467e-03.\n",
      "Epoch 178101, Training Loss: 27028, Validation Loss: 55662, 97984.68757895322\n",
      "Epoch 178136: reducing learning rate of group 0 to 1.7449e-03.\n",
      "Epoch 178201, Training Loss: 26342, Validation Loss: 58459, 126717.49917855341\n",
      "Epoch 178237: reducing learning rate of group 0 to 1.7432e-03.\n",
      "Epoch 178301, Training Loss: 28960, Validation Loss: 55283, 116262.17887478966\n",
      "Epoch 178338: reducing learning rate of group 0 to 1.7414e-03.\n",
      "Epoch 178401, Training Loss: 27665, Validation Loss: 54202, 97938.77608872381\n",
      "Epoch 178439: reducing learning rate of group 0 to 1.7397e-03.\n",
      "Epoch 178501, Training Loss: 27193, Validation Loss: 55423, 96627.20626583956\n",
      "Epoch 178540: reducing learning rate of group 0 to 1.7380e-03.\n",
      "Epoch 178601, Training Loss: 26989, Validation Loss: 53870, 79681.55692147174\n",
      "Epoch 178641: reducing learning rate of group 0 to 1.7362e-03.\n",
      "Epoch 178701, Training Loss: 27834, Validation Loss: 56576, 101154.60816939299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178742: reducing learning rate of group 0 to 1.7345e-03.\n",
      "Epoch 178801, Training Loss: 29429, Validation Loss: 55656, 89047.41479060224\n",
      "Epoch 178843: reducing learning rate of group 0 to 1.7327e-03.\n",
      "Epoch 178901, Training Loss: 27905, Validation Loss: 54696, 89238.25091289787\n",
      "Epoch 178944: reducing learning rate of group 0 to 1.7310e-03.\n",
      "Epoch 179001, Training Loss: 27767, Validation Loss: 55518, 108661.24560193093\n",
      "Epoch 179045: reducing learning rate of group 0 to 1.7293e-03.\n",
      "Epoch 179101, Training Loss: 26653, Validation Loss: 53637, 80652.70451981915\n",
      "Epoch 179146: reducing learning rate of group 0 to 1.7276e-03.\n",
      "Epoch 179201, Training Loss: 27318, Validation Loss: 55719, 97469.4807372754\n",
      "Epoch 179247: reducing learning rate of group 0 to 1.7258e-03.\n",
      "Epoch 179301, Training Loss: 31504, Validation Loss: 54930, 88615.4236590501\n",
      "Epoch 179348: reducing learning rate of group 0 to 1.7241e-03.\n",
      "Epoch 179401, Training Loss: 26599, Validation Loss: 55081, 82275.11805154358\n",
      "Epoch 179449: reducing learning rate of group 0 to 1.7224e-03.\n",
      "Epoch 179501, Training Loss: 26263, Validation Loss: 54136, 78129.56329105097\n",
      "Epoch 179550: reducing learning rate of group 0 to 1.7207e-03.\n",
      "Epoch 179601, Training Loss: 28943, Validation Loss: 55129, 92235.0423371436\n",
      "Epoch 179651: reducing learning rate of group 0 to 1.7189e-03.\n",
      "Epoch 179701, Training Loss: 25260, Validation Loss: 56133, 76000.2249080228\n",
      "Epoch 179752: reducing learning rate of group 0 to 1.7172e-03.\n",
      "Epoch 179801, Training Loss: 27792, Validation Loss: 54302, 80322.4353386557\n",
      "Epoch 179853: reducing learning rate of group 0 to 1.7155e-03.\n",
      "Epoch 179901, Training Loss: 29640, Validation Loss: 56386, 82699.45785482477\n",
      "Epoch 179954: reducing learning rate of group 0 to 1.7138e-03.\n",
      "Epoch 180001, Training Loss: 27933, Validation Loss: 56619, 83734.04466933986\n",
      "Epoch 180055: reducing learning rate of group 0 to 1.7121e-03.\n",
      "Epoch 180101, Training Loss: 27312, Validation Loss: 55635, 79120.42652748308\n",
      "Epoch 180156: reducing learning rate of group 0 to 1.7104e-03.\n",
      "Epoch 180201, Training Loss: 27669, Validation Loss: 54929, 70551.26558198461\n",
      "Epoch 180257: reducing learning rate of group 0 to 1.7086e-03.\n",
      "Epoch 180301, Training Loss: 29696, Validation Loss: 54344, 73843.08536716625\n",
      "Epoch 180358: reducing learning rate of group 0 to 1.7069e-03.\n",
      "Epoch 180401, Training Loss: 27145, Validation Loss: 55865, 78323.27451117568\n",
      "Epoch 180459: reducing learning rate of group 0 to 1.7052e-03.\n",
      "Epoch 180501, Training Loss: 29524, Validation Loss: 55216, 118364.65134009968\n",
      "Epoch 180560: reducing learning rate of group 0 to 1.7035e-03.\n",
      "Epoch 180601, Training Loss: 29162, Validation Loss: 54834, 84500.75652649347\n",
      "Epoch 180661: reducing learning rate of group 0 to 1.7018e-03.\n",
      "Epoch 180701, Training Loss: 28492, Validation Loss: 55097, 101392.87733274972\n",
      "Epoch 180762: reducing learning rate of group 0 to 1.7001e-03.\n",
      "Epoch 180801, Training Loss: 28729, Validation Loss: 59401, 83426.90538362022\n",
      "Epoch 180863: reducing learning rate of group 0 to 1.6984e-03.\n",
      "Epoch 180901, Training Loss: 27525, Validation Loss: 55180, 90682.67172578612\n",
      "Epoch 180964: reducing learning rate of group 0 to 1.6967e-03.\n",
      "Epoch 181001, Training Loss: 28116, Validation Loss: 55838, 86482.60600721418\n",
      "Epoch 181065: reducing learning rate of group 0 to 1.6950e-03.\n",
      "Epoch 181101, Training Loss: 26053, Validation Loss: 54780, 76010.84687826289\n",
      "Epoch 181166: reducing learning rate of group 0 to 1.6933e-03.\n",
      "Epoch 181201, Training Loss: 27772, Validation Loss: 56178, 97843.93398822083\n",
      "Epoch 181267: reducing learning rate of group 0 to 1.6916e-03.\n",
      "Epoch 181301, Training Loss: 28704, Validation Loss: 53323, 96541.26001946419\n",
      "Epoch 181368: reducing learning rate of group 0 to 1.6899e-03.\n",
      "Epoch 181401, Training Loss: 30379, Validation Loss: 55931, 110836.0519518945\n",
      "Epoch 181469: reducing learning rate of group 0 to 1.6883e-03.\n",
      "Epoch 181501, Training Loss: 27276, Validation Loss: 55341, 91216.21021191932\n",
      "Epoch 181570: reducing learning rate of group 0 to 1.6866e-03.\n",
      "Epoch 181601, Training Loss: 26303, Validation Loss: 55768, 101711.12916671742\n",
      "Epoch 181671: reducing learning rate of group 0 to 1.6849e-03.\n",
      "Epoch 181701, Training Loss: 30030, Validation Loss: 54729, 82919.22751483563\n",
      "Epoch 181772: reducing learning rate of group 0 to 1.6832e-03.\n",
      "Epoch 181801, Training Loss: 27485, Validation Loss: 54083, 82867.46642966071\n",
      "Epoch 181873: reducing learning rate of group 0 to 1.6815e-03.\n",
      "Epoch 181901, Training Loss: 28930, Validation Loss: 55402, 89168.6327788583\n",
      "Epoch 181974: reducing learning rate of group 0 to 1.6798e-03.\n",
      "Epoch 182001, Training Loss: 27057, Validation Loss: 54751, 92035.90099956044\n",
      "Epoch 182075: reducing learning rate of group 0 to 1.6782e-03.\n",
      "Epoch 182101, Training Loss: 26951, Validation Loss: 54719, 85594.6065505982\n",
      "Epoch 182176: reducing learning rate of group 0 to 1.6765e-03.\n",
      "Epoch 182201, Training Loss: 27024, Validation Loss: 55612, 101360.90544648394\n",
      "Epoch 182277: reducing learning rate of group 0 to 1.6748e-03.\n",
      "Epoch 182301, Training Loss: 27072, Validation Loss: 54505, 91587.85595546919\n",
      "Epoch 182378: reducing learning rate of group 0 to 1.6731e-03.\n",
      "Epoch 182401, Training Loss: 27564, Validation Loss: 54075, 87783.15387146074\n",
      "Epoch 182479: reducing learning rate of group 0 to 1.6714e-03.\n",
      "Epoch 182501, Training Loss: 27052, Validation Loss: 56139, 75075.22838007688\n",
      "Epoch 182580: reducing learning rate of group 0 to 1.6698e-03.\n",
      "Epoch 182601, Training Loss: 28565, Validation Loss: 55597, 107030.61536616436\n",
      "Epoch 182681: reducing learning rate of group 0 to 1.6681e-03.\n",
      "Epoch 182701, Training Loss: 27518, Validation Loss: 54579, 104659.62612190866\n",
      "Epoch 182782: reducing learning rate of group 0 to 1.6664e-03.\n",
      "Epoch 182801, Training Loss: 27812, Validation Loss: 54137, 86426.5518475428\n",
      "Epoch 182883: reducing learning rate of group 0 to 1.6648e-03.\n",
      "Epoch 182901, Training Loss: 27317, Validation Loss: 55498, 100385.50094852773\n",
      "Epoch 182984: reducing learning rate of group 0 to 1.6631e-03.\n",
      "Epoch 183001, Training Loss: 26633, Validation Loss: 55189, 97772.5227990022\n",
      "Epoch 183085: reducing learning rate of group 0 to 1.6614e-03.\n",
      "Epoch 183101, Training Loss: 28454, Validation Loss: 54575, 67966.69064020937\n",
      "Epoch 183186: reducing learning rate of group 0 to 1.6598e-03.\n",
      "Epoch 183201, Training Loss: 29158, Validation Loss: 55180, 118665.124369269\n",
      "Epoch 183287: reducing learning rate of group 0 to 1.6581e-03.\n",
      "Epoch 183301, Training Loss: 30432, Validation Loss: 55526, 101419.97537387349\n",
      "Epoch 183388: reducing learning rate of group 0 to 1.6565e-03.\n",
      "Epoch 183401, Training Loss: 27220, Validation Loss: 55948, 78983.74301812086\n",
      "Epoch 183489: reducing learning rate of group 0 to 1.6548e-03.\n",
      "Epoch 183501, Training Loss: 26535, Validation Loss: 56516, 96147.83387542493\n",
      "Epoch 183590: reducing learning rate of group 0 to 1.6532e-03.\n",
      "Epoch 183601, Training Loss: 30552, Validation Loss: 55027, 106666.2253313121\n",
      "Epoch 183691: reducing learning rate of group 0 to 1.6515e-03.\n",
      "Epoch 183701, Training Loss: 28522, Validation Loss: 54323, 84320.32938853581\n",
      "Epoch 183792: reducing learning rate of group 0 to 1.6498e-03.\n",
      "Epoch 183801, Training Loss: 29122, Validation Loss: 54880, 84467.79961036942\n",
      "Epoch 183893: reducing learning rate of group 0 to 1.6482e-03.\n",
      "Epoch 183901, Training Loss: 26726, Validation Loss: 54804, 109738.27806469034\n",
      "Epoch 183994: reducing learning rate of group 0 to 1.6466e-03.\n",
      "Epoch 184001, Training Loss: 27900, Validation Loss: 55729, 106136.32600487716\n",
      "Epoch 184095: reducing learning rate of group 0 to 1.6449e-03.\n",
      "Epoch 184101, Training Loss: 26563, Validation Loss: 54270, 77244.13839143538\n",
      "Epoch 184196: reducing learning rate of group 0 to 1.6433e-03.\n",
      "Epoch 184201, Training Loss: 27692, Validation Loss: 55810, 98930.39614319045\n",
      "Epoch 184297: reducing learning rate of group 0 to 1.6416e-03.\n",
      "Epoch 184301, Training Loss: 27310, Validation Loss: 54483, 80783.48659612922\n",
      "Epoch 184398: reducing learning rate of group 0 to 1.6400e-03.\n",
      "Epoch 184401, Training Loss: 24735, Validation Loss: 55177, 88129.68058028068\n",
      "Epoch 184499: reducing learning rate of group 0 to 1.6383e-03.\n",
      "Epoch 184501, Training Loss: 30730, Validation Loss: 55385, 86056.58805271758\n",
      "Epoch 184600: reducing learning rate of group 0 to 1.6367e-03.\n",
      "Epoch 184601, Training Loss: 26616, Validation Loss: 54813, 84669.14388912266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184701: reducing learning rate of group 0 to 1.6351e-03.\n",
      "Epoch 184701, Training Loss: 27763, Validation Loss: 55253, 95685.89573885726\n",
      "Epoch 184801, Training Loss: 29234, Validation Loss: 57303, 92628.49454694008\n",
      "Epoch 184802: reducing learning rate of group 0 to 1.6334e-03.\n",
      "Epoch 184901, Training Loss: 29688, Validation Loss: 55861, 91374.79851001171\n",
      "Epoch 184903: reducing learning rate of group 0 to 1.6318e-03.\n",
      "Epoch 185001, Training Loss: 26824, Validation Loss: 54494, 84779.94756480114\n",
      "Epoch 185004: reducing learning rate of group 0 to 1.6302e-03.\n",
      "Epoch 185101, Training Loss: 30897, Validation Loss: 56247, 101626.92556945991\n",
      "Epoch 185105: reducing learning rate of group 0 to 1.6285e-03.\n",
      "Epoch 185201, Training Loss: 26270, Validation Loss: 53934, 89435.81929617951\n",
      "Epoch 185206: reducing learning rate of group 0 to 1.6269e-03.\n",
      "Epoch 185301, Training Loss: 27650, Validation Loss: 53911, 100305.56820334122\n",
      "Epoch 185307: reducing learning rate of group 0 to 1.6253e-03.\n",
      "Epoch 185401, Training Loss: 28459, Validation Loss: 55433, 90789.55010618956\n",
      "Epoch 185408: reducing learning rate of group 0 to 1.6236e-03.\n",
      "Epoch 185501, Training Loss: 26645, Validation Loss: 55626, 86812.93301897046\n",
      "Epoch 185509: reducing learning rate of group 0 to 1.6220e-03.\n",
      "Epoch 185601, Training Loss: 25989, Validation Loss: 54407, 79552.57672890586\n",
      "Epoch 185610: reducing learning rate of group 0 to 1.6204e-03.\n",
      "Epoch 185701, Training Loss: 27922, Validation Loss: 55173, 103116.9798278568\n",
      "Epoch 185711: reducing learning rate of group 0 to 1.6188e-03.\n",
      "Epoch 185801, Training Loss: 27786, Validation Loss: 54480, 96381.7996171429\n",
      "Epoch 185812: reducing learning rate of group 0 to 1.6172e-03.\n",
      "Epoch 185901, Training Loss: 28976, Validation Loss: 54168, 82640.68822339585\n",
      "Epoch 185913: reducing learning rate of group 0 to 1.6155e-03.\n",
      "Epoch 186001, Training Loss: 30231, Validation Loss: 57736, 97706.00176536986\n",
      "Epoch 186014: reducing learning rate of group 0 to 1.6139e-03.\n",
      "Epoch 186101, Training Loss: 31373, Validation Loss: 54572, 109100.24539519737\n",
      "Epoch 186115: reducing learning rate of group 0 to 1.6123e-03.\n",
      "Epoch 186201, Training Loss: 26913, Validation Loss: 55389, 96896.17519046931\n",
      "Epoch 186216: reducing learning rate of group 0 to 1.6107e-03.\n",
      "Epoch 186301, Training Loss: 29575, Validation Loss: 54886, 117307.0380168024\n",
      "Epoch 186317: reducing learning rate of group 0 to 1.6091e-03.\n",
      "Epoch 186401, Training Loss: 27359, Validation Loss: 56206, 86455.78338671401\n",
      "Epoch 186418: reducing learning rate of group 0 to 1.6075e-03.\n",
      "Epoch 186501, Training Loss: 27817, Validation Loss: 53653, 94100.78553323184\n",
      "Epoch 186519: reducing learning rate of group 0 to 1.6059e-03.\n",
      "Epoch 186601, Training Loss: 26184, Validation Loss: 55444, 105782.47770566458\n",
      "Epoch 186620: reducing learning rate of group 0 to 1.6043e-03.\n",
      "Epoch 186701, Training Loss: 26983, Validation Loss: 55295, 86736.62844274892\n",
      "Epoch 186721: reducing learning rate of group 0 to 1.6027e-03.\n",
      "Epoch 186801, Training Loss: 26487, Validation Loss: 55352, 98670.54672679065\n",
      "Epoch 186822: reducing learning rate of group 0 to 1.6011e-03.\n",
      "Epoch 186901, Training Loss: 26221, Validation Loss: 54720, 92804.29358638274\n",
      "Epoch 186923: reducing learning rate of group 0 to 1.5995e-03.\n",
      "Epoch 187001, Training Loss: 26601, Validation Loss: 54997, 105962.49630037653\n",
      "Epoch 187024: reducing learning rate of group 0 to 1.5979e-03.\n",
      "Epoch 187101, Training Loss: 28903, Validation Loss: 55548, 89483.95611185522\n",
      "Epoch 187125: reducing learning rate of group 0 to 1.5963e-03.\n",
      "Epoch 187201, Training Loss: 27451, Validation Loss: 54021, 84329.44876817308\n",
      "Epoch 187226: reducing learning rate of group 0 to 1.5947e-03.\n",
      "Epoch 187301, Training Loss: 26702, Validation Loss: 55671, 117309.53804377653\n",
      "Epoch 187327: reducing learning rate of group 0 to 1.5931e-03.\n",
      "Epoch 187401, Training Loss: 26651, Validation Loss: 55733, 90184.08889403079\n",
      "Epoch 187428: reducing learning rate of group 0 to 1.5915e-03.\n",
      "Epoch 187501, Training Loss: 30803, Validation Loss: 56584, 91754.88647780033\n",
      "Epoch 187529: reducing learning rate of group 0 to 1.5899e-03.\n",
      "Epoch 187601, Training Loss: 29302, Validation Loss: 55356, 96170.03286071273\n",
      "Epoch 187630: reducing learning rate of group 0 to 1.5883e-03.\n",
      "Epoch 187701, Training Loss: 31336, Validation Loss: 54010, 82604.1858006798\n",
      "Epoch 187731: reducing learning rate of group 0 to 1.5867e-03.\n",
      "Epoch 187801, Training Loss: 27044, Validation Loss: 55103, 89548.75831834046\n",
      "Epoch 187832: reducing learning rate of group 0 to 1.5851e-03.\n",
      "Epoch 187901, Training Loss: 28416, Validation Loss: 54690, 88265.58991214965\n",
      "Epoch 187933: reducing learning rate of group 0 to 1.5835e-03.\n",
      "Epoch 188001, Training Loss: 29295, Validation Loss: 53884, 100875.63673532133\n",
      "Epoch 188034: reducing learning rate of group 0 to 1.5820e-03.\n",
      "Epoch 188101, Training Loss: 26972, Validation Loss: 54712, 91694.42075106622\n",
      "Epoch 188135: reducing learning rate of group 0 to 1.5804e-03.\n",
      "Epoch 188201, Training Loss: 25848, Validation Loss: 56651, 94004.69910232276\n",
      "Epoch 188236: reducing learning rate of group 0 to 1.5788e-03.\n",
      "Epoch 188301, Training Loss: 26311, Validation Loss: 54342, 101832.18848100312\n",
      "Epoch 188337: reducing learning rate of group 0 to 1.5772e-03.\n",
      "Epoch 188401, Training Loss: 28355, Validation Loss: 56492, 121876.8357495987\n",
      "Epoch 188438: reducing learning rate of group 0 to 1.5756e-03.\n",
      "Epoch 188501, Training Loss: 28963, Validation Loss: 55241, 92428.55851953989\n",
      "Epoch 188539: reducing learning rate of group 0 to 1.5741e-03.\n",
      "Epoch 188601, Training Loss: 28185, Validation Loss: 55146, 88832.39286634962\n",
      "Epoch 188640: reducing learning rate of group 0 to 1.5725e-03.\n",
      "Epoch 188701, Training Loss: 29264, Validation Loss: 54617, 85987.07322116602\n",
      "Epoch 188741: reducing learning rate of group 0 to 1.5709e-03.\n",
      "Epoch 188801, Training Loss: 29980, Validation Loss: 56284, 110634.81774473916\n",
      "Epoch 188842: reducing learning rate of group 0 to 1.5693e-03.\n",
      "Epoch 188901, Training Loss: 28329, Validation Loss: 55441, 108388.43006243515\n",
      "Epoch 188943: reducing learning rate of group 0 to 1.5678e-03.\n",
      "Epoch 189001, Training Loss: 27616, Validation Loss: 55759, 100238.56764161284\n",
      "Epoch 189044: reducing learning rate of group 0 to 1.5662e-03.\n",
      "Epoch 189101, Training Loss: 26836, Validation Loss: 55186, 81251.27192818646\n",
      "Epoch 189145: reducing learning rate of group 0 to 1.5646e-03.\n",
      "Epoch 189201, Training Loss: 26770, Validation Loss: 54309, 98043.59121535766\n",
      "Epoch 189246: reducing learning rate of group 0 to 1.5631e-03.\n",
      "Epoch 189301, Training Loss: 29886, Validation Loss: 55521, 92437.6690815548\n",
      "Epoch 189347: reducing learning rate of group 0 to 1.5615e-03.\n",
      "Epoch 189401, Training Loss: 28244, Validation Loss: 56130, 85091.38514282223\n",
      "Epoch 189448: reducing learning rate of group 0 to 1.5600e-03.\n",
      "Epoch 189501, Training Loss: 27040, Validation Loss: 55631, 80344.41703749022\n",
      "Epoch 189549: reducing learning rate of group 0 to 1.5584e-03.\n",
      "Epoch 189601, Training Loss: 26431, Validation Loss: 56552, 84587.65992845311\n",
      "Epoch 189650: reducing learning rate of group 0 to 1.5568e-03.\n",
      "Epoch 189701, Training Loss: 31558, Validation Loss: 55455, 87351.38550999404\n",
      "Epoch 189751: reducing learning rate of group 0 to 1.5553e-03.\n",
      "Epoch 189801, Training Loss: 28455, Validation Loss: 57169, 97332.45029749633\n",
      "Epoch 189852: reducing learning rate of group 0 to 1.5537e-03.\n",
      "Epoch 189901, Training Loss: 29234, Validation Loss: 55961, 75869.9545567044\n",
      "Epoch 189953: reducing learning rate of group 0 to 1.5522e-03.\n",
      "Epoch 190001, Training Loss: 25818, Validation Loss: 53984, 90237.14542132728\n",
      "Epoch 190054: reducing learning rate of group 0 to 1.5506e-03.\n",
      "Epoch 190101, Training Loss: 26518, Validation Loss: 54643, 99419.50154613944\n",
      "Epoch 190155: reducing learning rate of group 0 to 1.5491e-03.\n",
      "Epoch 190201, Training Loss: 27235, Validation Loss: 54953, 114472.909998161\n",
      "Epoch 190256: reducing learning rate of group 0 to 1.5475e-03.\n",
      "Epoch 190301, Training Loss: 27881, Validation Loss: 55979, 85901.3991361927\n",
      "Epoch 190357: reducing learning rate of group 0 to 1.5460e-03.\n",
      "Epoch 190401, Training Loss: 29337, Validation Loss: 55797, 103011.7858406315\n",
      "Epoch 190458: reducing learning rate of group 0 to 1.5444e-03.\n",
      "Epoch 190501, Training Loss: 26259, Validation Loss: 55290, 73003.59031657942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190559: reducing learning rate of group 0 to 1.5429e-03.\n",
      "Epoch 190601, Training Loss: 26070, Validation Loss: 56566, 102756.77555736166\n",
      "Epoch 190660: reducing learning rate of group 0 to 1.5413e-03.\n",
      "Epoch 190701, Training Loss: 29697, Validation Loss: 56893, 101799.98641316935\n",
      "Epoch 190761: reducing learning rate of group 0 to 1.5398e-03.\n",
      "Epoch 190801, Training Loss: 27449, Validation Loss: 56198, 88311.7112614473\n",
      "Epoch 190862: reducing learning rate of group 0 to 1.5383e-03.\n",
      "Epoch 190901, Training Loss: 27893, Validation Loss: 55404, 83740.89827573077\n",
      "Epoch 190963: reducing learning rate of group 0 to 1.5367e-03.\n",
      "Epoch 191001, Training Loss: 28097, Validation Loss: 54808, 96424.30604303667\n",
      "Epoch 191064: reducing learning rate of group 0 to 1.5352e-03.\n",
      "Epoch 191101, Training Loss: 28733, Validation Loss: 54736, 91131.98751651047\n",
      "Epoch 191165: reducing learning rate of group 0 to 1.5336e-03.\n",
      "Epoch 191201, Training Loss: 28291, Validation Loss: 55743, 92111.60757047993\n",
      "Epoch 191266: reducing learning rate of group 0 to 1.5321e-03.\n",
      "Epoch 191301, Training Loss: 27301, Validation Loss: 54736, 87916.46242390737\n",
      "Epoch 191367: reducing learning rate of group 0 to 1.5306e-03.\n",
      "Epoch 191401, Training Loss: 27281, Validation Loss: 55092, 74941.86685672365\n",
      "Epoch 191468: reducing learning rate of group 0 to 1.5290e-03.\n",
      "Epoch 191501, Training Loss: 27772, Validation Loss: 55225, 106442.36955938376\n",
      "Epoch 191569: reducing learning rate of group 0 to 1.5275e-03.\n",
      "Epoch 191601, Training Loss: 30088, Validation Loss: 53919, 91291.69289506316\n",
      "Epoch 191670: reducing learning rate of group 0 to 1.5260e-03.\n",
      "Epoch 191701, Training Loss: 28146, Validation Loss: 54961, 90676.64589132479\n",
      "Epoch 191771: reducing learning rate of group 0 to 1.5245e-03.\n",
      "Epoch 191801, Training Loss: 27268, Validation Loss: 54459, 84115.55276497074\n",
      "Epoch 191872: reducing learning rate of group 0 to 1.5229e-03.\n",
      "Epoch 191901, Training Loss: 28567, Validation Loss: 56687, 87781.87004192155\n",
      "Epoch 191973: reducing learning rate of group 0 to 1.5214e-03.\n",
      "Epoch 192001, Training Loss: 26989, Validation Loss: 55963, 108181.87955279798\n",
      "Epoch 192074: reducing learning rate of group 0 to 1.5199e-03.\n",
      "Epoch 192101, Training Loss: 27652, Validation Loss: 54902, 100126.62521986761\n",
      "Epoch 192175: reducing learning rate of group 0 to 1.5184e-03.\n",
      "Epoch 192201, Training Loss: 29047, Validation Loss: 55335, 94749.71629135679\n",
      "Epoch 192276: reducing learning rate of group 0 to 1.5169e-03.\n",
      "Epoch 192301, Training Loss: 27989, Validation Loss: 56287, 105345.04477896747\n",
      "Epoch 192377: reducing learning rate of group 0 to 1.5153e-03.\n",
      "Epoch 192401, Training Loss: 26625, Validation Loss: 55256, 104129.6008609181\n",
      "Epoch 192478: reducing learning rate of group 0 to 1.5138e-03.\n",
      "Epoch 192501, Training Loss: 26777, Validation Loss: 55066, 79909.18433885161\n",
      "Epoch 192579: reducing learning rate of group 0 to 1.5123e-03.\n",
      "Epoch 192601, Training Loss: 28369, Validation Loss: 54743, 107290.52810194262\n",
      "Epoch 192680: reducing learning rate of group 0 to 1.5108e-03.\n",
      "Epoch 192701, Training Loss: 27546, Validation Loss: 55725, 108997.03772498382\n",
      "Epoch 192781: reducing learning rate of group 0 to 1.5093e-03.\n",
      "Epoch 192801, Training Loss: 29640, Validation Loss: 53444, 105040.4845192681\n",
      "Epoch 192882: reducing learning rate of group 0 to 1.5078e-03.\n",
      "Epoch 192901, Training Loss: 29281, Validation Loss: 55957, 92259.25518840877\n",
      "Epoch 192983: reducing learning rate of group 0 to 1.5063e-03.\n",
      "Epoch 193001, Training Loss: 26779, Validation Loss: 56026, 88704.91621686758\n",
      "Epoch 193084: reducing learning rate of group 0 to 1.5048e-03.\n",
      "Epoch 193101, Training Loss: 25084, Validation Loss: 54687, 92103.32832444595\n",
      "Epoch 193185: reducing learning rate of group 0 to 1.5033e-03.\n",
      "Epoch 193201, Training Loss: 27577, Validation Loss: 55059, 107451.27323757073\n",
      "Epoch 193286: reducing learning rate of group 0 to 1.5018e-03.\n",
      "Epoch 193301, Training Loss: 24946, Validation Loss: 54731, 95295.00281410356\n",
      "Epoch 193387: reducing learning rate of group 0 to 1.5003e-03.\n",
      "Epoch 193401, Training Loss: 27552, Validation Loss: 55887, 114862.26990227163\n",
      "Epoch 193488: reducing learning rate of group 0 to 1.4988e-03.\n",
      "Epoch 193501, Training Loss: 27694, Validation Loss: 54677, 100980.25253123457\n",
      "Epoch 193589: reducing learning rate of group 0 to 1.4973e-03.\n",
      "Epoch 193601, Training Loss: 29091, Validation Loss: 55852, 85179.63030301488\n",
      "Epoch 193690: reducing learning rate of group 0 to 1.4958e-03.\n",
      "Epoch 193701, Training Loss: 28446, Validation Loss: 54458, 115286.49371033204\n",
      "Epoch 193791: reducing learning rate of group 0 to 1.4943e-03.\n",
      "Epoch 193801, Training Loss: 27979, Validation Loss: 53516, 116955.94924049328\n",
      "Epoch 193892: reducing learning rate of group 0 to 1.4928e-03.\n",
      "Epoch 193901, Training Loss: 26351, Validation Loss: 55398, 91697.86972133715\n",
      "Epoch 193993: reducing learning rate of group 0 to 1.4913e-03.\n",
      "Epoch 194001, Training Loss: 28227, Validation Loss: 55617, 110634.84259216092\n",
      "Epoch 194094: reducing learning rate of group 0 to 1.4898e-03.\n",
      "Epoch 194101, Training Loss: 28329, Validation Loss: 56032, 113851.42051779368\n",
      "Epoch 194195: reducing learning rate of group 0 to 1.4883e-03.\n",
      "Epoch 194201, Training Loss: 27269, Validation Loss: 56522, 86898.35335625662\n",
      "Epoch 194296: reducing learning rate of group 0 to 1.4868e-03.\n",
      "Epoch 194301, Training Loss: 28290, Validation Loss: 55981, 103019.84003094264\n",
      "Epoch 194397: reducing learning rate of group 0 to 1.4853e-03.\n",
      "Epoch 194401, Training Loss: 28183, Validation Loss: 54985, 94647.8631855694\n",
      "Epoch 194498: reducing learning rate of group 0 to 1.4838e-03.\n",
      "Epoch 194501, Training Loss: 27435, Validation Loss: 56975, 82644.04966011073\n",
      "Epoch 194599: reducing learning rate of group 0 to 1.4824e-03.\n",
      "Epoch 194601, Training Loss: 27439, Validation Loss: 56453, 106680.68469938241\n",
      "Epoch 194700: reducing learning rate of group 0 to 1.4809e-03.\n",
      "Epoch 194701, Training Loss: 26605, Validation Loss: 55668, 88240.04867298137\n",
      "Epoch 194801: reducing learning rate of group 0 to 1.4794e-03.\n",
      "Epoch 194801, Training Loss: 27694, Validation Loss: 55301, 91808.9553057336\n",
      "Epoch 194901, Training Loss: 27872, Validation Loss: 54767, 94781.4028118311\n",
      "Epoch 194902: reducing learning rate of group 0 to 1.4779e-03.\n",
      "Epoch 195001, Training Loss: 27360, Validation Loss: 55828, 108879.23363813541\n",
      "Epoch 195003: reducing learning rate of group 0 to 1.4764e-03.\n",
      "Epoch 195101, Training Loss: 26690, Validation Loss: 55744, 86016.92559020159\n",
      "Epoch 195104: reducing learning rate of group 0 to 1.4750e-03.\n",
      "Epoch 195201, Training Loss: 26467, Validation Loss: 55574, 85543.05520623435\n",
      "Epoch 195205: reducing learning rate of group 0 to 1.4735e-03.\n",
      "Epoch 195301, Training Loss: 28597, Validation Loss: 57089, 101908.93804916787\n",
      "Epoch 195306: reducing learning rate of group 0 to 1.4720e-03.\n",
      "Epoch 195401, Training Loss: 28710, Validation Loss: 55704, 108608.55213767616\n",
      "Epoch 195407: reducing learning rate of group 0 to 1.4705e-03.\n",
      "Epoch 195501, Training Loss: 28433, Validation Loss: 55219, 105243.22772156115\n",
      "Epoch 195508: reducing learning rate of group 0 to 1.4691e-03.\n",
      "Epoch 195601, Training Loss: 28665, Validation Loss: 56563, 92377.66206194542\n",
      "Epoch 195609: reducing learning rate of group 0 to 1.4676e-03.\n",
      "Epoch 195701, Training Loss: 26350, Validation Loss: 54227, 74272.41783173296\n",
      "Epoch 195710: reducing learning rate of group 0 to 1.4661e-03.\n",
      "Epoch 195801, Training Loss: 27573, Validation Loss: 54704, 95873.22488646446\n",
      "Epoch 195811: reducing learning rate of group 0 to 1.4647e-03.\n",
      "Epoch 195901, Training Loss: 28556, Validation Loss: 55894, 88121.75474553817\n",
      "Epoch 195912: reducing learning rate of group 0 to 1.4632e-03.\n",
      "Epoch 196001, Training Loss: 31439, Validation Loss: 55021, 95836.34991713123\n",
      "Epoch 196013: reducing learning rate of group 0 to 1.4617e-03.\n",
      "Epoch 196101, Training Loss: 28680, Validation Loss: 54274, 101506.14976147178\n",
      "Epoch 196114: reducing learning rate of group 0 to 1.4603e-03.\n",
      "Epoch 196201, Training Loss: 27763, Validation Loss: 56429, 91111.23601664562\n",
      "Epoch 196215: reducing learning rate of group 0 to 1.4588e-03.\n",
      "Epoch 196301, Training Loss: 26389, Validation Loss: 55655, 91299.33257024137\n",
      "Epoch 196316: reducing learning rate of group 0 to 1.4574e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196401, Training Loss: 26447, Validation Loss: 54636, 83251.28877815166\n",
      "Epoch 196417: reducing learning rate of group 0 to 1.4559e-03.\n",
      "Epoch 196501, Training Loss: 28033, Validation Loss: 55153, 87851.96167142077\n",
      "Epoch 196518: reducing learning rate of group 0 to 1.4544e-03.\n",
      "Epoch 196601, Training Loss: 25622, Validation Loss: 55647, 96118.58560090464\n",
      "Epoch 196619: reducing learning rate of group 0 to 1.4530e-03.\n",
      "Epoch 196701, Training Loss: 27586, Validation Loss: 53927, 100210.62205763487\n",
      "Epoch 196720: reducing learning rate of group 0 to 1.4515e-03.\n",
      "Epoch 196801, Training Loss: 27745, Validation Loss: 57246, 87159.05061996124\n",
      "Epoch 196821: reducing learning rate of group 0 to 1.4501e-03.\n",
      "Epoch 196901, Training Loss: 28587, Validation Loss: 55241, 106733.68710090325\n",
      "Epoch 196922: reducing learning rate of group 0 to 1.4486e-03.\n",
      "Epoch 197001, Training Loss: 25671, Validation Loss: 55071, 81088.15687925612\n",
      "Epoch 197023: reducing learning rate of group 0 to 1.4472e-03.\n",
      "Epoch 197101, Training Loss: 27788, Validation Loss: 55861, 100814.94593925937\n",
      "Epoch 197124: reducing learning rate of group 0 to 1.4457e-03.\n",
      "Epoch 197201, Training Loss: 25693, Validation Loss: 55641, 94412.41431895865\n",
      "Epoch 197225: reducing learning rate of group 0 to 1.4443e-03.\n",
      "Epoch 197301, Training Loss: 28573, Validation Loss: 55415, 90799.75534871618\n",
      "Epoch 197326: reducing learning rate of group 0 to 1.4428e-03.\n",
      "Epoch 197401, Training Loss: 26450, Validation Loss: 56669, 100493.85777107718\n",
      "Epoch 197427: reducing learning rate of group 0 to 1.4414e-03.\n",
      "Epoch 197501, Training Loss: 25233, Validation Loss: 55610, 102715.47004966585\n",
      "Epoch 197528: reducing learning rate of group 0 to 1.4400e-03.\n",
      "Epoch 197601, Training Loss: 29433, Validation Loss: 56197, 75183.25950787729\n",
      "Epoch 197629: reducing learning rate of group 0 to 1.4385e-03.\n",
      "Epoch 197701, Training Loss: 27759, Validation Loss: 54660, 89988.18750406797\n",
      "Epoch 197730: reducing learning rate of group 0 to 1.4371e-03.\n",
      "Epoch 197801, Training Loss: 25698, Validation Loss: 55942, 82292.85399461909\n",
      "Epoch 197831: reducing learning rate of group 0 to 1.4356e-03.\n",
      "Epoch 197901, Training Loss: 26590, Validation Loss: 56512, 96441.84327216128\n",
      "Epoch 197932: reducing learning rate of group 0 to 1.4342e-03.\n",
      "Epoch 198001, Training Loss: 26571, Validation Loss: 56624, 99789.8387565248\n",
      "Epoch 198033: reducing learning rate of group 0 to 1.4328e-03.\n",
      "Epoch 198101, Training Loss: 27502, Validation Loss: 54510, 98908.66134051094\n",
      "Epoch 198134: reducing learning rate of group 0 to 1.4313e-03.\n",
      "Epoch 198201, Training Loss: 27563, Validation Loss: 54674, 90709.68423263624\n",
      "Epoch 198235: reducing learning rate of group 0 to 1.4299e-03.\n",
      "Epoch 198301, Training Loss: 25882, Validation Loss: 55210, 89371.38700095825\n",
      "Epoch 198336: reducing learning rate of group 0 to 1.4285e-03.\n",
      "Epoch 198401, Training Loss: 25608, Validation Loss: 56768, 105901.21798026162\n",
      "Epoch 198437: reducing learning rate of group 0 to 1.4271e-03.\n",
      "Epoch 198501, Training Loss: 25398, Validation Loss: 56432, 88341.11172126785\n",
      "Epoch 198538: reducing learning rate of group 0 to 1.4256e-03.\n",
      "Epoch 198601, Training Loss: 26148, Validation Loss: 55672, 78256.04336852703\n",
      "Epoch 198639: reducing learning rate of group 0 to 1.4242e-03.\n",
      "Epoch 198701, Training Loss: 24943, Validation Loss: 56092, 109924.76590111048\n",
      "Epoch 198740: reducing learning rate of group 0 to 1.4228e-03.\n",
      "Epoch 198801, Training Loss: 29307, Validation Loss: 53557, 84586.58014389595\n",
      "Epoch 198841: reducing learning rate of group 0 to 1.4214e-03.\n",
      "Epoch 198901, Training Loss: 29094, Validation Loss: 55814, 97423.0111760085\n",
      "Epoch 198942: reducing learning rate of group 0 to 1.4199e-03.\n",
      "Epoch 199001, Training Loss: 26153, Validation Loss: 55122, 95141.8043843644\n",
      "Epoch 199043: reducing learning rate of group 0 to 1.4185e-03.\n",
      "Epoch 199101, Training Loss: 25475, Validation Loss: 54949, 70592.43370570791\n",
      "Epoch 199144: reducing learning rate of group 0 to 1.4171e-03.\n",
      "Epoch 199201, Training Loss: 26592, Validation Loss: 54883, 100551.42414911072\n",
      "Epoch 199245: reducing learning rate of group 0 to 1.4157e-03.\n",
      "Epoch 199301, Training Loss: 25422, Validation Loss: 56937, 80289.94457448869\n",
      "Epoch 199346: reducing learning rate of group 0 to 1.4143e-03.\n",
      "Epoch 199401, Training Loss: 28698, Validation Loss: 56591, 101694.69824567146\n",
      "Epoch 199447: reducing learning rate of group 0 to 1.4128e-03.\n",
      "Epoch 199501, Training Loss: 29322, Validation Loss: 55032, 114037.87127825445\n",
      "Epoch 199548: reducing learning rate of group 0 to 1.4114e-03.\n",
      "Epoch 199601, Training Loss: 29652, Validation Loss: 56180, 80475.71288038394\n",
      "Epoch 199649: reducing learning rate of group 0 to 1.4100e-03.\n",
      "Epoch 199701, Training Loss: 26927, Validation Loss: 57687, 83568.06141122253\n",
      "Epoch 199750: reducing learning rate of group 0 to 1.4086e-03.\n",
      "Epoch 199801, Training Loss: 25808, Validation Loss: 54896, 80410.93204417345\n",
      "Epoch 199851: reducing learning rate of group 0 to 1.4072e-03.\n",
      "Epoch 199901, Training Loss: 27580, Validation Loss: 55995, 89293.5357678155\n",
      "Epoch 199952: reducing learning rate of group 0 to 1.4058e-03.\n",
      "Epoch 200001, Training Loss: 26253, Validation Loss: 54392, 92570.94592971262\n",
      "Epoch 200053: reducing learning rate of group 0 to 1.4044e-03.\n",
      "Epoch 200101, Training Loss: 28060, Validation Loss: 55690, 90428.08444174462\n",
      "Epoch 200154: reducing learning rate of group 0 to 1.4030e-03.\n",
      "Epoch 200201, Training Loss: 25236, Validation Loss: 54715, 94004.54245498906\n",
      "Epoch 200255: reducing learning rate of group 0 to 1.4016e-03.\n",
      "Epoch 200301, Training Loss: 27779, Validation Loss: 54857, 125563.7410357645\n",
      "Epoch 200356: reducing learning rate of group 0 to 1.4002e-03.\n",
      "Epoch 200401, Training Loss: 27501, Validation Loss: 54766, 112342.87886258111\n",
      "Epoch 200457: reducing learning rate of group 0 to 1.3988e-03.\n",
      "Epoch 200501, Training Loss: 26221, Validation Loss: 55672, 92003.03672367113\n",
      "Epoch 200558: reducing learning rate of group 0 to 1.3974e-03.\n",
      "Epoch 200601, Training Loss: 25416, Validation Loss: 54580, 82406.84906388572\n",
      "Epoch 200659: reducing learning rate of group 0 to 1.3960e-03.\n",
      "Epoch 200701, Training Loss: 26855, Validation Loss: 54258, 92842.52351105206\n",
      "Epoch 200760: reducing learning rate of group 0 to 1.3946e-03.\n",
      "Epoch 200801, Training Loss: 27468, Validation Loss: 54669, 99054.48146045786\n",
      "Epoch 200861: reducing learning rate of group 0 to 1.3932e-03.\n",
      "Epoch 200901, Training Loss: 29462, Validation Loss: 54552, 103725.0476512146\n",
      "Epoch 200962: reducing learning rate of group 0 to 1.3918e-03.\n",
      "Epoch 201001, Training Loss: 27342, Validation Loss: 55106, 98640.63943436487\n",
      "Epoch 201063: reducing learning rate of group 0 to 1.3904e-03.\n",
      "Epoch 201101, Training Loss: 27826, Validation Loss: 56242, 116561.53387157102\n",
      "Epoch 201164: reducing learning rate of group 0 to 1.3890e-03.\n",
      "Epoch 201201, Training Loss: 27172, Validation Loss: 56637, 101064.3911304566\n",
      "Epoch 201265: reducing learning rate of group 0 to 1.3876e-03.\n",
      "Epoch 201301, Training Loss: 26711, Validation Loss: 55076, 75966.02241281599\n",
      "Epoch 201366: reducing learning rate of group 0 to 1.3862e-03.\n",
      "Epoch 201401, Training Loss: 27839, Validation Loss: 55980, 87381.3744157772\n",
      "Epoch 201467: reducing learning rate of group 0 to 1.3849e-03.\n",
      "Epoch 201501, Training Loss: 27182, Validation Loss: 54136, 92996.26405538058\n",
      "Epoch 201568: reducing learning rate of group 0 to 1.3835e-03.\n",
      "Epoch 201601, Training Loss: 29406, Validation Loss: 54711, 96342.03194563021\n",
      "Epoch 201669: reducing learning rate of group 0 to 1.3821e-03.\n",
      "Epoch 201701, Training Loss: 28271, Validation Loss: 55650, 93470.53961586351\n",
      "Epoch 201770: reducing learning rate of group 0 to 1.3807e-03.\n",
      "Epoch 201801, Training Loss: 25785, Validation Loss: 55667, 85436.29654253171\n",
      "Epoch 201871: reducing learning rate of group 0 to 1.3793e-03.\n",
      "Epoch 201901, Training Loss: 26332, Validation Loss: 55032, 111729.10640155466\n",
      "Epoch 201972: reducing learning rate of group 0 to 1.3779e-03.\n",
      "Epoch 202001, Training Loss: 31030, Validation Loss: 55830, 93064.77597075338\n",
      "Epoch 202073: reducing learning rate of group 0 to 1.3766e-03.\n",
      "Epoch 202101, Training Loss: 28008, Validation Loss: 53675, 97294.66186781152\n",
      "Epoch 202174: reducing learning rate of group 0 to 1.3752e-03.\n",
      "Epoch 202201, Training Loss: 25850, Validation Loss: 55575, 86330.23079985364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202275: reducing learning rate of group 0 to 1.3738e-03.\n",
      "Epoch 202301, Training Loss: 26568, Validation Loss: 55581, 100917.1373294932\n",
      "Epoch 202376: reducing learning rate of group 0 to 1.3724e-03.\n",
      "Epoch 202401, Training Loss: 26126, Validation Loss: 56321, 112097.73840642931\n",
      "Epoch 202477: reducing learning rate of group 0 to 1.3711e-03.\n",
      "Epoch 202501, Training Loss: 28627, Validation Loss: 54822, 92515.58056352237\n",
      "Epoch 202578: reducing learning rate of group 0 to 1.3697e-03.\n",
      "Epoch 202601, Training Loss: 26978, Validation Loss: 55175, 90329.68178519794\n",
      "Epoch 202679: reducing learning rate of group 0 to 1.3683e-03.\n",
      "Epoch 202701, Training Loss: 30688, Validation Loss: 56774, 83850.93032704142\n",
      "Epoch 202780: reducing learning rate of group 0 to 1.3670e-03.\n",
      "Epoch 202801, Training Loss: 25556, Validation Loss: 54069, 79794.51640018848\n",
      "Epoch 202881: reducing learning rate of group 0 to 1.3656e-03.\n",
      "Epoch 202901, Training Loss: 27266, Validation Loss: 55701, 124010.82454318453\n",
      "Epoch 202982: reducing learning rate of group 0 to 1.3642e-03.\n",
      "Epoch 203001, Training Loss: 24676, Validation Loss: 54089, 98082.05075830295\n",
      "Epoch 203083: reducing learning rate of group 0 to 1.3629e-03.\n",
      "Epoch 203101, Training Loss: 27842, Validation Loss: 55495, 98958.06403922716\n",
      "Epoch 203184: reducing learning rate of group 0 to 1.3615e-03.\n",
      "Epoch 203201, Training Loss: 27937, Validation Loss: 56921, 94985.16502458467\n",
      "Epoch 203285: reducing learning rate of group 0 to 1.3601e-03.\n",
      "Epoch 203301, Training Loss: 27404, Validation Loss: 56700, 94877.94442813242\n",
      "Epoch 203386: reducing learning rate of group 0 to 1.3588e-03.\n",
      "Epoch 203401, Training Loss: 26726, Validation Loss: 55666, 133317.9562981576\n",
      "Epoch 203487: reducing learning rate of group 0 to 1.3574e-03.\n",
      "Epoch 203501, Training Loss: 28408, Validation Loss: 55376, 95923.98664591368\n",
      "Epoch 203588: reducing learning rate of group 0 to 1.3561e-03.\n",
      "Epoch 203601, Training Loss: 25086, Validation Loss: 55721, 86324.16614009827\n",
      "Epoch 203689: reducing learning rate of group 0 to 1.3547e-03.\n",
      "Epoch 203701, Training Loss: 27169, Validation Loss: 56579, 95945.7273670814\n",
      "Epoch 203790: reducing learning rate of group 0 to 1.3534e-03.\n",
      "Epoch 203801, Training Loss: 29584, Validation Loss: 57417, 101296.79746031672\n",
      "Epoch 203891: reducing learning rate of group 0 to 1.3520e-03.\n",
      "Epoch 203901, Training Loss: 29965, Validation Loss: 54771, 112524.40247159568\n",
      "Epoch 203992: reducing learning rate of group 0 to 1.3506e-03.\n",
      "Epoch 204001, Training Loss: 28116, Validation Loss: 57325, 101024.32290525682\n",
      "Epoch 204093: reducing learning rate of group 0 to 1.3493e-03.\n",
      "Epoch 204101, Training Loss: 27483, Validation Loss: 56814, 94199.20368009697\n",
      "Epoch 204194: reducing learning rate of group 0 to 1.3479e-03.\n",
      "Epoch 204201, Training Loss: 27996, Validation Loss: 54741, 89739.79184760289\n",
      "Epoch 204295: reducing learning rate of group 0 to 1.3466e-03.\n",
      "Epoch 204301, Training Loss: 26393, Validation Loss: 56922, 102885.5623165844\n",
      "Epoch 204396: reducing learning rate of group 0 to 1.3453e-03.\n",
      "Epoch 204401, Training Loss: 27234, Validation Loss: 55645, 82949.75132314656\n",
      "Epoch 204497: reducing learning rate of group 0 to 1.3439e-03.\n",
      "Epoch 204501, Training Loss: 26317, Validation Loss: 55199, 102934.71592309973\n",
      "Epoch 204598: reducing learning rate of group 0 to 1.3426e-03.\n",
      "Epoch 204601, Training Loss: 27082, Validation Loss: 54815, 87277.43782161362\n",
      "Epoch 204699: reducing learning rate of group 0 to 1.3412e-03.\n",
      "Epoch 204701, Training Loss: 26222, Validation Loss: 57154, 116497.26412805724\n",
      "Epoch 204800: reducing learning rate of group 0 to 1.3399e-03.\n",
      "Epoch 204801, Training Loss: 26569, Validation Loss: 56614, 91973.10321263784\n",
      "Epoch 204901: reducing learning rate of group 0 to 1.3385e-03.\n",
      "Epoch 204901, Training Loss: 27049, Validation Loss: 55641, 103885.40335253284\n",
      "Epoch 205001, Training Loss: 26784, Validation Loss: 55931, 85749.62702215915\n",
      "Epoch 205002: reducing learning rate of group 0 to 1.3372e-03.\n",
      "Epoch 205101, Training Loss: 26341, Validation Loss: 54444, 98591.66013174888\n",
      "Epoch 205103: reducing learning rate of group 0 to 1.3359e-03.\n",
      "Epoch 205201, Training Loss: 25767, Validation Loss: 55262, 92829.13706768933\n",
      "Epoch 205204: reducing learning rate of group 0 to 1.3345e-03.\n",
      "Epoch 205301, Training Loss: 26410, Validation Loss: 55203, 71345.54935147877\n",
      "Epoch 205305: reducing learning rate of group 0 to 1.3332e-03.\n",
      "Epoch 205401, Training Loss: 27228, Validation Loss: 56979, 103512.7234142099\n",
      "Epoch 205406: reducing learning rate of group 0 to 1.3319e-03.\n",
      "Epoch 205501, Training Loss: 27963, Validation Loss: 54563, 83320.67617273454\n",
      "Epoch 205507: reducing learning rate of group 0 to 1.3305e-03.\n",
      "Epoch 205601, Training Loss: 26886, Validation Loss: 54516, 97433.69031685284\n",
      "Epoch 205608: reducing learning rate of group 0 to 1.3292e-03.\n",
      "Epoch 205701, Training Loss: 28538, Validation Loss: 55677, 88105.01261150162\n",
      "Epoch 205709: reducing learning rate of group 0 to 1.3279e-03.\n",
      "Epoch 205801, Training Loss: 28438, Validation Loss: 55619, 107708.88250911224\n",
      "Epoch 205810: reducing learning rate of group 0 to 1.3265e-03.\n",
      "Epoch 205901, Training Loss: 28292, Validation Loss: 55882, 91323.02509223197\n",
      "Epoch 205911: reducing learning rate of group 0 to 1.3252e-03.\n",
      "Epoch 206001, Training Loss: 27622, Validation Loss: 56589, 88640.41381833346\n",
      "Epoch 206012: reducing learning rate of group 0 to 1.3239e-03.\n",
      "Epoch 206101, Training Loss: 27555, Validation Loss: 55178, 116845.17164693521\n",
      "Epoch 206113: reducing learning rate of group 0 to 1.3226e-03.\n",
      "Epoch 206201, Training Loss: 27195, Validation Loss: 55394, 92233.66999188329\n",
      "Epoch 206214: reducing learning rate of group 0 to 1.3212e-03.\n",
      "Epoch 206301, Training Loss: 27446, Validation Loss: 55053, 82967.19907434747\n",
      "Epoch 206315: reducing learning rate of group 0 to 1.3199e-03.\n",
      "Epoch 206401, Training Loss: 27449, Validation Loss: 55161, 95467.02696483902\n",
      "Epoch 206416: reducing learning rate of group 0 to 1.3186e-03.\n",
      "Epoch 206501, Training Loss: 26730, Validation Loss: 55172, 96491.23391807605\n",
      "Epoch 206517: reducing learning rate of group 0 to 1.3173e-03.\n",
      "Epoch 206601, Training Loss: 28031, Validation Loss: 55201, 89641.39891612863\n",
      "Epoch 206618: reducing learning rate of group 0 to 1.3160e-03.\n",
      "Epoch 206701, Training Loss: 25856, Validation Loss: 55515, 89064.22548923532\n",
      "Epoch 206719: reducing learning rate of group 0 to 1.3146e-03.\n",
      "Epoch 206801, Training Loss: 27833, Validation Loss: 54843, 91225.43985198608\n",
      "Epoch 206820: reducing learning rate of group 0 to 1.3133e-03.\n",
      "Epoch 206901, Training Loss: 29758, Validation Loss: 55947, 88970.34592305298\n",
      "Epoch 206921: reducing learning rate of group 0 to 1.3120e-03.\n",
      "Epoch 207001, Training Loss: 26778, Validation Loss: 55995, 89948.06506564755\n",
      "Epoch 207022: reducing learning rate of group 0 to 1.3107e-03.\n",
      "Epoch 207101, Training Loss: 25991, Validation Loss: 56052, 88426.78149145153\n",
      "Epoch 207123: reducing learning rate of group 0 to 1.3094e-03.\n",
      "Epoch 207201, Training Loss: 29957, Validation Loss: 55735, 104794.99113227108\n",
      "Epoch 207224: reducing learning rate of group 0 to 1.3081e-03.\n",
      "Epoch 207301, Training Loss: 27599, Validation Loss: 54744, 92286.7009754689\n",
      "Epoch 207325: reducing learning rate of group 0 to 1.3068e-03.\n",
      "Epoch 207401, Training Loss: 28501, Validation Loss: 55652, 78821.00511627774\n",
      "Epoch 207426: reducing learning rate of group 0 to 1.3055e-03.\n",
      "Epoch 207501, Training Loss: 26170, Validation Loss: 55286, 103158.23748174857\n",
      "Epoch 207527: reducing learning rate of group 0 to 1.3042e-03.\n",
      "Epoch 207601, Training Loss: 26390, Validation Loss: 55354, 106164.96621718637\n",
      "Epoch 207628: reducing learning rate of group 0 to 1.3029e-03.\n",
      "Epoch 207701, Training Loss: 25917, Validation Loss: 55428, 110089.45627997218\n",
      "Epoch 207729: reducing learning rate of group 0 to 1.3016e-03.\n",
      "Epoch 207801, Training Loss: 29029, Validation Loss: 55714, 103602.17011840564\n",
      "Epoch 207830: reducing learning rate of group 0 to 1.3003e-03.\n",
      "Epoch 207901, Training Loss: 28685, Validation Loss: 55465, 106733.36287962482\n",
      "Epoch 207931: reducing learning rate of group 0 to 1.2990e-03.\n",
      "Epoch 208001, Training Loss: 25758, Validation Loss: 55761, 112494.5234227766\n",
      "Epoch 208032: reducing learning rate of group 0 to 1.2977e-03.\n",
      "Epoch 208101, Training Loss: 27373, Validation Loss: 54877, 93769.23587823451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208133: reducing learning rate of group 0 to 1.2964e-03.\n",
      "Epoch 208201, Training Loss: 29057, Validation Loss: 55034, 113760.76437112004\n",
      "Epoch 208234: reducing learning rate of group 0 to 1.2951e-03.\n",
      "Epoch 208301, Training Loss: 26874, Validation Loss: 55122, 106802.37552853541\n",
      "Epoch 208335: reducing learning rate of group 0 to 1.2938e-03.\n",
      "Epoch 208401, Training Loss: 27372, Validation Loss: 54879, 88469.29328426854\n",
      "Epoch 208436: reducing learning rate of group 0 to 1.2925e-03.\n",
      "Epoch 208501, Training Loss: 26379, Validation Loss: 54823, 99178.72778620078\n",
      "Epoch 208537: reducing learning rate of group 0 to 1.2912e-03.\n",
      "Epoch 208601, Training Loss: 26258, Validation Loss: 54803, 86209.05222159276\n",
      "Epoch 208638: reducing learning rate of group 0 to 1.2899e-03.\n",
      "Epoch 208701, Training Loss: 29123, Validation Loss: 55821, 82044.35870519903\n",
      "Epoch 208739: reducing learning rate of group 0 to 1.2886e-03.\n",
      "Epoch 208801, Training Loss: 26926, Validation Loss: 55387, 92669.26354462151\n",
      "Epoch 208840: reducing learning rate of group 0 to 1.2873e-03.\n",
      "Epoch 208901, Training Loss: 27444, Validation Loss: 55324, 90520.19754187921\n",
      "Epoch 208941: reducing learning rate of group 0 to 1.2860e-03.\n",
      "Epoch 209001, Training Loss: 26481, Validation Loss: 55790, 92980.1998227699\n",
      "Epoch 209042: reducing learning rate of group 0 to 1.2847e-03.\n",
      "Epoch 209101, Training Loss: 27277, Validation Loss: 54343, 82373.63031453891\n",
      "Epoch 209143: reducing learning rate of group 0 to 1.2835e-03.\n",
      "Epoch 209201, Training Loss: 24953, Validation Loss: 55979, 98357.29569450988\n",
      "Epoch 209244: reducing learning rate of group 0 to 1.2822e-03.\n",
      "Epoch 209301, Training Loss: 27411, Validation Loss: 54692, 106141.66195089086\n",
      "Epoch 209345: reducing learning rate of group 0 to 1.2809e-03.\n",
      "Epoch 209401, Training Loss: 28500, Validation Loss: 55864, 117143.33958639042\n",
      "Epoch 209446: reducing learning rate of group 0 to 1.2796e-03.\n",
      "Epoch 209501, Training Loss: 28534, Validation Loss: 54371, 90729.09735617456\n",
      "Epoch 209547: reducing learning rate of group 0 to 1.2783e-03.\n",
      "Epoch 209601, Training Loss: 25619, Validation Loss: 54963, 108157.16410825285\n",
      "Epoch 209648: reducing learning rate of group 0 to 1.2771e-03.\n",
      "Epoch 209701, Training Loss: 29231, Validation Loss: 54830, 101315.19848367461\n",
      "Epoch 209749: reducing learning rate of group 0 to 1.2758e-03.\n",
      "Epoch 209801, Training Loss: 28778, Validation Loss: 56801, 86332.51553392799\n",
      "Epoch 209850: reducing learning rate of group 0 to 1.2745e-03.\n",
      "Epoch 209901, Training Loss: 26670, Validation Loss: 54845, 82369.09487701306\n",
      "Epoch 209951: reducing learning rate of group 0 to 1.2732e-03.\n",
      "Epoch 210001, Training Loss: 25833, Validation Loss: 55184, 88165.78543147487\n",
      "Epoch 210052: reducing learning rate of group 0 to 1.2720e-03.\n",
      "Epoch 210101, Training Loss: 28918, Validation Loss: 55008, 106393.25230307259\n",
      "Epoch 210153: reducing learning rate of group 0 to 1.2707e-03.\n",
      "Epoch 210201, Training Loss: 26636, Validation Loss: 55240, 90707.54155660215\n",
      "Epoch 210254: reducing learning rate of group 0 to 1.2694e-03.\n",
      "Epoch 210301, Training Loss: 27022, Validation Loss: 54722, 96139.12452935947\n",
      "Epoch 210355: reducing learning rate of group 0 to 1.2681e-03.\n",
      "Epoch 210401, Training Loss: 28765, Validation Loss: 55118, 100545.97688546126\n",
      "Epoch 210456: reducing learning rate of group 0 to 1.2669e-03.\n",
      "Epoch 210501, Training Loss: 27460, Validation Loss: 55239, 84366.31780402167\n",
      "Epoch 210557: reducing learning rate of group 0 to 1.2656e-03.\n",
      "Epoch 210601, Training Loss: 26959, Validation Loss: 55680, 86254.00473155202\n",
      "Epoch 210658: reducing learning rate of group 0 to 1.2643e-03.\n",
      "Epoch 210701, Training Loss: 25892, Validation Loss: 56099, 88783.40710821889\n",
      "Epoch 210759: reducing learning rate of group 0 to 1.2631e-03.\n",
      "Epoch 210801, Training Loss: 25176, Validation Loss: 54720, 128285.20410641027\n",
      "Epoch 210860: reducing learning rate of group 0 to 1.2618e-03.\n",
      "Epoch 210901, Training Loss: 27379, Validation Loss: 54158, 91571.44209067083\n",
      "Epoch 210961: reducing learning rate of group 0 to 1.2606e-03.\n",
      "Epoch 211001, Training Loss: 28260, Validation Loss: 55778, 85171.11255124502\n",
      "Epoch 211062: reducing learning rate of group 0 to 1.2593e-03.\n",
      "Epoch 211101, Training Loss: 26087, Validation Loss: 55947, 115154.20256561814\n",
      "Epoch 211163: reducing learning rate of group 0 to 1.2580e-03.\n",
      "Epoch 211201, Training Loss: 27106, Validation Loss: 56933, 94002.68064986786\n",
      "Epoch 211264: reducing learning rate of group 0 to 1.2568e-03.\n",
      "Epoch 211301, Training Loss: 25747, Validation Loss: 55322, 93931.74186476278\n",
      "Epoch 211365: reducing learning rate of group 0 to 1.2555e-03.\n",
      "Epoch 211401, Training Loss: 28227, Validation Loss: 54656, 99502.86129223886\n",
      "Epoch 211466: reducing learning rate of group 0 to 1.2543e-03.\n",
      "Epoch 211501, Training Loss: 27027, Validation Loss: 56556, 86724.83876306\n",
      "Epoch 211567: reducing learning rate of group 0 to 1.2530e-03.\n",
      "Epoch 211601, Training Loss: 25312, Validation Loss: 55030, 104603.59535481218\n",
      "Epoch 211668: reducing learning rate of group 0 to 1.2518e-03.\n",
      "Epoch 211701, Training Loss: 26137, Validation Loss: 56057, 87106.53674786293\n",
      "Epoch 211769: reducing learning rate of group 0 to 1.2505e-03.\n",
      "Epoch 211801, Training Loss: 28091, Validation Loss: 55342, 100596.60127972637\n",
      "Epoch 211870: reducing learning rate of group 0 to 1.2493e-03.\n",
      "Epoch 211901, Training Loss: 28320, Validation Loss: 56172, 111408.85017845902\n",
      "Epoch 211971: reducing learning rate of group 0 to 1.2480e-03.\n",
      "Epoch 212001, Training Loss: 26042, Validation Loss: 56255, 104380.6916089563\n",
      "Epoch 212072: reducing learning rate of group 0 to 1.2468e-03.\n",
      "Epoch 212101, Training Loss: 27175, Validation Loss: 54778, 122914.76600057962\n",
      "Epoch 212173: reducing learning rate of group 0 to 1.2455e-03.\n",
      "Epoch 212201, Training Loss: 26460, Validation Loss: 55059, 99934.0823111051\n",
      "Epoch 212274: reducing learning rate of group 0 to 1.2443e-03.\n",
      "Epoch 212301, Training Loss: 26794, Validation Loss: 56539, 103649.73477778524\n",
      "Epoch 212375: reducing learning rate of group 0 to 1.2430e-03.\n",
      "Epoch 212401, Training Loss: 27589, Validation Loss: 55292, 99712.44358515019\n",
      "Epoch 212476: reducing learning rate of group 0 to 1.2418e-03.\n",
      "Epoch 212501, Training Loss: 26821, Validation Loss: 56019, 115718.50023813476\n",
      "Epoch 212577: reducing learning rate of group 0 to 1.2405e-03.\n",
      "Epoch 212601, Training Loss: 27190, Validation Loss: 55983, 99742.89379753261\n",
      "Epoch 212678: reducing learning rate of group 0 to 1.2393e-03.\n",
      "Epoch 212701, Training Loss: 25508, Validation Loss: 55721, 97771.4449320848\n",
      "Epoch 212779: reducing learning rate of group 0 to 1.2381e-03.\n",
      "Epoch 212801, Training Loss: 26750, Validation Loss: 55244, 97250.56492257184\n",
      "Epoch 212880: reducing learning rate of group 0 to 1.2368e-03.\n",
      "Epoch 212901, Training Loss: 27081, Validation Loss: 56215, 87629.65842635582\n",
      "Epoch 212981: reducing learning rate of group 0 to 1.2356e-03.\n",
      "Epoch 213001, Training Loss: 26443, Validation Loss: 55646, 107007.06635429007\n",
      "Epoch 213082: reducing learning rate of group 0 to 1.2343e-03.\n",
      "Epoch 213101, Training Loss: 27675, Validation Loss: 56175, 94074.76839000794\n",
      "Epoch 213183: reducing learning rate of group 0 to 1.2331e-03.\n",
      "Epoch 213201, Training Loss: 25924, Validation Loss: 54674, 95402.8664008507\n",
      "Epoch 213284: reducing learning rate of group 0 to 1.2319e-03.\n",
      "Epoch 213301, Training Loss: 26960, Validation Loss: 55858, 88793.40020498236\n",
      "Epoch 213385: reducing learning rate of group 0 to 1.2306e-03.\n",
      "Epoch 213401, Training Loss: 26540, Validation Loss: 56954, 96992.23524409845\n",
      "Epoch 213486: reducing learning rate of group 0 to 1.2294e-03.\n",
      "Epoch 213501, Training Loss: 25960, Validation Loss: 55502, 88348.10451054596\n",
      "Epoch 213587: reducing learning rate of group 0 to 1.2282e-03.\n",
      "Epoch 213601, Training Loss: 27437, Validation Loss: 55797, 98565.85137634615\n",
      "Epoch 213688: reducing learning rate of group 0 to 1.2270e-03.\n",
      "Epoch 213701, Training Loss: 27873, Validation Loss: 56353, 111500.36401482852\n",
      "Epoch 213789: reducing learning rate of group 0 to 1.2257e-03.\n",
      "Epoch 213801, Training Loss: 27600, Validation Loss: 55125, 117585.38400764496\n",
      "Epoch 213890: reducing learning rate of group 0 to 1.2245e-03.\n",
      "Epoch 213901, Training Loss: 26924, Validation Loss: 55929, 104318.637021244\n",
      "Epoch 213991: reducing learning rate of group 0 to 1.2233e-03.\n",
      "Epoch 214001, Training Loss: 29207, Validation Loss: 56113, 117523.763407173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214092: reducing learning rate of group 0 to 1.2221e-03.\n",
      "Epoch 214101, Training Loss: 26988, Validation Loss: 55481, 101821.89352640758\n",
      "Epoch 214193: reducing learning rate of group 0 to 1.2208e-03.\n",
      "Epoch 214201, Training Loss: 24025, Validation Loss: 54620, 112288.80855987211\n",
      "Epoch 214294: reducing learning rate of group 0 to 1.2196e-03.\n",
      "Epoch 214301, Training Loss: 28807, Validation Loss: 54997, 96554.84846650493\n",
      "Epoch 214395: reducing learning rate of group 0 to 1.2184e-03.\n",
      "Epoch 214401, Training Loss: 28058, Validation Loss: 54687, 112953.31506429698\n",
      "Epoch 214496: reducing learning rate of group 0 to 1.2172e-03.\n",
      "Epoch 214501, Training Loss: 25971, Validation Loss: 55558, 103046.76576138659\n",
      "Epoch 214597: reducing learning rate of group 0 to 1.2160e-03.\n",
      "Epoch 214601, Training Loss: 27289, Validation Loss: 56605, 88457.93164637852\n",
      "Epoch 214698: reducing learning rate of group 0 to 1.2147e-03.\n",
      "Epoch 214701, Training Loss: 26332, Validation Loss: 54585, 104275.19044096256\n",
      "Epoch 214799: reducing learning rate of group 0 to 1.2135e-03.\n",
      "Epoch 214801, Training Loss: 25413, Validation Loss: 54635, 95830.69685630231\n",
      "Epoch 214900: reducing learning rate of group 0 to 1.2123e-03.\n",
      "Epoch 214901, Training Loss: 24658, Validation Loss: 54865, 94533.48960161494\n",
      "Epoch 215001: reducing learning rate of group 0 to 1.2111e-03.\n",
      "Epoch 215001, Training Loss: 28782, Validation Loss: 54816, 95306.35585660559\n",
      "Epoch 215101, Training Loss: 27482, Validation Loss: 54842, 97076.12241541217\n",
      "Epoch 215102: reducing learning rate of group 0 to 1.2099e-03.\n",
      "Epoch 215201, Training Loss: 28400, Validation Loss: 56651, 105201.02932112078\n",
      "Epoch 215203: reducing learning rate of group 0 to 1.2087e-03.\n",
      "Epoch 215301, Training Loss: 26760, Validation Loss: 54728, 78502.73732444194\n",
      "Epoch 215304: reducing learning rate of group 0 to 1.2075e-03.\n",
      "Epoch 215401, Training Loss: 26007, Validation Loss: 55335, 105810.1634454126\n",
      "Epoch 215405: reducing learning rate of group 0 to 1.2063e-03.\n",
      "Epoch 215501, Training Loss: 28300, Validation Loss: 55236, 97507.55516165386\n",
      "Epoch 215506: reducing learning rate of group 0 to 1.2051e-03.\n",
      "Epoch 215601, Training Loss: 26081, Validation Loss: 55063, 122280.46920372301\n",
      "Epoch 215607: reducing learning rate of group 0 to 1.2039e-03.\n",
      "Epoch 215701, Training Loss: 25543, Validation Loss: 55181, 99649.3886410063\n",
      "Epoch 215708: reducing learning rate of group 0 to 1.2026e-03.\n",
      "Epoch 215801, Training Loss: 27489, Validation Loss: 55245, 93531.41937704601\n",
      "Epoch 215809: reducing learning rate of group 0 to 1.2014e-03.\n",
      "Epoch 215901, Training Loss: 27050, Validation Loss: 56097, 88781.43920806481\n",
      "Epoch 215910: reducing learning rate of group 0 to 1.2002e-03.\n",
      "Epoch 216001, Training Loss: 25842, Validation Loss: 55911, 112913.77286563965\n",
      "Epoch 216011: reducing learning rate of group 0 to 1.1990e-03.\n",
      "Epoch 216101, Training Loss: 28286, Validation Loss: 55356, 109593.54609861271\n",
      "Epoch 216112: reducing learning rate of group 0 to 1.1978e-03.\n",
      "Epoch 216201, Training Loss: 29008, Validation Loss: 54544, 84686.77618115564\n",
      "Epoch 216213: reducing learning rate of group 0 to 1.1966e-03.\n",
      "Epoch 216301, Training Loss: 25705, Validation Loss: 55326, 91552.23175384937\n",
      "Epoch 216314: reducing learning rate of group 0 to 1.1955e-03.\n",
      "Epoch 216401, Training Loss: 26577, Validation Loss: 55751, 92884.48274095559\n",
      "Epoch 216415: reducing learning rate of group 0 to 1.1943e-03.\n",
      "Epoch 216501, Training Loss: 29031, Validation Loss: 55400, 93761.94835307501\n",
      "Epoch 216516: reducing learning rate of group 0 to 1.1931e-03.\n",
      "Epoch 216601, Training Loss: 27701, Validation Loss: 55806, 97947.0412764791\n",
      "Epoch 216617: reducing learning rate of group 0 to 1.1919e-03.\n",
      "Epoch 216701, Training Loss: 27717, Validation Loss: 56072, 94756.40519076904\n",
      "Epoch 216718: reducing learning rate of group 0 to 1.1907e-03.\n",
      "Epoch 216801, Training Loss: 28344, Validation Loss: 55243, 85287.58029263883\n",
      "Epoch 216819: reducing learning rate of group 0 to 1.1895e-03.\n",
      "Epoch 216901, Training Loss: 28068, Validation Loss: 55688, 98500.21434416343\n",
      "Epoch 216920: reducing learning rate of group 0 to 1.1883e-03.\n",
      "Epoch 217001, Training Loss: 27399, Validation Loss: 56728, 81323.93977517945\n",
      "Epoch 217021: reducing learning rate of group 0 to 1.1871e-03.\n",
      "Epoch 217101, Training Loss: 28196, Validation Loss: 55866, 89302.62289741536\n",
      "Epoch 217122: reducing learning rate of group 0 to 1.1859e-03.\n",
      "Epoch 217201, Training Loss: 28437, Validation Loss: 54894, 92588.98919238611\n",
      "Epoch 217223: reducing learning rate of group 0 to 1.1847e-03.\n",
      "Epoch 217301, Training Loss: 28786, Validation Loss: 55687, 108157.85321934464\n",
      "Epoch 217324: reducing learning rate of group 0 to 1.1835e-03.\n",
      "Epoch 217401, Training Loss: 26626, Validation Loss: 55695, 105848.95381099822\n",
      "Epoch 217425: reducing learning rate of group 0 to 1.1824e-03.\n",
      "Epoch 217501, Training Loss: 26558, Validation Loss: 54960, 99560.64743303756\n",
      "Epoch 217526: reducing learning rate of group 0 to 1.1812e-03.\n",
      "Epoch 217601, Training Loss: 28102, Validation Loss: 55414, 113692.97816389683\n",
      "Epoch 217627: reducing learning rate of group 0 to 1.1800e-03.\n",
      "Epoch 217701, Training Loss: 27784, Validation Loss: 55822, 94755.72575931657\n",
      "Epoch 217728: reducing learning rate of group 0 to 1.1788e-03.\n",
      "Epoch 217801, Training Loss: 28510, Validation Loss: 56158, 98819.93799063477\n",
      "Epoch 217829: reducing learning rate of group 0 to 1.1776e-03.\n",
      "Epoch 217901, Training Loss: 25762, Validation Loss: 55870, 90567.46777800277\n",
      "Epoch 217930: reducing learning rate of group 0 to 1.1765e-03.\n",
      "Epoch 218001, Training Loss: 27113, Validation Loss: 55837, 93911.76129191025\n",
      "Epoch 218031: reducing learning rate of group 0 to 1.1753e-03.\n",
      "Epoch 218101, Training Loss: 28427, Validation Loss: 56571, 111335.2030534605\n",
      "Epoch 218132: reducing learning rate of group 0 to 1.1741e-03.\n",
      "Epoch 218201, Training Loss: 25716, Validation Loss: 55381, 98108.39277671935\n",
      "Epoch 218233: reducing learning rate of group 0 to 1.1729e-03.\n",
      "Epoch 218301, Training Loss: 27202, Validation Loss: 55079, 93161.06582021265\n",
      "Epoch 218334: reducing learning rate of group 0 to 1.1718e-03.\n",
      "Epoch 218401, Training Loss: 27311, Validation Loss: 55998, 93809.39139836955\n",
      "Epoch 218435: reducing learning rate of group 0 to 1.1706e-03.\n",
      "Epoch 218501, Training Loss: 28836, Validation Loss: 56108, 119197.00594792231\n",
      "Epoch 218536: reducing learning rate of group 0 to 1.1694e-03.\n",
      "Epoch 218601, Training Loss: 25883, Validation Loss: 55926, 104537.06950660511\n",
      "Epoch 218637: reducing learning rate of group 0 to 1.1683e-03.\n",
      "Epoch 218701, Training Loss: 26229, Validation Loss: 53992, 92855.72823954264\n",
      "Epoch 218738: reducing learning rate of group 0 to 1.1671e-03.\n",
      "Epoch 218801, Training Loss: 29245, Validation Loss: 56562, 126445.34119592425\n",
      "Epoch 218839: reducing learning rate of group 0 to 1.1659e-03.\n",
      "Epoch 218901, Training Loss: 27781, Validation Loss: 54779, 115471.77308648382\n",
      "Epoch 218940: reducing learning rate of group 0 to 1.1648e-03.\n",
      "Epoch 219001, Training Loss: 25127, Validation Loss: 55675, 89721.45321558563\n",
      "Epoch 219041: reducing learning rate of group 0 to 1.1636e-03.\n",
      "Epoch 219101, Training Loss: 28061, Validation Loss: 55129, 101329.37386281788\n",
      "Epoch 219142: reducing learning rate of group 0 to 1.1624e-03.\n",
      "Epoch 219201, Training Loss: 24933, Validation Loss: 55403, 102769.88407937442\n",
      "Epoch 219243: reducing learning rate of group 0 to 1.1613e-03.\n",
      "Epoch 219301, Training Loss: 26497, Validation Loss: 55274, 77336.93073724002\n",
      "Epoch 219344: reducing learning rate of group 0 to 1.1601e-03.\n",
      "Epoch 219401, Training Loss: 27313, Validation Loss: 54518, 102772.41699570895\n",
      "Epoch 219445: reducing learning rate of group 0 to 1.1589e-03.\n",
      "Epoch 219501, Training Loss: 28612, Validation Loss: 55021, 111500.83228574491\n",
      "Epoch 219546: reducing learning rate of group 0 to 1.1578e-03.\n",
      "Epoch 219601, Training Loss: 27975, Validation Loss: 57516, 101571.27857354491\n",
      "Epoch 219647: reducing learning rate of group 0 to 1.1566e-03.\n",
      "Epoch 219701, Training Loss: 27107, Validation Loss: 55759, 97015.38525763353\n",
      "Epoch 219748: reducing learning rate of group 0 to 1.1555e-03.\n",
      "Epoch 219801, Training Loss: 26887, Validation Loss: 55982, 98339.36239066627\n",
      "Epoch 219849: reducing learning rate of group 0 to 1.1543e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219901, Training Loss: 26804, Validation Loss: 55090, 105783.41173265106\n",
      "Epoch 219950: reducing learning rate of group 0 to 1.1532e-03.\n",
      "Epoch 220001, Training Loss: 27774, Validation Loss: 55673, 109749.64169011521\n",
      "Epoch 220051: reducing learning rate of group 0 to 1.1520e-03.\n",
      "Epoch 220101, Training Loss: 32261, Validation Loss: 56007, 104036.43271844789\n",
      "Epoch 220152: reducing learning rate of group 0 to 1.1509e-03.\n",
      "Epoch 220201, Training Loss: 28266, Validation Loss: 54163, 99633.12246816584\n",
      "Epoch 220253: reducing learning rate of group 0 to 1.1497e-03.\n",
      "Epoch 220301, Training Loss: 26653, Validation Loss: 56234, 83527.26681435578\n",
      "Epoch 220354: reducing learning rate of group 0 to 1.1486e-03.\n",
      "Epoch 220401, Training Loss: 27467, Validation Loss: 56703, 102328.3991908045\n",
      "Epoch 220455: reducing learning rate of group 0 to 1.1474e-03.\n",
      "Epoch 220501, Training Loss: 27401, Validation Loss: 55730, 94785.491219175\n",
      "Epoch 220556: reducing learning rate of group 0 to 1.1463e-03.\n",
      "Epoch 220601, Training Loss: 27361, Validation Loss: 55858, 94482.11303675927\n",
      "Epoch 220657: reducing learning rate of group 0 to 1.1451e-03.\n",
      "Epoch 220701, Training Loss: 30029, Validation Loss: 56442, 130706.79955122569\n",
      "Epoch 220758: reducing learning rate of group 0 to 1.1440e-03.\n",
      "Epoch 220801, Training Loss: 27256, Validation Loss: 55476, 91530.41490723792\n",
      "Epoch 220859: reducing learning rate of group 0 to 1.1428e-03.\n",
      "Epoch 220901, Training Loss: 27159, Validation Loss: 54818, 106275.29348435868\n",
      "Epoch 220960: reducing learning rate of group 0 to 1.1417e-03.\n",
      "Epoch 221001, Training Loss: 25221, Validation Loss: 55316, 107281.96170803784\n",
      "Epoch 221061: reducing learning rate of group 0 to 1.1405e-03.\n",
      "Epoch 221101, Training Loss: 27270, Validation Loss: 56247, 97008.82298797656\n",
      "Epoch 221162: reducing learning rate of group 0 to 1.1394e-03.\n",
      "Epoch 221201, Training Loss: 27927, Validation Loss: 55362, 94523.65565973571\n",
      "Epoch 221263: reducing learning rate of group 0 to 1.1383e-03.\n",
      "Epoch 221301, Training Loss: 27166, Validation Loss: 55718, 122775.77442301996\n",
      "Epoch 221364: reducing learning rate of group 0 to 1.1371e-03.\n",
      "Epoch 221401, Training Loss: 25454, Validation Loss: 54741, 90309.1952138152\n",
      "Epoch 221465: reducing learning rate of group 0 to 1.1360e-03.\n",
      "Epoch 221501, Training Loss: 26218, Validation Loss: 55390, 98743.63210856776\n",
      "Epoch 221566: reducing learning rate of group 0 to 1.1348e-03.\n",
      "Epoch 221601, Training Loss: 29305, Validation Loss: 56974, 101970.4309878655\n",
      "Epoch 221667: reducing learning rate of group 0 to 1.1337e-03.\n",
      "Epoch 221701, Training Loss: 27854, Validation Loss: 54917, 134790.94158096178\n",
      "Epoch 221768: reducing learning rate of group 0 to 1.1326e-03.\n",
      "Epoch 221801, Training Loss: 26023, Validation Loss: 53935, 84690.86363443083\n",
      "Epoch 221869: reducing learning rate of group 0 to 1.1314e-03.\n",
      "Epoch 221901, Training Loss: 26520, Validation Loss: 54387, 96808.43241239665\n",
      "Epoch 221970: reducing learning rate of group 0 to 1.1303e-03.\n",
      "Epoch 222001, Training Loss: 26553, Validation Loss: 55284, 100077.28476265994\n",
      "Epoch 222071: reducing learning rate of group 0 to 1.1292e-03.\n",
      "Epoch 222101, Training Loss: 29016, Validation Loss: 56317, 85811.31501450698\n",
      "Epoch 222172: reducing learning rate of group 0 to 1.1281e-03.\n",
      "Epoch 222201, Training Loss: 26993, Validation Loss: 55806, 121945.86151184887\n",
      "Epoch 222273: reducing learning rate of group 0 to 1.1269e-03.\n",
      "Epoch 222301, Training Loss: 28519, Validation Loss: 56553, 98803.48914661891\n",
      "Epoch 222374: reducing learning rate of group 0 to 1.1258e-03.\n",
      "Epoch 222401, Training Loss: 26305, Validation Loss: 56749, 114730.9357603086\n",
      "Epoch 222475: reducing learning rate of group 0 to 1.1247e-03.\n",
      "Epoch 222501, Training Loss: 27232, Validation Loss: 56402, 111724.6906798281\n",
      "Epoch 222576: reducing learning rate of group 0 to 1.1235e-03.\n",
      "Epoch 222601, Training Loss: 27398, Validation Loss: 55207, 126508.59877373833\n",
      "Epoch 222677: reducing learning rate of group 0 to 1.1224e-03.\n",
      "Epoch 222701, Training Loss: 26573, Validation Loss: 56377, 85732.4861620326\n",
      "Epoch 222778: reducing learning rate of group 0 to 1.1213e-03.\n",
      "Epoch 222801, Training Loss: 24900, Validation Loss: 56329, 112636.068731036\n",
      "Epoch 222879: reducing learning rate of group 0 to 1.1202e-03.\n",
      "Epoch 222901, Training Loss: 26483, Validation Loss: 54287, 106712.35145835696\n",
      "Epoch 222980: reducing learning rate of group 0 to 1.1191e-03.\n",
      "Epoch 223001, Training Loss: 26044, Validation Loss: 54795, 110063.56499894585\n",
      "Epoch 223081: reducing learning rate of group 0 to 1.1179e-03.\n",
      "Epoch 223101, Training Loss: 27176, Validation Loss: 55103, 113505.76052603296\n",
      "Epoch 223182: reducing learning rate of group 0 to 1.1168e-03.\n",
      "Epoch 223201, Training Loss: 29861, Validation Loss: 55502, 114633.06934507222\n",
      "Epoch 223283: reducing learning rate of group 0 to 1.1157e-03.\n",
      "Epoch 223301, Training Loss: 25545, Validation Loss: 55097, 81101.20811695747\n",
      "Epoch 223384: reducing learning rate of group 0 to 1.1146e-03.\n",
      "Epoch 223401, Training Loss: 27269, Validation Loss: 55605, 96502.0702625393\n",
      "Epoch 223485: reducing learning rate of group 0 to 1.1135e-03.\n",
      "Epoch 223501, Training Loss: 28090, Validation Loss: 55225, 112442.60336129407\n",
      "Epoch 223586: reducing learning rate of group 0 to 1.1124e-03.\n",
      "Epoch 223601, Training Loss: 24271, Validation Loss: 57157, 100388.83852344584\n",
      "Epoch 223687: reducing learning rate of group 0 to 1.1113e-03.\n",
      "Epoch 223701, Training Loss: 26587, Validation Loss: 55704, 113156.96068058722\n",
      "Epoch 223788: reducing learning rate of group 0 to 1.1101e-03.\n",
      "Epoch 223801, Training Loss: 26946, Validation Loss: 55894, 89119.22290859406\n",
      "Epoch 223889: reducing learning rate of group 0 to 1.1090e-03.\n",
      "Epoch 223901, Training Loss: 26197, Validation Loss: 56008, 105774.40067737926\n",
      "Epoch 223990: reducing learning rate of group 0 to 1.1079e-03.\n",
      "Epoch 224001, Training Loss: 27676, Validation Loss: 56622, 95535.27014098018\n",
      "Epoch 224091: reducing learning rate of group 0 to 1.1068e-03.\n",
      "Epoch 224101, Training Loss: 27423, Validation Loss: 56774, 103618.18864271311\n",
      "Epoch 224192: reducing learning rate of group 0 to 1.1057e-03.\n",
      "Epoch 224201, Training Loss: 24549, Validation Loss: 55170, 93563.89100315724\n",
      "Epoch 224293: reducing learning rate of group 0 to 1.1046e-03.\n",
      "Epoch 224301, Training Loss: 26482, Validation Loss: 56160, 88322.36471465415\n",
      "Epoch 224394: reducing learning rate of group 0 to 1.1035e-03.\n",
      "Epoch 224401, Training Loss: 26603, Validation Loss: 57013, 76362.28083771867\n",
      "Epoch 224495: reducing learning rate of group 0 to 1.1024e-03.\n",
      "Epoch 224501, Training Loss: 26811, Validation Loss: 56557, 142076.32656173152\n",
      "Epoch 224596: reducing learning rate of group 0 to 1.1013e-03.\n",
      "Epoch 224601, Training Loss: 27031, Validation Loss: 55290, 109340.17005080043\n",
      "Epoch 224697: reducing learning rate of group 0 to 1.1002e-03.\n",
      "Epoch 224701, Training Loss: 25003, Validation Loss: 55969, 96294.98972178868\n",
      "Epoch 224798: reducing learning rate of group 0 to 1.0991e-03.\n",
      "Epoch 224801, Training Loss: 25776, Validation Loss: 55212, 95891.44049716281\n",
      "Epoch 224899: reducing learning rate of group 0 to 1.0980e-03.\n",
      "Epoch 224901, Training Loss: 27117, Validation Loss: 55488, 96916.10208233578\n",
      "Epoch 225000: reducing learning rate of group 0 to 1.0969e-03.\n",
      "Epoch 225001, Training Loss: 27268, Validation Loss: 56428, 100404.4790737158\n",
      "Epoch 225101: reducing learning rate of group 0 to 1.0958e-03.\n",
      "Epoch 225101, Training Loss: 25698, Validation Loss: 54892, 96116.42579406535\n",
      "Epoch 225201, Training Loss: 26212, Validation Loss: 55282, 86719.66386886033\n",
      "Epoch 225202: reducing learning rate of group 0 to 1.0947e-03.\n",
      "Epoch 225301, Training Loss: 28306, Validation Loss: 54974, 108653.5819337552\n",
      "Epoch 225303: reducing learning rate of group 0 to 1.0936e-03.\n",
      "Epoch 225401, Training Loss: 27456, Validation Loss: 56444, 105664.8228439837\n",
      "Epoch 225404: reducing learning rate of group 0 to 1.0925e-03.\n",
      "Epoch 225501, Training Loss: 27860, Validation Loss: 55805, 97944.21285044336\n",
      "Epoch 225505: reducing learning rate of group 0 to 1.0914e-03.\n",
      "Epoch 225601, Training Loss: 29661, Validation Loss: 54897, 95588.16296517958\n",
      "Epoch 225606: reducing learning rate of group 0 to 1.0903e-03.\n",
      "Epoch 225701, Training Loss: 28718, Validation Loss: 55289, 134872.6113157842\n",
      "Epoch 225707: reducing learning rate of group 0 to 1.0892e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225801, Training Loss: 24935, Validation Loss: 55144, 99194.38278326021\n",
      "Epoch 225808: reducing learning rate of group 0 to 1.0881e-03.\n",
      "Epoch 225901, Training Loss: 26991, Validation Loss: 55947, 105389.65315803797\n",
      "Epoch 225909: reducing learning rate of group 0 to 1.0871e-03.\n",
      "Epoch 226001, Training Loss: 26807, Validation Loss: 54761, 101145.12069085364\n",
      "Epoch 226010: reducing learning rate of group 0 to 1.0860e-03.\n",
      "Epoch 226101, Training Loss: 29562, Validation Loss: 55600, 116208.77523744595\n",
      "Epoch 226111: reducing learning rate of group 0 to 1.0849e-03.\n",
      "Epoch 226201, Training Loss: 28608, Validation Loss: 55975, 99536.24168364068\n",
      "Epoch 226212: reducing learning rate of group 0 to 1.0838e-03.\n",
      "Epoch 226301, Training Loss: 28569, Validation Loss: 55309, 89183.15604309102\n",
      "Epoch 226313: reducing learning rate of group 0 to 1.0827e-03.\n",
      "Epoch 226401, Training Loss: 28433, Validation Loss: 55405, 101359.21299077729\n",
      "Epoch 226414: reducing learning rate of group 0 to 1.0816e-03.\n",
      "Epoch 226501, Training Loss: 27469, Validation Loss: 55700, 107682.3415707994\n",
      "Epoch 226515: reducing learning rate of group 0 to 1.0806e-03.\n",
      "Epoch 226601, Training Loss: 28277, Validation Loss: 55659, 110109.65325729937\n",
      "Epoch 226616: reducing learning rate of group 0 to 1.0795e-03.\n",
      "Epoch 226701, Training Loss: 26603, Validation Loss: 55769, 82799.22235218777\n",
      "Epoch 226717: reducing learning rate of group 0 to 1.0784e-03.\n",
      "Epoch 226801, Training Loss: 25672, Validation Loss: 55063, 98261.03612274637\n",
      "Epoch 226818: reducing learning rate of group 0 to 1.0773e-03.\n",
      "Epoch 226901, Training Loss: 26586, Validation Loss: 55330, 127777.28050721576\n",
      "Epoch 226919: reducing learning rate of group 0 to 1.0762e-03.\n",
      "Epoch 227001, Training Loss: 28278, Validation Loss: 54925, 95853.87769134222\n",
      "Epoch 227020: reducing learning rate of group 0 to 1.0752e-03.\n",
      "Epoch 227101, Training Loss: 26450, Validation Loss: 54405, 102991.24112202617\n",
      "Epoch 227121: reducing learning rate of group 0 to 1.0741e-03.\n",
      "Epoch 227201, Training Loss: 27222, Validation Loss: 55056, 90048.47498018527\n",
      "Epoch 227222: reducing learning rate of group 0 to 1.0730e-03.\n",
      "Epoch 227301, Training Loss: 26399, Validation Loss: 56144, 114602.03017562088\n",
      "Epoch 227323: reducing learning rate of group 0 to 1.0719e-03.\n",
      "Epoch 227401, Training Loss: 29429, Validation Loss: 55184, 86036.74173744411\n",
      "Epoch 227424: reducing learning rate of group 0 to 1.0709e-03.\n",
      "Epoch 227501, Training Loss: 25835, Validation Loss: 56118, 95143.8741176001\n",
      "Epoch 227525: reducing learning rate of group 0 to 1.0698e-03.\n",
      "Epoch 227601, Training Loss: 28886, Validation Loss: 55110, 113144.5333933044\n",
      "Epoch 227626: reducing learning rate of group 0 to 1.0687e-03.\n",
      "Epoch 227701, Training Loss: 26680, Validation Loss: 55000, 100689.80284946912\n",
      "Epoch 227727: reducing learning rate of group 0 to 1.0677e-03.\n",
      "Epoch 227801, Training Loss: 28875, Validation Loss: 55379, 108036.95922520272\n",
      "Epoch 227828: reducing learning rate of group 0 to 1.0666e-03.\n",
      "Epoch 227901, Training Loss: 26480, Validation Loss: 55353, 89795.6124230342\n",
      "Epoch 227929: reducing learning rate of group 0 to 1.0655e-03.\n",
      "Epoch 228001, Training Loss: 26393, Validation Loss: 56085, 94075.21569161578\n",
      "Epoch 228030: reducing learning rate of group 0 to 1.0645e-03.\n",
      "Epoch 228101, Training Loss: 26729, Validation Loss: 55349, 116627.00252824556\n",
      "Epoch 228131: reducing learning rate of group 0 to 1.0634e-03.\n",
      "Epoch 228201, Training Loss: 25953, Validation Loss: 54503, 92317.95893884839\n",
      "Epoch 228232: reducing learning rate of group 0 to 1.0623e-03.\n",
      "Epoch 228301, Training Loss: 25496, Validation Loss: 54891, 101148.62192511768\n",
      "Epoch 228333: reducing learning rate of group 0 to 1.0613e-03.\n",
      "Epoch 228401, Training Loss: 27355, Validation Loss: 55756, 101009.0283748348\n",
      "Epoch 228434: reducing learning rate of group 0 to 1.0602e-03.\n",
      "Epoch 228501, Training Loss: 26308, Validation Loss: 57027, 114541.8796322421\n",
      "Epoch 228535: reducing learning rate of group 0 to 1.0591e-03.\n",
      "Epoch 228601, Training Loss: 25565, Validation Loss: 56001, 91224.16862915928\n",
      "Epoch 228636: reducing learning rate of group 0 to 1.0581e-03.\n",
      "Epoch 228701, Training Loss: 26433, Validation Loss: 56636, 107646.78236152184\n",
      "Epoch 228737: reducing learning rate of group 0 to 1.0570e-03.\n",
      "Epoch 228801, Training Loss: 28369, Validation Loss: 55812, 101504.07358188299\n",
      "Epoch 228838: reducing learning rate of group 0 to 1.0560e-03.\n",
      "Epoch 228901, Training Loss: 28458, Validation Loss: 55716, 100015.05416776886\n",
      "Epoch 228939: reducing learning rate of group 0 to 1.0549e-03.\n",
      "Epoch 229001, Training Loss: 26961, Validation Loss: 55323, 121048.33629218892\n",
      "Epoch 229040: reducing learning rate of group 0 to 1.0539e-03.\n",
      "Epoch 229101, Training Loss: 25812, Validation Loss: 55751, 107858.72610376735\n",
      "Epoch 229141: reducing learning rate of group 0 to 1.0528e-03.\n",
      "Epoch 229201, Training Loss: 26725, Validation Loss: 56308, 90296.15014250616\n",
      "Epoch 229242: reducing learning rate of group 0 to 1.0518e-03.\n",
      "Epoch 229301, Training Loss: 27492, Validation Loss: 55966, 105238.67471820021\n",
      "Epoch 229343: reducing learning rate of group 0 to 1.0507e-03.\n",
      "Epoch 229401, Training Loss: 27765, Validation Loss: 57774, 103323.52132229933\n",
      "Epoch 229444: reducing learning rate of group 0 to 1.0497e-03.\n",
      "Epoch 229501, Training Loss: 27238, Validation Loss: 56291, 105524.00092996238\n",
      "Epoch 229545: reducing learning rate of group 0 to 1.0486e-03.\n",
      "Epoch 229601, Training Loss: 27186, Validation Loss: 55514, 99999.1688007657\n",
      "Epoch 229646: reducing learning rate of group 0 to 1.0476e-03.\n",
      "Epoch 229701, Training Loss: 26251, Validation Loss: 56107, 95976.23145002969\n",
      "Epoch 229747: reducing learning rate of group 0 to 1.0465e-03.\n",
      "Epoch 229801, Training Loss: 30631, Validation Loss: 55639, 87160.43783571261\n",
      "Epoch 229848: reducing learning rate of group 0 to 1.0455e-03.\n",
      "Epoch 229901, Training Loss: 26837, Validation Loss: 56364, 93179.24655183875\n",
      "Epoch 229949: reducing learning rate of group 0 to 1.0444e-03.\n",
      "Epoch 230001, Training Loss: 25905, Validation Loss: 56569, 103218.38703964975\n",
      "Epoch 230050: reducing learning rate of group 0 to 1.0434e-03.\n",
      "Epoch 230101, Training Loss: 28278, Validation Loss: 55286, 128512.61549717207\n",
      "Epoch 230151: reducing learning rate of group 0 to 1.0423e-03.\n",
      "Epoch 230201, Training Loss: 26518, Validation Loss: 56753, 107697.895009285\n",
      "Epoch 230252: reducing learning rate of group 0 to 1.0413e-03.\n",
      "Epoch 230301, Training Loss: 28040, Validation Loss: 55860, 106308.85812409424\n",
      "Epoch 230353: reducing learning rate of group 0 to 1.0402e-03.\n",
      "Epoch 230401, Training Loss: 26671, Validation Loss: 57429, 89562.68326020334\n",
      "Epoch 230454: reducing learning rate of group 0 to 1.0392e-03.\n",
      "Epoch 230501, Training Loss: 25949, Validation Loss: 56371, 113178.8341278383\n",
      "Epoch 230555: reducing learning rate of group 0 to 1.0382e-03.\n",
      "Epoch 230601, Training Loss: 26613, Validation Loss: 55295, 85832.54625083094\n",
      "Epoch 230656: reducing learning rate of group 0 to 1.0371e-03.\n",
      "Epoch 230701, Training Loss: 26050, Validation Loss: 56828, 99703.8774678749\n",
      "Epoch 230757: reducing learning rate of group 0 to 1.0361e-03.\n",
      "Epoch 230801, Training Loss: 26735, Validation Loss: 56586, 91332.0222062093\n",
      "Epoch 230858: reducing learning rate of group 0 to 1.0351e-03.\n",
      "Epoch 230901, Training Loss: 26102, Validation Loss: 55701, 90848.18157213989\n",
      "Epoch 230959: reducing learning rate of group 0 to 1.0340e-03.\n",
      "Epoch 231001, Training Loss: 26063, Validation Loss: 55207, 112200.1408616376\n",
      "Epoch 231060: reducing learning rate of group 0 to 1.0330e-03.\n",
      "Epoch 231101, Training Loss: 27134, Validation Loss: 55419, 96605.373888854\n",
      "Epoch 231161: reducing learning rate of group 0 to 1.0319e-03.\n",
      "Epoch 231201, Training Loss: 27901, Validation Loss: 55448, 97607.06802292303\n",
      "Epoch 231262: reducing learning rate of group 0 to 1.0309e-03.\n",
      "Epoch 231301, Training Loss: 25334, Validation Loss: 55958, 106267.7579972043\n",
      "Epoch 231363: reducing learning rate of group 0 to 1.0299e-03.\n",
      "Epoch 231401, Training Loss: 25585, Validation Loss: 56494, 118267.91716619942\n",
      "Epoch 231464: reducing learning rate of group 0 to 1.0289e-03.\n",
      "Epoch 231501, Training Loss: 26241, Validation Loss: 55908, 100103.63339803675\n",
      "Epoch 231565: reducing learning rate of group 0 to 1.0278e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231601, Training Loss: 25207, Validation Loss: 55075, 98074.37989637016\n",
      "Epoch 231666: reducing learning rate of group 0 to 1.0268e-03.\n",
      "Epoch 231701, Training Loss: 25247, Validation Loss: 56585, 141676.4816155023\n",
      "Epoch 231767: reducing learning rate of group 0 to 1.0258e-03.\n",
      "Epoch 231801, Training Loss: 27977, Validation Loss: 57052, 118297.50934471095\n",
      "Epoch 231868: reducing learning rate of group 0 to 1.0247e-03.\n",
      "Epoch 231901, Training Loss: 26839, Validation Loss: 56168, 98207.50203762641\n",
      "Epoch 231969: reducing learning rate of group 0 to 1.0237e-03.\n",
      "Epoch 232001, Training Loss: 26927, Validation Loss: 54288, 93522.72742906299\n",
      "Epoch 232070: reducing learning rate of group 0 to 1.0227e-03.\n",
      "Epoch 232101, Training Loss: 26061, Validation Loss: 55465, 85620.60496880404\n",
      "Epoch 232171: reducing learning rate of group 0 to 1.0217e-03.\n",
      "Epoch 232201, Training Loss: 24754, Validation Loss: 55736, 119587.30061069755\n",
      "Epoch 232272: reducing learning rate of group 0 to 1.0207e-03.\n",
      "Epoch 232301, Training Loss: 27691, Validation Loss: 55379, 117490.51754386474\n",
      "Epoch 232373: reducing learning rate of group 0 to 1.0196e-03.\n",
      "Epoch 232401, Training Loss: 26931, Validation Loss: 55931, 107017.31449478578\n",
      "Epoch 232474: reducing learning rate of group 0 to 1.0186e-03.\n",
      "Epoch 232501, Training Loss: 26862, Validation Loss: 55263, 111054.84419394005\n",
      "Epoch 232575: reducing learning rate of group 0 to 1.0176e-03.\n",
      "Epoch 232601, Training Loss: 26590, Validation Loss: 54774, 89826.52310129085\n",
      "Epoch 232676: reducing learning rate of group 0 to 1.0166e-03.\n",
      "Epoch 232701, Training Loss: 24770, Validation Loss: 55909, 101686.28952601914\n",
      "Epoch 232777: reducing learning rate of group 0 to 1.0156e-03.\n",
      "Epoch 232801, Training Loss: 27347, Validation Loss: 55867, 104108.42356836815\n",
      "Epoch 232878: reducing learning rate of group 0 to 1.0145e-03.\n",
      "Epoch 232901, Training Loss: 26061, Validation Loss: 54942, 102836.70251340081\n",
      "Epoch 232979: reducing learning rate of group 0 to 1.0135e-03.\n",
      "Epoch 233001, Training Loss: 27935, Validation Loss: 55724, 98490.91676148109\n",
      "Epoch 233080: reducing learning rate of group 0 to 1.0125e-03.\n",
      "Epoch 233101, Training Loss: 25695, Validation Loss: 56144, 92013.30695596624\n",
      "Epoch 233181: reducing learning rate of group 0 to 1.0115e-03.\n",
      "Epoch 233201, Training Loss: 25323, Validation Loss: 56101, 80204.71323590014\n",
      "Epoch 233282: reducing learning rate of group 0 to 1.0105e-03.\n",
      "Epoch 233301, Training Loss: 25557, Validation Loss: 57046, 79566.64276346989\n",
      "Epoch 233383: reducing learning rate of group 0 to 1.0095e-03.\n",
      "Epoch 233401, Training Loss: 27403, Validation Loss: 55986, 83600.20180210343\n",
      "Epoch 233484: reducing learning rate of group 0 to 1.0085e-03.\n",
      "Epoch 233501, Training Loss: 25719, Validation Loss: 57204, 99011.36765077808\n",
      "Epoch 233585: reducing learning rate of group 0 to 1.0075e-03.\n",
      "Epoch 233601, Training Loss: 27535, Validation Loss: 54853, 101484.20255776966\n",
      "Epoch 233686: reducing learning rate of group 0 to 1.0065e-03.\n",
      "Epoch 233701, Training Loss: 25103, Validation Loss: 55525, 90376.395381917\n",
      "Epoch 233787: reducing learning rate of group 0 to 1.0055e-03.\n",
      "Epoch 233801, Training Loss: 27009, Validation Loss: 56160, 87181.58840265298\n",
      "Epoch 233888: reducing learning rate of group 0 to 1.0044e-03.\n",
      "Epoch 233901, Training Loss: 27487, Validation Loss: 55715, 95403.13430270663\n",
      "Epoch 233989: reducing learning rate of group 0 to 1.0034e-03.\n",
      "Epoch 234001, Training Loss: 26992, Validation Loss: 55488, 104416.96638899914\n",
      "Epoch 234090: reducing learning rate of group 0 to 1.0024e-03.\n",
      "Epoch 234101, Training Loss: 25685, Validation Loss: 55319, 91907.59635601375\n",
      "Epoch 234191: reducing learning rate of group 0 to 1.0014e-03.\n",
      "Epoch 234201, Training Loss: 27229, Validation Loss: 56792, 124202.04891644686\n",
      "Epoch 234292: reducing learning rate of group 0 to 1.0004e-03.\n",
      "Epoch 234301, Training Loss: 24815, Validation Loss: 55547, 87790.47861288769\n",
      "Epoch 234393: reducing learning rate of group 0 to 9.9943e-04.\n",
      "Epoch 234401, Training Loss: 27098, Validation Loss: 55186, 103101.98453415964\n",
      "Epoch 234494: reducing learning rate of group 0 to 9.9843e-04.\n",
      "Epoch 234501, Training Loss: 26387, Validation Loss: 54903, 80857.31262662647\n",
      "Epoch 234595: reducing learning rate of group 0 to 9.9744e-04.\n",
      "Epoch 234601, Training Loss: 28107, Validation Loss: 55587, 88917.21734374586\n",
      "Epoch 234696: reducing learning rate of group 0 to 9.9644e-04.\n",
      "Epoch 234701, Training Loss: 26467, Validation Loss: 56041, 87020.2939540765\n",
      "Epoch 234797: reducing learning rate of group 0 to 9.9544e-04.\n",
      "Epoch 234801, Training Loss: 24450, Validation Loss: 55682, 105248.73275195353\n",
      "Epoch 234898: reducing learning rate of group 0 to 9.9445e-04.\n",
      "Epoch 234901, Training Loss: 25372, Validation Loss: 55083, 83156.81363231898\n",
      "Epoch 234999: reducing learning rate of group 0 to 9.9345e-04.\n",
      "Epoch 235001, Training Loss: 25885, Validation Loss: 55956, 101711.16212597804\n",
      "Epoch 235100: reducing learning rate of group 0 to 9.9246e-04.\n",
      "Epoch 235101, Training Loss: 28268, Validation Loss: 55690, 115159.31998231665\n",
      "Epoch 235201: reducing learning rate of group 0 to 9.9147e-04.\n",
      "Epoch 235201, Training Loss: 24446, Validation Loss: 55179, 101031.84391137474\n",
      "Epoch 235301, Training Loss: 27676, Validation Loss: 55166, 106724.31725612255\n",
      "Epoch 235302: reducing learning rate of group 0 to 9.9047e-04.\n",
      "Epoch 235401, Training Loss: 26119, Validation Loss: 56598, 95189.35850757489\n",
      "Epoch 235403: reducing learning rate of group 0 to 9.8948e-04.\n",
      "Epoch 235501, Training Loss: 27744, Validation Loss: 56237, 104584.86589845274\n",
      "Epoch 235504: reducing learning rate of group 0 to 9.8849e-04.\n",
      "Epoch 235601, Training Loss: 25426, Validation Loss: 55147, 99874.6383276834\n",
      "Epoch 235605: reducing learning rate of group 0 to 9.8751e-04.\n",
      "Epoch 235701, Training Loss: 25480, Validation Loss: 55726, 124041.38516052479\n",
      "Epoch 235706: reducing learning rate of group 0 to 9.8652e-04.\n",
      "Epoch 235801, Training Loss: 26187, Validation Loss: 56100, 121271.002496327\n",
      "Epoch 235807: reducing learning rate of group 0 to 9.8553e-04.\n",
      "Epoch 235901, Training Loss: 26618, Validation Loss: 54303, 100359.81304410515\n",
      "Epoch 235908: reducing learning rate of group 0 to 9.8455e-04.\n",
      "Epoch 236001, Training Loss: 27659, Validation Loss: 55504, 104174.88024516059\n",
      "Epoch 236009: reducing learning rate of group 0 to 9.8356e-04.\n",
      "Epoch 236101, Training Loss: 27000, Validation Loss: 55738, 94535.27049494263\n",
      "Epoch 236110: reducing learning rate of group 0 to 9.8258e-04.\n",
      "Epoch 236201, Training Loss: 27476, Validation Loss: 55672, 86670.18238031825\n",
      "Epoch 236211: reducing learning rate of group 0 to 9.8160e-04.\n",
      "Epoch 236301, Training Loss: 24891, Validation Loss: 55242, 95830.93344419582\n",
      "Epoch 236312: reducing learning rate of group 0 to 9.8061e-04.\n",
      "Epoch 236401, Training Loss: 25268, Validation Loss: 55738, 104691.64579287637\n",
      "Epoch 236413: reducing learning rate of group 0 to 9.7963e-04.\n",
      "Epoch 236501, Training Loss: 25682, Validation Loss: 56124, 100778.82209270692\n",
      "Epoch 236514: reducing learning rate of group 0 to 9.7865e-04.\n",
      "Epoch 236601, Training Loss: 28685, Validation Loss: 55628, 117273.31717977526\n",
      "Epoch 236615: reducing learning rate of group 0 to 9.7768e-04.\n",
      "Epoch 236701, Training Loss: 24682, Validation Loss: 56215, 140926.43445267834\n",
      "Epoch 236716: reducing learning rate of group 0 to 9.7670e-04.\n",
      "Epoch 236801, Training Loss: 27426, Validation Loss: 55371, 92203.28347621248\n",
      "Epoch 236817: reducing learning rate of group 0 to 9.7572e-04.\n",
      "Epoch 236901, Training Loss: 26285, Validation Loss: 54684, 107618.84067670473\n",
      "Epoch 236918: reducing learning rate of group 0 to 9.7475e-04.\n",
      "Epoch 237001, Training Loss: 28447, Validation Loss: 56653, 114971.09529903058\n",
      "Epoch 237019: reducing learning rate of group 0 to 9.7377e-04.\n",
      "Epoch 237101, Training Loss: 26957, Validation Loss: 56431, 107925.53754307482\n",
      "Epoch 237120: reducing learning rate of group 0 to 9.7280e-04.\n",
      "Epoch 237201, Training Loss: 25288, Validation Loss: 54541, 115410.74489518882\n",
      "Epoch 237221: reducing learning rate of group 0 to 9.7182e-04.\n",
      "Epoch 237301, Training Loss: 27478, Validation Loss: 56249, 108629.14937716874\n",
      "Epoch 237322: reducing learning rate of group 0 to 9.7085e-04.\n",
      "Epoch 237401, Training Loss: 26901, Validation Loss: 57285, 101474.2695846627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237423: reducing learning rate of group 0 to 9.6988e-04.\n",
      "Epoch 237501, Training Loss: 25659, Validation Loss: 55005, 99606.78165998119\n",
      "Epoch 237524: reducing learning rate of group 0 to 9.6891e-04.\n",
      "Epoch 237601, Training Loss: 26495, Validation Loss: 55342, 138708.62334739018\n",
      "Epoch 237625: reducing learning rate of group 0 to 9.6794e-04.\n",
      "Epoch 237701, Training Loss: 27539, Validation Loss: 55844, 119643.1816660278\n",
      "Epoch 237726: reducing learning rate of group 0 to 9.6697e-04.\n",
      "Epoch 237801, Training Loss: 28792, Validation Loss: 56479, 107382.65827268781\n",
      "Epoch 237827: reducing learning rate of group 0 to 9.6601e-04.\n",
      "Epoch 237901, Training Loss: 27787, Validation Loss: 56414, 107867.4244360657\n",
      "Epoch 237928: reducing learning rate of group 0 to 9.6504e-04.\n",
      "Epoch 238001, Training Loss: 24216, Validation Loss: 54746, 106136.33558781986\n",
      "Epoch 238029: reducing learning rate of group 0 to 9.6408e-04.\n",
      "Epoch 238101, Training Loss: 25495, Validation Loss: 55065, 99553.36094321072\n",
      "Epoch 238130: reducing learning rate of group 0 to 9.6311e-04.\n",
      "Epoch 238201, Training Loss: 27120, Validation Loss: 56542, 109297.76783953718\n",
      "Epoch 238231: reducing learning rate of group 0 to 9.6215e-04.\n",
      "Epoch 238301, Training Loss: 27827, Validation Loss: 55787, 95220.99313567982\n",
      "Epoch 238332: reducing learning rate of group 0 to 9.6119e-04.\n",
      "Epoch 238401, Training Loss: 26321, Validation Loss: 54521, 92022.40740736516\n",
      "Epoch 238433: reducing learning rate of group 0 to 9.6023e-04.\n",
      "Epoch 238501, Training Loss: 25050, Validation Loss: 54309, 107363.84446818793\n",
      "Epoch 238534: reducing learning rate of group 0 to 9.5927e-04.\n",
      "Epoch 238601, Training Loss: 27330, Validation Loss: 55239, 102526.22494780726\n",
      "Epoch 238635: reducing learning rate of group 0 to 9.5831e-04.\n",
      "Epoch 238701, Training Loss: 25106, Validation Loss: 55019, 103403.57740277755\n",
      "Epoch 238736: reducing learning rate of group 0 to 9.5735e-04.\n",
      "Epoch 238801, Training Loss: 25575, Validation Loss: 56825, 104540.42828061288\n",
      "Epoch 238837: reducing learning rate of group 0 to 9.5639e-04.\n",
      "Epoch 238901, Training Loss: 26424, Validation Loss: 54763, 126654.91593342127\n",
      "Epoch 238938: reducing learning rate of group 0 to 9.5543e-04.\n",
      "Epoch 239001, Training Loss: 26371, Validation Loss: 56552, 102447.58874722086\n",
      "Epoch 239039: reducing learning rate of group 0 to 9.5448e-04.\n",
      "Epoch 239101, Training Loss: 26532, Validation Loss: 55813, 113626.02818779075\n",
      "Epoch 239140: reducing learning rate of group 0 to 9.5352e-04.\n",
      "Epoch 239201, Training Loss: 26079, Validation Loss: 55823, 110457.13958058733\n",
      "Epoch 239241: reducing learning rate of group 0 to 9.5257e-04.\n",
      "Epoch 239301, Training Loss: 27416, Validation Loss: 56204, 119310.87301639731\n",
      "Epoch 239342: reducing learning rate of group 0 to 9.5162e-04.\n",
      "Epoch 239401, Training Loss: 24817, Validation Loss: 55278, 102900.1062301586\n",
      "Epoch 239443: reducing learning rate of group 0 to 9.5067e-04.\n",
      "Epoch 239501, Training Loss: 26698, Validation Loss: 56345, 111337.52305827448\n",
      "Epoch 239544: reducing learning rate of group 0 to 9.4972e-04.\n",
      "Epoch 239601, Training Loss: 26321, Validation Loss: 56020, 101687.9552685549\n",
      "Epoch 239645: reducing learning rate of group 0 to 9.4877e-04.\n",
      "Epoch 239701, Training Loss: 27851, Validation Loss: 55247, 116506.56719829868\n",
      "Epoch 239746: reducing learning rate of group 0 to 9.4782e-04.\n",
      "Epoch 239801, Training Loss: 24823, Validation Loss: 55393, 117153.25638891132\n",
      "Epoch 239847: reducing learning rate of group 0 to 9.4687e-04.\n",
      "Epoch 239901, Training Loss: 24529, Validation Loss: 54951, 106371.07104966232\n",
      "Epoch 239948: reducing learning rate of group 0 to 9.4592e-04.\n",
      "Epoch 240001, Training Loss: 26657, Validation Loss: 55762, 108929.02032223564\n",
      "Epoch 240049: reducing learning rate of group 0 to 9.4498e-04.\n",
      "Epoch 240101, Training Loss: 26769, Validation Loss: 55882, 109545.10586718144\n",
      "Epoch 240150: reducing learning rate of group 0 to 9.4403e-04.\n",
      "Epoch 240201, Training Loss: 28938, Validation Loss: 56031, 126392.75616513554\n",
      "Epoch 240251: reducing learning rate of group 0 to 9.4309e-04.\n",
      "Epoch 240301, Training Loss: 27259, Validation Loss: 55544, 95984.9313446451\n",
      "Epoch 240352: reducing learning rate of group 0 to 9.4214e-04.\n",
      "Epoch 240401, Training Loss: 26678, Validation Loss: 55656, 116644.09296918422\n",
      "Epoch 240453: reducing learning rate of group 0 to 9.4120e-04.\n",
      "Epoch 240501, Training Loss: 27305, Validation Loss: 55316, 111299.25598705497\n",
      "Epoch 240554: reducing learning rate of group 0 to 9.4026e-04.\n",
      "Epoch 240601, Training Loss: 28061, Validation Loss: 55905, 143376.71767911204\n",
      "Epoch 240655: reducing learning rate of group 0 to 9.3932e-04.\n",
      "Epoch 240701, Training Loss: 26843, Validation Loss: 56110, 122338.63023323081\n",
      "Epoch 240756: reducing learning rate of group 0 to 9.3838e-04.\n",
      "Epoch 240801, Training Loss: 27287, Validation Loss: 55035, 117476.89011748979\n",
      "Epoch 240857: reducing learning rate of group 0 to 9.3744e-04.\n",
      "Epoch 240901, Training Loss: 24156, Validation Loss: 56061, 84849.83881013845\n",
      "Epoch 240958: reducing learning rate of group 0 to 9.3651e-04.\n",
      "Epoch 241001, Training Loss: 31818, Validation Loss: 56268, 106471.83374580792\n",
      "Epoch 241059: reducing learning rate of group 0 to 9.3557e-04.\n",
      "Epoch 241101, Training Loss: 26020, Validation Loss: 55438, 98955.17794329811\n",
      "Epoch 241160: reducing learning rate of group 0 to 9.3463e-04.\n",
      "Epoch 241201, Training Loss: 25496, Validation Loss: 56234, 125628.65854753491\n",
      "Epoch 241261: reducing learning rate of group 0 to 9.3370e-04.\n",
      "Epoch 241301, Training Loss: 28129, Validation Loss: 55631, 115980.37125786017\n",
      "Epoch 241362: reducing learning rate of group 0 to 9.3277e-04.\n",
      "Epoch 241401, Training Loss: 27912, Validation Loss: 56245, 110455.24317695115\n",
      "Epoch 241463: reducing learning rate of group 0 to 9.3183e-04.\n",
      "Epoch 241501, Training Loss: 27212, Validation Loss: 56283, 93701.51524905792\n",
      "Epoch 241564: reducing learning rate of group 0 to 9.3090e-04.\n",
      "Epoch 241601, Training Loss: 25412, Validation Loss: 55817, 104551.89708005071\n",
      "Epoch 241665: reducing learning rate of group 0 to 9.2997e-04.\n",
      "Epoch 241701, Training Loss: 26667, Validation Loss: 55104, 115971.26118009747\n",
      "Epoch 241766: reducing learning rate of group 0 to 9.2904e-04.\n",
      "Epoch 241801, Training Loss: 26980, Validation Loss: 54704, 133375.74682892297\n",
      "Epoch 241867: reducing learning rate of group 0 to 9.2811e-04.\n",
      "Epoch 241901, Training Loss: 27912, Validation Loss: 56521, 104538.86065222701\n",
      "Epoch 241968: reducing learning rate of group 0 to 9.2718e-04.\n",
      "Epoch 242001, Training Loss: 27142, Validation Loss: 56062, 82550.18062802327\n",
      "Epoch 242069: reducing learning rate of group 0 to 9.2626e-04.\n",
      "Epoch 242101, Training Loss: 25797, Validation Loss: 55986, 135876.43451955105\n",
      "Epoch 242170: reducing learning rate of group 0 to 9.2533e-04.\n",
      "Epoch 242201, Training Loss: 27399, Validation Loss: 56024, 112970.85661059646\n",
      "Epoch 242271: reducing learning rate of group 0 to 9.2440e-04.\n",
      "Epoch 242301, Training Loss: 27295, Validation Loss: 55879, 109167.72751996324\n",
      "Epoch 242372: reducing learning rate of group 0 to 9.2348e-04.\n",
      "Epoch 242401, Training Loss: 25236, Validation Loss: 56728, 105759.06909516074\n",
      "Epoch 242473: reducing learning rate of group 0 to 9.2256e-04.\n",
      "Epoch 242501, Training Loss: 26893, Validation Loss: 55646, 89299.8011196651\n",
      "Epoch 242574: reducing learning rate of group 0 to 9.2163e-04.\n",
      "Epoch 242601, Training Loss: 26129, Validation Loss: 55614, 102537.01672634727\n",
      "Epoch 242675: reducing learning rate of group 0 to 9.2071e-04.\n",
      "Epoch 242701, Training Loss: 25819, Validation Loss: 55663, 117411.04412552582\n",
      "Epoch 242776: reducing learning rate of group 0 to 9.1979e-04.\n",
      "Epoch 242801, Training Loss: 26009, Validation Loss: 56096, 96915.01863816574\n",
      "Epoch 242877: reducing learning rate of group 0 to 9.1887e-04.\n",
      "Epoch 242901, Training Loss: 24453, Validation Loss: 54667, 112102.06374623312\n",
      "Epoch 242978: reducing learning rate of group 0 to 9.1795e-04.\n",
      "Epoch 243001, Training Loss: 25274, Validation Loss: 56086, 98059.93255859043\n",
      "Epoch 243079: reducing learning rate of group 0 to 9.1703e-04.\n",
      "Epoch 243101, Training Loss: 26977, Validation Loss: 56049, 107530.40312080305\n",
      "Epoch 243180: reducing learning rate of group 0 to 9.1612e-04.\n",
      "Epoch 243201, Training Loss: 29380, Validation Loss: 55068, 99979.49673213279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243281: reducing learning rate of group 0 to 9.1520e-04.\n",
      "Epoch 243301, Training Loss: 26751, Validation Loss: 55489, 104647.57221318975\n",
      "Epoch 243382: reducing learning rate of group 0 to 9.1429e-04.\n",
      "Epoch 243401, Training Loss: 26671, Validation Loss: 55810, 111631.00629270128\n",
      "Epoch 243483: reducing learning rate of group 0 to 9.1337e-04.\n",
      "Epoch 243501, Training Loss: 26563, Validation Loss: 56005, 111235.71221299369\n",
      "Epoch 243584: reducing learning rate of group 0 to 9.1246e-04.\n",
      "Epoch 243601, Training Loss: 26577, Validation Loss: 54753, 96843.85928522861\n",
      "Epoch 243685: reducing learning rate of group 0 to 9.1155e-04.\n",
      "Epoch 243701, Training Loss: 27199, Validation Loss: 55181, 105901.52216575394\n",
      "Epoch 243786: reducing learning rate of group 0 to 9.1063e-04.\n",
      "Epoch 243801, Training Loss: 25486, Validation Loss: 55409, 96729.53474408267\n",
      "Epoch 243887: reducing learning rate of group 0 to 9.0972e-04.\n",
      "Epoch 243901, Training Loss: 27012, Validation Loss: 56273, 91265.71150497043\n",
      "Epoch 243988: reducing learning rate of group 0 to 9.0881e-04.\n",
      "Epoch 244001, Training Loss: 26911, Validation Loss: 55513, 120045.16873316122\n",
      "Epoch 244089: reducing learning rate of group 0 to 9.0791e-04.\n",
      "Epoch 244101, Training Loss: 27552, Validation Loss: 57231, 115664.3436753162\n",
      "Epoch 244190: reducing learning rate of group 0 to 9.0700e-04.\n",
      "Epoch 244201, Training Loss: 34886, Validation Loss: 56079, 121545.79229375486\n",
      "Epoch 244291: reducing learning rate of group 0 to 9.0609e-04.\n",
      "Epoch 244301, Training Loss: 27780, Validation Loss: 55774, 101428.2760884862\n",
      "Epoch 244392: reducing learning rate of group 0 to 9.0518e-04.\n",
      "Epoch 244401, Training Loss: 26118, Validation Loss: 54996, 110940.08174528372\n",
      "Epoch 244493: reducing learning rate of group 0 to 9.0428e-04.\n",
      "Epoch 244501, Training Loss: 26797, Validation Loss: 55296, 94092.07079351721\n",
      "Epoch 244594: reducing learning rate of group 0 to 9.0338e-04.\n",
      "Epoch 244601, Training Loss: 25535, Validation Loss: 55722, 122723.62429426103\n",
      "Epoch 244695: reducing learning rate of group 0 to 9.0247e-04.\n",
      "Epoch 244701, Training Loss: 26336, Validation Loss: 55579, 96431.30002361827\n",
      "Epoch 244796: reducing learning rate of group 0 to 9.0157e-04.\n",
      "Epoch 244801, Training Loss: 27783, Validation Loss: 55862, 91934.64969962061\n",
      "Epoch 244897: reducing learning rate of group 0 to 9.0067e-04.\n",
      "Epoch 244901, Training Loss: 25699, Validation Loss: 55332, 101462.40815293993\n",
      "Epoch 244998: reducing learning rate of group 0 to 8.9977e-04.\n",
      "Epoch 245001, Training Loss: 26497, Validation Loss: 56021, 102896.90454471279\n",
      "Epoch 245099: reducing learning rate of group 0 to 8.9887e-04.\n",
      "Epoch 245101, Training Loss: 24871, Validation Loss: 54554, 101206.26402447889\n",
      "Epoch 245200: reducing learning rate of group 0 to 8.9797e-04.\n",
      "Epoch 245201, Training Loss: 25452, Validation Loss: 55773, 128036.93490541218\n",
      "Epoch 245301: reducing learning rate of group 0 to 8.9707e-04.\n",
      "Epoch 245301, Training Loss: 26173, Validation Loss: 57331, 88789.91930412588\n",
      "Epoch 245401, Training Loss: 26124, Validation Loss: 55132, 111883.0955371994\n",
      "Epoch 245402: reducing learning rate of group 0 to 8.9617e-04.\n",
      "Epoch 245501, Training Loss: 25622, Validation Loss: 55475, 106122.03550784818\n",
      "Epoch 245503: reducing learning rate of group 0 to 8.9528e-04.\n",
      "Epoch 245601, Training Loss: 25211, Validation Loss: 55428, 101013.35366600279\n",
      "Epoch 245604: reducing learning rate of group 0 to 8.9438e-04.\n",
      "Epoch 245701, Training Loss: 27104, Validation Loss: 55502, 88820.3065719555\n",
      "Epoch 245705: reducing learning rate of group 0 to 8.9349e-04.\n",
      "Epoch 245801, Training Loss: 25695, Validation Loss: 56977, 101139.57726162627\n",
      "Epoch 245806: reducing learning rate of group 0 to 8.9259e-04.\n",
      "Epoch 245901, Training Loss: 26698, Validation Loss: 55624, 105600.87212418324\n",
      "Epoch 245907: reducing learning rate of group 0 to 8.9170e-04.\n",
      "Epoch 246001, Training Loss: 26229, Validation Loss: 55316, 108678.45248414588\n",
      "Epoch 246008: reducing learning rate of group 0 to 8.9081e-04.\n",
      "Epoch 246101, Training Loss: 27749, Validation Loss: 54997, 134723.63095840614\n",
      "Epoch 246109: reducing learning rate of group 0 to 8.8992e-04.\n",
      "Epoch 246201, Training Loss: 26798, Validation Loss: 55420, 134012.91983045734\n",
      "Epoch 246210: reducing learning rate of group 0 to 8.8903e-04.\n",
      "Epoch 246301, Training Loss: 23968, Validation Loss: 54911, 125179.25923957855\n",
      "Epoch 246311: reducing learning rate of group 0 to 8.8814e-04.\n",
      "Epoch 246401, Training Loss: 26740, Validation Loss: 56097, 99574.02561894323\n",
      "Epoch 246412: reducing learning rate of group 0 to 8.8725e-04.\n",
      "Epoch 246501, Training Loss: 26778, Validation Loss: 54795, 99270.62023084442\n",
      "Epoch 246513: reducing learning rate of group 0 to 8.8636e-04.\n",
      "Epoch 246601, Training Loss: 27346, Validation Loss: 55242, 127670.56658982275\n",
      "Epoch 246614: reducing learning rate of group 0 to 8.8548e-04.\n",
      "Epoch 246701, Training Loss: 26749, Validation Loss: 55328, 109991.3801630219\n",
      "Epoch 246715: reducing learning rate of group 0 to 8.8459e-04.\n",
      "Epoch 246801, Training Loss: 26142, Validation Loss: 55554, 107592.28117680586\n",
      "Epoch 246816: reducing learning rate of group 0 to 8.8371e-04.\n",
      "Epoch 246901, Training Loss: 26610, Validation Loss: 55728, 116276.95996942262\n",
      "Epoch 246917: reducing learning rate of group 0 to 8.8282e-04.\n",
      "Epoch 247001, Training Loss: 24046, Validation Loss: 55876, 96114.29758326746\n",
      "Epoch 247018: reducing learning rate of group 0 to 8.8194e-04.\n",
      "Epoch 247101, Training Loss: 26889, Validation Loss: 55737, 115221.24017754535\n",
      "Epoch 247119: reducing learning rate of group 0 to 8.8106e-04.\n",
      "Epoch 247201, Training Loss: 25875, Validation Loss: 55136, 89516.07645718202\n",
      "Epoch 247220: reducing learning rate of group 0 to 8.8018e-04.\n",
      "Epoch 247301, Training Loss: 25424, Validation Loss: 55093, 98061.16375077954\n",
      "Epoch 247321: reducing learning rate of group 0 to 8.7930e-04.\n",
      "Epoch 247401, Training Loss: 26385, Validation Loss: 55995, 105589.36969684223\n",
      "Epoch 247422: reducing learning rate of group 0 to 8.7842e-04.\n",
      "Epoch 247501, Training Loss: 27303, Validation Loss: 55210, 114068.55737319325\n",
      "Epoch 247523: reducing learning rate of group 0 to 8.7754e-04.\n",
      "Epoch 247601, Training Loss: 24171, Validation Loss: 55892, 109999.06843836453\n",
      "Epoch 247624: reducing learning rate of group 0 to 8.7666e-04.\n",
      "Epoch 247701, Training Loss: 24478, Validation Loss: 55116, 134123.8011850747\n",
      "Epoch 247725: reducing learning rate of group 0 to 8.7579e-04.\n",
      "Epoch 247801, Training Loss: 27649, Validation Loss: 55782, 102711.87204809014\n",
      "Epoch 247826: reducing learning rate of group 0 to 8.7491e-04.\n",
      "Epoch 247901, Training Loss: 26146, Validation Loss: 55222, 94230.32556861533\n",
      "Epoch 247927: reducing learning rate of group 0 to 8.7404e-04.\n",
      "Epoch 248001, Training Loss: 25436, Validation Loss: 56676, 121436.69637425372\n",
      "Epoch 248028: reducing learning rate of group 0 to 8.7316e-04.\n",
      "Epoch 248101, Training Loss: 25348, Validation Loss: 55168, 101982.6387050111\n",
      "Epoch 248129: reducing learning rate of group 0 to 8.7229e-04.\n",
      "Epoch 248201, Training Loss: 26143, Validation Loss: 55199, 95752.61039997234\n",
      "Epoch 248230: reducing learning rate of group 0 to 8.7142e-04.\n",
      "Epoch 248301, Training Loss: 24840, Validation Loss: 55724, 116364.20038758707\n",
      "Epoch 248331: reducing learning rate of group 0 to 8.7055e-04.\n",
      "Epoch 248401, Training Loss: 26523, Validation Loss: 55604, 95909.70908637224\n",
      "Epoch 248432: reducing learning rate of group 0 to 8.6967e-04.\n",
      "Epoch 248501, Training Loss: 27419, Validation Loss: 56075, 97393.65115897842\n",
      "Epoch 248533: reducing learning rate of group 0 to 8.6880e-04.\n",
      "Epoch 248601, Training Loss: 25187, Validation Loss: 55340, 116280.54185771418\n",
      "Epoch 248634: reducing learning rate of group 0 to 8.6794e-04.\n",
      "Epoch 248701, Training Loss: 26992, Validation Loss: 56181, 111416.95909278635\n",
      "Epoch 248735: reducing learning rate of group 0 to 8.6707e-04.\n",
      "Epoch 248801, Training Loss: 26574, Validation Loss: 55751, 111397.36092143849\n",
      "Epoch 248836: reducing learning rate of group 0 to 8.6620e-04.\n",
      "Epoch 248901, Training Loss: 25831, Validation Loss: 55179, 101349.22623590648\n",
      "Epoch 248937: reducing learning rate of group 0 to 8.6533e-04.\n",
      "Epoch 249001, Training Loss: 26364, Validation Loss: 56565, 99333.85258011498\n",
      "Epoch 249038: reducing learning rate of group 0 to 8.6447e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249101, Training Loss: 25490, Validation Loss: 55112, 125065.99982402485\n",
      "Epoch 249139: reducing learning rate of group 0 to 8.6361e-04.\n",
      "Epoch 249201, Training Loss: 26573, Validation Loss: 55477, 105528.22471112676\n",
      "Epoch 249240: reducing learning rate of group 0 to 8.6274e-04.\n",
      "Epoch 249301, Training Loss: 26626, Validation Loss: 55342, 97744.2275453667\n",
      "Epoch 249341: reducing learning rate of group 0 to 8.6188e-04.\n",
      "Epoch 249401, Training Loss: 24730, Validation Loss: 55469, 118943.65056965947\n",
      "Epoch 249442: reducing learning rate of group 0 to 8.6102e-04.\n",
      "Epoch 249501, Training Loss: 27595, Validation Loss: 55049, 136329.1714361111\n",
      "Epoch 249543: reducing learning rate of group 0 to 8.6016e-04.\n",
      "Epoch 249601, Training Loss: 25091, Validation Loss: 55397, 144757.2719729384\n",
      "Epoch 249644: reducing learning rate of group 0 to 8.5930e-04.\n",
      "Epoch 249701, Training Loss: 26176, Validation Loss: 56223, 99819.72398978908\n",
      "Epoch 249745: reducing learning rate of group 0 to 8.5844e-04.\n",
      "Epoch 249801, Training Loss: 26801, Validation Loss: 55762, 98801.24531841784\n",
      "Epoch 249846: reducing learning rate of group 0 to 8.5758e-04.\n",
      "Epoch 249901, Training Loss: 27330, Validation Loss: 56356, 98024.36232432642\n",
      "Epoch 249947: reducing learning rate of group 0 to 8.5672e-04.\n",
      "Epoch 250001, Training Loss: 27232, Validation Loss: 55480, 108042.53991015402\n",
      "Epoch 250048: reducing learning rate of group 0 to 8.5586e-04.\n",
      "Epoch 250101, Training Loss: 27012, Validation Loss: 55663, 90888.56145245601\n",
      "Epoch 250149: reducing learning rate of group 0 to 8.5501e-04.\n",
      "Epoch 250201, Training Loss: 26061, Validation Loss: 56551, 87369.26204955562\n",
      "Epoch 250250: reducing learning rate of group 0 to 8.5415e-04.\n",
      "Epoch 250301, Training Loss: 24322, Validation Loss: 56468, 118337.19791754015\n",
      "Epoch 250351: reducing learning rate of group 0 to 8.5330e-04.\n",
      "Epoch 250401, Training Loss: 26651, Validation Loss: 54901, 125228.5967594579\n",
      "Epoch 250452: reducing learning rate of group 0 to 8.5245e-04.\n",
      "Epoch 250501, Training Loss: 27246, Validation Loss: 55617, 133677.04329260087\n",
      "Epoch 250553: reducing learning rate of group 0 to 8.5159e-04.\n",
      "Epoch 250601, Training Loss: 26069, Validation Loss: 55663, 106784.37668989554\n",
      "Epoch 250654: reducing learning rate of group 0 to 8.5074e-04.\n",
      "Epoch 250701, Training Loss: 26361, Validation Loss: 55679, 118253.38909316975\n",
      "Epoch 250755: reducing learning rate of group 0 to 8.4989e-04.\n",
      "Epoch 250801, Training Loss: 26968, Validation Loss: 55583, 122321.10242521844\n",
      "Epoch 250856: reducing learning rate of group 0 to 8.4904e-04.\n",
      "Epoch 250901, Training Loss: 27176, Validation Loss: 55361, 124047.69144427816\n",
      "Epoch 250957: reducing learning rate of group 0 to 8.4819e-04.\n",
      "Epoch 251001, Training Loss: 26878, Validation Loss: 55408, 111841.27866324833\n",
      "Epoch 251058: reducing learning rate of group 0 to 8.4734e-04.\n",
      "Epoch 251101, Training Loss: 26628, Validation Loss: 55780, 92680.18866186764\n",
      "Epoch 251159: reducing learning rate of group 0 to 8.4650e-04.\n",
      "Epoch 251201, Training Loss: 25103, Validation Loss: 54868, 87349.99901977686\n",
      "Epoch 251260: reducing learning rate of group 0 to 8.4565e-04.\n",
      "Epoch 251301, Training Loss: 24974, Validation Loss: 55943, 97848.83952867391\n",
      "Epoch 251361: reducing learning rate of group 0 to 8.4480e-04.\n",
      "Epoch 251401, Training Loss: 26362, Validation Loss: 55870, 120448.80068188017\n",
      "Epoch 251462: reducing learning rate of group 0 to 8.4396e-04.\n",
      "Epoch 251501, Training Loss: 30304, Validation Loss: 55151, 127190.9724712926\n",
      "Epoch 251563: reducing learning rate of group 0 to 8.4312e-04.\n",
      "Epoch 251601, Training Loss: 26360, Validation Loss: 56086, 98012.9544904192\n",
      "Epoch 251664: reducing learning rate of group 0 to 8.4227e-04.\n",
      "Epoch 251701, Training Loss: 27344, Validation Loss: 55123, 122214.73194407024\n",
      "Epoch 251765: reducing learning rate of group 0 to 8.4143e-04.\n",
      "Epoch 251801, Training Loss: 27514, Validation Loss: 56128, 113548.45057187074\n",
      "Epoch 251866: reducing learning rate of group 0 to 8.4059e-04.\n",
      "Epoch 251901, Training Loss: 25572, Validation Loss: 55328, 104419.69375010302\n",
      "Epoch 251967: reducing learning rate of group 0 to 8.3975e-04.\n",
      "Epoch 252001, Training Loss: 28233, Validation Loss: 56105, 126099.68548693201\n",
      "Epoch 252068: reducing learning rate of group 0 to 8.3891e-04.\n",
      "Epoch 252101, Training Loss: 26538, Validation Loss: 55360, 127241.53577377553\n",
      "Epoch 252169: reducing learning rate of group 0 to 8.3807e-04.\n",
      "Epoch 252201, Training Loss: 27005, Validation Loss: 56127, 91706.37950352568\n",
      "Epoch 252270: reducing learning rate of group 0 to 8.3723e-04.\n",
      "Epoch 252301, Training Loss: 26733, Validation Loss: 55984, 99497.86038305452\n",
      "Epoch 252371: reducing learning rate of group 0 to 8.3639e-04.\n",
      "Epoch 252401, Training Loss: 26328, Validation Loss: 56042, 98486.18078822336\n",
      "Epoch 252472: reducing learning rate of group 0 to 8.3556e-04.\n",
      "Epoch 252501, Training Loss: 25651, Validation Loss: 55870, 105117.09155236887\n",
      "Epoch 252573: reducing learning rate of group 0 to 8.3472e-04.\n",
      "Epoch 252601, Training Loss: 26060, Validation Loss: 55615, 121629.19584984111\n",
      "Epoch 252674: reducing learning rate of group 0 to 8.3389e-04.\n",
      "Epoch 252701, Training Loss: 26991, Validation Loss: 54800, 144325.2544951092\n",
      "Epoch 252775: reducing learning rate of group 0 to 8.3305e-04.\n",
      "Epoch 252801, Training Loss: 31734, Validation Loss: 55496, 111607.05132564677\n",
      "Epoch 252876: reducing learning rate of group 0 to 8.3222e-04.\n",
      "Epoch 252901, Training Loss: 27005, Validation Loss: 55675, 99930.75096201163\n",
      "Epoch 252977: reducing learning rate of group 0 to 8.3139e-04.\n",
      "Epoch 253001, Training Loss: 27616, Validation Loss: 56283, 99281.00059770001\n",
      "Epoch 253078: reducing learning rate of group 0 to 8.3056e-04.\n",
      "Epoch 253101, Training Loss: 26836, Validation Loss: 55855, 94731.84442623786\n",
      "Epoch 253179: reducing learning rate of group 0 to 8.2973e-04.\n",
      "Epoch 253201, Training Loss: 27589, Validation Loss: 55793, 104262.62038657822\n",
      "Epoch 253280: reducing learning rate of group 0 to 8.2890e-04.\n",
      "Epoch 253301, Training Loss: 26492, Validation Loss: 55956, 126613.0194022085\n",
      "Epoch 253381: reducing learning rate of group 0 to 8.2807e-04.\n",
      "Epoch 253401, Training Loss: 24983, Validation Loss: 56025, 123625.55262474973\n",
      "Epoch 253482: reducing learning rate of group 0 to 8.2724e-04.\n",
      "Epoch 253501, Training Loss: 26409, Validation Loss: 55151, 118693.53957131978\n",
      "Epoch 253583: reducing learning rate of group 0 to 8.2641e-04.\n",
      "Epoch 253601, Training Loss: 25219, Validation Loss: 55185, 94846.26393789006\n",
      "Epoch 253684: reducing learning rate of group 0 to 8.2559e-04.\n",
      "Epoch 253701, Training Loss: 25596, Validation Loss: 55155, 115350.27244619369\n",
      "Epoch 253785: reducing learning rate of group 0 to 8.2476e-04.\n",
      "Epoch 253801, Training Loss: 26766, Validation Loss: 56090, 117607.91052618648\n",
      "Epoch 253886: reducing learning rate of group 0 to 8.2394e-04.\n",
      "Epoch 253901, Training Loss: 26762, Validation Loss: 55467, 106197.67189039507\n",
      "Epoch 253987: reducing learning rate of group 0 to 8.2311e-04.\n",
      "Epoch 254001, Training Loss: 26188, Validation Loss: 56157, 103941.80110260588\n",
      "Epoch 254088: reducing learning rate of group 0 to 8.2229e-04.\n",
      "Epoch 254101, Training Loss: 30032, Validation Loss: 55200, 137766.19615812125\n",
      "Epoch 254189: reducing learning rate of group 0 to 8.2147e-04.\n",
      "Epoch 254201, Training Loss: 26093, Validation Loss: 57109, 101696.50389996353\n",
      "Epoch 254290: reducing learning rate of group 0 to 8.2064e-04.\n",
      "Epoch 254301, Training Loss: 27254, Validation Loss: 54956, 118352.5644170881\n",
      "Epoch 254391: reducing learning rate of group 0 to 8.1982e-04.\n",
      "Epoch 254401, Training Loss: 24471, Validation Loss: 55408, 87799.29554859268\n",
      "Epoch 254492: reducing learning rate of group 0 to 8.1900e-04.\n",
      "Epoch 254501, Training Loss: 26556, Validation Loss: 55928, 97257.46227224007\n",
      "Epoch 254593: reducing learning rate of group 0 to 8.1819e-04.\n",
      "Epoch 254601, Training Loss: 25603, Validation Loss: 54981, 104304.91180115826\n",
      "Epoch 254694: reducing learning rate of group 0 to 8.1737e-04.\n",
      "Epoch 254701, Training Loss: 26946, Validation Loss: 54830, 94771.71666231293\n",
      "Epoch 254795: reducing learning rate of group 0 to 8.1655e-04.\n",
      "Epoch 254801, Training Loss: 25107, Validation Loss: 55180, 105822.0679396763\n",
      "Epoch 254896: reducing learning rate of group 0 to 8.1573e-04.\n",
      "Epoch 254901, Training Loss: 25159, Validation Loss: 54475, 93585.78953805131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254997: reducing learning rate of group 0 to 8.1492e-04.\n",
      "Epoch 255001, Training Loss: 26239, Validation Loss: 55843, 125257.96042273148\n",
      "Epoch 255098: reducing learning rate of group 0 to 8.1410e-04.\n",
      "Epoch 255101, Training Loss: 27345, Validation Loss: 55452, 105017.81855020493\n",
      "Epoch 255199: reducing learning rate of group 0 to 8.1329e-04.\n",
      "Epoch 255201, Training Loss: 27081, Validation Loss: 55621, 111750.86788522918\n",
      "Epoch 255300: reducing learning rate of group 0 to 8.1247e-04.\n",
      "Epoch 255301, Training Loss: 27149, Validation Loss: 56779, 114413.1773595931\n",
      "Epoch 255401: reducing learning rate of group 0 to 8.1166e-04.\n",
      "Epoch 255401, Training Loss: 26259, Validation Loss: 55373, 104221.84630939446\n",
      "Epoch 255501, Training Loss: 27619, Validation Loss: 56113, 120973.54980958482\n",
      "Epoch 255502: reducing learning rate of group 0 to 8.1085e-04.\n",
      "Epoch 255601, Training Loss: 24606, Validation Loss: 55665, 101412.51720591821\n",
      "Epoch 255603: reducing learning rate of group 0 to 8.1004e-04.\n",
      "Epoch 255701, Training Loss: 25427, Validation Loss: 55574, 108273.28803415886\n",
      "Epoch 255704: reducing learning rate of group 0 to 8.0923e-04.\n",
      "Epoch 255801, Training Loss: 25382, Validation Loss: 55369, 121604.19251191271\n",
      "Epoch 255805: reducing learning rate of group 0 to 8.0842e-04.\n",
      "Epoch 255901, Training Loss: 24444, Validation Loss: 55596, 103036.52021242744\n",
      "Epoch 255906: reducing learning rate of group 0 to 8.0761e-04.\n",
      "Epoch 256001, Training Loss: 25319, Validation Loss: 55626, 114297.22728928369\n",
      "Epoch 256007: reducing learning rate of group 0 to 8.0680e-04.\n",
      "Epoch 256101, Training Loss: 24492, Validation Loss: 55881, 125670.60875523179\n",
      "Epoch 256108: reducing learning rate of group 0 to 8.0600e-04.\n",
      "Epoch 256201, Training Loss: 27014, Validation Loss: 55488, 94295.49666357682\n",
      "Epoch 256209: reducing learning rate of group 0 to 8.0519e-04.\n",
      "Epoch 256301, Training Loss: 26482, Validation Loss: 57055, 115183.41120299492\n",
      "Epoch 256310: reducing learning rate of group 0 to 8.0439e-04.\n",
      "Epoch 256401, Training Loss: 25597, Validation Loss: 55500, 89477.24084730416\n",
      "Epoch 256411: reducing learning rate of group 0 to 8.0358e-04.\n",
      "Epoch 256501, Training Loss: 26166, Validation Loss: 55208, 97794.68582157815\n",
      "Epoch 256512: reducing learning rate of group 0 to 8.0278e-04.\n",
      "Epoch 256601, Training Loss: 26582, Validation Loss: 55436, 106585.51077055535\n",
      "Epoch 256613: reducing learning rate of group 0 to 8.0198e-04.\n",
      "Epoch 256701, Training Loss: 26191, Validation Loss: 55181, 106166.88831815003\n",
      "Epoch 256714: reducing learning rate of group 0 to 8.0117e-04.\n",
      "Epoch 256801, Training Loss: 27646, Validation Loss: 55492, 89448.10348686027\n",
      "Epoch 256815: reducing learning rate of group 0 to 8.0037e-04.\n",
      "Epoch 256901, Training Loss: 26732, Validation Loss: 56490, 114135.5561547622\n",
      "Epoch 256916: reducing learning rate of group 0 to 7.9957e-04.\n",
      "Epoch 257001, Training Loss: 26342, Validation Loss: 56573, 115819.81758239772\n",
      "Epoch 257017: reducing learning rate of group 0 to 7.9877e-04.\n",
      "Epoch 257101, Training Loss: 26679, Validation Loss: 54771, 125945.9049985796\n",
      "Epoch 257118: reducing learning rate of group 0 to 7.9797e-04.\n",
      "Epoch 257201, Training Loss: 25040, Validation Loss: 55564, 118369.10152488407\n",
      "Epoch 257219: reducing learning rate of group 0 to 7.9718e-04.\n",
      "Epoch 257301, Training Loss: 24986, Validation Loss: 55631, 106478.20793878842\n",
      "Epoch 257320: reducing learning rate of group 0 to 7.9638e-04.\n",
      "Epoch 257401, Training Loss: 25723, Validation Loss: 56219, 130161.74062498\n",
      "Epoch 257421: reducing learning rate of group 0 to 7.9558e-04.\n",
      "Epoch 257501, Training Loss: 26051, Validation Loss: 55622, 118167.46408658968\n",
      "Epoch 257522: reducing learning rate of group 0 to 7.9479e-04.\n",
      "Epoch 257601, Training Loss: 24914, Validation Loss: 55941, 101182.42992463219\n",
      "Epoch 257623: reducing learning rate of group 0 to 7.9399e-04.\n",
      "Epoch 257701, Training Loss: 24807, Validation Loss: 56618, 90773.54902336957\n",
      "Epoch 257724: reducing learning rate of group 0 to 7.9320e-04.\n",
      "Epoch 257801, Training Loss: 28527, Validation Loss: 55338, 99791.13341904401\n",
      "Epoch 257825: reducing learning rate of group 0 to 7.9240e-04.\n",
      "Epoch 257901, Training Loss: 26278, Validation Loss: 57042, 98193.04123086004\n",
      "Epoch 257926: reducing learning rate of group 0 to 7.9161e-04.\n",
      "Epoch 258001, Training Loss: 25655, Validation Loss: 56432, 118145.2452065838\n",
      "Epoch 258027: reducing learning rate of group 0 to 7.9082e-04.\n",
      "Epoch 258101, Training Loss: 28016, Validation Loss: 55469, 109718.03982609879\n",
      "Epoch 258128: reducing learning rate of group 0 to 7.9003e-04.\n",
      "Epoch 258201, Training Loss: 26725, Validation Loss: 55723, 100636.8911221332\n",
      "Epoch 258229: reducing learning rate of group 0 to 7.8924e-04.\n",
      "Epoch 258301, Training Loss: 25756, Validation Loss: 54843, 125357.440521774\n",
      "Epoch 258330: reducing learning rate of group 0 to 7.8845e-04.\n",
      "Epoch 258401, Training Loss: 25748, Validation Loss: 56390, 100088.25873080084\n",
      "Epoch 258431: reducing learning rate of group 0 to 7.8766e-04.\n",
      "Epoch 258501, Training Loss: 24816, Validation Loss: 56107, 95601.17866175847\n",
      "Epoch 258532: reducing learning rate of group 0 to 7.8687e-04.\n",
      "Epoch 258601, Training Loss: 25263, Validation Loss: 55252, 110851.85663480342\n",
      "Epoch 258633: reducing learning rate of group 0 to 7.8609e-04.\n",
      "Epoch 258701, Training Loss: 24730, Validation Loss: 55414, 123523.62893727329\n",
      "Epoch 258734: reducing learning rate of group 0 to 7.8530e-04.\n",
      "Epoch 258801, Training Loss: 24772, Validation Loss: 55725, 97753.62757946404\n",
      "Epoch 258835: reducing learning rate of group 0 to 7.8452e-04.\n",
      "Epoch 258901, Training Loss: 27278, Validation Loss: 54636, 101846.89105766534\n",
      "Epoch 258936: reducing learning rate of group 0 to 7.8373e-04.\n",
      "Epoch 259001, Training Loss: 26325, Validation Loss: 55105, 134048.89381959348\n",
      "Epoch 259037: reducing learning rate of group 0 to 7.8295e-04.\n",
      "Epoch 259101, Training Loss: 27328, Validation Loss: 55800, 104537.86303067813\n",
      "Epoch 259138: reducing learning rate of group 0 to 7.8217e-04.\n",
      "Epoch 259201, Training Loss: 26895, Validation Loss: 55142, 90774.4330998794\n",
      "Epoch 259239: reducing learning rate of group 0 to 7.8138e-04.\n",
      "Epoch 259301, Training Loss: 28639, Validation Loss: 55371, 89008.8908444264\n",
      "Epoch 259340: reducing learning rate of group 0 to 7.8060e-04.\n",
      "Epoch 259401, Training Loss: 25673, Validation Loss: 54938, 106994.74444833117\n",
      "Epoch 259441: reducing learning rate of group 0 to 7.7982e-04.\n",
      "Epoch 259501, Training Loss: 24670, Validation Loss: 56100, 108727.23768031744\n",
      "Epoch 259542: reducing learning rate of group 0 to 7.7904e-04.\n",
      "Epoch 259601, Training Loss: 25285, Validation Loss: 55798, 112202.21962705192\n",
      "Epoch 259643: reducing learning rate of group 0 to 7.7826e-04.\n",
      "Epoch 259701, Training Loss: 26588, Validation Loss: 55222, 123583.51244861916\n",
      "Epoch 259744: reducing learning rate of group 0 to 7.7748e-04.\n",
      "Epoch 259801, Training Loss: 25400, Validation Loss: 55585, 125828.01863124773\n",
      "Epoch 259845: reducing learning rate of group 0 to 7.7671e-04.\n",
      "Epoch 259901, Training Loss: 24993, Validation Loss: 55741, 90149.76225613563\n",
      "Epoch 259946: reducing learning rate of group 0 to 7.7593e-04.\n",
      "Epoch 260001, Training Loss: 26845, Validation Loss: 55690, 111813.0048974069\n",
      "Epoch 260047: reducing learning rate of group 0 to 7.7515e-04.\n",
      "Epoch 260101, Training Loss: 25354, Validation Loss: 54927, 132383.45667646805\n",
      "Epoch 260148: reducing learning rate of group 0 to 7.7438e-04.\n",
      "Epoch 260201, Training Loss: 28701, Validation Loss: 55041, 125521.12588556383\n",
      "Epoch 260249: reducing learning rate of group 0 to 7.7360e-04.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-2,\n",
    "    weight_decay=3e-4\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999, \n",
    "    patience=100, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa48b8",
   "metadata": {},
   "source": [
    "# Saving. Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b53599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabularFFNNSimple(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "#         super(TabularFFNNSimple, self).__init__()\n",
    "#         hidden_size = 48\n",
    "#         self.ffnn = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.BatchNorm1d(hidden_size),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.5),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#         for m in self.ffnn:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 init.xavier_uniform_(m.weight)\n",
    "#                 m.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         # print(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.ffnn(x)\n",
    "#         return x\n",
    "    \n",
    "# # Split the data into features and target\n",
    "# X = data.drop('price', axis=1)\n",
    "# y = data['price']\n",
    "\n",
    "# # Standardize the features\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "# y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# # Split the data into training and combined validation and testing sets\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "#                                                             test_size=0.4, random_state=42)\n",
    "\n",
    "# # Split the combined validation and testing sets\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create DataLoader for training, validation, and testing\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# val_data = TensorDataset(X_val, y_val)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 256\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check if the dimensions match the expected input size for the model\n",
    "# input_size = X_train.shape[1]\n",
    "\n",
    "# # Output\n",
    "# # input_size, train_loader, test_loader\n",
    "\n",
    "# model = TabularFFNNSimple(\n",
    "#     input_size = input_size,\n",
    "#     output_size = 1\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 300000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# epochs_suc = [] # to have a reference to it\n",
    "# grad_norms = []\n",
    "\n",
    "# def get_gradient_norm(model):\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=9e-3,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion_abs = torch.nn.L1Loss()\n",
    "# criterion = criterion_abs\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min', \n",
    "#     factor=0.999999, \n",
    "#     patience=10, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     l1_losses = []\n",
    "#     grad_norm = 0\n",
    "#     for tuple_ in train_loader:\n",
    "#         datas, prices = tuple_\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(datas)\n",
    "#         prices_viewed = prices.view(-1, 1).float()\n",
    "#         loss = criterion(outputs, prices_viewed)\n",
    "#         loss.backward()\n",
    "#         grad_norm += get_gradient_norm(model)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     grad_norms.append(grad_norm / len(train_loader))\n",
    "#     train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for tuple_ in val_loader:\n",
    "#             datas, prices = tuple_\n",
    "#             outputs = model(datas)  # Forward pass\n",
    "#             prices_viewed = prices.view(-1, 1).float()\n",
    "#             loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "#             val_loss += loss.item()  # Accumulate the loss\n",
    "#             l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "#     val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "#     l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "#     # Print epoch's summary\n",
    "#     epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "#     if epoch % 100 == 0:\n",
    "#         tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "#         vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "#         l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "#         dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "#         print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ade95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
