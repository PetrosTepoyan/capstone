{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e3dbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1850 entries, 0 to 1849\n",
      "Data columns (total 31 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Unnamed: 0                         1850 non-null   int64  \n",
      " 1   price                              1850 non-null   float64\n",
      " 2   area                               1850 non-null   float64\n",
      " 3   rooms                              1850 non-null   float64\n",
      " 4   floor                              1850 non-null   float64\n",
      " 5   storeys                            1850 non-null   float64\n",
      " 6   bathroom_count                     1850 non-null   float64\n",
      " 7   new_building                       1850 non-null   int64  \n",
      " 8   F_Balcony                          1850 non-null   int64  \n",
      " 9   F_Internet                         1850 non-null   int64  \n",
      " 10  F_Security                         1850 non-null   int64  \n",
      " 11  F_Elevator                         1850 non-null   int64  \n",
      " 12  F_Air Conditioner                  1850 non-null   int64  \n",
      " 13  F_Heating System                   1850 non-null   int64  \n",
      " 14  F_Furniture                        1850 non-null   int64  \n",
      " 15  SL_opera                           1850 non-null   int64  \n",
      " 16  SL_zvartnoc                        1850 non-null   int64  \n",
      " 17  SL_central_cemetery                1850 non-null   int64  \n",
      " 18  SL_shengavit_subway                1850 non-null   int64  \n",
      " 19  L_pharmacy                         1850 non-null   int64  \n",
      " 20  L_atm                              1850 non-null   int64  \n",
      " 21  L_parking                          1850 non-null   int64  \n",
      " 22  L_university                       1850 non-null   int64  \n",
      " 23  building_type_Monolit              1850 non-null   int64  \n",
      " 24  building_type_Other                1850 non-null   int64  \n",
      " 25  building_type_Panel                1850 non-null   int64  \n",
      " 26  building_type_Stone                1850 non-null   int64  \n",
      " 27  condition_Good Condition           1850 non-null   int64  \n",
      " 28  condition_Needs Renovation/Repair  1850 non-null   int64  \n",
      " 29  condition_New/Excellent Condition  1850 non-null   int64  \n",
      " 30  condition_Other                    1850 non-null   int64  \n",
      "dtypes: float64(6), int64(25)\n",
      "memory usage: 448.2 KB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data2/data.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\", \n",
    "                                                           \"L_clinic\", \"L_place_of_worship\",\n",
    "                                                           \"L_hospital\", \"L_kindergarten\",\n",
    "                                                           \"SL_baghramyan_subway\"])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        hidden_size = 38\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.18),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                            test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "grad_norms = []\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 40873, Validation Loss: 55250, 43881.01812298637\n",
      "Epoch 101, Training Loss: 41770, Validation Loss: 52374, 54903.14687717877\n",
      "Epoch 201, Training Loss: 39315, Validation Loss: 52437, 46666.86084051796\n",
      "Epoch 301, Training Loss: 39924, Validation Loss: 51639, 65471.4257335229\n",
      "Epoch 401, Training Loss: 41900, Validation Loss: 52806, 54478.88132301194\n",
      "Epoch 501, Training Loss: 41906, Validation Loss: 52206, 49472.67279412418\n",
      "Epoch 601, Training Loss: 42142, Validation Loss: 52602, 48046.830027355056\n",
      "Epoch 701, Training Loss: 39649, Validation Loss: 52268, 56334.89509836953\n",
      "Epoch 801, Training Loss: 42329, Validation Loss: 53621, 58215.38811172798\n",
      "Epoch 901, Training Loss: 37938, Validation Loss: 53256, 42224.095141359234\n",
      "Epoch 1001, Training Loss: 42328, Validation Loss: 53968, 42641.26653488868\n",
      "Epoch 1101, Training Loss: 39008, Validation Loss: 53049, 47289.72894930482\n",
      "Epoch 1201, Training Loss: 38951, Validation Loss: 52991, 53327.02726185828\n",
      "Epoch 1301, Training Loss: 41365, Validation Loss: 53330, 52789.913494834065\n",
      "Epoch 1401, Training Loss: 41309, Validation Loss: 52401, 53538.78671161338\n",
      "Epoch 1501, Training Loss: 40589, Validation Loss: 52845, 59356.02590738811\n",
      "Epoch 1601, Training Loss: 41318, Validation Loss: 54137, 45866.62581916404\n",
      "Epoch 1701, Training Loss: 42751, Validation Loss: 52238, 49063.14478810699\n",
      "Epoch 1801, Training Loss: 39187, Validation Loss: 52916, 63177.30479187567\n",
      "Epoch 1901, Training Loss: 41813, Validation Loss: 53325, 50005.97591482693\n",
      "Epoch 2001, Training Loss: 39199, Validation Loss: 53504, 57320.34054626441\n",
      "Epoch 2101, Training Loss: 38541, Validation Loss: 53915, 67856.27997127779\n",
      "Epoch 2201, Training Loss: 41947, Validation Loss: 52758, 36889.33648392266\n",
      "Epoch 2301, Training Loss: 41785, Validation Loss: 53032, 57597.553023394525\n",
      "Epoch 2401, Training Loss: 41902, Validation Loss: 52801, 54848.96847176835\n",
      "Epoch 2501, Training Loss: 40009, Validation Loss: 52901, 55332.183553198316\n",
      "Epoch 2601, Training Loss: 41205, Validation Loss: 54007, 56818.469000110264\n",
      "Epoch 2701, Training Loss: 40931, Validation Loss: 53548, 57337.74168200346\n",
      "Epoch 2801, Training Loss: 42403, Validation Loss: 52578, 43655.888769027886\n",
      "Epoch 2901, Training Loss: 39176, Validation Loss: 54106, 50989.90967110756\n",
      "Epoch 3001, Training Loss: 42618, Validation Loss: 52245, 45000.24984008016\n",
      "Epoch 3101, Training Loss: 44815, Validation Loss: 53027, 63258.75862929638\n",
      "Epoch 3201, Training Loss: 41083, Validation Loss: 53583, 57676.39657564036\n",
      "Epoch 3301, Training Loss: 41369, Validation Loss: 53794, 47787.35108878867\n",
      "Epoch 3401, Training Loss: 42598, Validation Loss: 56058, 53891.1020920224\n",
      "Epoch 3501, Training Loss: 41152, Validation Loss: 51782, 63317.50551982156\n",
      "Epoch 3601, Training Loss: 41775, Validation Loss: 53248, 55787.37211888912\n",
      "Epoch 3701, Training Loss: 42667, Validation Loss: 52355, 67128.4667283339\n",
      "Epoch 3801, Training Loss: 42028, Validation Loss: 53098, 53444.71771582572\n",
      "Epoch 3901, Training Loss: 39018, Validation Loss: 53339, 58283.39374747562\n",
      "Epoch 4001, Training Loss: 40792, Validation Loss: 52711, 60811.259271938856\n",
      "Epoch 4101, Training Loss: 42951, Validation Loss: 52370, 54808.32086140272\n",
      "Epoch 4201, Training Loss: 39011, Validation Loss: 52977, 53782.293427327175\n",
      "Epoch 4301, Training Loss: 39672, Validation Loss: 52571, 45770.71518892294\n",
      "Epoch 4401, Training Loss: 39496, Validation Loss: 52830, 47828.71940449258\n",
      "Epoch 4501, Training Loss: 39797, Validation Loss: 52035, 50033.5416138971\n",
      "Epoch 4601, Training Loss: 40610, Validation Loss: 52636, 57923.23237573176\n",
      "Epoch 4701, Training Loss: 42614, Validation Loss: 52965, 58327.13500979545\n",
      "Epoch 4801, Training Loss: 40845, Validation Loss: 53169, 56363.15136966482\n",
      "Epoch 4901, Training Loss: 41414, Validation Loss: 52273, 57031.209683299814\n",
      "Epoch 5001, Training Loss: 42812, Validation Loss: 54355, 44839.452983456744\n",
      "Epoch 5101, Training Loss: 39586, Validation Loss: 53143, 62195.90454601279\n",
      "Epoch 5201, Training Loss: 41656, Validation Loss: 52313, 41497.53419745437\n",
      "Epoch 5301, Training Loss: 41188, Validation Loss: 52963, 53166.91604061675\n",
      "Epoch 5401, Training Loss: 43495, Validation Loss: 53527, 58332.24763193232\n",
      "Epoch 5501, Training Loss: 40156, Validation Loss: 54599, 51947.391182071595\n",
      "Epoch 5601, Training Loss: 41095, Validation Loss: 53387, 48645.748055462805\n",
      "Epoch 5701, Training Loss: 44299, Validation Loss: 53970, 48670.0761308664\n",
      "Epoch 5801, Training Loss: 39470, Validation Loss: 52236, 62693.28251061802\n",
      "Epoch 5901, Training Loss: 41323, Validation Loss: 54192, 63009.837746125435\n",
      "Epoch 6001, Training Loss: 42696, Validation Loss: 52867, 59871.102148773265\n",
      "Epoch 6101, Training Loss: 40560, Validation Loss: 53204, 45825.55334703767\n",
      "Epoch 6201, Training Loss: 39611, Validation Loss: 52647, 59902.090170662945\n",
      "Epoch 6301, Training Loss: 41300, Validation Loss: 53107, 44471.937402575415\n",
      "Epoch 6401, Training Loss: 40026, Validation Loss: 53442, 55296.728878272814\n",
      "Epoch 6501, Training Loss: 43422, Validation Loss: 52578, 52729.93505253348\n",
      "Epoch 6601, Training Loss: 40767, Validation Loss: 52756, 59385.39266817127\n",
      "Epoch 6701, Training Loss: 43684, Validation Loss: 54183, 44651.65710950476\n",
      "Epoch 6801, Training Loss: 41556, Validation Loss: 53755, 51512.376647763966\n",
      "Epoch 6901, Training Loss: 41017, Validation Loss: 52887, 56932.96817322989\n",
      "Epoch 7001, Training Loss: 39508, Validation Loss: 54592, 50528.89027593994\n",
      "Epoch 7101, Training Loss: 42753, Validation Loss: 53735, 46945.67296656376\n",
      "Epoch 7201, Training Loss: 41940, Validation Loss: 52767, 59281.88723599158\n",
      "Epoch 7301, Training Loss: 41248, Validation Loss: 52948, 57680.15641681071\n",
      "Epoch 7401, Training Loss: 38907, Validation Loss: 53157, 61427.03900478662\n",
      "Epoch 7501, Training Loss: 41743, Validation Loss: 53717, 63324.699421538004\n",
      "Epoch 7601, Training Loss: 41421, Validation Loss: 51934, 67194.7839466494\n",
      "Epoch 7701, Training Loss: 40582, Validation Loss: 53340, 53184.13402253556\n",
      "Epoch 7801, Training Loss: 40196, Validation Loss: 53002, 44334.511882288476\n",
      "Epoch 7901, Training Loss: 42404, Validation Loss: 52438, 57274.55118501085\n",
      "Epoch 8001, Training Loss: 42074, Validation Loss: 54742, 54036.99899096459\n",
      "Epoch 8101, Training Loss: 40125, Validation Loss: 54372, 67968.1168271007\n",
      "Epoch 8201, Training Loss: 37880, Validation Loss: 55154, 55169.13343756614\n",
      "Epoch 8301, Training Loss: 41942, Validation Loss: 53327, 51974.92748314703\n",
      "Epoch 8401, Training Loss: 43162, Validation Loss: 53920, 55842.68383808471\n",
      "Epoch 8501, Training Loss: 40241, Validation Loss: 54302, 49825.1320204651\n",
      "Epoch 8601, Training Loss: 40873, Validation Loss: 52896, 61296.753889400126\n",
      "Epoch 8701, Training Loss: 40173, Validation Loss: 54334, 67361.66548959671\n",
      "Epoch 8801, Training Loss: 41674, Validation Loss: 53649, 65200.484205513414\n",
      "Epoch 8901, Training Loss: 43944, Validation Loss: 52810, 58581.599634134815\n",
      "Epoch 9001, Training Loss: 43185, Validation Loss: 52981, 59479.183820120816\n",
      "Epoch 9101, Training Loss: 42186, Validation Loss: 54041, 55634.91935928127\n",
      "Epoch 9201, Training Loss: 38551, Validation Loss: 53787, 47213.05611682095\n",
      "Epoch 9301, Training Loss: 44136, Validation Loss: 53241, 55386.932604222384\n",
      "Epoch 9401, Training Loss: 40355, Validation Loss: 52944, 66985.58919579825\n",
      "Epoch 9501, Training Loss: 38296, Validation Loss: 52412, 54555.739529941195\n",
      "Epoch 9601, Training Loss: 40200, Validation Loss: 52317, 50815.60337842274\n",
      "Epoch 9701, Training Loss: 40034, Validation Loss: 53600, 65581.27259047677\n",
      "Epoch 9801, Training Loss: 43656, Validation Loss: 53891, 55185.90169852776\n",
      "Epoch 9901, Training Loss: 39962, Validation Loss: 53467, 54574.69962185456\n",
      "Epoch 10001, Training Loss: 39539, Validation Loss: 52057, 48229.22974355746\n",
      "Epoch 10101, Training Loss: 42074, Validation Loss: 52471, 59368.19319118058\n",
      "Epoch 10201, Training Loss: 39334, Validation Loss: 53096, 62910.40049559786\n",
      "Epoch 10301, Training Loss: 41208, Validation Loss: 53337, 56768.77275316452\n",
      "Epoch 10401, Training Loss: 42903, Validation Loss: 53466, 52245.02481375163\n",
      "Epoch 10501, Training Loss: 41430, Validation Loss: 52781, 56983.41800820036\n",
      "Epoch 10601, Training Loss: 39943, Validation Loss: 51984, 51643.583330873495\n",
      "Epoch 10701, Training Loss: 38864, Validation Loss: 51327, 44471.088388699536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10801, Training Loss: 41710, Validation Loss: 53534, 46382.29227231829\n",
      "Epoch 10901, Training Loss: 45582, Validation Loss: 54518, 39093.54639008077\n",
      "Epoch 11001, Training Loss: 39503, Validation Loss: 53755, 52899.41440465231\n",
      "Epoch 11101, Training Loss: 41047, Validation Loss: 52756, 59599.934907961695\n",
      "Epoch 11201, Training Loss: 41336, Validation Loss: 52979, 61692.62621602591\n",
      "Epoch 11301, Training Loss: 43651, Validation Loss: 53964, 49913.144025888585\n",
      "Epoch 11401, Training Loss: 41018, Validation Loss: 53594, 44649.201963373016\n",
      "Epoch 11501, Training Loss: 38696, Validation Loss: 53045, 50537.698466819966\n",
      "Epoch 11601, Training Loss: 38400, Validation Loss: 53301, 55279.02200429249\n",
      "Epoch 11701, Training Loss: 39683, Validation Loss: 53137, 51747.541817893216\n",
      "Epoch 11801, Training Loss: 42029, Validation Loss: 52786, 45395.78679783945\n",
      "Epoch 11901, Training Loss: 38636, Validation Loss: 51745, 49124.209506319254\n",
      "Epoch 12001, Training Loss: 41595, Validation Loss: 54598, 63218.017673307586\n",
      "Epoch 12101, Training Loss: 37541, Validation Loss: 54142, 62633.847703212865\n",
      "Epoch 12201, Training Loss: 39776, Validation Loss: 51871, 56284.686215255584\n",
      "Epoch 12301, Training Loss: 39493, Validation Loss: 53580, 53714.31883247546\n",
      "Epoch 12401, Training Loss: 39353, Validation Loss: 53512, 72307.9397254036\n",
      "Epoch 12501, Training Loss: 42080, Validation Loss: 52516, 74892.63602234847\n",
      "Epoch 12601, Training Loss: 40609, Validation Loss: 52565, 66040.18214501721\n",
      "Epoch 12701, Training Loss: 39279, Validation Loss: 53746, 59170.5937025214\n",
      "Epoch 12801, Training Loss: 41654, Validation Loss: 53325, 58867.68131911838\n",
      "Epoch 12901, Training Loss: 42454, Validation Loss: 51411, 54364.42052897962\n",
      "Epoch 13001, Training Loss: 39947, Validation Loss: 52608, 54217.1932674963\n",
      "Epoch 13101, Training Loss: 38404, Validation Loss: 52832, 71400.54157820066\n",
      "Epoch 13201, Training Loss: 39621, Validation Loss: 54266, 66593.88514252486\n",
      "Epoch 13301, Training Loss: 41157, Validation Loss: 53325, 51135.15582427245\n",
      "Epoch 13401, Training Loss: 39527, Validation Loss: 53723, 59067.80441897481\n",
      "Epoch 13501, Training Loss: 40750, Validation Loss: 52448, 47764.9618255971\n",
      "Epoch 13601, Training Loss: 40920, Validation Loss: 53800, 55250.13630956126\n",
      "Epoch 13701, Training Loss: 39670, Validation Loss: 52251, 61639.0822517337\n",
      "Epoch 13801, Training Loss: 40945, Validation Loss: 52623, 57621.999457927916\n",
      "Epoch 13901, Training Loss: 40696, Validation Loss: 53908, 59079.01498988868\n",
      "Epoch 14001, Training Loss: 41497, Validation Loss: 52549, 60620.984009484404\n",
      "Epoch 14101, Training Loss: 41386, Validation Loss: 53531, 49290.77327808269\n",
      "Epoch 14201, Training Loss: 42038, Validation Loss: 54052, 53636.45300846937\n",
      "Epoch 14301, Training Loss: 42214, Validation Loss: 54087, 68372.02935765675\n",
      "Epoch 14401, Training Loss: 39860, Validation Loss: 53983, 55098.430888186966\n",
      "Epoch 14501, Training Loss: 40614, Validation Loss: 53513, 58143.88517715253\n",
      "Epoch 14601, Training Loss: 43453, Validation Loss: 54893, 52113.6679358407\n",
      "Epoch 14701, Training Loss: 41964, Validation Loss: 53398, 72635.54092918777\n",
      "Epoch 14801, Training Loss: 39974, Validation Loss: 53513, 51712.79779243675\n",
      "Epoch 14901, Training Loss: 38900, Validation Loss: 54076, 48499.74666467722\n",
      "Epoch 15001, Training Loss: 41019, Validation Loss: 53566, 55488.919159832236\n",
      "Epoch 15101, Training Loss: 39645, Validation Loss: 52767, 53757.36828880719\n",
      "Epoch 15201, Training Loss: 42516, Validation Loss: 55620, 54316.94094277699\n",
      "Epoch 15301, Training Loss: 41271, Validation Loss: 52839, 52609.27146870893\n",
      "Epoch 15401, Training Loss: 40050, Validation Loss: 54343, 58412.53897558773\n",
      "Epoch 15501, Training Loss: 42459, Validation Loss: 52467, 48659.31511311565\n",
      "Epoch 15601, Training Loss: 40333, Validation Loss: 53633, 58912.08392159221\n",
      "Epoch 15701, Training Loss: 38276, Validation Loss: 53877, 60469.27871603796\n",
      "Epoch 15801, Training Loss: 42698, Validation Loss: 53518, 62456.85762556327\n",
      "Epoch 15901, Training Loss: 40059, Validation Loss: 55573, 50331.0982574971\n",
      "Epoch 16001, Training Loss: 38817, Validation Loss: 51977, 66609.52503261583\n",
      "Epoch 16101, Training Loss: 39739, Validation Loss: 53263, 61170.19684360574\n",
      "Epoch 16201, Training Loss: 41645, Validation Loss: 58624, 76372.84054252945\n",
      "Epoch 16301, Training Loss: 40202, Validation Loss: 52425, 60169.64631970252\n",
      "Epoch 16401, Training Loss: 38838, Validation Loss: 53247, 53038.24545071376\n",
      "Epoch 16501, Training Loss: 40500, Validation Loss: 53531, 43166.51962513046\n",
      "Epoch 16601, Training Loss: 40579, Validation Loss: 52735, 63799.494094702626\n",
      "Epoch 16701, Training Loss: 40943, Validation Loss: 53069, 53562.1988386354\n",
      "Epoch 16801, Training Loss: 39650, Validation Loss: 53389, 78301.72531583799\n",
      "Epoch 16901, Training Loss: 40403, Validation Loss: 53842, 82108.90533975726\n",
      "Epoch 17001, Training Loss: 38544, Validation Loss: 52951, 68443.69480415591\n",
      "Epoch 17101, Training Loss: 39356, Validation Loss: 53914, 50548.813216484676\n",
      "Epoch 17201, Training Loss: 43270, Validation Loss: 53790, 61800.11227400622\n",
      "Epoch 17301, Training Loss: 38747, Validation Loss: 54460, 63865.94476477892\n",
      "Epoch 17401, Training Loss: 40778, Validation Loss: 53547, 54502.26998767739\n",
      "Epoch 17501, Training Loss: 40954, Validation Loss: 54954, 63388.74781157926\n",
      "Epoch 17601, Training Loss: 42675, Validation Loss: 52715, 60289.78501651867\n",
      "Epoch 17701, Training Loss: 39664, Validation Loss: 53180, 62330.32671405715\n",
      "Epoch 17801, Training Loss: 38524, Validation Loss: 52553, 50144.09067079072\n",
      "Epoch 17901, Training Loss: 40332, Validation Loss: 52906, 67646.20479275654\n",
      "Epoch 18001, Training Loss: 39568, Validation Loss: 52363, 59303.85170790858\n",
      "Epoch 18101, Training Loss: 38627, Validation Loss: 53381, 61364.611452482895\n",
      "Epoch 18201, Training Loss: 38131, Validation Loss: 53305, 51470.332326152966\n",
      "Epoch 18301, Training Loss: 43799, Validation Loss: 53307, 71411.89308903435\n",
      "Epoch 18401, Training Loss: 40825, Validation Loss: 54356, 73005.9973245412\n",
      "Epoch 18501, Training Loss: 39597, Validation Loss: 51932, 57179.27688646165\n",
      "Epoch 18601, Training Loss: 42408, Validation Loss: 53428, 62458.65583720168\n",
      "Epoch 18701, Training Loss: 40819, Validation Loss: 53945, 55238.49369425611\n",
      "Epoch 18801, Training Loss: 42760, Validation Loss: 54472, 57310.03447427907\n",
      "Epoch 18901, Training Loss: 44167, Validation Loss: 52457, 56059.15238488663\n",
      "Epoch 19001, Training Loss: 41883, Validation Loss: 54053, 57049.44679724746\n",
      "Epoch 19101, Training Loss: 39277, Validation Loss: 53344, 57714.48685814981\n",
      "Epoch 19201, Training Loss: 40114, Validation Loss: 53569, 66003.43210633876\n",
      "Epoch 19301, Training Loss: 39702, Validation Loss: 51571, 51017.548571279134\n",
      "Epoch 19401, Training Loss: 40578, Validation Loss: 52100, 51004.16352614007\n",
      "Epoch 19501, Training Loss: 42717, Validation Loss: 53124, 58685.199990053545\n",
      "Epoch 19601, Training Loss: 40211, Validation Loss: 53854, 56818.605004453006\n",
      "Epoch 19701, Training Loss: 39934, Validation Loss: 52480, 58391.416468823234\n",
      "Epoch 19801, Training Loss: 39133, Validation Loss: 54034, 61998.81416726285\n",
      "Epoch 19901, Training Loss: 39652, Validation Loss: 53449, 52779.33103790961\n",
      "Epoch 20001, Training Loss: 39264, Validation Loss: 52432, 45859.269527300086\n",
      "Epoch 20101, Training Loss: 41918, Validation Loss: 53222, 58666.733677056596\n",
      "Epoch 20201, Training Loss: 39935, Validation Loss: 51822, 61159.068907380606\n",
      "Epoch 20301, Training Loss: 41584, Validation Loss: 52132, 66601.4764102357\n",
      "Epoch 20401, Training Loss: 45040, Validation Loss: 53845, 54278.92768299851\n",
      "Epoch 20501, Training Loss: 43510, Validation Loss: 52811, 65553.2423597387\n",
      "Epoch 20601, Training Loss: 39602, Validation Loss: 52987, 57206.14839609901\n",
      "Epoch 20701, Training Loss: 40703, Validation Loss: 52421, 57620.09503604396\n",
      "Epoch 20801, Training Loss: 40945, Validation Loss: 54340, 54634.46819266651\n",
      "Epoch 20901, Training Loss: 40193, Validation Loss: 53447, 53462.47223587857\n",
      "Epoch 21001, Training Loss: 40345, Validation Loss: 53316, 56271.27011861108\n",
      "Epoch 21101, Training Loss: 40104, Validation Loss: 52456, 64799.833361843164\n",
      "Epoch 21201, Training Loss: 38487, Validation Loss: 52834, 49184.31647068241\n",
      "Epoch 21301, Training Loss: 40065, Validation Loss: 53270, 63329.19960356457\n",
      "Epoch 21401, Training Loss: 39924, Validation Loss: 52458, 61758.53048533106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21501, Training Loss: 41818, Validation Loss: 53489, 62646.337458242684\n",
      "Epoch 21601, Training Loss: 40722, Validation Loss: 53065, 55175.66669336651\n",
      "Epoch 21701, Training Loss: 39423, Validation Loss: 53632, 63159.65481442122\n",
      "Epoch 21801, Training Loss: 38125, Validation Loss: 53063, 52470.93728237188\n",
      "Epoch 21901, Training Loss: 40355, Validation Loss: 52869, 51325.36080723612\n",
      "Epoch 22001, Training Loss: 41237, Validation Loss: 52569, 46155.13617729465\n",
      "Epoch 22101, Training Loss: 41326, Validation Loss: 52669, 60514.2894333957\n",
      "Epoch 22201, Training Loss: 42202, Validation Loss: 54842, 66362.03049279797\n",
      "Epoch 22301, Training Loss: 40065, Validation Loss: 53339, 62038.630686625496\n",
      "Epoch 22401, Training Loss: 38318, Validation Loss: 54661, 57356.399210456344\n",
      "Epoch 22501, Training Loss: 42466, Validation Loss: 53872, 60826.43503802243\n",
      "Epoch 22601, Training Loss: 39506, Validation Loss: 54054, 65509.83525171815\n",
      "Epoch 22701, Training Loss: 40004, Validation Loss: 54606, 59345.52571350065\n",
      "Epoch 22801, Training Loss: 37709, Validation Loss: 52803, 62410.8988674811\n",
      "Epoch 22901, Training Loss: 40767, Validation Loss: 53887, 62005.11322357333\n",
      "Epoch 23001, Training Loss: 38749, Validation Loss: 53198, 62580.16187253671\n",
      "Epoch 23101, Training Loss: 38665, Validation Loss: 54737, 55728.726346293486\n",
      "Epoch 23201, Training Loss: 41251, Validation Loss: 51786, 57622.494198175285\n",
      "Epoch 23301, Training Loss: 40424, Validation Loss: 53818, 57952.16756689465\n",
      "Epoch 23401, Training Loss: 38291, Validation Loss: 54380, 66865.67299149245\n",
      "Epoch 23501, Training Loss: 40617, Validation Loss: 54877, 61351.78556733866\n",
      "Epoch 23601, Training Loss: 40431, Validation Loss: 53025, 57667.26608894419\n",
      "Epoch 23701, Training Loss: 40100, Validation Loss: 53343, 66395.5342342209\n",
      "Epoch 23801, Training Loss: 39562, Validation Loss: 54446, 55701.26821430362\n",
      "Epoch 23901, Training Loss: 40868, Validation Loss: 53357, 55384.38004270908\n",
      "Epoch 24001, Training Loss: 38895, Validation Loss: 52542, 77278.10116904757\n",
      "Epoch 24101, Training Loss: 41738, Validation Loss: 54247, 64726.85804293884\n",
      "Epoch 24201, Training Loss: 41928, Validation Loss: 53267, 64676.64450513562\n",
      "Epoch 24301, Training Loss: 40745, Validation Loss: 53501, 56704.53924825962\n",
      "Epoch 24401, Training Loss: 41170, Validation Loss: 53250, 60108.855681807356\n",
      "Epoch 24501, Training Loss: 39967, Validation Loss: 52895, 65058.08148136363\n",
      "Epoch 24601, Training Loss: 41657, Validation Loss: 52359, 67142.45274942962\n",
      "Epoch 24701, Training Loss: 38500, Validation Loss: 53319, 68641.81651657974\n",
      "Epoch 24801, Training Loss: 38777, Validation Loss: 53576, 56929.81152458999\n",
      "Epoch 24901, Training Loss: 39756, Validation Loss: 52608, 52805.60823108803\n",
      "Epoch 25001, Training Loss: 42085, Validation Loss: 52984, 67634.02099581022\n",
      "Epoch 25101, Training Loss: 39539, Validation Loss: 51698, 64441.18368564741\n",
      "Epoch 25201, Training Loss: 44676, Validation Loss: 54353, 65468.554705920185\n",
      "Epoch 25301, Training Loss: 41372, Validation Loss: 54962, 53765.60693911119\n",
      "Epoch 25401, Training Loss: 38670, Validation Loss: 52586, 59513.883719446596\n",
      "Epoch 25501, Training Loss: 39443, Validation Loss: 53124, 62609.06508867531\n",
      "Epoch 25601, Training Loss: 42119, Validation Loss: 55666, 50978.60722134503\n",
      "Epoch 25701, Training Loss: 38185, Validation Loss: 55744, 68343.46743244436\n",
      "Epoch 25801, Training Loss: 42274, Validation Loss: 55211, 56208.945430001746\n",
      "Epoch 25901, Training Loss: 39411, Validation Loss: 53995, 74882.1341007584\n",
      "Epoch 26001, Training Loss: 37682, Validation Loss: 53783, 50406.69465738485\n",
      "Epoch 26101, Training Loss: 40090, Validation Loss: 51989, 71153.70108859974\n",
      "Epoch 26201, Training Loss: 42768, Validation Loss: 52459, 63066.53229607275\n",
      "Epoch 26301, Training Loss: 39248, Validation Loss: 52799, 59058.29757816457\n",
      "Epoch 26401, Training Loss: 41042, Validation Loss: 52364, 64160.17584453651\n",
      "Epoch 26501, Training Loss: 41526, Validation Loss: 53163, 58841.09130592374\n",
      "Epoch 26601, Training Loss: 45112, Validation Loss: 53627, 57424.321761021616\n",
      "Epoch 26701, Training Loss: 40756, Validation Loss: 53186, 63616.16874714013\n",
      "Epoch 26801, Training Loss: 42014, Validation Loss: 53188, 50817.6692662042\n",
      "Epoch 26901, Training Loss: 37812, Validation Loss: 53540, 71992.03532284866\n",
      "Epoch 27001, Training Loss: 38478, Validation Loss: 54167, 67826.33351391238\n",
      "Epoch 27101, Training Loss: 40004, Validation Loss: 52299, 65167.714321433195\n",
      "Epoch 27201, Training Loss: 37461, Validation Loss: 53728, 59862.60473810148\n",
      "Epoch 27301, Training Loss: 40511, Validation Loss: 52428, 69647.26350207206\n",
      "Epoch 27401, Training Loss: 39890, Validation Loss: 53523, 63617.57397391032\n",
      "Epoch 27501, Training Loss: 38638, Validation Loss: 55327, 72691.88356546509\n",
      "Epoch 27601, Training Loss: 42136, Validation Loss: 53058, 62155.0374397482\n",
      "Epoch 27701, Training Loss: 42458, Validation Loss: 52816, 69217.02139949502\n",
      "Epoch 27801, Training Loss: 38996, Validation Loss: 53692, 72294.16293670134\n",
      "Epoch 27901, Training Loss: 38501, Validation Loss: 53236, 69281.28067783696\n",
      "Epoch 28001, Training Loss: 38487, Validation Loss: 53458, 55053.29694003138\n",
      "Epoch 28101, Training Loss: 40229, Validation Loss: 53621, 77282.39414699373\n",
      "Epoch 28201, Training Loss: 39162, Validation Loss: 53997, 57033.30957245238\n",
      "Epoch 28301, Training Loss: 39857, Validation Loss: 52186, 72936.99323108859\n",
      "Epoch 28401, Training Loss: 41665, Validation Loss: 54192, 69167.52163991312\n",
      "Epoch 28501, Training Loss: 38971, Validation Loss: 51576, 73344.40629440745\n",
      "Epoch 28601, Training Loss: 39521, Validation Loss: 52231, 71479.19299280031\n",
      "Epoch 28701, Training Loss: 40269, Validation Loss: 53740, 73921.95868658736\n",
      "Epoch 28801, Training Loss: 41067, Validation Loss: 53319, 61461.02992409451\n",
      "Epoch 28901, Training Loss: 42070, Validation Loss: 52802, 60955.94925567692\n",
      "Epoch 29001, Training Loss: 42226, Validation Loss: 53142, 67853.99816546071\n",
      "Epoch 29101, Training Loss: 42815, Validation Loss: 53254, 64022.93291828664\n",
      "Epoch 29201, Training Loss: 42231, Validation Loss: 53233, 61846.85065464423\n",
      "Epoch 29301, Training Loss: 40733, Validation Loss: 53516, 74101.05502979337\n",
      "Epoch 29401, Training Loss: 41223, Validation Loss: 53259, 67915.64927016893\n",
      "Epoch 29501, Training Loss: 39992, Validation Loss: 52295, 69882.04502710118\n",
      "Epoch 29601, Training Loss: 41738, Validation Loss: 54349, 70244.34975580599\n",
      "Epoch 29701, Training Loss: 39122, Validation Loss: 54519, 61674.1858403309\n",
      "Epoch 29801, Training Loss: 40715, Validation Loss: 53542, 65162.19340458384\n",
      "Epoch 29901, Training Loss: 41484, Validation Loss: 53977, 59764.94113968344\n",
      "Epoch 30001, Training Loss: 41083, Validation Loss: 53289, 62248.188229060885\n",
      "Epoch 30101, Training Loss: 40985, Validation Loss: 52255, 54932.79250429452\n",
      "Epoch 30201, Training Loss: 38660, Validation Loss: 53484, 75304.89699235508\n",
      "Epoch 30301, Training Loss: 38486, Validation Loss: 52749, 54803.75027510159\n",
      "Epoch 30401, Training Loss: 40379, Validation Loss: 53908, 61134.0869259765\n",
      "Epoch 30501, Training Loss: 37787, Validation Loss: 53242, 56319.25893883995\n",
      "Epoch 30601, Training Loss: 42195, Validation Loss: 53585, 58997.427592602944\n",
      "Epoch 30701, Training Loss: 39819, Validation Loss: 53860, 61075.90907716072\n",
      "Epoch 30801, Training Loss: 40967, Validation Loss: 52923, 63545.673136410885\n",
      "Epoch 30901, Training Loss: 39044, Validation Loss: 53934, 56038.459568990256\n",
      "Epoch 31001, Training Loss: 41911, Validation Loss: 54550, 63593.72749594601\n",
      "Epoch 31101, Training Loss: 41259, Validation Loss: 54081, 58858.131701199454\n",
      "Epoch 31201, Training Loss: 39925, Validation Loss: 54620, 52638.85971375784\n",
      "Epoch 31301, Training Loss: 40713, Validation Loss: 53435, 54636.98423644876\n",
      "Epoch 31401, Training Loss: 40396, Validation Loss: 53184, 62724.16555096917\n",
      "Epoch 31501, Training Loss: 39678, Validation Loss: 53265, 64688.514844802885\n",
      "Epoch 31601, Training Loss: 40006, Validation Loss: 54335, 48960.44693052303\n",
      "Epoch 31701, Training Loss: 41490, Validation Loss: 52556, 53334.558901466\n",
      "Epoch 31801, Training Loss: 40386, Validation Loss: 52941, 59225.4566959301\n",
      "Epoch 31901, Training Loss: 42122, Validation Loss: 54235, 62550.43569066226\n",
      "Epoch 32001, Training Loss: 41162, Validation Loss: 53116, 70806.91247648689\n",
      "Epoch 32101, Training Loss: 37454, Validation Loss: 54082, 81703.76266217676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32201, Training Loss: 38252, Validation Loss: 54551, 46320.6108282691\n",
      "Epoch 32301, Training Loss: 38629, Validation Loss: 53700, 54030.845543571886\n",
      "Epoch 32401, Training Loss: 38539, Validation Loss: 52740, 61094.5123054228\n",
      "Epoch 32501, Training Loss: 40138, Validation Loss: 53495, 55919.82116007022\n",
      "Epoch 32601, Training Loss: 40625, Validation Loss: 52605, 48764.0347227641\n",
      "Epoch 32701, Training Loss: 38563, Validation Loss: 53299, 59539.02106747223\n",
      "Epoch 32801, Training Loss: 40217, Validation Loss: 53640, 62161.77643665342\n",
      "Epoch 32901, Training Loss: 39953, Validation Loss: 54579, 63609.24465244211\n",
      "Epoch 33001, Training Loss: 39816, Validation Loss: 53405, 65261.85434973048\n",
      "Epoch 33101, Training Loss: 37633, Validation Loss: 53334, 54301.138289613715\n",
      "Epoch 33201, Training Loss: 39571, Validation Loss: 53552, 69475.53540860393\n",
      "Epoch 33301, Training Loss: 42813, Validation Loss: 53072, 60182.06661790918\n",
      "Epoch 33401, Training Loss: 38438, Validation Loss: 53740, 60802.61717656903\n",
      "Epoch 33501, Training Loss: 37793, Validation Loss: 55358, 68097.11605791363\n",
      "Epoch 33601, Training Loss: 41415, Validation Loss: 53821, 63901.74064251323\n",
      "Epoch 33701, Training Loss: 39231, Validation Loss: 53753, 62160.067100593275\n",
      "Epoch 33801, Training Loss: 38464, Validation Loss: 53817, 68895.24623400629\n",
      "Epoch 33901, Training Loss: 36284, Validation Loss: 53123, 51835.04319135287\n",
      "Epoch 34001, Training Loss: 41871, Validation Loss: 55139, 69232.81141108695\n",
      "Epoch 34101, Training Loss: 41888, Validation Loss: 52962, 70457.96176032949\n",
      "Epoch 34201, Training Loss: 40625, Validation Loss: 53599, 61428.694315151755\n",
      "Epoch 34301, Training Loss: 41307, Validation Loss: 53253, 69150.03816390301\n",
      "Epoch 34401, Training Loss: 39818, Validation Loss: 54049, 52595.51446478666\n",
      "Epoch 34501, Training Loss: 39313, Validation Loss: 53818, 62717.89969452886\n",
      "Epoch 34601, Training Loss: 38416, Validation Loss: 54162, 51421.72846627945\n",
      "Epoch 34701, Training Loss: 40377, Validation Loss: 52082, 59148.22488822212\n",
      "Epoch 34801, Training Loss: 39660, Validation Loss: 53539, 60425.47111018202\n",
      "Epoch 34901, Training Loss: 37519, Validation Loss: 51790, 66666.82661221112\n",
      "Epoch 35001, Training Loss: 37608, Validation Loss: 53110, 59855.69453873815\n",
      "Epoch 35101, Training Loss: 42368, Validation Loss: 53324, 62800.802446951246\n",
      "Epoch 35201, Training Loss: 41968, Validation Loss: 53790, 55455.21987960635\n",
      "Epoch 35301, Training Loss: 40655, Validation Loss: 53584, 63414.63253255524\n",
      "Epoch 35401, Training Loss: 40971, Validation Loss: 54287, 67237.2263419237\n",
      "Epoch 35501, Training Loss: 40878, Validation Loss: 53694, 56133.93640602814\n",
      "Epoch 35601, Training Loss: 37400, Validation Loss: 54379, 64109.62202035379\n",
      "Epoch 35701, Training Loss: 40095, Validation Loss: 54920, 54663.94267494567\n",
      "Epoch 35801, Training Loss: 42899, Validation Loss: 54056, 63582.50473823459\n",
      "Epoch 35901, Training Loss: 40570, Validation Loss: 55904, 55642.761395543406\n",
      "Epoch 36001, Training Loss: 37611, Validation Loss: 53781, 64965.58042304241\n",
      "Epoch 36101, Training Loss: 39918, Validation Loss: 54302, 69024.28730449169\n",
      "Epoch 36201, Training Loss: 37739, Validation Loss: 52637, 63286.02047562279\n",
      "Epoch 36301, Training Loss: 38748, Validation Loss: 53130, 62099.78001417103\n",
      "Epoch 36401, Training Loss: 42412, Validation Loss: 52558, 57085.952865204476\n",
      "Epoch 36501, Training Loss: 41495, Validation Loss: 53945, 62237.33676990169\n",
      "Epoch 36601, Training Loss: 39686, Validation Loss: 52851, 65818.01061760703\n",
      "Epoch 36701, Training Loss: 40045, Validation Loss: 53084, 72130.38347243275\n",
      "Epoch 36801, Training Loss: 42412, Validation Loss: 52570, 60342.94158353404\n",
      "Epoch 36901, Training Loss: 39672, Validation Loss: 52911, 63308.84457806935\n",
      "Epoch 37001, Training Loss: 41000, Validation Loss: 52212, 66433.19432377398\n",
      "Epoch 37101, Training Loss: 40838, Validation Loss: 53840, 58115.984230957176\n",
      "Epoch 37201, Training Loss: 39280, Validation Loss: 53606, 61234.18626335278\n",
      "Epoch 37301, Training Loss: 40332, Validation Loss: 52989, 69386.39681981353\n",
      "Epoch 37401, Training Loss: 39632, Validation Loss: 52354, 58800.819984656635\n",
      "Epoch 37501, Training Loss: 40178, Validation Loss: 53875, 69641.70065093083\n",
      "Epoch 37601, Training Loss: 37597, Validation Loss: 52843, 63997.8292425561\n",
      "Epoch 37701, Training Loss: 38826, Validation Loss: 51913, 58877.81952152198\n",
      "Epoch 37801, Training Loss: 37722, Validation Loss: 53250, 71685.96197626789\n",
      "Epoch 37901, Training Loss: 39717, Validation Loss: 52807, 54252.503539570134\n",
      "Epoch 38001, Training Loss: 41094, Validation Loss: 53162, 62915.1815163191\n",
      "Epoch 38101, Training Loss: 39346, Validation Loss: 51705, 68069.32689441838\n",
      "Epoch 38201, Training Loss: 40571, Validation Loss: 53007, 59553.05608274235\n",
      "Epoch 38301, Training Loss: 39210, Validation Loss: 52721, 73058.83325639724\n",
      "Epoch 38401, Training Loss: 42154, Validation Loss: 54827, 71650.55143229057\n",
      "Epoch 38501, Training Loss: 37600, Validation Loss: 53576, 57223.08517423519\n",
      "Epoch 38601, Training Loss: 40215, Validation Loss: 53069, 60212.14390477821\n",
      "Epoch 38701, Training Loss: 41572, Validation Loss: 51980, 62147.05893974835\n",
      "Epoch 38801, Training Loss: 39123, Validation Loss: 53588, 66296.08259138757\n",
      "Epoch 38901, Training Loss: 37063, Validation Loss: 53105, 77449.34256963804\n",
      "Epoch 39001, Training Loss: 41123, Validation Loss: 52888, 72448.63182699979\n",
      "Epoch 39101, Training Loss: 38975, Validation Loss: 53525, 62355.52406533379\n",
      "Epoch 39201, Training Loss: 39269, Validation Loss: 53446, 64627.45582458797\n",
      "Epoch 39301, Training Loss: 38036, Validation Loss: 54008, 62878.46435543549\n",
      "Epoch 39401, Training Loss: 39956, Validation Loss: 54248, 68355.70870346113\n",
      "Epoch 39501, Training Loss: 43066, Validation Loss: 52540, 66539.69341438613\n",
      "Epoch 39601, Training Loss: 41797, Validation Loss: 54623, 65701.39223700948\n",
      "Epoch 39701, Training Loss: 39801, Validation Loss: 54606, 71697.92603462352\n",
      "Epoch 39801, Training Loss: 41380, Validation Loss: 54627, 57794.02603448002\n",
      "Epoch 39901, Training Loss: 39396, Validation Loss: 52501, 67618.05702877586\n",
      "Epoch 40001, Training Loss: 41349, Validation Loss: 52821, 56973.7333965396\n",
      "Epoch 40101, Training Loss: 40538, Validation Loss: 53604, 72917.5653932941\n",
      "Epoch 40201, Training Loss: 40553, Validation Loss: 53246, 82156.11134161019\n",
      "Epoch 40301, Training Loss: 39817, Validation Loss: 52708, 67300.61200953704\n",
      "Epoch 40401, Training Loss: 39964, Validation Loss: 53417, 73810.38832144115\n",
      "Epoch 40501, Training Loss: 41551, Validation Loss: 54140, 81505.94325098126\n",
      "Epoch 40601, Training Loss: 40691, Validation Loss: 52996, 71658.01128381635\n",
      "Epoch 40701, Training Loss: 39535, Validation Loss: 54572, 69228.12096844014\n",
      "Epoch 40801, Training Loss: 39621, Validation Loss: 54299, 67275.0282486559\n",
      "Epoch 40901, Training Loss: 40139, Validation Loss: 54310, 87444.14015644847\n",
      "Epoch 41001, Training Loss: 40159, Validation Loss: 52976, 58048.93820814649\n",
      "Epoch 41101, Training Loss: 41431, Validation Loss: 51677, 68095.27681714293\n",
      "Epoch 41201, Training Loss: 39413, Validation Loss: 55216, 54851.090396225154\n",
      "Epoch 41301, Training Loss: 40394, Validation Loss: 52966, 58432.50024941653\n",
      "Epoch 41401, Training Loss: 38972, Validation Loss: 54882, 67971.98747233396\n",
      "Epoch 41501, Training Loss: 40295, Validation Loss: 52866, 69522.13548466902\n",
      "Epoch 41601, Training Loss: 38562, Validation Loss: 51663, 59692.387713975135\n",
      "Epoch 41701, Training Loss: 42131, Validation Loss: 56337, 67161.27229017629\n",
      "Epoch 41801, Training Loss: 40532, Validation Loss: 54327, 63219.23300192253\n",
      "Epoch 41901, Training Loss: 40774, Validation Loss: 52126, 66293.11679008459\n",
      "Epoch 42001, Training Loss: 37976, Validation Loss: 55275, 53057.521907344264\n",
      "Epoch 42101, Training Loss: 40408, Validation Loss: 53393, 62347.222867407974\n",
      "Epoch 42201, Training Loss: 37785, Validation Loss: 54751, 59907.17118401235\n",
      "Epoch 42301, Training Loss: 41660, Validation Loss: 54413, 67562.936621633\n",
      "Epoch 42401, Training Loss: 44665, Validation Loss: 52582, 58017.94359090155\n",
      "Epoch 42501, Training Loss: 40043, Validation Loss: 53644, 75547.10835800208\n",
      "Epoch 42601, Training Loss: 40293, Validation Loss: 52787, 69502.24163196239\n",
      "Epoch 42701, Training Loss: 36841, Validation Loss: 54516, 55832.81725021048\n",
      "Epoch 42801, Training Loss: 40226, Validation Loss: 53574, 72055.41723851225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42901, Training Loss: 40991, Validation Loss: 55971, 66563.34680129284\n",
      "Epoch 43001, Training Loss: 42412, Validation Loss: 52374, 63643.23343001106\n",
      "Epoch 43101, Training Loss: 38478, Validation Loss: 53802, 83772.8756149111\n",
      "Epoch 43201, Training Loss: 40066, Validation Loss: 51107, 77444.22891751997\n",
      "Epoch 43301, Training Loss: 39825, Validation Loss: 52708, 84969.53404287466\n",
      "Epoch 43401, Training Loss: 38682, Validation Loss: 54362, 54563.30038462954\n",
      "Epoch 43501, Training Loss: 40392, Validation Loss: 52922, 57903.8554406667\n",
      "Epoch 43601, Training Loss: 39698, Validation Loss: 52404, 67955.22859276745\n",
      "Epoch 43701, Training Loss: 39696, Validation Loss: 53909, 52038.97396551416\n",
      "Epoch 43801, Training Loss: 38738, Validation Loss: 53611, 66476.81015301528\n",
      "Epoch 43901, Training Loss: 40227, Validation Loss: 54282, 66380.07608816882\n",
      "Epoch 44001, Training Loss: 38851, Validation Loss: 52613, 80478.88583437732\n",
      "Epoch 44101, Training Loss: 39977, Validation Loss: 55454, 67525.4320071976\n",
      "Epoch 44201, Training Loss: 38330, Validation Loss: 53679, 59175.61562783853\n",
      "Epoch 44301, Training Loss: 40681, Validation Loss: 53465, 73484.22200712074\n",
      "Epoch 44401, Training Loss: 40624, Validation Loss: 53430, 66024.1537989204\n",
      "Epoch 44501, Training Loss: 40124, Validation Loss: 54539, 59079.199415522184\n",
      "Epoch 44601, Training Loss: 36910, Validation Loss: 53335, 69503.76703691426\n",
      "Epoch 44701, Training Loss: 40042, Validation Loss: 52561, 65981.65377951205\n",
      "Epoch 44801, Training Loss: 41100, Validation Loss: 53771, 65969.83831913718\n",
      "Epoch 44901, Training Loss: 39083, Validation Loss: 52965, 68083.62104291616\n",
      "Epoch 45001, Training Loss: 38195, Validation Loss: 53942, 67241.26959390049\n",
      "Epoch 45101, Training Loss: 38709, Validation Loss: 53758, 66642.19421939073\n",
      "Epoch 45201, Training Loss: 41881, Validation Loss: 55441, 66700.67664132266\n",
      "Epoch 45301, Training Loss: 39624, Validation Loss: 52228, 56647.79708824828\n",
      "Epoch 45401, Training Loss: 41506, Validation Loss: 53162, 76554.12628196065\n",
      "Epoch 45501, Training Loss: 41355, Validation Loss: 53724, 55040.40706824337\n",
      "Epoch 45601, Training Loss: 37873, Validation Loss: 52588, 67387.49334718\n",
      "Epoch 45701, Training Loss: 39910, Validation Loss: 53925, 59549.048622569906\n",
      "Epoch 45801, Training Loss: 40101, Validation Loss: 54624, 73124.79651920802\n",
      "Epoch 45901, Training Loss: 38323, Validation Loss: 54248, 69136.596959701\n",
      "Epoch 46001, Training Loss: 40172, Validation Loss: 56041, 68513.40661324073\n",
      "Epoch 46101, Training Loss: 41846, Validation Loss: 53538, 65886.55097667627\n",
      "Epoch 46201, Training Loss: 40372, Validation Loss: 54068, 66791.22239799057\n",
      "Epoch 46301, Training Loss: 40061, Validation Loss: 53430, 68508.80151579712\n",
      "Epoch 46401, Training Loss: 39715, Validation Loss: 52824, 59653.30068603635\n",
      "Epoch 46501, Training Loss: 38527, Validation Loss: 54448, 49840.3698414615\n",
      "Epoch 46601, Training Loss: 38427, Validation Loss: 53066, 71909.54758351734\n",
      "Epoch 46701, Training Loss: 41151, Validation Loss: 52027, 79693.97711379234\n",
      "Epoch 46801, Training Loss: 38746, Validation Loss: 51789, 54807.90510682794\n",
      "Epoch 46901, Training Loss: 36305, Validation Loss: 53710, 72571.95681410174\n",
      "Epoch 47001, Training Loss: 38756, Validation Loss: 54382, 73191.97489930084\n",
      "Epoch 47101, Training Loss: 39016, Validation Loss: 54160, 74832.03605271649\n",
      "Epoch 47201, Training Loss: 39283, Validation Loss: 53556, 64573.04212734124\n",
      "Epoch 47301, Training Loss: 40112, Validation Loss: 52509, 70271.37056808115\n",
      "Epoch 47401, Training Loss: 42889, Validation Loss: 52613, 75824.27714602508\n",
      "Epoch 47501, Training Loss: 38304, Validation Loss: 53978, 56368.50807507008\n",
      "Epoch 47601, Training Loss: 41842, Validation Loss: 53199, 63810.676224366885\n",
      "Epoch 47701, Training Loss: 40474, Validation Loss: 54439, 74332.41330983613\n",
      "Epoch 47801, Training Loss: 37699, Validation Loss: 53409, 64341.82012784118\n",
      "Epoch 47901, Training Loss: 36604, Validation Loss: 54275, 82274.5700627196\n",
      "Epoch 48001, Training Loss: 40279, Validation Loss: 53841, 79480.60766289632\n",
      "Epoch 48101, Training Loss: 41022, Validation Loss: 54163, 60042.678255501494\n",
      "Epoch 48201, Training Loss: 42583, Validation Loss: 53766, 83872.38547497155\n",
      "Epoch 48301, Training Loss: 40239, Validation Loss: 54604, 84946.52768287859\n",
      "Epoch 48401, Training Loss: 40639, Validation Loss: 54651, 65516.30575821869\n",
      "Epoch 48501, Training Loss: 43326, Validation Loss: 55027, 91452.70443416959\n",
      "Epoch 48601, Training Loss: 42064, Validation Loss: 52204, 63923.92638663055\n",
      "Epoch 48701, Training Loss: 39812, Validation Loss: 54806, 65648.90723131655\n",
      "Epoch 48801, Training Loss: 37331, Validation Loss: 52544, 73905.09203126318\n",
      "Epoch 48901, Training Loss: 40780, Validation Loss: 53574, 64133.494675426926\n",
      "Epoch 49001, Training Loss: 41871, Validation Loss: 52978, 74098.88061385126\n",
      "Epoch 49101, Training Loss: 40657, Validation Loss: 54293, 67283.93137335771\n",
      "Epoch 49201, Training Loss: 40181, Validation Loss: 52950, 53090.288967679895\n",
      "Epoch 49301, Training Loss: 40471, Validation Loss: 54376, 79422.73112306678\n",
      "Epoch 49401, Training Loss: 40344, Validation Loss: 52901, 68927.71974334003\n",
      "Epoch 49501, Training Loss: 40722, Validation Loss: 53883, 72426.40098538391\n",
      "Epoch 49601, Training Loss: 38821, Validation Loss: 53853, 56596.95388489752\n",
      "Epoch 49701, Training Loss: 39603, Validation Loss: 53065, 79980.30914288262\n",
      "Epoch 49801, Training Loss: 40891, Validation Loss: 52235, 68763.14595398454\n",
      "Epoch 49901, Training Loss: 38094, Validation Loss: 52923, 64213.792618233354\n",
      "Epoch 50001, Training Loss: 40084, Validation Loss: 53273, 70419.70803210749\n",
      "Epoch 50101, Training Loss: 38376, Validation Loss: 54200, 72746.53796821478\n",
      "Epoch 50201, Training Loss: 40878, Validation Loss: 53763, 69220.2066370634\n",
      "Epoch 50301, Training Loss: 39882, Validation Loss: 53710, 68440.6601643436\n",
      "Epoch 50401, Training Loss: 38021, Validation Loss: 53486, 57672.263874887976\n",
      "Epoch 50501, Training Loss: 37593, Validation Loss: 53585, 78097.270069304\n",
      "Epoch 50601, Training Loss: 41593, Validation Loss: 54660, 76875.7575837298\n",
      "Epoch 50701, Training Loss: 40776, Validation Loss: 54144, 57432.82575109925\n",
      "Epoch 50801, Training Loss: 42312, Validation Loss: 53252, 89639.26780948097\n",
      "Epoch 50901, Training Loss: 40324, Validation Loss: 56423, 68204.63135238858\n",
      "Epoch 51001, Training Loss: 39143, Validation Loss: 52831, 57793.4755046255\n",
      "Epoch 51101, Training Loss: 38773, Validation Loss: 54315, 84338.13695194038\n",
      "Epoch 51201, Training Loss: 38731, Validation Loss: 53217, 72848.33093169067\n",
      "Epoch 51301, Training Loss: 41253, Validation Loss: 52377, 71735.86207041341\n",
      "Epoch 51401, Training Loss: 39974, Validation Loss: 52942, 68663.59910221503\n",
      "Epoch 51501, Training Loss: 38147, Validation Loss: 53326, 74840.110789702\n",
      "Epoch 51601, Training Loss: 39660, Validation Loss: 52683, 96058.13645763822\n",
      "Epoch 51701, Training Loss: 38539, Validation Loss: 55493, 66549.66783164623\n",
      "Epoch 51801, Training Loss: 39451, Validation Loss: 54496, 75514.64009311738\n",
      "Epoch 51901, Training Loss: 39857, Validation Loss: 54505, 64798.81560268709\n",
      "Epoch 52001, Training Loss: 38239, Validation Loss: 52455, 50367.05553164434\n",
      "Epoch 52101, Training Loss: 40030, Validation Loss: 53952, 56587.715575590846\n",
      "Epoch 52201, Training Loss: 39492, Validation Loss: 52381, 80528.26153568755\n",
      "Epoch 52301, Training Loss: 41141, Validation Loss: 53524, 71693.6778914663\n",
      "Epoch 52401, Training Loss: 40832, Validation Loss: 52031, 64871.07515413626\n",
      "Epoch 52501, Training Loss: 40318, Validation Loss: 53732, 71724.48361020988\n",
      "Epoch 52601, Training Loss: 40652, Validation Loss: 52519, 66653.55616984807\n",
      "Epoch 52701, Training Loss: 40832, Validation Loss: 54765, 68284.05432658191\n",
      "Epoch 52801, Training Loss: 36204, Validation Loss: 54743, 69131.26060942393\n",
      "Epoch 52901, Training Loss: 40162, Validation Loss: 53602, 74879.26781719741\n",
      "Epoch 53001, Training Loss: 38633, Validation Loss: 53879, 60332.30247713705\n",
      "Epoch 53101, Training Loss: 38913, Validation Loss: 54747, 73177.5658064669\n",
      "Epoch 53201, Training Loss: 40822, Validation Loss: 51976, 60333.09512874789\n",
      "Epoch 53301, Training Loss: 40603, Validation Loss: 54676, 70949.15875745376\n",
      "Epoch 53401, Training Loss: 39701, Validation Loss: 55524, 84994.49349547112\n",
      "Epoch 53501, Training Loss: 40185, Validation Loss: 53387, 60850.72054083105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53601, Training Loss: 37245, Validation Loss: 52809, 77191.87945234252\n",
      "Epoch 53701, Training Loss: 42072, Validation Loss: 54446, 69757.89947411134\n",
      "Epoch 53801, Training Loss: 39377, Validation Loss: 52998, 79238.26487779416\n",
      "Epoch 53901, Training Loss: 37988, Validation Loss: 52383, 83183.88092875476\n",
      "Epoch 54001, Training Loss: 39301, Validation Loss: 52452, 62912.98368872448\n",
      "Epoch 54101, Training Loss: 38622, Validation Loss: 54629, 70739.0202087576\n",
      "Epoch 54201, Training Loss: 39706, Validation Loss: 55581, 66259.22317681776\n",
      "Epoch 54301, Training Loss: 39146, Validation Loss: 52595, 75246.70438166911\n",
      "Epoch 54401, Training Loss: 40253, Validation Loss: 53058, 72362.14246518833\n",
      "Epoch 54501, Training Loss: 40013, Validation Loss: 52838, 85572.05230441208\n",
      "Epoch 54601, Training Loss: 41247, Validation Loss: 53349, 76208.07459708043\n",
      "Epoch 54701, Training Loss: 38631, Validation Loss: 53997, 83750.3182914043\n",
      "Epoch 54801, Training Loss: 39343, Validation Loss: 52251, 87304.63941099854\n",
      "Epoch 54901, Training Loss: 39180, Validation Loss: 53136, 83735.93116398495\n",
      "Epoch 55001, Training Loss: 38399, Validation Loss: 52228, 81310.23055002169\n",
      "Epoch 55101, Training Loss: 40531, Validation Loss: 53895, 87269.05664833261\n",
      "Epoch 55201, Training Loss: 39506, Validation Loss: 54976, 57170.361626392034\n",
      "Epoch 55301, Training Loss: 41310, Validation Loss: 54288, 81428.50431492717\n",
      "Epoch 55401, Training Loss: 39848, Validation Loss: 52235, 76298.49112907803\n",
      "Epoch 55501, Training Loss: 39243, Validation Loss: 53952, 65047.3071347321\n",
      "Epoch 55601, Training Loss: 39071, Validation Loss: 54731, 80392.99161243405\n",
      "Epoch 55701, Training Loss: 38143, Validation Loss: 54686, 70157.29870778718\n",
      "Epoch 55801, Training Loss: 40814, Validation Loss: 52873, 68505.56982595335\n",
      "Epoch 55901, Training Loss: 39161, Validation Loss: 53911, 78202.3322106391\n",
      "Epoch 56001, Training Loss: 39635, Validation Loss: 52854, 67189.57043070895\n",
      "Epoch 56101, Training Loss: 38415, Validation Loss: 52762, 71935.20486395364\n",
      "Epoch 56201, Training Loss: 38522, Validation Loss: 54055, 77317.62774083279\n",
      "Epoch 56301, Training Loss: 40082, Validation Loss: 52223, 89413.34628983872\n",
      "Epoch 56401, Training Loss: 38849, Validation Loss: 52827, 72372.49037800972\n",
      "Epoch 56501, Training Loss: 39494, Validation Loss: 55414, 70962.76075227949\n",
      "Epoch 56601, Training Loss: 39211, Validation Loss: 54117, 89038.76489699657\n",
      "Epoch 56701, Training Loss: 39353, Validation Loss: 54007, 67508.79099111952\n",
      "Epoch 56801, Training Loss: 40262, Validation Loss: 54858, 63605.423582906136\n",
      "Epoch 56901, Training Loss: 40599, Validation Loss: 53863, 89067.5727291293\n",
      "Epoch 57001, Training Loss: 38083, Validation Loss: 53579, 68077.41591371114\n",
      "Epoch 57101, Training Loss: 38382, Validation Loss: 52922, 83035.18875883409\n",
      "Epoch 57201, Training Loss: 41232, Validation Loss: 53629, 70079.92950443231\n",
      "Epoch 57301, Training Loss: 38599, Validation Loss: 54257, 68690.44641388276\n",
      "Epoch 57401, Training Loss: 39423, Validation Loss: 55894, 72033.80389195384\n",
      "Epoch 57501, Training Loss: 39799, Validation Loss: 55602, 70028.58723764682\n",
      "Epoch 57601, Training Loss: 39490, Validation Loss: 55729, 76263.6348934493\n",
      "Epoch 57701, Training Loss: 38169, Validation Loss: 54113, 103615.35295343741\n",
      "Epoch 57801, Training Loss: 40910, Validation Loss: 52559, 103294.77423507604\n",
      "Epoch 57901, Training Loss: 38422, Validation Loss: 54149, 80955.40628964204\n",
      "Epoch 58001, Training Loss: 40357, Validation Loss: 54613, 86068.5452729115\n",
      "Epoch 58101, Training Loss: 44459, Validation Loss: 50954, 75799.16470524612\n",
      "Epoch 58201, Training Loss: 42709, Validation Loss: 55674, 73706.6026979747\n",
      "Epoch 58301, Training Loss: 40963, Validation Loss: 53231, 77548.85495047663\n",
      "Epoch 58401, Training Loss: 38860, Validation Loss: 53027, 97570.89178116569\n",
      "Epoch 58501, Training Loss: 37784, Validation Loss: 53388, 67538.3090154162\n",
      "Epoch 58601, Training Loss: 40301, Validation Loss: 53731, 79208.08562477346\n",
      "Epoch 58701, Training Loss: 38827, Validation Loss: 53865, 68328.20242992278\n",
      "Epoch 58801, Training Loss: 37572, Validation Loss: 54149, 78051.65778419276\n",
      "Epoch 58901, Training Loss: 40698, Validation Loss: 53779, 81556.96961820537\n",
      "Epoch 59001, Training Loss: 39278, Validation Loss: 53541, 91331.60072965546\n",
      "Epoch 59101, Training Loss: 39006, Validation Loss: 52947, 81721.72404385997\n",
      "Epoch 59201, Training Loss: 42404, Validation Loss: 53729, 82687.92387886683\n",
      "Epoch 59301, Training Loss: 38413, Validation Loss: 53633, 81327.01625017717\n",
      "Epoch 59401, Training Loss: 38652, Validation Loss: 53166, 80551.7980958036\n",
      "Epoch 59501, Training Loss: 40094, Validation Loss: 55060, 77051.25281319693\n",
      "Epoch 59601, Training Loss: 38733, Validation Loss: 53798, 85481.26645087422\n",
      "Epoch 59701, Training Loss: 38605, Validation Loss: 55677, 82641.74806549163\n",
      "Epoch 59801, Training Loss: 39266, Validation Loss: 53472, 73582.12812072352\n",
      "Epoch 59901, Training Loss: 43824, Validation Loss: 55414, 78196.1013742481\n",
      "Epoch 60001, Training Loss: 40799, Validation Loss: 53348, 75984.81494604728\n",
      "Epoch 60101, Training Loss: 38580, Validation Loss: 54360, 75696.14255652296\n",
      "Epoch 60201, Training Loss: 38636, Validation Loss: 53805, 73891.79907239543\n",
      "Epoch 60301, Training Loss: 39254, Validation Loss: 53180, 114948.43165837647\n",
      "Epoch 60401, Training Loss: 36461, Validation Loss: 55266, 75860.10148665086\n",
      "Epoch 60501, Training Loss: 39602, Validation Loss: 52251, 90141.53891696472\n",
      "Epoch 60601, Training Loss: 39914, Validation Loss: 54405, 82082.45387090625\n",
      "Epoch 60701, Training Loss: 39866, Validation Loss: 53245, 106121.30116551708\n",
      "Epoch 60801, Training Loss: 38664, Validation Loss: 53191, 73909.86085766742\n",
      "Epoch 60901, Training Loss: 38120, Validation Loss: 53165, 85857.33862869164\n",
      "Epoch 61001, Training Loss: 38494, Validation Loss: 53138, 74652.79628107072\n",
      "Epoch 61101, Training Loss: 39717, Validation Loss: 53659, 83873.18346878867\n",
      "Epoch 61201, Training Loss: 40302, Validation Loss: 53674, 108703.1086283527\n",
      "Epoch 61301, Training Loss: 42880, Validation Loss: 54197, 94114.05320859775\n",
      "Epoch 61401, Training Loss: 37216, Validation Loss: 51878, 71708.723135916\n",
      "Epoch 61501, Training Loss: 38469, Validation Loss: 53286, 86410.63655537048\n",
      "Epoch 61601, Training Loss: 44986, Validation Loss: 54179, 76426.85013319214\n",
      "Epoch 61701, Training Loss: 38316, Validation Loss: 54888, 70033.09118917775\n",
      "Epoch 61801, Training Loss: 39398, Validation Loss: 53127, 79986.86504658328\n",
      "Epoch 61901, Training Loss: 38844, Validation Loss: 54600, 116121.81649255485\n",
      "Epoch 62001, Training Loss: 39969, Validation Loss: 53034, 90410.96218673412\n",
      "Epoch 62101, Training Loss: 40316, Validation Loss: 51975, 93630.11931279505\n",
      "Epoch 62201, Training Loss: 37605, Validation Loss: 53620, 74179.5241002121\n",
      "Epoch 62301, Training Loss: 37691, Validation Loss: 54242, 88899.92329405673\n",
      "Epoch 62401, Training Loss: 38591, Validation Loss: 53562, 101426.17278472563\n",
      "Epoch 62501, Training Loss: 42286, Validation Loss: 53450, 76610.89188838007\n",
      "Epoch 62601, Training Loss: 42977, Validation Loss: 55382, 104689.34314405695\n",
      "Epoch 62701, Training Loss: 38896, Validation Loss: 53707, 187255.4649643423\n",
      "Epoch 62801, Training Loss: 40097, Validation Loss: 54787, 74161.35893983519\n",
      "Epoch 62901, Training Loss: 41725, Validation Loss: 55520, 84808.34999388194\n",
      "Epoch 63001, Training Loss: 41271, Validation Loss: 53819, 99867.84200279698\n",
      "Epoch 63101, Training Loss: 39517, Validation Loss: 52444, 76625.94752855855\n",
      "Epoch 63201, Training Loss: 39213, Validation Loss: 53838, 83137.96319138251\n",
      "Epoch 63301, Training Loss: 38082, Validation Loss: 53982, 80847.0066396947\n",
      "Epoch 63401, Training Loss: 37993, Validation Loss: 53489, 99381.23239778476\n",
      "Epoch 63501, Training Loss: 37584, Validation Loss: 53639, 68069.42099881898\n",
      "Epoch 63601, Training Loss: 40192, Validation Loss: 54347, 85082.70619025509\n",
      "Epoch 63701, Training Loss: 38442, Validation Loss: 54605, 70521.58039390307\n",
      "Epoch 63801, Training Loss: 41011, Validation Loss: 53221, 97240.30878343481\n",
      "Epoch 63901, Training Loss: 38564, Validation Loss: 52684, 98193.23931484348\n",
      "Epoch 64001, Training Loss: 42091, Validation Loss: 54108, 76261.47073775524\n",
      "Epoch 64101, Training Loss: 39041, Validation Loss: 53176, 64880.50343061884\n",
      "Epoch 64201, Training Loss: 41499, Validation Loss: 54033, 77888.08631702246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64301, Training Loss: 38249, Validation Loss: 55110, 66467.86805579659\n",
      "Epoch 64401, Training Loss: 40855, Validation Loss: 53257, 84810.91022252142\n",
      "Epoch 64501, Training Loss: 39240, Validation Loss: 54265, 81486.37675400107\n",
      "Epoch 64601, Training Loss: 40889, Validation Loss: 52825, 75729.32916387882\n",
      "Epoch 64701, Training Loss: 40011, Validation Loss: 53625, 116529.6234970255\n",
      "Epoch 64801, Training Loss: 39949, Validation Loss: 53709, 90051.8551586537\n",
      "Epoch 64901, Training Loss: 36549, Validation Loss: 54223, 84010.72840081352\n",
      "Epoch 65001, Training Loss: 41861, Validation Loss: 53133, 70670.20491104096\n",
      "Epoch 65101, Training Loss: 41775, Validation Loss: 53817, 104513.72687519359\n",
      "Epoch 65201, Training Loss: 39232, Validation Loss: 53499, 86728.99388408492\n",
      "Epoch 65301, Training Loss: 41137, Validation Loss: 53346, 86590.28910651043\n",
      "Epoch 65401, Training Loss: 41340, Validation Loss: 53518, 75203.46254215935\n",
      "Epoch 65501, Training Loss: 41733, Validation Loss: 52582, 70218.10287927072\n",
      "Epoch 65601, Training Loss: 37767, Validation Loss: 54715, 91014.01658057277\n",
      "Epoch 65701, Training Loss: 40016, Validation Loss: 53277, 74638.78551386508\n",
      "Epoch 65801, Training Loss: 40822, Validation Loss: 52747, 77114.67088549606\n",
      "Epoch 65901, Training Loss: 39123, Validation Loss: 53903, 74324.43173227769\n",
      "Epoch 66001, Training Loss: 37829, Validation Loss: 53998, 82662.71943743255\n",
      "Epoch 66101, Training Loss: 39299, Validation Loss: 53846, 76799.68483452938\n",
      "Epoch 66201, Training Loss: 38899, Validation Loss: 54997, 82038.7194459291\n",
      "Epoch 66301, Training Loss: 39618, Validation Loss: 53403, 80280.01270013954\n",
      "Epoch 66401, Training Loss: 39473, Validation Loss: 53178, 114588.94037343173\n",
      "Epoch 66501, Training Loss: 39548, Validation Loss: 53179, 73530.23571670715\n",
      "Epoch 66601, Training Loss: 39415, Validation Loss: 52053, 78323.22030589629\n",
      "Epoch 66701, Training Loss: 39505, Validation Loss: 54851, 97806.3781972502\n",
      "Epoch 66801, Training Loss: 38781, Validation Loss: 52218, 106455.30883679318\n",
      "Epoch 66901, Training Loss: 44135, Validation Loss: 53498, 83241.86236866696\n",
      "Epoch 67001, Training Loss: 39378, Validation Loss: 53021, 79024.24471021749\n",
      "Epoch 67101, Training Loss: 36880, Validation Loss: 52250, 79130.3200651447\n",
      "Epoch 67201, Training Loss: 37521, Validation Loss: 51941, 88450.8976463119\n",
      "Epoch 67301, Training Loss: 38257, Validation Loss: 53369, 71742.59934424888\n",
      "Epoch 67401, Training Loss: 35879, Validation Loss: 55414, 67091.1062384745\n",
      "Epoch 67501, Training Loss: 41285, Validation Loss: 53785, 80050.32098133466\n",
      "Epoch 67601, Training Loss: 39677, Validation Loss: 53454, 74950.44416176736\n",
      "Epoch 67701, Training Loss: 35639, Validation Loss: 55188, 71460.76445946275\n",
      "Epoch 67801, Training Loss: 39269, Validation Loss: 54527, 78874.45509550779\n",
      "Epoch 67901, Training Loss: 39884, Validation Loss: 55011, 77527.3970250674\n",
      "Epoch 68001, Training Loss: 41657, Validation Loss: 54088, 85307.05632429666\n",
      "Epoch 68101, Training Loss: 41312, Validation Loss: 52315, 72869.41329556494\n",
      "Epoch 68201, Training Loss: 44045, Validation Loss: 53718, 88524.58656274881\n",
      "Epoch 68301, Training Loss: 37719, Validation Loss: 55258, 81006.9142731051\n",
      "Epoch 68401, Training Loss: 38038, Validation Loss: 54621, 78600.51416537976\n",
      "Epoch 68501, Training Loss: 39512, Validation Loss: 52621, 98389.40400533336\n",
      "Epoch 68601, Training Loss: 38393, Validation Loss: 54729, 73402.69980210169\n",
      "Epoch 68701, Training Loss: 42948, Validation Loss: 54698, 79489.25681878153\n",
      "Epoch 68801, Training Loss: 38705, Validation Loss: 54311, 94932.39812067618\n",
      "Epoch 68901, Training Loss: 39602, Validation Loss: 56078, 84861.26608385176\n",
      "Epoch 69001, Training Loss: 38148, Validation Loss: 52848, 101667.46317005817\n",
      "Epoch 69101, Training Loss: 40082, Validation Loss: 53821, 107842.80004755709\n",
      "Epoch 69201, Training Loss: 37563, Validation Loss: 53064, 89100.56082780233\n",
      "Epoch 69301, Training Loss: 39007, Validation Loss: 54295, 54853.84944270019\n",
      "Epoch 69401, Training Loss: 38199, Validation Loss: 53405, 74273.29507947272\n",
      "Epoch 69501, Training Loss: 39206, Validation Loss: 52143, 94511.7130326559\n",
      "Epoch 69601, Training Loss: 39365, Validation Loss: 51830, 74841.12956268745\n",
      "Epoch 69701, Training Loss: 38922, Validation Loss: 52887, 85143.13855414184\n",
      "Epoch 69801, Training Loss: 41800, Validation Loss: 54554, 84205.61575821595\n",
      "Epoch 69901, Training Loss: 35751, Validation Loss: 56200, 87052.36413921652\n",
      "Epoch 70001, Training Loss: 38440, Validation Loss: 54383, 66670.49969202022\n",
      "Epoch 70101, Training Loss: 38783, Validation Loss: 51519, 96235.61118472915\n",
      "Epoch 70201, Training Loss: 37625, Validation Loss: 54222, 68182.37447089587\n",
      "Epoch 70301, Training Loss: 38423, Validation Loss: 52238, 99910.82312394901\n",
      "Epoch 70401, Training Loss: 37140, Validation Loss: 53453, 66509.6831406307\n",
      "Epoch 70501, Training Loss: 36477, Validation Loss: 52225, 86391.02550379492\n",
      "Epoch 70601, Training Loss: 41096, Validation Loss: 54136, 80275.66171767238\n",
      "Epoch 70701, Training Loss: 37597, Validation Loss: 52978, 69414.97219083716\n",
      "Epoch 70801, Training Loss: 41460, Validation Loss: 53034, 83665.70905204669\n",
      "Epoch 70901, Training Loss: 44040, Validation Loss: 54608, 105656.81696670363\n",
      "Epoch 71001, Training Loss: 40983, Validation Loss: 53383, 73356.13329114765\n",
      "Epoch 71101, Training Loss: 39263, Validation Loss: 55299, 85885.5303110657\n",
      "Epoch 71201, Training Loss: 37309, Validation Loss: 52005, 75861.20708735417\n",
      "Epoch 71301, Training Loss: 43323, Validation Loss: 53307, 83563.06987842865\n",
      "Epoch 71401, Training Loss: 39542, Validation Loss: 53809, 76814.7100131417\n",
      "Epoch 71501, Training Loss: 40402, Validation Loss: 52829, 76594.98217713628\n",
      "Epoch 71601, Training Loss: 39905, Validation Loss: 53576, 83916.56604547189\n",
      "Epoch 71701, Training Loss: 40101, Validation Loss: 53898, 74487.23787036818\n",
      "Epoch 71801, Training Loss: 41100, Validation Loss: 53319, 74079.66056667351\n",
      "Epoch 71901, Training Loss: 39049, Validation Loss: 53542, 112463.48447673442\n",
      "Epoch 72001, Training Loss: 35685, Validation Loss: 53618, 87947.76644921718\n",
      "Epoch 72101, Training Loss: 41240, Validation Loss: 53603, 76014.11925021744\n",
      "Epoch 72201, Training Loss: 38241, Validation Loss: 52184, 87796.5107772468\n",
      "Epoch 72301, Training Loss: 38748, Validation Loss: 53480, 96497.74945822377\n",
      "Epoch 72401, Training Loss: 42012, Validation Loss: 52558, 84956.82391504338\n",
      "Epoch 72501, Training Loss: 37901, Validation Loss: 53211, 73000.29790119012\n",
      "Epoch 72601, Training Loss: 38348, Validation Loss: 53424, 63928.87369376963\n",
      "Epoch 72701, Training Loss: 40124, Validation Loss: 52975, 96658.48011431731\n",
      "Epoch 72801, Training Loss: 40068, Validation Loss: 52522, 74792.53007470914\n",
      "Epoch 72901, Training Loss: 38739, Validation Loss: 53710, 90182.15953532234\n",
      "Epoch 73001, Training Loss: 37363, Validation Loss: 53385, 86413.2387627682\n",
      "Epoch 73101, Training Loss: 39757, Validation Loss: 53849, 71567.61962058059\n",
      "Epoch 73201, Training Loss: 41070, Validation Loss: 54226, 81168.22584934926\n",
      "Epoch 73301, Training Loss: 40853, Validation Loss: 53061, 78626.01596273591\n",
      "Epoch 73401, Training Loss: 39916, Validation Loss: 52573, 98359.67962182639\n",
      "Epoch 73501, Training Loss: 38377, Validation Loss: 54948, 81791.3264530688\n",
      "Epoch 73601, Training Loss: 38718, Validation Loss: 53401, 65087.93788648377\n",
      "Epoch 73701, Training Loss: 38829, Validation Loss: 54365, 84275.18982794376\n",
      "Epoch 73801, Training Loss: 39162, Validation Loss: 53744, 78420.84833597168\n",
      "Epoch 73901, Training Loss: 38263, Validation Loss: 53702, 71979.3051471197\n",
      "Epoch 74001, Training Loss: 39907, Validation Loss: 56007, 70077.7070969849\n",
      "Epoch 74101, Training Loss: 40696, Validation Loss: 53212, 77140.58666055936\n",
      "Epoch 74201, Training Loss: 36012, Validation Loss: 54100, 70095.36552176416\n",
      "Epoch 74301, Training Loss: 37458, Validation Loss: 54121, 87232.91869081135\n",
      "Epoch 74401, Training Loss: 37593, Validation Loss: 53043, 70460.14637153852\n",
      "Epoch 74501, Training Loss: 39009, Validation Loss: 53702, 86386.99664127926\n",
      "Epoch 74601, Training Loss: 40163, Validation Loss: 52781, 71487.37727891414\n",
      "Epoch 74701, Training Loss: 40799, Validation Loss: 54196, 74489.32976507656\n",
      "Epoch 74801, Training Loss: 37402, Validation Loss: 53617, 92404.63600376209\n",
      "Epoch 74901, Training Loss: 36835, Validation Loss: 54148, 73096.11508785316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75001, Training Loss: 39764, Validation Loss: 53513, 80363.29885932138\n",
      "Epoch 75101, Training Loss: 37220, Validation Loss: 53495, 72247.40109600265\n",
      "Epoch 75201, Training Loss: 37898, Validation Loss: 51733, 82461.04068257565\n",
      "Epoch 75301, Training Loss: 40138, Validation Loss: 52840, 70713.50233632071\n",
      "Epoch 75401, Training Loss: 39504, Validation Loss: 52805, 101826.25331995093\n",
      "Epoch 75501, Training Loss: 37902, Validation Loss: 53423, 80087.62456034015\n",
      "Epoch 75601, Training Loss: 36993, Validation Loss: 53050, 70453.99292540232\n",
      "Epoch 75701, Training Loss: 38089, Validation Loss: 52468, 82965.41538719044\n",
      "Epoch 75801, Training Loss: 39720, Validation Loss: 54610, 76957.76246295361\n",
      "Epoch 75901, Training Loss: 39528, Validation Loss: 52987, 67350.55037493334\n",
      "Epoch 76001, Training Loss: 38544, Validation Loss: 53643, 91437.88017322125\n",
      "Epoch 76101, Training Loss: 40076, Validation Loss: 53497, 89399.09951966493\n",
      "Epoch 76201, Training Loss: 40123, Validation Loss: 55266, 80863.20582953664\n",
      "Epoch 76301, Training Loss: 37555, Validation Loss: 53837, 98037.76730788215\n",
      "Epoch 76401, Training Loss: 38487, Validation Loss: 54530, 73767.95344583751\n",
      "Epoch 76501, Training Loss: 40360, Validation Loss: 52543, 72553.10161317587\n",
      "Epoch 76601, Training Loss: 39501, Validation Loss: 52464, 80728.07391331393\n",
      "Epoch 76701, Training Loss: 38318, Validation Loss: 53053, 70707.42324349859\n",
      "Epoch 76801, Training Loss: 39893, Validation Loss: 55949, 87260.17550034422\n",
      "Epoch 76901, Training Loss: 38542, Validation Loss: 52255, 78107.04205285506\n",
      "Epoch 77001, Training Loss: 39581, Validation Loss: 53159, 67699.16268853936\n",
      "Epoch 77101, Training Loss: 38221, Validation Loss: 54004, 88266.13200980774\n",
      "Epoch 77201, Training Loss: 37574, Validation Loss: 55506, 79000.45668303505\n",
      "Epoch 77301, Training Loss: 40432, Validation Loss: 53052, 66423.1517995043\n",
      "Epoch 77401, Training Loss: 39033, Validation Loss: 52432, 97570.96548327948\n",
      "Epoch 77501, Training Loss: 38177, Validation Loss: 53985, 78220.97482063671\n",
      "Epoch 77601, Training Loss: 38515, Validation Loss: 54348, 80394.56454805528\n",
      "Epoch 77701, Training Loss: 38754, Validation Loss: 51908, 78925.76996204579\n",
      "Epoch 77801, Training Loss: 40097, Validation Loss: 54759, 68454.35509694507\n",
      "Epoch 77901, Training Loss: 39337, Validation Loss: 55801, 79820.78549795625\n",
      "Epoch 78001, Training Loss: 40843, Validation Loss: 55029, 65020.72341621688\n",
      "Epoch 78101, Training Loss: 38358, Validation Loss: 52547, 87644.78055892738\n",
      "Epoch 78201, Training Loss: 39365, Validation Loss: 52522, 83893.20164582216\n",
      "Epoch 78301, Training Loss: 36540, Validation Loss: 52529, 79541.4497963395\n",
      "Epoch 78401, Training Loss: 36830, Validation Loss: 54473, 90308.33891751517\n",
      "Epoch 78501, Training Loss: 38657, Validation Loss: 53138, 67840.30092930705\n",
      "Epoch 78601, Training Loss: 39420, Validation Loss: 52744, 72524.74713267421\n",
      "Epoch 78701, Training Loss: 43784, Validation Loss: 55267, 71091.11015017972\n",
      "Epoch 78801, Training Loss: 39750, Validation Loss: 52770, 64285.29420455911\n",
      "Epoch 78901, Training Loss: 37461, Validation Loss: 53012, 104992.7236830638\n",
      "Epoch 79001, Training Loss: 40855, Validation Loss: 53921, 86866.03854406446\n",
      "Epoch 79101, Training Loss: 41832, Validation Loss: 55386, 93952.57074042433\n",
      "Epoch 79201, Training Loss: 36926, Validation Loss: 54605, 86618.37247862184\n",
      "Epoch 79301, Training Loss: 40064, Validation Loss: 53914, 97918.25084092094\n",
      "Epoch 79401, Training Loss: 37747, Validation Loss: 52933, 78289.29236616388\n",
      "Epoch 79501, Training Loss: 39424, Validation Loss: 54917, 87001.28939395554\n",
      "Epoch 79601, Training Loss: 37461, Validation Loss: 54557, 84387.56445542364\n",
      "Epoch 79701, Training Loss: 40496, Validation Loss: 52447, 96989.28256006977\n",
      "Epoch 79801, Training Loss: 38317, Validation Loss: 52751, 80872.02550935831\n",
      "Epoch 79901, Training Loss: 38829, Validation Loss: 53169, 84020.13907464889\n",
      "Epoch 80001, Training Loss: 37651, Validation Loss: 53456, 83389.43583830718\n",
      "Epoch 80101, Training Loss: 37772, Validation Loss: 53188, 90059.65585418406\n",
      "Epoch 80201, Training Loss: 37487, Validation Loss: 54156, 77915.52409673553\n",
      "Epoch 80301, Training Loss: 38257, Validation Loss: 54696, 88662.94006837875\n",
      "Epoch 80401, Training Loss: 43162, Validation Loss: 53702, 101548.34806354367\n",
      "Epoch 80501, Training Loss: 39729, Validation Loss: 54817, 79194.67800493915\n",
      "Epoch 80601, Training Loss: 39558, Validation Loss: 53002, 85196.6554625144\n",
      "Epoch 80701, Training Loss: 41520, Validation Loss: 54860, 86641.23437808648\n",
      "Epoch 80801, Training Loss: 38843, Validation Loss: 53367, 69426.70240626116\n",
      "Epoch 80901, Training Loss: 38584, Validation Loss: 54012, 75890.0571108759\n",
      "Epoch 81001, Training Loss: 38362, Validation Loss: 53731, 75624.92134146414\n",
      "Epoch 81101, Training Loss: 37321, Validation Loss: 52963, 129081.52361693764\n",
      "Epoch 81201, Training Loss: 36021, Validation Loss: 52527, 68547.17773711435\n",
      "Epoch 81301, Training Loss: 38561, Validation Loss: 55217, 100718.50020240828\n",
      "Epoch 81401, Training Loss: 39408, Validation Loss: 52611, 95568.5744440182\n",
      "Epoch 81501, Training Loss: 39879, Validation Loss: 53629, 90592.55245915985\n",
      "Epoch 81601, Training Loss: 37673, Validation Loss: 53901, 96180.59599003685\n",
      "Epoch 81701, Training Loss: 42228, Validation Loss: 54117, 89424.12919777351\n",
      "Epoch 81801, Training Loss: 41468, Validation Loss: 54403, 85622.92785471065\n",
      "Epoch 81901, Training Loss: 37496, Validation Loss: 52628, 135226.70923353982\n",
      "Epoch 82001, Training Loss: 40132, Validation Loss: 52560, 109417.58390992384\n",
      "Epoch 82101, Training Loss: 41338, Validation Loss: 53303, 88498.94998084732\n",
      "Epoch 82201, Training Loss: 39419, Validation Loss: 54302, 76058.14944472203\n",
      "Epoch 82301, Training Loss: 36973, Validation Loss: 54188, 73618.00113722529\n",
      "Epoch 82401, Training Loss: 40012, Validation Loss: 52536, 95296.06046442136\n",
      "Epoch 82501, Training Loss: 37967, Validation Loss: 56594, 85346.80404867831\n",
      "Epoch 82601, Training Loss: 38351, Validation Loss: 56325, 85490.3406909542\n",
      "Epoch 82701, Training Loss: 38457, Validation Loss: 56102, 77764.58054960199\n",
      "Epoch 82801, Training Loss: 40158, Validation Loss: 54363, 102988.84023797134\n",
      "Epoch 82901, Training Loss: 37358, Validation Loss: 53128, 68469.4074702672\n",
      "Epoch 83001, Training Loss: 38440, Validation Loss: 54086, 80564.11713518058\n",
      "Epoch 83101, Training Loss: 39901, Validation Loss: 53080, 87498.24835618626\n",
      "Epoch 83201, Training Loss: 40964, Validation Loss: 54297, 97242.17168652771\n",
      "Epoch 83301, Training Loss: 38247, Validation Loss: 53855, 88400.88558677\n",
      "Epoch 83401, Training Loss: 37779, Validation Loss: 53201, 730139.0225243616\n",
      "Epoch 83501, Training Loss: 38864, Validation Loss: 53196, 84116.3966756529\n",
      "Epoch 83601, Training Loss: 40081, Validation Loss: 55717, 109972.35111411092\n",
      "Epoch 83701, Training Loss: 41231, Validation Loss: 52615, 77594.69832678908\n",
      "Epoch 83801, Training Loss: 36073, Validation Loss: 56890, 108853.3958142834\n",
      "Epoch 83901, Training Loss: 38580, Validation Loss: 54163, 77128.5490919208\n",
      "Epoch 84001, Training Loss: 40023, Validation Loss: 52801, 80075.45894547277\n",
      "Epoch 84101, Training Loss: 40319, Validation Loss: 52224, 83781.40565254955\n",
      "Epoch 84201, Training Loss: 41781, Validation Loss: 51406, 81437.26057537382\n",
      "Epoch 84301, Training Loss: 37519, Validation Loss: 53899, 104397.91008030076\n",
      "Epoch 84401, Training Loss: 41113, Validation Loss: 52569, 76416.14543907485\n",
      "Epoch 84501, Training Loss: 37974, Validation Loss: 54068, 91181.05003617138\n",
      "Epoch 84601, Training Loss: 38592, Validation Loss: 53097, 97687.93706380099\n",
      "Epoch 84701, Training Loss: 40161, Validation Loss: 54773, 103143.5292333458\n",
      "Epoch 84801, Training Loss: 40769, Validation Loss: 55229, 72820.69593884608\n",
      "Epoch 84901, Training Loss: 40201, Validation Loss: 54402, 105767.86564291072\n",
      "Epoch 85001, Training Loss: 37842, Validation Loss: 53632, 78094.60640932067\n",
      "Epoch 85101, Training Loss: 38013, Validation Loss: 55023, 88082.302691746\n",
      "Epoch 85201, Training Loss: 36721, Validation Loss: 53559, 69754.01489277817\n",
      "Epoch 85301, Training Loss: 36171, Validation Loss: 53723, 90041.54316918719\n",
      "Epoch 85401, Training Loss: 37584, Validation Loss: 54238, 67769.43246958374\n",
      "Epoch 85501, Training Loss: 39822, Validation Loss: 52225, 97669.07945135864\n",
      "Epoch 85601, Training Loss: 41348, Validation Loss: 53948, 90480.4345306051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85701, Training Loss: 36940, Validation Loss: 52965, 81531.31373268199\n",
      "Epoch 85801, Training Loss: 35709, Validation Loss: 53149, 103039.71688768514\n",
      "Epoch 85901, Training Loss: 39980, Validation Loss: 52418, 85214.82973395092\n",
      "Epoch 86001, Training Loss: 40180, Validation Loss: 53795, 80113.71206487095\n",
      "Epoch 86101, Training Loss: 36510, Validation Loss: 52802, 104066.25506610495\n",
      "Epoch 86201, Training Loss: 38189, Validation Loss: 54197, 74792.10687661101\n",
      "Epoch 86301, Training Loss: 38768, Validation Loss: 54129, 75648.65740972078\n",
      "Epoch 86401, Training Loss: 38880, Validation Loss: 53128, 87497.5743781601\n",
      "Epoch 86501, Training Loss: 38012, Validation Loss: 53061, 98377.40589589257\n",
      "Epoch 86601, Training Loss: 37683, Validation Loss: 53038, 77320.58537846767\n",
      "Epoch 86701, Training Loss: 37895, Validation Loss: 53148, 110065.4727760991\n",
      "Epoch 86801, Training Loss: 39066, Validation Loss: 52616, 75766.36655118533\n",
      "Epoch 86901, Training Loss: 38997, Validation Loss: 53144, 79690.2364160294\n",
      "Epoch 87001, Training Loss: 40804, Validation Loss: 53623, 80894.27574354318\n",
      "Epoch 87101, Training Loss: 40162, Validation Loss: 52424, 81799.74989774304\n",
      "Epoch 87201, Training Loss: 38482, Validation Loss: 54264, 103603.13614147085\n",
      "Epoch 87301, Training Loss: 37810, Validation Loss: 53495, 79606.63554886256\n",
      "Epoch 87401, Training Loss: 38779, Validation Loss: 53478, 90145.8052573731\n",
      "Epoch 87501, Training Loss: 39263, Validation Loss: 53179, 86251.62392705462\n",
      "Epoch 87601, Training Loss: 39454, Validation Loss: 53255, 103271.37025055657\n",
      "Epoch 87701, Training Loss: 38311, Validation Loss: 53312, 155329.64996687302\n",
      "Epoch 87801, Training Loss: 40633, Validation Loss: 54742, 90515.43686888617\n",
      "Epoch 87901, Training Loss: 40249, Validation Loss: 53004, 83564.08382296027\n",
      "Epoch 88001, Training Loss: 39210, Validation Loss: 56055, 86842.26601796444\n",
      "Epoch 88101, Training Loss: 41166, Validation Loss: 55416, 76161.60470198705\n",
      "Epoch 88201, Training Loss: 37456, Validation Loss: 54282, 80404.26232381244\n",
      "Epoch 88301, Training Loss: 38330, Validation Loss: 54417, 85253.93635899977\n",
      "Epoch 88401, Training Loss: 40426, Validation Loss: 52529, 80003.45624541557\n",
      "Epoch 88501, Training Loss: 36619, Validation Loss: 52865, 99140.56062902509\n",
      "Epoch 88601, Training Loss: 35843, Validation Loss: 52911, 91026.75771541211\n",
      "Epoch 88701, Training Loss: 38615, Validation Loss: 53723, 128717.52449967805\n",
      "Epoch 88801, Training Loss: 38839, Validation Loss: 53728, 96076.75367869763\n",
      "Epoch 88901, Training Loss: 38838, Validation Loss: 52705, 90676.58415143228\n",
      "Epoch 89001, Training Loss: 35639, Validation Loss: 52910, 80788.14440536284\n",
      "Epoch 89101, Training Loss: 39687, Validation Loss: 52988, 97951.06851106929\n",
      "Epoch 89201, Training Loss: 39282, Validation Loss: 54177, 107701.3696046404\n",
      "Epoch 89301, Training Loss: 38065, Validation Loss: 53340, 96390.7980203324\n",
      "Epoch 89401, Training Loss: 37637, Validation Loss: 53222, 106081.72006543698\n",
      "Epoch 89501, Training Loss: 37575, Validation Loss: 54407, 85393.40115262955\n",
      "Epoch 89601, Training Loss: 38560, Validation Loss: 54364, 126348.28945707425\n",
      "Epoch 89701, Training Loss: 37357, Validation Loss: 53537, 81219.54178049888\n",
      "Epoch 89801, Training Loss: 37179, Validation Loss: 54961, 89971.29052121476\n",
      "Epoch 89901, Training Loss: 38475, Validation Loss: 53018, 89100.57202472586\n",
      "Epoch 90001, Training Loss: 36710, Validation Loss: 54065, 85718.84086500217\n",
      "Epoch 90101, Training Loss: 37903, Validation Loss: 53824, 113476.86136750832\n",
      "Epoch 90201, Training Loss: 36907, Validation Loss: 51917, 79622.3534811738\n",
      "Epoch 90301, Training Loss: 37882, Validation Loss: 53160, 96109.49501939506\n",
      "Epoch 90401, Training Loss: 36484, Validation Loss: 55001, 86650.00317366706\n",
      "Epoch 90501, Training Loss: 37060, Validation Loss: 53458, 88043.56140554849\n",
      "Epoch 90601, Training Loss: 39262, Validation Loss: 52341, 89660.84751428709\n",
      "Epoch 90701, Training Loss: 36621, Validation Loss: 53975, 98347.02891308034\n",
      "Epoch 90801, Training Loss: 37550, Validation Loss: 53395, 106482.55558245811\n",
      "Epoch 90901, Training Loss: 38395, Validation Loss: 53733, 95148.18180551675\n",
      "Epoch 91001, Training Loss: 38148, Validation Loss: 53311, 96553.7176628041\n",
      "Epoch 91101, Training Loss: 39262, Validation Loss: 52660, 77901.26085116605\n",
      "Epoch 91201, Training Loss: 40499, Validation Loss: 54057, 100735.45693674246\n",
      "Epoch 91301, Training Loss: 38055, Validation Loss: 52882, 112587.4344968325\n",
      "Epoch 91401, Training Loss: 35981, Validation Loss: 54063, 82679.39920005594\n",
      "Epoch 91501, Training Loss: 39256, Validation Loss: 52748, 91856.60421941117\n",
      "Epoch 91601, Training Loss: 39198, Validation Loss: 53629, 87050.98531260667\n",
      "Epoch 91701, Training Loss: 39789, Validation Loss: 51755, 101105.57795730275\n",
      "Epoch 91801, Training Loss: 37545, Validation Loss: 53208, 83884.97156645868\n",
      "Epoch 91901, Training Loss: 36308, Validation Loss: 54007, 96252.57061690075\n",
      "Epoch 92001, Training Loss: 38703, Validation Loss: 52453, 92973.79989891638\n",
      "Epoch 92101, Training Loss: 39441, Validation Loss: 53296, 101763.28706289401\n",
      "Epoch 92201, Training Loss: 37290, Validation Loss: 54031, 86990.2238556462\n",
      "Epoch 92301, Training Loss: 36291, Validation Loss: 52984, 79011.0786183658\n",
      "Epoch 92401, Training Loss: 38966, Validation Loss: 53807, 90720.4939005768\n",
      "Epoch 92501, Training Loss: 39208, Validation Loss: 53683, 110932.58092857238\n",
      "Epoch 92601, Training Loss: 37982, Validation Loss: 53394, 106520.23439993414\n",
      "Epoch 92701, Training Loss: 37764, Validation Loss: 52618, 113546.48555937794\n",
      "Epoch 92801, Training Loss: 40023, Validation Loss: 52828, 104226.59849072318\n",
      "Epoch 92901, Training Loss: 36383, Validation Loss: 52799, 101646.7603613382\n",
      "Epoch 93001, Training Loss: 40525, Validation Loss: 53369, 101810.42763646158\n",
      "Epoch 93101, Training Loss: 39095, Validation Loss: 52835, 94198.44124017074\n",
      "Epoch 93201, Training Loss: 37812, Validation Loss: 55107, 115738.19544719947\n",
      "Epoch 93301, Training Loss: 38877, Validation Loss: 53502, 80610.34011522954\n",
      "Epoch 93401, Training Loss: 40644, Validation Loss: 53951, 86073.65338751125\n",
      "Epoch 93501, Training Loss: 35621, Validation Loss: 52352, 96898.28013038798\n",
      "Epoch 93601, Training Loss: 39586, Validation Loss: 53696, 82338.60116422486\n",
      "Epoch 93701, Training Loss: 39780, Validation Loss: 54844, 99370.62735202152\n",
      "Epoch 93801, Training Loss: 38415, Validation Loss: 54214, 102755.58458461515\n",
      "Epoch 93901, Training Loss: 37610, Validation Loss: 52839, 98499.38551697116\n",
      "Epoch 94001, Training Loss: 39808, Validation Loss: 53031, 86218.57455469569\n",
      "Epoch 94101, Training Loss: 38187, Validation Loss: 54453, 112260.82298583392\n",
      "Epoch 94201, Training Loss: 39705, Validation Loss: 54050, 95746.64651369127\n",
      "Epoch 94301, Training Loss: 39140, Validation Loss: 53706, 78116.78648235352\n",
      "Epoch 94401, Training Loss: 39210, Validation Loss: 54790, 126844.51321770025\n",
      "Epoch 94501, Training Loss: 40665, Validation Loss: 52787, 114623.42754756042\n",
      "Epoch 94601, Training Loss: 36366, Validation Loss: 53813, 85137.45160016128\n",
      "Epoch 94701, Training Loss: 38963, Validation Loss: 55165, 92145.00032064893\n",
      "Epoch 94801, Training Loss: 39238, Validation Loss: 53028, 93711.57848707448\n",
      "Epoch 94901, Training Loss: 40825, Validation Loss: 53864, 109741.23575564862\n",
      "Epoch 95001, Training Loss: 43537, Validation Loss: 54882, 112229.45716941329\n",
      "Epoch 95101, Training Loss: 38141, Validation Loss: 52220, 87698.05883505913\n",
      "Epoch 95201, Training Loss: 40181, Validation Loss: 53672, 85583.38909300943\n",
      "Epoch 95301, Training Loss: 36096, Validation Loss: 52507, 88815.94590513893\n",
      "Epoch 95401, Training Loss: 37285, Validation Loss: 52953, 93042.08888259571\n",
      "Epoch 95501, Training Loss: 39384, Validation Loss: 55229, 106288.36011428502\n",
      "Epoch 95601, Training Loss: 41287, Validation Loss: 55154, 90945.63751364934\n",
      "Epoch 95701, Training Loss: 38825, Validation Loss: 54004, 92976.96884089033\n",
      "Epoch 95801, Training Loss: 38341, Validation Loss: 54369, 102798.26460752776\n",
      "Epoch 95901, Training Loss: 33363, Validation Loss: 53175, 86866.30020798714\n",
      "Epoch 96001, Training Loss: 39460, Validation Loss: 53213, 106549.92036187269\n",
      "Epoch 96101, Training Loss: 43429, Validation Loss: 52951, 106266.1990852938\n",
      "Epoch 96201, Training Loss: 37616, Validation Loss: 54184, 93240.96957350378\n",
      "Epoch 96301, Training Loss: 37488, Validation Loss: 54405, 104347.36687217338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96401, Training Loss: 40349, Validation Loss: 53815, 81594.36195463977\n",
      "Epoch 96501, Training Loss: 37173, Validation Loss: 53361, 90610.72895807911\n",
      "Epoch 96601, Training Loss: 38742, Validation Loss: 54276, 119696.70915669063\n",
      "Epoch 96701, Training Loss: 38737, Validation Loss: 53976, 105038.05432884193\n",
      "Epoch 96801, Training Loss: 39361, Validation Loss: 53108, 96678.47794029338\n",
      "Epoch 96901, Training Loss: 41285, Validation Loss: 52289, 104878.62253355549\n",
      "Epoch 97001, Training Loss: 42155, Validation Loss: 52515, 79007.82737677592\n",
      "Epoch 97101, Training Loss: 37398, Validation Loss: 53026, 100355.46664650373\n",
      "Epoch 97201, Training Loss: 38205, Validation Loss: 53018, 104344.7460188145\n",
      "Epoch 97301, Training Loss: 42532, Validation Loss: 54136, 111687.03055872205\n",
      "Epoch 97401, Training Loss: 38930, Validation Loss: 55553, 98133.70592667798\n",
      "Epoch 97501, Training Loss: 36668, Validation Loss: 51869, 87376.76985877282\n",
      "Epoch 97601, Training Loss: 36498, Validation Loss: 51866, 111387.38534535008\n",
      "Epoch 97701, Training Loss: 40334, Validation Loss: 53994, 91911.88162927916\n",
      "Epoch 97801, Training Loss: 39141, Validation Loss: 54812, 93100.8117978727\n",
      "Epoch 97901, Training Loss: 35505, Validation Loss: 51765, 93788.38060241379\n",
      "Epoch 98001, Training Loss: 41912, Validation Loss: 54509, 129368.86354175869\n",
      "Epoch 98101, Training Loss: 37435, Validation Loss: 53509, 82006.58363062795\n",
      "Epoch 98201, Training Loss: 40906, Validation Loss: 55784, 87452.41831339097\n",
      "Epoch 98301, Training Loss: 39490, Validation Loss: 52734, 97023.02821647946\n",
      "Epoch 98401, Training Loss: 36613, Validation Loss: 55934, 88958.42927029234\n",
      "Epoch 98501, Training Loss: 40278, Validation Loss: 54491, 101912.31689790632\n",
      "Epoch 98601, Training Loss: 37575, Validation Loss: 53041, 100452.20295272407\n",
      "Epoch 98701, Training Loss: 40272, Validation Loss: 54027, 105431.50895206265\n",
      "Epoch 98801, Training Loss: 39052, Validation Loss: 52770, 112119.34862560216\n",
      "Epoch 98901, Training Loss: 38087, Validation Loss: 53482, 105274.04042360037\n",
      "Epoch 99001, Training Loss: 36236, Validation Loss: 53218, 76009.32859225624\n",
      "Epoch 99101, Training Loss: 36630, Validation Loss: 54658, 87792.11465398833\n",
      "Epoch 99201, Training Loss: 36752, Validation Loss: 53519, 98073.89129939454\n",
      "Epoch 99301, Training Loss: 39335, Validation Loss: 54362, 124566.46038993316\n",
      "Epoch 99401, Training Loss: 40685, Validation Loss: 54000, 104051.31183359493\n",
      "Epoch 99501, Training Loss: 37072, Validation Loss: 52706, 97508.36954085813\n",
      "Epoch 99601, Training Loss: 39312, Validation Loss: 52269, 89908.80472122201\n",
      "Epoch 99701, Training Loss: 39107, Validation Loss: 55408, 100968.50058099226\n",
      "Epoch 99801, Training Loss: 37335, Validation Loss: 53631, 120177.33861789136\n",
      "Epoch 99901, Training Loss: 36044, Validation Loss: 53550, 92331.83519696286\n",
      "Epoch 100001, Training Loss: 38301, Validation Loss: 52939, 108730.92512659088\n",
      "Epoch 100101, Training Loss: 41855, Validation Loss: 54218, 149344.66486663674\n",
      "Epoch 100201, Training Loss: 38661, Validation Loss: 52455, 115966.04210790701\n",
      "Epoch 100301, Training Loss: 37844, Validation Loss: 53451, 96159.93940830207\n",
      "Epoch 100401, Training Loss: 36569, Validation Loss: 55220, 116983.13230212405\n",
      "Epoch 100501, Training Loss: 38664, Validation Loss: 53409, 90390.3265537547\n",
      "Epoch 100601, Training Loss: 38044, Validation Loss: 54909, 86094.5446033204\n",
      "Epoch 100701, Training Loss: 39269, Validation Loss: 52898, 91433.54942811244\n",
      "Epoch 100801, Training Loss: 41368, Validation Loss: 52310, 119436.30790036242\n",
      "Epoch 100901, Training Loss: 38659, Validation Loss: 53273, 99033.43334571253\n",
      "Epoch 101001, Training Loss: 38048, Validation Loss: 54668, 102034.23419783196\n",
      "Epoch 101101, Training Loss: 38172, Validation Loss: 53102, 136284.62373609314\n",
      "Epoch 101201, Training Loss: 39848, Validation Loss: 53640, 102687.19506299346\n",
      "Epoch 101301, Training Loss: 38215, Validation Loss: 53755, 117570.57784359784\n",
      "Epoch 101401, Training Loss: 40105, Validation Loss: 53517, 94741.72592059219\n",
      "Epoch 101501, Training Loss: 37258, Validation Loss: 52304, 91811.41865584324\n",
      "Epoch 101601, Training Loss: 38163, Validation Loss: 53708, 90961.7392813639\n",
      "Epoch 101701, Training Loss: 39511, Validation Loss: 53274, 99385.84510923277\n",
      "Epoch 101801, Training Loss: 36889, Validation Loss: 53729, 98633.33556147316\n",
      "Epoch 101901, Training Loss: 43695, Validation Loss: 53640, 99255.29100589773\n",
      "Epoch 102001, Training Loss: 35816, Validation Loss: 52398, 131077.94210936362\n",
      "Epoch 102101, Training Loss: 38240, Validation Loss: 53584, 86082.07817440834\n",
      "Epoch 102201, Training Loss: 37282, Validation Loss: 54487, 82587.69915884273\n",
      "Epoch 102301, Training Loss: 39798, Validation Loss: 54443, 98224.7171096491\n",
      "Epoch 102401, Training Loss: 37867, Validation Loss: 52844, 92637.50202811246\n",
      "Epoch 102501, Training Loss: 41174, Validation Loss: 52981, 139098.14407055476\n",
      "Epoch 102601, Training Loss: 40193, Validation Loss: 54229, 81005.76279861982\n",
      "Epoch 102701, Training Loss: 38618, Validation Loss: 53483, 118477.59775313661\n",
      "Epoch 102801, Training Loss: 44030, Validation Loss: 53751, 108300.33608774575\n",
      "Epoch 102901, Training Loss: 37804, Validation Loss: 52345, 99492.51850792195\n",
      "Epoch 103001, Training Loss: 39262, Validation Loss: 53669, 101903.01694213004\n",
      "Epoch 103101, Training Loss: 38650, Validation Loss: 54083, 89655.54205882242\n",
      "Epoch 103201, Training Loss: 38966, Validation Loss: 52679, 100517.84257482518\n",
      "Epoch 103301, Training Loss: 36796, Validation Loss: 55693, 112861.10916955136\n",
      "Epoch 103401, Training Loss: 38787, Validation Loss: 54114, 99634.69430312562\n",
      "Epoch 103501, Training Loss: 41516, Validation Loss: 54249, 77387.15250270815\n",
      "Epoch 103601, Training Loss: 37646, Validation Loss: 53441, 108347.97911132203\n",
      "Epoch 103701, Training Loss: 36788, Validation Loss: 51728, 109398.481801128\n",
      "Epoch 103801, Training Loss: 37689, Validation Loss: 52804, 84599.72841331045\n",
      "Epoch 103901, Training Loss: 40254, Validation Loss: 52095, 92148.984621673\n",
      "Epoch 104001, Training Loss: 40087, Validation Loss: 54819, 96441.12294964427\n",
      "Epoch 104101, Training Loss: 39760, Validation Loss: 54780, 90290.32625821361\n",
      "Epoch 104201, Training Loss: 39997, Validation Loss: 52729, 88285.54164772516\n",
      "Epoch 104301, Training Loss: 41239, Validation Loss: 52334, 159523.74858469694\n",
      "Epoch 104401, Training Loss: 34049, Validation Loss: 53406, 84716.3745220595\n",
      "Epoch 104501, Training Loss: 39285, Validation Loss: 53660, 94242.9644548566\n",
      "Epoch 104601, Training Loss: 37776, Validation Loss: 53098, 82077.83814413786\n",
      "Epoch 104701, Training Loss: 38880, Validation Loss: 53661, 74593.71177370891\n",
      "Epoch 104801, Training Loss: 37756, Validation Loss: 52620, 129725.88249308383\n",
      "Epoch 104901, Training Loss: 38830, Validation Loss: 53174, 83310.75797657952\n",
      "Epoch 105001, Training Loss: 39122, Validation Loss: 54018, 101262.5775910208\n",
      "Epoch 105101, Training Loss: 39416, Validation Loss: 53501, 117906.13139242314\n",
      "Epoch 105201, Training Loss: 39580, Validation Loss: 54107, 95998.78002990538\n",
      "Epoch 105301, Training Loss: 36050, Validation Loss: 54322, 94611.10510926628\n",
      "Epoch 105401, Training Loss: 38434, Validation Loss: 53884, 95808.51034081167\n",
      "Epoch 105501, Training Loss: 37947, Validation Loss: 52959, 81206.86574928687\n",
      "Epoch 105601, Training Loss: 38508, Validation Loss: 52237, 100251.75907287413\n",
      "Epoch 105701, Training Loss: 37668, Validation Loss: 54871, 87920.86787668201\n",
      "Epoch 105801, Training Loss: 36547, Validation Loss: 55169, 102694.43638979795\n",
      "Epoch 105901, Training Loss: 39318, Validation Loss: 53501, 96005.92633735626\n",
      "Epoch 106001, Training Loss: 36825, Validation Loss: 53558, 118595.75614649564\n",
      "Epoch 106101, Training Loss: 37860, Validation Loss: 51904, 90615.57193862148\n",
      "Epoch 106201, Training Loss: 38446, Validation Loss: 54947, 114508.29830429707\n",
      "Epoch 106301, Training Loss: 37552, Validation Loss: 53952, 92923.18130442219\n",
      "Epoch 106401, Training Loss: 38101, Validation Loss: 54178, 90134.78220535554\n",
      "Epoch 106501, Training Loss: 38730, Validation Loss: 51903, 110557.5731888786\n",
      "Epoch 106601, Training Loss: 38550, Validation Loss: 52830, 77247.74375668715\n",
      "Epoch 106701, Training Loss: 38893, Validation Loss: 54039, 113805.14269294862\n",
      "Epoch 106801, Training Loss: 35790, Validation Loss: 55055, 99474.05840442616\n",
      "Epoch 106901, Training Loss: 38008, Validation Loss: 54915, 113812.84414100733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107001, Training Loss: 40740, Validation Loss: 55076, 71115.05322822521\n",
      "Epoch 107101, Training Loss: 38478, Validation Loss: 52637, 91343.09102349635\n",
      "Epoch 107201, Training Loss: 37372, Validation Loss: 52423, 101521.08510724765\n",
      "Epoch 107301, Training Loss: 38346, Validation Loss: 53762, 151567.55189449884\n",
      "Epoch 107401, Training Loss: 39218, Validation Loss: 54793, 129247.8060596857\n",
      "Epoch 107501, Training Loss: 41275, Validation Loss: 54253, 77533.14189043886\n",
      "Epoch 107601, Training Loss: 38030, Validation Loss: 53498, 124106.77525130322\n",
      "Epoch 107701, Training Loss: 38955, Validation Loss: 53182, 79073.71705598231\n",
      "Epoch 107801, Training Loss: 37214, Validation Loss: 51995, 83563.62878617334\n",
      "Epoch 107901, Training Loss: 41258, Validation Loss: 53952, 100750.98436607141\n",
      "Epoch 108001, Training Loss: 39603, Validation Loss: 52149, 87431.98329217143\n",
      "Epoch 108101, Training Loss: 39183, Validation Loss: 52160, 101172.47655685234\n",
      "Epoch 108201, Training Loss: 39062, Validation Loss: 53131, 76613.50043760148\n",
      "Epoch 108301, Training Loss: 37693, Validation Loss: 53111, 85504.78364266544\n",
      "Epoch 108401, Training Loss: 37618, Validation Loss: 54666, 106686.55273781293\n",
      "Epoch 108501, Training Loss: 38628, Validation Loss: 53813, 113839.25317414897\n",
      "Epoch 108601, Training Loss: 39251, Validation Loss: 52818, 121930.75659583356\n",
      "Epoch 108701, Training Loss: 41084, Validation Loss: 53939, 108995.95058901729\n",
      "Epoch 108801, Training Loss: 36842, Validation Loss: 53550, 105433.8690605554\n",
      "Epoch 108901, Training Loss: 37705, Validation Loss: 53616, 124173.73024395993\n",
      "Epoch 109001, Training Loss: 38619, Validation Loss: 53557, 93123.0282209032\n",
      "Epoch 109101, Training Loss: 38572, Validation Loss: 53854, 87249.86039989076\n",
      "Epoch 109201, Training Loss: 39717, Validation Loss: 53995, 103536.22860236272\n",
      "Epoch 109301, Training Loss: 40144, Validation Loss: 54478, 97513.86162800388\n",
      "Epoch 109401, Training Loss: 41050, Validation Loss: 52721, 133892.31673464502\n",
      "Epoch 109501, Training Loss: 38335, Validation Loss: 53376, 143299.83313437013\n",
      "Epoch 109601, Training Loss: 38170, Validation Loss: 53156, 96979.58001062322\n",
      "Epoch 109701, Training Loss: 38969, Validation Loss: 54587, 105952.29327814643\n",
      "Epoch 109801, Training Loss: 39805, Validation Loss: 54595, 90203.47779884352\n",
      "Epoch 109901, Training Loss: 37864, Validation Loss: 53715, 140026.820154381\n",
      "Epoch 110001, Training Loss: 38398, Validation Loss: 54620, 97728.3800693379\n",
      "Epoch 110101, Training Loss: 38024, Validation Loss: 52140, 95442.57067875938\n",
      "Epoch 110201, Training Loss: 37447, Validation Loss: 54553, 114779.98551154528\n",
      "Epoch 110301, Training Loss: 37927, Validation Loss: 55268, 100029.25729203505\n",
      "Epoch 110401, Training Loss: 40911, Validation Loss: 53143, 108201.60618456826\n",
      "Epoch 110501, Training Loss: 37526, Validation Loss: 53592, 83082.5445302053\n",
      "Epoch 110601, Training Loss: 38481, Validation Loss: 53675, 108055.59837975125\n",
      "Epoch 110701, Training Loss: 37198, Validation Loss: 55813, 83802.25817157289\n",
      "Epoch 110801, Training Loss: 36030, Validation Loss: 54191, 105981.3419237355\n",
      "Epoch 110901, Training Loss: 38252, Validation Loss: 53777, 116759.2678818981\n",
      "Epoch 111001, Training Loss: 39792, Validation Loss: 55137, 95561.34101855436\n",
      "Epoch 111101, Training Loss: 36826, Validation Loss: 52998, 101016.50608177634\n",
      "Epoch 111201, Training Loss: 38273, Validation Loss: 53277, 113695.15971791213\n",
      "Epoch 111301, Training Loss: 38151, Validation Loss: 54048, 83069.86319041792\n",
      "Epoch 111401, Training Loss: 37256, Validation Loss: 56226, 144813.6222176554\n",
      "Epoch 111501, Training Loss: 39207, Validation Loss: 53029, 114336.50121962996\n",
      "Epoch 111601, Training Loss: 39758, Validation Loss: 54458, 88549.96174152424\n",
      "Epoch 111701, Training Loss: 37767, Validation Loss: 53481, 99495.77034183964\n",
      "Epoch 111801, Training Loss: 36141, Validation Loss: 51666, 111369.77017247747\n",
      "Epoch 111901, Training Loss: 41038, Validation Loss: 53312, 92656.54028372387\n",
      "Epoch 112001, Training Loss: 37067, Validation Loss: 53898, 113100.34806704072\n",
      "Epoch 112101, Training Loss: 36811, Validation Loss: 53856, 102441.5059206052\n",
      "Epoch 112201, Training Loss: 37914, Validation Loss: 53418, 108894.19650893642\n",
      "Epoch 112301, Training Loss: 39232, Validation Loss: 53074, 133465.98233375818\n",
      "Epoch 112401, Training Loss: 37656, Validation Loss: 53469, 100098.69629044361\n",
      "Epoch 112501, Training Loss: 37184, Validation Loss: 53632, 105771.46340555362\n",
      "Epoch 112601, Training Loss: 38892, Validation Loss: 56025, 116085.67081860274\n",
      "Epoch 112701, Training Loss: 40601, Validation Loss: 56041, 112769.06630681113\n",
      "Epoch 112801, Training Loss: 37432, Validation Loss: 53525, 80652.067557177\n",
      "Epoch 112901, Training Loss: 38271, Validation Loss: 53349, 100225.46533877002\n",
      "Epoch 113001, Training Loss: 38713, Validation Loss: 53476, 91817.18324894724\n",
      "Epoch 113101, Training Loss: 40143, Validation Loss: 52839, 100406.40540666059\n",
      "Epoch 113201, Training Loss: 37364, Validation Loss: 53590, 77875.05944328064\n",
      "Epoch 113301, Training Loss: 36016, Validation Loss: 54307, 97280.58976585702\n",
      "Epoch 113401, Training Loss: 37646, Validation Loss: 53997, 71326.15059365203\n",
      "Epoch 113501, Training Loss: 38725, Validation Loss: 53888, 106611.79346586154\n",
      "Epoch 113601, Training Loss: 40323, Validation Loss: 55857, 108305.63574319928\n",
      "Epoch 113701, Training Loss: 38879, Validation Loss: 52805, 82746.33302396713\n",
      "Epoch 113801, Training Loss: 41356, Validation Loss: 54771, 95479.16463643564\n",
      "Epoch 113901, Training Loss: 35980, Validation Loss: 53722, 88714.6004806626\n",
      "Epoch 114001, Training Loss: 38804, Validation Loss: 54722, 94592.41917896428\n",
      "Epoch 114101, Training Loss: 39493, Validation Loss: 54369, 102932.93491990463\n",
      "Epoch 114201, Training Loss: 38466, Validation Loss: 53098, 109281.94401063165\n",
      "Epoch 114301, Training Loss: 39576, Validation Loss: 52547, 109371.24642201136\n",
      "Epoch 114401, Training Loss: 37827, Validation Loss: 54041, 99258.879274485\n",
      "Epoch 114501, Training Loss: 37560, Validation Loss: 52035, 101421.68574869516\n",
      "Epoch 114601, Training Loss: 39699, Validation Loss: 53485, 119322.39264897583\n",
      "Epoch 114701, Training Loss: 40138, Validation Loss: 55584, 101388.99637719295\n",
      "Epoch 114801, Training Loss: 36017, Validation Loss: 54281, 91877.31735973984\n",
      "Epoch 114901, Training Loss: 38721, Validation Loss: 53238, 112458.91753248217\n",
      "Epoch 115001, Training Loss: 38843, Validation Loss: 53828, 92640.82714392336\n",
      "Epoch 115101, Training Loss: 38756, Validation Loss: 53286, 101880.61112432503\n",
      "Epoch 115201, Training Loss: 39343, Validation Loss: 54082, 105659.2299289244\n",
      "Epoch 115301, Training Loss: 40739, Validation Loss: 52539, 119270.91353902865\n",
      "Epoch 115401, Training Loss: 40558, Validation Loss: 53837, 83619.43228085664\n",
      "Epoch 115501, Training Loss: 39229, Validation Loss: 53334, 94363.95849847955\n",
      "Epoch 115601, Training Loss: 42776, Validation Loss: 53786, 101960.85232211942\n",
      "Epoch 115701, Training Loss: 40726, Validation Loss: 52661, 107773.96952757437\n",
      "Epoch 115801, Training Loss: 39677, Validation Loss: 52444, 104572.54328398588\n",
      "Epoch 115901, Training Loss: 35327, Validation Loss: 54939, 89944.17941831671\n",
      "Epoch 116001, Training Loss: 40801, Validation Loss: 54247, 92540.37505146074\n",
      "Epoch 116101, Training Loss: 37941, Validation Loss: 51680, 87625.8195819317\n",
      "Epoch 116201, Training Loss: 38611, Validation Loss: 52933, 129547.4799443999\n",
      "Epoch 116301, Training Loss: 37592, Validation Loss: 53210, 84806.78747966577\n",
      "Epoch 116401, Training Loss: 36372, Validation Loss: 54564, 100760.85732547409\n",
      "Epoch 116501, Training Loss: 36477, Validation Loss: 54542, 75936.67804611176\n",
      "Epoch 116601, Training Loss: 37401, Validation Loss: 53926, 101333.14793824968\n",
      "Epoch 116701, Training Loss: 38258, Validation Loss: 53148, 112813.48523323594\n",
      "Epoch 116801, Training Loss: 38379, Validation Loss: 55406, 111331.72441482967\n",
      "Epoch 116901, Training Loss: 40195, Validation Loss: 53568, 93396.39436291969\n",
      "Epoch 117001, Training Loss: 39500, Validation Loss: 54821, 87466.24701067181\n",
      "Epoch 117101, Training Loss: 39104, Validation Loss: 52088, 95010.57404405404\n",
      "Epoch 117201, Training Loss: 37252, Validation Loss: 53284, 126750.31967910774\n",
      "Epoch 117301, Training Loss: 38412, Validation Loss: 53882, 96326.18339501349\n",
      "Epoch 117401, Training Loss: 40125, Validation Loss: 54672, 100107.12618569184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117501, Training Loss: 38988, Validation Loss: 52404, 80412.35244129311\n",
      "Epoch 117601, Training Loss: 38934, Validation Loss: 53369, 106120.49942123133\n",
      "Epoch 117701, Training Loss: 36178, Validation Loss: 53441, 95572.58541198832\n",
      "Epoch 117801, Training Loss: 39145, Validation Loss: 54211, 87724.36525103996\n",
      "Epoch 117901, Training Loss: 36186, Validation Loss: 52237, 94611.89433923566\n",
      "Epoch 118001, Training Loss: 38142, Validation Loss: 52529, 172245.2781681945\n",
      "Epoch 118101, Training Loss: 37674, Validation Loss: 53298, 129504.79289040838\n",
      "Epoch 118201, Training Loss: 36365, Validation Loss: 52539, 113320.04110223982\n",
      "Epoch 118301, Training Loss: 39646, Validation Loss: 54350, 120728.21881818846\n",
      "Epoch 118401, Training Loss: 35951, Validation Loss: 53389, 93033.75742539433\n",
      "Epoch 118501, Training Loss: 38291, Validation Loss: 55767, 98254.67073094299\n",
      "Epoch 118601, Training Loss: 39485, Validation Loss: 55155, 115765.85335016577\n",
      "Epoch 118701, Training Loss: 37276, Validation Loss: 51938, 101428.82210178177\n",
      "Epoch 118801, Training Loss: 37683, Validation Loss: 52513, 115710.36947719693\n",
      "Epoch 118901, Training Loss: 37009, Validation Loss: 53169, 85266.68204062324\n",
      "Epoch 119001, Training Loss: 39885, Validation Loss: 55766, 97514.11928653713\n",
      "Epoch 119101, Training Loss: 39667, Validation Loss: 53306, 112972.13925604826\n",
      "Epoch 119201, Training Loss: 38197, Validation Loss: 54560, 107221.48471795431\n",
      "Epoch 119301, Training Loss: 38565, Validation Loss: 55296, 111239.85426154519\n",
      "Epoch 119401, Training Loss: 37312, Validation Loss: 54035, 116393.27039014133\n",
      "Epoch 119501, Training Loss: 36390, Validation Loss: 54298, 110259.14590573085\n",
      "Epoch 119601, Training Loss: 37521, Validation Loss: 53132, 123753.58534446468\n",
      "Epoch 119701, Training Loss: 35750, Validation Loss: 55625, 114399.769236515\n",
      "Epoch 119801, Training Loss: 38033, Validation Loss: 53585, 114434.19579051982\n",
      "Epoch 119901, Training Loss: 39049, Validation Loss: 54627, 84341.74112724862\n",
      "Epoch 120001, Training Loss: 39960, Validation Loss: 54010, 92645.09506865614\n",
      "Epoch 120101, Training Loss: 40259, Validation Loss: 55482, 116729.48054251658\n",
      "Epoch 120201, Training Loss: 38197, Validation Loss: 55295, 83989.77815418543\n",
      "Epoch 120301, Training Loss: 38627, Validation Loss: 52338, 135978.55937909623\n",
      "Epoch 120401, Training Loss: 38210, Validation Loss: 53151, 78539.71382408807\n",
      "Epoch 120501, Training Loss: 39221, Validation Loss: 55130, 102537.63579050441\n",
      "Epoch 120601, Training Loss: 37414, Validation Loss: 54421, 99171.62803995032\n",
      "Epoch 120701, Training Loss: 38836, Validation Loss: 54901, 98799.0311403039\n",
      "Epoch 120801, Training Loss: 39439, Validation Loss: 53871, 92527.29950405925\n",
      "Epoch 120901, Training Loss: 38220, Validation Loss: 53642, 94497.70039904137\n",
      "Epoch 121001, Training Loss: 37544, Validation Loss: 55348, 138047.349207033\n",
      "Epoch 121101, Training Loss: 36614, Validation Loss: 54016, 96944.7374914431\n",
      "Epoch 121201, Training Loss: 38668, Validation Loss: 54355, 111241.76836206797\n",
      "Epoch 121301, Training Loss: 36878, Validation Loss: 54619, 105128.31459876285\n",
      "Epoch 121401, Training Loss: 38775, Validation Loss: 53410, 87670.86618696085\n",
      "Epoch 121501, Training Loss: 37883, Validation Loss: 52785, 96562.57600446785\n",
      "Epoch 121601, Training Loss: 38473, Validation Loss: 55524, 81659.40998788588\n",
      "Epoch 121701, Training Loss: 37387, Validation Loss: 53456, 112859.3530582352\n",
      "Epoch 121801, Training Loss: 41174, Validation Loss: 55010, 160118.47067373854\n",
      "Epoch 121901, Training Loss: 38887, Validation Loss: 53486, 99306.73675427627\n",
      "Epoch 122001, Training Loss: 40936, Validation Loss: 53593, 110999.9892343583\n",
      "Epoch 122101, Training Loss: 37203, Validation Loss: 54478, 113073.25962499122\n",
      "Epoch 122201, Training Loss: 39468, Validation Loss: 54246, 111274.47820054342\n",
      "Epoch 122301, Training Loss: 38758, Validation Loss: 53033, 113135.58277089459\n",
      "Epoch 122401, Training Loss: 38865, Validation Loss: 53147, 99716.3668516788\n",
      "Epoch 122501, Training Loss: 38858, Validation Loss: 52427, 115057.97150313582\n",
      "Epoch 122601, Training Loss: 38930, Validation Loss: 53889, 119754.33964998244\n",
      "Epoch 122701, Training Loss: 36798, Validation Loss: 53178, 87077.0681195587\n",
      "Epoch 122801, Training Loss: 36149, Validation Loss: 54158, 94220.54331304943\n",
      "Epoch 122901, Training Loss: 37787, Validation Loss: 54650, 93283.64825164368\n",
      "Epoch 123001, Training Loss: 37924, Validation Loss: 52902, 94325.67547684273\n",
      "Epoch 123101, Training Loss: 37123, Validation Loss: 54604, 95907.47276551498\n",
      "Epoch 123201, Training Loss: 39181, Validation Loss: 53544, 100111.05456043214\n",
      "Epoch 123301, Training Loss: 37326, Validation Loss: 53969, 91382.23524742427\n",
      "Epoch 123401, Training Loss: 35331, Validation Loss: 54109, 95301.4548778577\n",
      "Epoch 123501, Training Loss: 38990, Validation Loss: 55198, 151565.43550625953\n",
      "Epoch 123601, Training Loss: 41389, Validation Loss: 52737, 117229.29928039426\n",
      "Epoch 123701, Training Loss: 39699, Validation Loss: 53912, 97684.28215655025\n",
      "Epoch 123801, Training Loss: 35790, Validation Loss: 53146, 107233.66596388219\n",
      "Epoch 123901, Training Loss: 39736, Validation Loss: 53076, 106184.27385096673\n",
      "Epoch 124001, Training Loss: 36035, Validation Loss: 52563, 115557.94992385805\n",
      "Epoch 124101, Training Loss: 37258, Validation Loss: 54698, 93342.66268809786\n",
      "Epoch 124201, Training Loss: 37679, Validation Loss: 53488, 95237.62806670187\n",
      "Epoch 124301, Training Loss: 36680, Validation Loss: 55066, 91900.38504714564\n",
      "Epoch 124401, Training Loss: 40397, Validation Loss: 54589, 104402.55279208087\n",
      "Epoch 124501, Training Loss: 37137, Validation Loss: 53625, 103094.69481671171\n",
      "Epoch 124601, Training Loss: 38920, Validation Loss: 53278, 101183.07027203463\n",
      "Epoch 124701, Training Loss: 36730, Validation Loss: 53493, 82668.98038175494\n",
      "Epoch 124801, Training Loss: 33930, Validation Loss: 53817, 95791.03185842877\n",
      "Epoch 124901, Training Loss: 36099, Validation Loss: 53479, 129960.30998858763\n",
      "Epoch 125001, Training Loss: 37945, Validation Loss: 52975, 122575.47836490208\n",
      "Epoch 125101, Training Loss: 37313, Validation Loss: 55858, 118523.0794256435\n",
      "Epoch 125201, Training Loss: 38211, Validation Loss: 53897, 106529.4683763192\n",
      "Epoch 125301, Training Loss: 35598, Validation Loss: 53412, 93498.13764526496\n",
      "Epoch 125401, Training Loss: 38293, Validation Loss: 53721, 108130.66993506886\n",
      "Epoch 125501, Training Loss: 39017, Validation Loss: 52705, 92125.82965624538\n",
      "Epoch 125601, Training Loss: 41777, Validation Loss: 52706, 188330.27597089473\n",
      "Epoch 125701, Training Loss: 35903, Validation Loss: 55107, 88750.46314604\n",
      "Epoch 125801, Training Loss: 34494, Validation Loss: 53620, 108229.50711724126\n",
      "Epoch 125901, Training Loss: 39965, Validation Loss: 53751, 103760.85799643603\n",
      "Epoch 126001, Training Loss: 36647, Validation Loss: 52710, 101323.93007643307\n",
      "Epoch 126101, Training Loss: 36862, Validation Loss: 54713, 127972.40809604239\n",
      "Epoch 126201, Training Loss: 36834, Validation Loss: 53455, 106316.30993788852\n",
      "Epoch 126301, Training Loss: 38773, Validation Loss: 55003, 101671.56069152057\n",
      "Epoch 126401, Training Loss: 38968, Validation Loss: 52155, 84000.39871495537\n",
      "Epoch 126501, Training Loss: 40419, Validation Loss: 54129, 128735.16219548433\n",
      "Epoch 126601, Training Loss: 35314, Validation Loss: 51669, 93865.4411605129\n",
      "Epoch 126701, Training Loss: 36088, Validation Loss: 54117, 105540.1637921052\n",
      "Epoch 126801, Training Loss: 37920, Validation Loss: 53355, 137410.4322621672\n",
      "Epoch 126901, Training Loss: 36500, Validation Loss: 55846, 113013.83005560939\n",
      "Epoch 127001, Training Loss: 40060, Validation Loss: 53824, 119656.81866734335\n",
      "Epoch 127101, Training Loss: 35761, Validation Loss: 54404, 95618.88463334514\n",
      "Epoch 127201, Training Loss: 40862, Validation Loss: 53773, 93451.83976419856\n",
      "Epoch 127301, Training Loss: 38087, Validation Loss: 54016, 136066.68149059336\n",
      "Epoch 127401, Training Loss: 37770, Validation Loss: 53885, 105572.82203415317\n",
      "Epoch 127501, Training Loss: 38098, Validation Loss: 53334, 106543.47212524351\n",
      "Epoch 127601, Training Loss: 38329, Validation Loss: 56024, 138828.05282867103\n",
      "Epoch 127701, Training Loss: 36005, Validation Loss: 53424, 119437.50535354162\n",
      "Epoch 127801, Training Loss: 37233, Validation Loss: 53926, 110761.3354211249\n",
      "Epoch 127901, Training Loss: 41913, Validation Loss: 53558, 100635.37527549434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128001, Training Loss: 37107, Validation Loss: 51816, 100210.42746012418\n",
      "Epoch 128101, Training Loss: 35905, Validation Loss: 54830, 104278.64105853764\n",
      "Epoch 128201, Training Loss: 37475, Validation Loss: 53426, 85606.45465668326\n",
      "Epoch 128301, Training Loss: 38681, Validation Loss: 54329, 108614.37235534005\n",
      "Epoch 128401, Training Loss: 35112, Validation Loss: 52867, 135438.2120479813\n",
      "Epoch 128501, Training Loss: 40985, Validation Loss: 54835, 125937.49510410329\n",
      "Epoch 128601, Training Loss: 38136, Validation Loss: 53460, 95263.93816949947\n",
      "Epoch 128701, Training Loss: 36310, Validation Loss: 53096, 98746.37248373406\n",
      "Epoch 128801, Training Loss: 39423, Validation Loss: 53516, 119053.37359465046\n",
      "Epoch 128901, Training Loss: 37946, Validation Loss: 54382, 124013.11981277836\n",
      "Epoch 129001, Training Loss: 40369, Validation Loss: 52581, 100774.92963329602\n",
      "Epoch 129101, Training Loss: 38044, Validation Loss: 53744, 98964.64692574793\n",
      "Epoch 129201, Training Loss: 38246, Validation Loss: 53184, 164336.51201487696\n",
      "Epoch 129301, Training Loss: 38228, Validation Loss: 56053, 95596.0530676893\n",
      "Epoch 129401, Training Loss: 37523, Validation Loss: 52912, 96746.02938292478\n",
      "Epoch 129501, Training Loss: 38835, Validation Loss: 52410, 108683.66256006045\n",
      "Epoch 129601, Training Loss: 36936, Validation Loss: 52408, 88997.48167014786\n",
      "Epoch 129701, Training Loss: 36859, Validation Loss: 54663, 99379.84234157449\n",
      "Epoch 129801, Training Loss: 39122, Validation Loss: 53798, 82948.44235984246\n",
      "Epoch 129901, Training Loss: 36128, Validation Loss: 55182, 120528.0225324713\n",
      "Epoch 130001, Training Loss: 34794, Validation Loss: 53024, 99166.23285204993\n",
      "Epoch 130101, Training Loss: 37881, Validation Loss: 54472, 151054.52613478006\n",
      "Epoch 130201, Training Loss: 39068, Validation Loss: 53865, 81462.19098407292\n",
      "Epoch 130301, Training Loss: 36597, Validation Loss: 53288, 80277.17241041221\n",
      "Epoch 130401, Training Loss: 37566, Validation Loss: 54259, 106270.50806296176\n",
      "Epoch 130501, Training Loss: 38847, Validation Loss: 53829, 109242.42765581126\n",
      "Epoch 130601, Training Loss: 38857, Validation Loss: 53412, 119260.80034429021\n",
      "Epoch 130701, Training Loss: 39277, Validation Loss: 54378, 137898.15737936806\n",
      "Epoch 130801, Training Loss: 37097, Validation Loss: 54983, 94948.79013395414\n",
      "Epoch 130901, Training Loss: 37290, Validation Loss: 54643, 120581.45130678902\n",
      "Epoch 131001, Training Loss: 36184, Validation Loss: 53869, 134523.92691154472\n",
      "Epoch 131101, Training Loss: 39592, Validation Loss: 53132, 101417.98042505588\n",
      "Epoch 131201, Training Loss: 36232, Validation Loss: 53352, 99722.09715013945\n",
      "Epoch 131301, Training Loss: 38220, Validation Loss: 54437, 118756.78744965151\n",
      "Epoch 131401, Training Loss: 39160, Validation Loss: 52475, 122801.74188141136\n",
      "Epoch 131501, Training Loss: 37634, Validation Loss: 52897, 107885.03136696779\n",
      "Epoch 131601, Training Loss: 36742, Validation Loss: 54642, 97156.16649400954\n",
      "Epoch 131701, Training Loss: 37354, Validation Loss: 55346, 142014.15912893938\n",
      "Epoch 131801, Training Loss: 36301, Validation Loss: 53338, 116935.49803044552\n",
      "Epoch 131901, Training Loss: 36626, Validation Loss: 53833, 99212.82643829238\n",
      "Epoch 132001, Training Loss: 37402, Validation Loss: 52705, 119268.44017681398\n",
      "Epoch 132101, Training Loss: 36496, Validation Loss: 52835, 103236.405472484\n",
      "Epoch 132201, Training Loss: 32716, Validation Loss: 55465, 102333.83204836877\n",
      "Epoch 132301, Training Loss: 38624, Validation Loss: 52760, 98513.47502897498\n",
      "Epoch 132401, Training Loss: 38126, Validation Loss: 54632, 92254.08981775546\n",
      "Epoch 132501, Training Loss: 39946, Validation Loss: 53926, 107719.58195423258\n",
      "Epoch 132601, Training Loss: 37168, Validation Loss: 53660, 85250.46009178982\n",
      "Epoch 132701, Training Loss: 37749, Validation Loss: 55750, 105189.76843936421\n",
      "Epoch 132801, Training Loss: 37484, Validation Loss: 54241, 108352.76994044706\n",
      "Epoch 132901, Training Loss: 37757, Validation Loss: 52664, 138821.39859424465\n",
      "Epoch 133001, Training Loss: 36897, Validation Loss: 53097, 130575.99293645023\n",
      "Epoch 133101, Training Loss: 40059, Validation Loss: 53753, 88458.16563561054\n",
      "Epoch 133201, Training Loss: 38446, Validation Loss: 55927, 95935.19723599554\n",
      "Epoch 133301, Training Loss: 38532, Validation Loss: 53238, 98474.10011289567\n",
      "Epoch 133401, Training Loss: 38483, Validation Loss: 55326, 109201.88520323462\n",
      "Epoch 133501, Training Loss: 36744, Validation Loss: 56495, 93295.36873978838\n",
      "Epoch 133601, Training Loss: 38783, Validation Loss: 52916, 126756.295623281\n",
      "Epoch 133701, Training Loss: 37286, Validation Loss: 53563, 97460.67047503607\n",
      "Epoch 133801, Training Loss: 40540, Validation Loss: 55799, 136924.38133987456\n",
      "Epoch 133901, Training Loss: 36994, Validation Loss: 57382, 107704.96041459874\n",
      "Epoch 134001, Training Loss: 37549, Validation Loss: 53797, 106860.49383038843\n",
      "Epoch 134101, Training Loss: 37473, Validation Loss: 55110, 143764.8842239251\n",
      "Epoch 134201, Training Loss: 36195, Validation Loss: 53109, 150464.4182615639\n",
      "Epoch 134301, Training Loss: 37847, Validation Loss: 54154, 101512.97141747472\n",
      "Epoch 134401, Training Loss: 35853, Validation Loss: 53916, 118537.76170178666\n",
      "Epoch 134501, Training Loss: 41328, Validation Loss: 53111, 93756.7127718858\n",
      "Epoch 134601, Training Loss: 38754, Validation Loss: 53207, 101769.28398109558\n",
      "Epoch 134701, Training Loss: 37137, Validation Loss: 53682, 104786.42453276868\n",
      "Epoch 134801, Training Loss: 37119, Validation Loss: 55160, 110737.29259612353\n",
      "Epoch 134901, Training Loss: 36329, Validation Loss: 54108, 117860.98459633987\n",
      "Epoch 135001, Training Loss: 38812, Validation Loss: 55424, 93019.42232390044\n",
      "Epoch 135101, Training Loss: 36227, Validation Loss: 52531, 112243.42991582742\n",
      "Epoch 135201, Training Loss: 42063, Validation Loss: 55321, 120932.0175599443\n",
      "Epoch 135301, Training Loss: 40119, Validation Loss: 54531, 98074.42585099798\n",
      "Epoch 135401, Training Loss: 38622, Validation Loss: 53214, 146951.57844558163\n",
      "Epoch 135501, Training Loss: 37893, Validation Loss: 54902, 132898.51412850656\n",
      "Epoch 135601, Training Loss: 35966, Validation Loss: 56226, 102342.62499007351\n",
      "Epoch 135701, Training Loss: 40300, Validation Loss: 54604, 98527.87471204805\n",
      "Epoch 135801, Training Loss: 37579, Validation Loss: 53913, 111704.98093988508\n",
      "Epoch 135901, Training Loss: 39149, Validation Loss: 53089, 115069.98769899651\n",
      "Epoch 136001, Training Loss: 37392, Validation Loss: 54195, 107113.89828334934\n",
      "Epoch 136101, Training Loss: 37895, Validation Loss: 53275, 110697.9398859684\n",
      "Epoch 136201, Training Loss: 39610, Validation Loss: 53631, 120443.30100387841\n",
      "Epoch 136301, Training Loss: 40380, Validation Loss: 54039, 112339.68484613839\n",
      "Epoch 136401, Training Loss: 39911, Validation Loss: 52218, 124171.46688265372\n",
      "Epoch 136501, Training Loss: 36868, Validation Loss: 55432, 140868.71240863958\n",
      "Epoch 136601, Training Loss: 37893, Validation Loss: 54093, 116031.20170461935\n",
      "Epoch 136701, Training Loss: 39019, Validation Loss: 53496, 132492.4929436945\n",
      "Epoch 136801, Training Loss: 36096, Validation Loss: 52639, 85285.15791716329\n",
      "Epoch 136901, Training Loss: 38245, Validation Loss: 53345, 111533.04239689601\n",
      "Epoch 137001, Training Loss: 39291, Validation Loss: 53053, 94920.92399183493\n",
      "Epoch 137101, Training Loss: 37526, Validation Loss: 53575, 104945.54424860752\n",
      "Epoch 137201, Training Loss: 38248, Validation Loss: 53472, 101936.97139781807\n",
      "Epoch 137301, Training Loss: 34538, Validation Loss: 55095, 113706.88563100738\n",
      "Epoch 137401, Training Loss: 37557, Validation Loss: 53568, 92847.7265948727\n",
      "Epoch 137501, Training Loss: 37882, Validation Loss: 54413, 149057.0405962652\n",
      "Epoch 137601, Training Loss: 37150, Validation Loss: 53289, 137206.30308504592\n",
      "Epoch 137701, Training Loss: 38839, Validation Loss: 53929, 118021.88844360512\n",
      "Epoch 137801, Training Loss: 40543, Validation Loss: 53035, 162624.61082153025\n",
      "Epoch 137901, Training Loss: 36211, Validation Loss: 54594, 109949.87110700132\n",
      "Epoch 138001, Training Loss: 36840, Validation Loss: 53919, 102723.71581522899\n",
      "Epoch 138101, Training Loss: 40464, Validation Loss: 54663, 110527.3836613188\n",
      "Epoch 138201, Training Loss: 35664, Validation Loss: 54158, 98537.14276234119\n",
      "Epoch 138301, Training Loss: 37888, Validation Loss: 52910, 115293.41959000501\n",
      "Epoch 138401, Training Loss: 37015, Validation Loss: 55858, 101328.04367032257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138501, Training Loss: 38745, Validation Loss: 52887, 120817.77180575735\n",
      "Epoch 138601, Training Loss: 38793, Validation Loss: 53291, 130823.69736607482\n",
      "Epoch 138701, Training Loss: 37371, Validation Loss: 52803, 117020.09633721726\n",
      "Epoch 138801, Training Loss: 37898, Validation Loss: 54506, 120710.74859981175\n",
      "Epoch 138901, Training Loss: 36448, Validation Loss: 52312, 89013.60611953413\n",
      "Epoch 139001, Training Loss: 38200, Validation Loss: 55836, 118026.51729157967\n",
      "Epoch 139101, Training Loss: 36121, Validation Loss: 53034, 106905.17715617863\n",
      "Epoch 139201, Training Loss: 37137, Validation Loss: 52511, 101862.71511074564\n",
      "Epoch 139301, Training Loss: 37143, Validation Loss: 53000, 97019.31283549815\n",
      "Epoch 139401, Training Loss: 34335, Validation Loss: 55195, 89647.22898208474\n",
      "Epoch 139501, Training Loss: 40277, Validation Loss: 54073, 138764.35728499325\n",
      "Epoch 139601, Training Loss: 37604, Validation Loss: 54876, 119062.32539648446\n",
      "Epoch 139701, Training Loss: 39581, Validation Loss: 52839, 122878.72621143032\n",
      "Epoch 139801, Training Loss: 36125, Validation Loss: 54306, 85987.83207198964\n",
      "Epoch 139901, Training Loss: 43852, Validation Loss: 53834, 126304.84032821412\n",
      "Epoch 140001, Training Loss: 37911, Validation Loss: 53480, 142151.42091097598\n",
      "Epoch 140101, Training Loss: 37749, Validation Loss: 54793, 94424.3835977423\n",
      "Epoch 140201, Training Loss: 38964, Validation Loss: 53748, 121346.7253396004\n",
      "Epoch 140301, Training Loss: 38230, Validation Loss: 55001, 106425.75166906741\n",
      "Epoch 140401, Training Loss: 40320, Validation Loss: 52227, 87760.73108218116\n",
      "Epoch 140501, Training Loss: 37606, Validation Loss: 53086, 116969.75017889396\n",
      "Epoch 140601, Training Loss: 35552, Validation Loss: 53297, 135774.08624388537\n",
      "Epoch 140701, Training Loss: 36941, Validation Loss: 54181, 130515.89739961685\n",
      "Epoch 140801, Training Loss: 39922, Validation Loss: 53189, 99114.2482097734\n",
      "Epoch 140901, Training Loss: 37881, Validation Loss: 54468, 102161.25945921498\n",
      "Epoch 141001, Training Loss: 37525, Validation Loss: 54115, 94676.24197116397\n",
      "Epoch 141101, Training Loss: 37586, Validation Loss: 52985, 85658.23266377278\n",
      "Epoch 141201, Training Loss: 34651, Validation Loss: 54150, 124606.84941363837\n",
      "Epoch 141301, Training Loss: 36321, Validation Loss: 53563, 115529.30951339523\n",
      "Epoch 141401, Training Loss: 38894, Validation Loss: 54824, 88640.41446839517\n",
      "Epoch 141501, Training Loss: 38797, Validation Loss: 54998, 115157.87133985572\n",
      "Epoch 141601, Training Loss: 40430, Validation Loss: 53700, 156245.98343743046\n",
      "Epoch 141701, Training Loss: 36884, Validation Loss: 53529, 101826.85539077564\n",
      "Epoch 141801, Training Loss: 38478, Validation Loss: 54393, 121191.85169530466\n",
      "Epoch 141901, Training Loss: 36506, Validation Loss: 53412, 136667.38481382196\n",
      "Epoch 142001, Training Loss: 37984, Validation Loss: 52764, 98706.79595307971\n",
      "Epoch 142101, Training Loss: 39258, Validation Loss: 54158, 89091.64398842164\n",
      "Epoch 142201, Training Loss: 39539, Validation Loss: 54224, 128325.14912765347\n",
      "Epoch 142301, Training Loss: 37906, Validation Loss: 53346, 132671.39545064795\n",
      "Epoch 142401, Training Loss: 38578, Validation Loss: 53735, 129496.37448014553\n",
      "Epoch 142501, Training Loss: 37462, Validation Loss: 53078, 114837.56767386889\n",
      "Epoch 142601, Training Loss: 36860, Validation Loss: 55081, 139854.2128589088\n",
      "Epoch 142701, Training Loss: 37065, Validation Loss: 53907, 114753.46052283233\n",
      "Epoch 142801, Training Loss: 37063, Validation Loss: 54699, 113864.77836744099\n",
      "Epoch 142901, Training Loss: 39018, Validation Loss: 53805, 107829.19224031219\n",
      "Epoch 143001, Training Loss: 39095, Validation Loss: 54184, 94595.42024410954\n",
      "Epoch 143101, Training Loss: 36973, Validation Loss: 53240, 104620.19494670404\n",
      "Epoch 143201, Training Loss: 40093, Validation Loss: 53867, 112596.36378945604\n",
      "Epoch 143301, Training Loss: 35548, Validation Loss: 54298, 103577.52921648702\n",
      "Epoch 143401, Training Loss: 41921, Validation Loss: 55384, 128227.8469952583\n",
      "Epoch 143501, Training Loss: 35238, Validation Loss: 53945, 119753.88055425724\n",
      "Epoch 143601, Training Loss: 39680, Validation Loss: 53508, 129805.4157430021\n",
      "Epoch 143701, Training Loss: 34340, Validation Loss: 54730, 98549.4777487911\n",
      "Epoch 143801, Training Loss: 38023, Validation Loss: 54593, 118777.58169405117\n",
      "Epoch 143901, Training Loss: 37768, Validation Loss: 53445, 145217.7853878391\n",
      "Epoch 144001, Training Loss: 37692, Validation Loss: 55039, 91921.87546696262\n",
      "Epoch 144101, Training Loss: 39155, Validation Loss: 54522, 97233.57630344394\n",
      "Epoch 144201, Training Loss: 36970, Validation Loss: 53961, 166994.4636380784\n",
      "Epoch 144301, Training Loss: 38789, Validation Loss: 53048, 162086.3731712616\n",
      "Epoch 144401, Training Loss: 38247, Validation Loss: 55414, 109077.22767288447\n",
      "Epoch 144501, Training Loss: 36931, Validation Loss: 54301, 109113.2559533586\n",
      "Epoch 144601, Training Loss: 35816, Validation Loss: 53977, 77554.64024464549\n",
      "Epoch 144701, Training Loss: 37985, Validation Loss: 55577, 95376.65887718584\n",
      "Epoch 144801, Training Loss: 39024, Validation Loss: 56052, 107536.23750296513\n",
      "Epoch 144901, Training Loss: 37916, Validation Loss: 53995, 91186.03949434788\n",
      "Epoch 145001, Training Loss: 38268, Validation Loss: 54835, 106194.52065882746\n",
      "Epoch 145101, Training Loss: 38254, Validation Loss: 53784, 127043.05508720032\n",
      "Epoch 145201, Training Loss: 34694, Validation Loss: 55899, 139576.2426977721\n",
      "Epoch 145301, Training Loss: 40315, Validation Loss: 55979, 111964.20668884383\n",
      "Epoch 145401, Training Loss: 37238, Validation Loss: 54405, 120281.55153737108\n",
      "Epoch 145501, Training Loss: 36426, Validation Loss: 54398, 104711.91001491633\n",
      "Epoch 145601, Training Loss: 36589, Validation Loss: 55754, 90627.11629100352\n",
      "Epoch 145701, Training Loss: 34841, Validation Loss: 54582, 197645.84602916404\n",
      "Epoch 145801, Training Loss: 38372, Validation Loss: 51928, 109113.21604372138\n",
      "Epoch 145901, Training Loss: 35758, Validation Loss: 53454, 115155.81358210435\n",
      "Epoch 146001, Training Loss: 36903, Validation Loss: 52767, 116301.28752611035\n",
      "Epoch 146101, Training Loss: 37876, Validation Loss: 52897, 150850.69144527602\n",
      "Epoch 146201, Training Loss: 37494, Validation Loss: 53926, 106262.85304159169\n",
      "Epoch 146301, Training Loss: 39236, Validation Loss: 54293, 132034.34619134592\n",
      "Epoch 146401, Training Loss: 38236, Validation Loss: 54594, 99527.39019546285\n",
      "Epoch 146501, Training Loss: 38652, Validation Loss: 54351, 119083.12266114145\n",
      "Epoch 146601, Training Loss: 38127, Validation Loss: 54347, 117277.40567485685\n",
      "Epoch 146701, Training Loss: 36346, Validation Loss: 54168, 114271.21041383124\n",
      "Epoch 146801, Training Loss: 37560, Validation Loss: 54442, 134739.3329328669\n",
      "Epoch 146901, Training Loss: 39662, Validation Loss: 53219, 118827.34882167353\n",
      "Epoch 147001, Training Loss: 38245, Validation Loss: 52565, 141694.09898523742\n",
      "Epoch 147101, Training Loss: 34553, Validation Loss: 53248, 86231.85115021998\n",
      "Epoch 147201, Training Loss: 38540, Validation Loss: 54444, 104015.1139488344\n",
      "Epoch 147301, Training Loss: 39408, Validation Loss: 53791, 106427.8480779121\n",
      "Epoch 147401, Training Loss: 37541, Validation Loss: 53519, 97867.59504012996\n",
      "Epoch 147501, Training Loss: 37445, Validation Loss: 52924, 114647.84624445741\n",
      "Epoch 147601, Training Loss: 38661, Validation Loss: 52793, 128640.08047944878\n",
      "Epoch 147701, Training Loss: 36403, Validation Loss: 54355, 148980.902917222\n",
      "Epoch 147801, Training Loss: 36346, Validation Loss: 55538, 131448.60213298243\n",
      "Epoch 147901, Training Loss: 35650, Validation Loss: 52406, 101882.00231785171\n",
      "Epoch 148001, Training Loss: 36490, Validation Loss: 53833, 119241.38704815146\n",
      "Epoch 148101, Training Loss: 37735, Validation Loss: 54804, 94229.76276639533\n",
      "Epoch 148201, Training Loss: 36808, Validation Loss: 52736, 110166.07330251897\n",
      "Epoch 148301, Training Loss: 35589, Validation Loss: 54140, 114475.34441360163\n",
      "Epoch 148401, Training Loss: 42503, Validation Loss: 54668, 141342.19156519714\n",
      "Epoch 148501, Training Loss: 39071, Validation Loss: 53826, 132071.38650403204\n",
      "Epoch 148601, Training Loss: 38691, Validation Loss: 53339, 127167.79454105422\n",
      "Epoch 148701, Training Loss: 38387, Validation Loss: 54762, 135667.64446827958\n",
      "Epoch 148801, Training Loss: 38052, Validation Loss: 55356, 95513.99968100041\n",
      "Epoch 148901, Training Loss: 37770, Validation Loss: 55011, 111777.07726841552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149001, Training Loss: 39089, Validation Loss: 53756, 110642.14481705119\n",
      "Epoch 149101, Training Loss: 38196, Validation Loss: 53909, 89430.22877896282\n",
      "Epoch 149201, Training Loss: 38029, Validation Loss: 52928, 138554.01034764794\n",
      "Epoch 149301, Training Loss: 38061, Validation Loss: 54427, 129623.41015815659\n",
      "Epoch 149401, Training Loss: 37633, Validation Loss: 56263, 109458.90525714704\n",
      "Epoch 149501, Training Loss: 37088, Validation Loss: 54785, 103859.58706757468\n",
      "Epoch 149601, Training Loss: 39007, Validation Loss: 54803, 116718.44013327984\n",
      "Epoch 149701, Training Loss: 39382, Validation Loss: 56057, 111001.11950383114\n",
      "Epoch 149801, Training Loss: 37567, Validation Loss: 54797, 116616.94931840262\n",
      "Epoch 149901, Training Loss: 34317, Validation Loss: 54371, 144375.70553642046\n",
      "Epoch 150001, Training Loss: 38228, Validation Loss: 54436, 123422.24720057416\n",
      "Epoch 150101, Training Loss: 38935, Validation Loss: 55041, 103839.70777849405\n",
      "Epoch 150201, Training Loss: 38148, Validation Loss: 52790, 97598.3894605741\n",
      "Epoch 150301, Training Loss: 36167, Validation Loss: 53289, 118711.80111953004\n",
      "Epoch 150401, Training Loss: 36879, Validation Loss: 54251, 116669.1549757708\n",
      "Epoch 150501, Training Loss: 36798, Validation Loss: 53096, 114726.86476023392\n",
      "Epoch 150601, Training Loss: 37967, Validation Loss: 54192, 97048.69054846623\n",
      "Epoch 150701, Training Loss: 37257, Validation Loss: 54507, 94320.13277921751\n",
      "Epoch 150801, Training Loss: 36343, Validation Loss: 54707, 147421.64944983166\n",
      "Epoch 150901, Training Loss: 37343, Validation Loss: 53284, 130432.55009122899\n",
      "Epoch 151001, Training Loss: 36880, Validation Loss: 53580, 101998.03578309967\n",
      "Epoch 151101, Training Loss: 39430, Validation Loss: 54489, 122754.81580130717\n",
      "Epoch 151201, Training Loss: 38284, Validation Loss: 54411, 105874.6037300824\n",
      "Epoch 151301, Training Loss: 36732, Validation Loss: 53895, 111885.3706057552\n",
      "Epoch 151401, Training Loss: 37931, Validation Loss: 52819, 112489.72488628405\n",
      "Epoch 151501, Training Loss: 37574, Validation Loss: 52754, 118055.58684883856\n",
      "Epoch 151601, Training Loss: 37050, Validation Loss: 54215, 96691.79596331553\n",
      "Epoch 151701, Training Loss: 35768, Validation Loss: 53710, 108045.99925585953\n",
      "Epoch 151801, Training Loss: 38371, Validation Loss: 52874, 114183.81617141185\n",
      "Epoch 151901, Training Loss: 37647, Validation Loss: 52797, 121914.84094938362\n",
      "Epoch 152001, Training Loss: 36543, Validation Loss: 53125, 120012.30824899809\n",
      "Epoch 152101, Training Loss: 37914, Validation Loss: 53843, 128642.38856646857\n",
      "Epoch 152201, Training Loss: 38813, Validation Loss: 53161, 128063.35298229876\n",
      "Epoch 152301, Training Loss: 38707, Validation Loss: 54075, 103012.7981880771\n",
      "Epoch 152401, Training Loss: 34678, Validation Loss: 53538, 102242.59661659373\n",
      "Epoch 152501, Training Loss: 37229, Validation Loss: 55776, 126533.57137480425\n",
      "Epoch 152601, Training Loss: 38053, Validation Loss: 53669, 138040.19246427264\n",
      "Epoch 152701, Training Loss: 36674, Validation Loss: 53200, 139826.2183658073\n",
      "Epoch 152801, Training Loss: 35441, Validation Loss: 52700, 102680.35616593517\n",
      "Epoch 152901, Training Loss: 36719, Validation Loss: 55258, 167480.96930696064\n",
      "Epoch 153001, Training Loss: 39486, Validation Loss: 53578, 134871.44030600376\n",
      "Epoch 153101, Training Loss: 40805, Validation Loss: 54433, 138817.4134566743\n",
      "Epoch 153201, Training Loss: 37281, Validation Loss: 54362, 101282.64668668139\n",
      "Epoch 153301, Training Loss: 37717, Validation Loss: 53839, 164581.646324627\n",
      "Epoch 153401, Training Loss: 36414, Validation Loss: 54492, 100782.99682167766\n",
      "Epoch 153501, Training Loss: 36125, Validation Loss: 54905, 130897.31319501495\n",
      "Epoch 153601, Training Loss: 39148, Validation Loss: 54932, 152829.1104257414\n",
      "Epoch 153701, Training Loss: 36494, Validation Loss: 54466, 121975.77337978431\n",
      "Epoch 153801, Training Loss: 36222, Validation Loss: 56250, 91024.28277638718\n",
      "Epoch 153901, Training Loss: 35990, Validation Loss: 53401, 99499.97730888863\n",
      "Epoch 154001, Training Loss: 40160, Validation Loss: 55256, 142158.01773600423\n",
      "Epoch 154101, Training Loss: 37458, Validation Loss: 54172, 108937.14442278333\n",
      "Epoch 154201, Training Loss: 35905, Validation Loss: 54294, 114613.43938269076\n",
      "Epoch 154301, Training Loss: 36860, Validation Loss: 53836, 133242.50520154982\n",
      "Epoch 154401, Training Loss: 37901, Validation Loss: 53389, 120024.45912683934\n",
      "Epoch 154501, Training Loss: 37325, Validation Loss: 53408, 104186.47622810789\n",
      "Epoch 154601, Training Loss: 35913, Validation Loss: 56197, 129440.10868058908\n",
      "Epoch 154701, Training Loss: 39093, Validation Loss: 53354, 109423.60796479035\n",
      "Epoch 154801, Training Loss: 38840, Validation Loss: 53427, 155602.18811488405\n",
      "Epoch 154901, Training Loss: 39854, Validation Loss: 56034, 144478.8711830839\n",
      "Epoch 155001, Training Loss: 40611, Validation Loss: 54396, 102316.98367640201\n",
      "Epoch 155101, Training Loss: 37235, Validation Loss: 55669, 126227.51565224952\n",
      "Epoch 155201, Training Loss: 36017, Validation Loss: 54981, 83757.85036273133\n",
      "Epoch 155301, Training Loss: 38251, Validation Loss: 53440, 134322.99373786812\n",
      "Epoch 155401, Training Loss: 39170, Validation Loss: 54603, 218814.29352924673\n",
      "Epoch 155501, Training Loss: 35624, Validation Loss: 54150, 107866.76749948626\n",
      "Epoch 155601, Training Loss: 37532, Validation Loss: 54110, 100519.0881325282\n",
      "Epoch 155701, Training Loss: 37350, Validation Loss: 55502, 133210.98064108423\n",
      "Epoch 155801, Training Loss: 38933, Validation Loss: 54576, 94055.49755030248\n",
      "Epoch 155901, Training Loss: 36311, Validation Loss: 54352, 118486.90806328022\n",
      "Epoch 156001, Training Loss: 38926, Validation Loss: 54186, 160599.7606998715\n",
      "Epoch 156101, Training Loss: 37435, Validation Loss: 54121, 113534.85385669464\n",
      "Epoch 156201, Training Loss: 36496, Validation Loss: 53973, 103773.32245365455\n",
      "Epoch 156301, Training Loss: 41120, Validation Loss: 57142, 95569.42363552241\n",
      "Epoch 156401, Training Loss: 39474, Validation Loss: 53981, 118102.00022388657\n",
      "Epoch 156501, Training Loss: 39663, Validation Loss: 53944, 106509.77774554638\n",
      "Epoch 156601, Training Loss: 39945, Validation Loss: 55004, 134520.2370312379\n",
      "Epoch 156701, Training Loss: 39641, Validation Loss: 56386, 141128.38301181284\n",
      "Epoch 156801, Training Loss: 38533, Validation Loss: 56296, 124566.8785848967\n",
      "Epoch 156901, Training Loss: 36157, Validation Loss: 54754, 140748.41056955408\n",
      "Epoch 157001, Training Loss: 38560, Validation Loss: 54476, 106127.86684677594\n",
      "Epoch 157101, Training Loss: 38548, Validation Loss: 54652, 159192.6872902841\n",
      "Epoch 157201, Training Loss: 35818, Validation Loss: 54374, 103427.3255261855\n",
      "Epoch 157301, Training Loss: 35648, Validation Loss: 54036, 102091.15811068972\n",
      "Epoch 157401, Training Loss: 37257, Validation Loss: 55018, 134461.10815628312\n",
      "Epoch 157501, Training Loss: 34245, Validation Loss: 54056, 95397.46744011082\n",
      "Epoch 157601, Training Loss: 37969, Validation Loss: 53326, 114601.73194531277\n",
      "Epoch 157701, Training Loss: 37013, Validation Loss: 53837, 151071.4552203991\n",
      "Epoch 157801, Training Loss: 35404, Validation Loss: 53873, 102385.1953618325\n",
      "Epoch 157901, Training Loss: 33297, Validation Loss: 54113, 105264.94186613547\n",
      "Epoch 158001, Training Loss: 37288, Validation Loss: 55788, 101208.13027478605\n",
      "Epoch 158101, Training Loss: 34918, Validation Loss: 53222, 113413.22941881872\n",
      "Epoch 158201, Training Loss: 42852, Validation Loss: 54649, 108479.94116795262\n",
      "Epoch 158301, Training Loss: 37593, Validation Loss: 53871, 110523.27131106098\n",
      "Epoch 158401, Training Loss: 36192, Validation Loss: 55222, 125447.68968381845\n",
      "Epoch 158501, Training Loss: 37561, Validation Loss: 54937, 76722.37431998969\n",
      "Epoch 158601, Training Loss: 38399, Validation Loss: 54855, 102480.67026189472\n",
      "Epoch 158701, Training Loss: 38644, Validation Loss: 53188, 121530.98624767784\n",
      "Epoch 158801, Training Loss: 38230, Validation Loss: 53166, 126773.01825333339\n",
      "Epoch 158901, Training Loss: 39745, Validation Loss: 55112, 147970.03134333366\n",
      "Epoch 159001, Training Loss: 37334, Validation Loss: 56013, 112279.62690820993\n",
      "Epoch 159101, Training Loss: 39123, Validation Loss: 53322, 102493.31384634173\n",
      "Epoch 159201, Training Loss: 35222, Validation Loss: 54878, 122550.75828489522\n",
      "Epoch 159301, Training Loss: 35490, Validation Loss: 54605, 165579.29505994587\n",
      "Epoch 159401, Training Loss: 37534, Validation Loss: 55338, 91479.17724291031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159501, Training Loss: 35342, Validation Loss: 52371, 102732.51707829925\n",
      "Epoch 159601, Training Loss: 36569, Validation Loss: 54651, 93331.10192915046\n",
      "Epoch 159701, Training Loss: 34859, Validation Loss: 54073, 160945.83320484732\n",
      "Epoch 159801, Training Loss: 39011, Validation Loss: 54100, 102080.30753427738\n",
      "Epoch 159901, Training Loss: 36329, Validation Loss: 53124, 117647.5525956457\n",
      "Epoch 160001, Training Loss: 37309, Validation Loss: 53098, 114807.40703045513\n",
      "Epoch 160101, Training Loss: 37846, Validation Loss: 54161, 120573.05858398597\n",
      "Epoch 160201, Training Loss: 36348, Validation Loss: 56189, 94022.20455562546\n",
      "Epoch 160301, Training Loss: 36067, Validation Loss: 53642, 159995.24724991233\n",
      "Epoch 160401, Training Loss: 37962, Validation Loss: 53493, 132086.9239740522\n",
      "Epoch 160501, Training Loss: 37492, Validation Loss: 53713, 131767.28131159474\n",
      "Epoch 160601, Training Loss: 37842, Validation Loss: 52461, 152307.75025764122\n",
      "Epoch 160701, Training Loss: 35818, Validation Loss: 54903, 109042.96946312296\n",
      "Epoch 160801, Training Loss: 35055, Validation Loss: 54599, 121780.87509428077\n",
      "Epoch 160901, Training Loss: 37649, Validation Loss: 54571, 107715.12560230024\n",
      "Epoch 161001, Training Loss: 37729, Validation Loss: 54984, 124452.6765720101\n",
      "Epoch 161101, Training Loss: 36679, Validation Loss: 53752, 94707.3446484751\n",
      "Epoch 161201, Training Loss: 35271, Validation Loss: 53333, 96667.67640308286\n",
      "Epoch 161301, Training Loss: 36415, Validation Loss: 53523, 91701.47109285998\n",
      "Epoch 161401, Training Loss: 40346, Validation Loss: 54707, 147266.01395495178\n",
      "Epoch 161501, Training Loss: 37755, Validation Loss: 53482, 98347.77024250297\n",
      "Epoch 161601, Training Loss: 36765, Validation Loss: 54654, 115218.51638316637\n",
      "Epoch 161701, Training Loss: 35374, Validation Loss: 55120, 167466.04597216754\n",
      "Epoch 161801, Training Loss: 38649, Validation Loss: 54288, 114495.98984957514\n",
      "Epoch 161901, Training Loss: 34796, Validation Loss: 53899, 111193.32408336883\n",
      "Epoch 162001, Training Loss: 37678, Validation Loss: 54032, 119363.95517803523\n",
      "Epoch 162101, Training Loss: 36810, Validation Loss: 54178, 109062.85184163573\n",
      "Epoch 162201, Training Loss: 37698, Validation Loss: 56826, 142122.5832007408\n",
      "Epoch 162301, Training Loss: 35231, Validation Loss: 52365, 105368.41835875077\n",
      "Epoch 162401, Training Loss: 35834, Validation Loss: 54210, 107616.75154311625\n",
      "Epoch 162501, Training Loss: 37715, Validation Loss: 54035, 121314.28193796395\n",
      "Epoch 162601, Training Loss: 35625, Validation Loss: 55679, 117729.38730580948\n",
      "Epoch 162701, Training Loss: 35539, Validation Loss: 53921, 112666.83920201503\n",
      "Epoch 162801, Training Loss: 39391, Validation Loss: 55681, 99606.46342414069\n",
      "Epoch 162901, Training Loss: 40761, Validation Loss: 53816, 120225.98347281758\n",
      "Epoch 163001, Training Loss: 37375, Validation Loss: 53631, 153695.77686942663\n",
      "Epoch 163101, Training Loss: 38460, Validation Loss: 53960, 113619.88579705276\n",
      "Epoch 163201, Training Loss: 36895, Validation Loss: 55222, 119385.30997547552\n",
      "Epoch 163301, Training Loss: 37906, Validation Loss: 52840, 108101.21400573954\n",
      "Epoch 163401, Training Loss: 36992, Validation Loss: 53929, 118365.32830536441\n",
      "Epoch 163501, Training Loss: 37551, Validation Loss: 55443, 140594.0715369413\n",
      "Epoch 163601, Training Loss: 37173, Validation Loss: 54483, 135106.07298619894\n",
      "Epoch 163701, Training Loss: 37020, Validation Loss: 55185, 110386.21616202257\n",
      "Epoch 163801, Training Loss: 38574, Validation Loss: 55544, 162458.5673156108\n",
      "Epoch 163901, Training Loss: 38637, Validation Loss: 53869, 142953.89004332465\n",
      "Epoch 164001, Training Loss: 38715, Validation Loss: 54461, 102335.11180119174\n",
      "Epoch 164101, Training Loss: 37277, Validation Loss: 53087, 142597.45041253453\n",
      "Epoch 164201, Training Loss: 36903, Validation Loss: 52615, 111904.45486890558\n",
      "Epoch 164301, Training Loss: 38498, Validation Loss: 54566, 120856.44071176356\n",
      "Epoch 164401, Training Loss: 36899, Validation Loss: 53223, 152142.36689220453\n",
      "Epoch 164501, Training Loss: 36718, Validation Loss: 54971, 113292.99356492491\n",
      "Epoch 164601, Training Loss: 40803, Validation Loss: 54705, 120906.07303879414\n",
      "Epoch 164701, Training Loss: 37299, Validation Loss: 53241, 97940.25735595277\n",
      "Epoch 164801, Training Loss: 36467, Validation Loss: 53878, 134426.91636472466\n",
      "Epoch 164901, Training Loss: 38777, Validation Loss: 54905, 123945.12100061942\n",
      "Epoch 165001, Training Loss: 37689, Validation Loss: 53589, 114251.95487702206\n",
      "Epoch 165101, Training Loss: 38978, Validation Loss: 55212, 138112.41169886268\n",
      "Epoch 165201, Training Loss: 37066, Validation Loss: 53235, 112385.61290993995\n",
      "Epoch 165301, Training Loss: 37801, Validation Loss: 53051, 94640.61581356313\n",
      "Epoch 165401, Training Loss: 37049, Validation Loss: 53889, 109498.38837917599\n",
      "Epoch 165501, Training Loss: 37649, Validation Loss: 52099, 109388.42385077632\n",
      "Epoch 165601, Training Loss: 37111, Validation Loss: 53468, 96040.29232304667\n",
      "Epoch 165701, Training Loss: 35748, Validation Loss: 53800, 98865.08246184651\n",
      "Epoch 165801, Training Loss: 37187, Validation Loss: 53091, 103429.30871994149\n",
      "Epoch 165901, Training Loss: 36029, Validation Loss: 53652, 126382.14565621437\n",
      "Epoch 166001, Training Loss: 38056, Validation Loss: 54091, 192026.32577008219\n",
      "Epoch 166101, Training Loss: 35536, Validation Loss: 53994, 117233.71287470315\n",
      "Epoch 166201, Training Loss: 35417, Validation Loss: 54348, 117542.32463802071\n",
      "Epoch 166301, Training Loss: 37545, Validation Loss: 53455, 116630.41104035317\n",
      "Epoch 166401, Training Loss: 35423, Validation Loss: 54900, 105833.8624605406\n",
      "Epoch 166501, Training Loss: 36050, Validation Loss: 54639, 121309.24561191387\n",
      "Epoch 166601, Training Loss: 36419, Validation Loss: 54049, 150604.55087632398\n",
      "Epoch 166701, Training Loss: 36418, Validation Loss: 53333, 125119.2743514427\n",
      "Epoch 166801, Training Loss: 37459, Validation Loss: 53623, 326512.5540895449\n",
      "Epoch 166901, Training Loss: 34979, Validation Loss: 53470, 120469.21693459457\n",
      "Epoch 167001, Training Loss: 37724, Validation Loss: 56073, 130233.83164147117\n",
      "Epoch 167101, Training Loss: 35681, Validation Loss: 55037, 126994.9964537804\n",
      "Epoch 167201, Training Loss: 37202, Validation Loss: 55865, 148117.09260615378\n",
      "Epoch 167301, Training Loss: 34509, Validation Loss: 55928, 98126.59778818616\n",
      "Epoch 167401, Training Loss: 38315, Validation Loss: 54474, 116666.14440809021\n",
      "Epoch 167501, Training Loss: 37731, Validation Loss: 53626, 132578.14258715842\n",
      "Epoch 167601, Training Loss: 38383, Validation Loss: 54628, 219726.0490231543\n",
      "Epoch 167701, Training Loss: 37196, Validation Loss: 55139, 143835.38283526982\n",
      "Epoch 167801, Training Loss: 36872, Validation Loss: 54985, 127694.26527079323\n",
      "Epoch 167901, Training Loss: 38292, Validation Loss: 54629, 120664.40542455611\n",
      "Epoch 168001, Training Loss: 36172, Validation Loss: 54267, 99434.66592505148\n",
      "Epoch 168101, Training Loss: 41567, Validation Loss: 56510, 173663.2465635469\n",
      "Epoch 168201, Training Loss: 37134, Validation Loss: 53761, 180282.59061013433\n",
      "Epoch 168301, Training Loss: 36897, Validation Loss: 53080, 109737.89196768613\n",
      "Epoch 168401, Training Loss: 39212, Validation Loss: 55679, 145039.6246953093\n",
      "Epoch 168501, Training Loss: 39385, Validation Loss: 54963, 149283.08640854535\n",
      "Epoch 168601, Training Loss: 37236, Validation Loss: 55496, 150672.60163270167\n",
      "Epoch 168701, Training Loss: 37229, Validation Loss: 53796, 111082.33496970306\n",
      "Epoch 168801, Training Loss: 36525, Validation Loss: 54172, 111481.22705789459\n",
      "Epoch 168901, Training Loss: 35630, Validation Loss: 54439, 127964.8080310994\n",
      "Epoch 169001, Training Loss: 34877, Validation Loss: 53395, 140346.63951288068\n",
      "Epoch 169101, Training Loss: 36235, Validation Loss: 54218, 146947.24222822842\n",
      "Epoch 169201, Training Loss: 37229, Validation Loss: 53988, 106614.1122675027\n",
      "Epoch 169301, Training Loss: 36227, Validation Loss: 53849, 182126.1197916375\n",
      "Epoch 169401, Training Loss: 34909, Validation Loss: 53789, 126765.60679336423\n",
      "Epoch 169501, Training Loss: 38052, Validation Loss: 53935, 125625.59929549291\n",
      "Epoch 169601, Training Loss: 36864, Validation Loss: 53453, 135131.69656614165\n",
      "Epoch 169701, Training Loss: 37890, Validation Loss: 54068, 131080.11587285154\n",
      "Epoch 169801, Training Loss: 38050, Validation Loss: 55079, 109497.01083448312\n",
      "Epoch 169901, Training Loss: 35230, Validation Loss: 54821, 140224.98934976294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170001, Training Loss: 35192, Validation Loss: 56636, 143839.81810468485\n",
      "Epoch 170101, Training Loss: 36576, Validation Loss: 55262, 122599.2901432404\n",
      "Epoch 170201, Training Loss: 36751, Validation Loss: 54226, 113997.11729506003\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,\n",
    "    weight_decay=3e-4\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999, \n",
    "    patience=100, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    grad_norm = 0\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        grad_norm += get_gradient_norm(model)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    grad_norms.append(grad_norm / len(train_loader))\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "#     scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        tl = f\"Training Loss: {int(train_losses[-1])}\"\n",
    "        vl = f\"Validation Loss: {int(val_losses[-1])}\"\n",
    "        l1 = f\"L1: {int(l1_mean_loss)}\"\n",
    "        dl = f'Epoch {epoch+1}, {tl}, {vl}, {grad_norms[-1]}'\n",
    "        print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"../visualizations/overfit_model_evaluation_full_dataset.png\", dpi=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
