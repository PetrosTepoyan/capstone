{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050bc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../data2/data_tiny.csv\").drop(columns = [\"id\", \"source\", \n",
    "                                                       \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28e331fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 211300, Validation Loss: 237232, L1: 237232\n",
      "Epoch 101, Training Loss: 191016, Validation Loss: 216251, L1: 216251\n",
      "Epoch 201, Training Loss: 103119, Validation Loss: 138556, L1: 138556\n",
      "Epoch 301, Training Loss: 75974, Validation Loss: 133005, L1: 133005\n",
      "Epoch 401, Training Loss: 75592, Validation Loss: 132507, L1: 132507\n",
      "Epoch 501, Training Loss: 75200, Validation Loss: 132394, L1: 132394\n",
      "Epoch 601, Training Loss: 74814, Validation Loss: 132115, L1: 132115\n",
      "Epoch 701, Training Loss: 74449, Validation Loss: 131689, L1: 131689\n",
      "Epoch 801, Training Loss: 74031, Validation Loss: 131211, L1: 131211\n",
      "Epoch 901, Training Loss: 73549, Validation Loss: 130659, L1: 130659\n",
      "Epoch 1001, Training Loss: 72985, Validation Loss: 130012, L1: 130012\n",
      "Epoch 1101, Training Loss: 72533, Validation Loss: 129465, L1: 129465\n",
      "Epoch 1201, Training Loss: 72223, Validation Loss: 129109, L1: 129109\n",
      "Epoch 1301, Training Loss: 71972, Validation Loss: 128660, L1: 128660\n",
      "Epoch 1401, Training Loss: 71743, Validation Loss: 128205, L1: 128205\n",
      "Epoch 1501, Training Loss: 71481, Validation Loss: 127655, L1: 127655\n",
      "Epoch 1601, Training Loss: 71171, Validation Loss: 126994, L1: 126994\n",
      "Epoch 1701, Training Loss: 70805, Validation Loss: 126221, L1: 126221\n",
      "Epoch 1801, Training Loss: 70383, Validation Loss: 125164, L1: 125164\n",
      "Epoch 1901, Training Loss: 70015, Validation Loss: 124143, L1: 124143\n",
      "Epoch 2001, Training Loss: 69508, Validation Loss: 122882, L1: 122882\n",
      "Epoch 2101, Training Loss: 68928, Validation Loss: 121225, L1: 121225\n",
      "Epoch 2201, Training Loss: 68259, Validation Loss: 119376, L1: 119376\n",
      "Epoch 2301, Training Loss: 67583, Validation Loss: 117787, L1: 117787\n",
      "Epoch 2401, Training Loss: 66859, Validation Loss: 116582, L1: 116582\n",
      "Epoch 2501, Training Loss: 66100, Validation Loss: 115301, L1: 115301\n",
      "Epoch 2601, Training Loss: 65311, Validation Loss: 113352, L1: 113352\n",
      "Epoch 2701, Training Loss: 64346, Validation Loss: 110738, L1: 110738\n",
      "Epoch 2801, Training Loss: 63643, Validation Loss: 109169, L1: 109169\n",
      "Epoch 2901, Training Loss: 63145, Validation Loss: 108021, L1: 108021\n",
      "Epoch 3001, Training Loss: 62775, Validation Loss: 107211, L1: 107211\n",
      "Epoch 3101, Training Loss: 62416, Validation Loss: 106380, L1: 106380\n",
      "Epoch 3201, Training Loss: 62066, Validation Loss: 105410, L1: 105410\n",
      "Epoch 3301, Training Loss: 61652, Validation Loss: 104345, L1: 104345\n",
      "Epoch 3401, Training Loss: 61181, Validation Loss: 103100, L1: 103100\n",
      "Epoch 3501, Training Loss: 60698, Validation Loss: 101984, L1: 101984\n",
      "Epoch 3601, Training Loss: 60155, Validation Loss: 101033, L1: 101033\n",
      "Epoch 3701, Training Loss: 59606, Validation Loss: 100288, L1: 100288\n",
      "Epoch 3801, Training Loss: 58959, Validation Loss: 99437, L1: 99437\n",
      "Epoch 3901, Training Loss: 58161, Validation Loss: 99284, L1: 99284\n",
      "Epoch 4001, Training Loss: 57387, Validation Loss: 97899, L1: 97899\n",
      "Epoch 4101, Training Loss: 56585, Validation Loss: 96131, L1: 96131\n",
      "Epoch 4201, Training Loss: 56007, Validation Loss: 94831, L1: 94831\n",
      "Epoch 4301, Training Loss: 55504, Validation Loss: 94041, L1: 94041\n",
      "Epoch 4401, Training Loss: 55066, Validation Loss: 93125, L1: 93125\n",
      "Epoch 4501, Training Loss: 54555, Validation Loss: 92090, L1: 92090\n",
      "Epoch 4601, Training Loss: 54100, Validation Loss: 91316, L1: 91316\n",
      "Epoch 4701, Training Loss: 53523, Validation Loss: 90496, L1: 90496\n",
      "Epoch 4801, Training Loss: 52980, Validation Loss: 89628, L1: 89628\n",
      "Epoch 4901, Training Loss: 52391, Validation Loss: 88741, L1: 88741\n",
      "Epoch 5001, Training Loss: 51792, Validation Loss: 87822, L1: 87822\n",
      "Epoch 5101, Training Loss: 51110, Validation Loss: 86881, L1: 86881\n",
      "Epoch 5201, Training Loss: 50428, Validation Loss: 85781, L1: 85781\n",
      "Epoch 5301, Training Loss: 49816, Validation Loss: 84328, L1: 84328\n",
      "Epoch 5401, Training Loss: 49165, Validation Loss: 83031, L1: 83031\n",
      "Epoch 5501, Training Loss: 48482, Validation Loss: 82282, L1: 82282\n",
      "Epoch 5601, Training Loss: 47809, Validation Loss: 81369, L1: 81369\n",
      "Epoch 5701, Training Loss: 47251, Validation Loss: 80466, L1: 80466\n",
      "Epoch 5801, Training Loss: 46752, Validation Loss: 79237, L1: 79237\n",
      "Epoch 5901, Training Loss: 46258, Validation Loss: 78500, L1: 78500\n",
      "Epoch 6001, Training Loss: 45696, Validation Loss: 77730, L1: 77730\n",
      "Epoch 6101, Training Loss: 45100, Validation Loss: 76467, L1: 76467\n",
      "Epoch 6201, Training Loss: 44425, Validation Loss: 75528, L1: 75528\n",
      "Epoch 6301, Training Loss: 43961, Validation Loss: 74645, L1: 74645\n",
      "Epoch 6401, Training Loss: 43575, Validation Loss: 73685, L1: 73685\n",
      "Epoch 6501, Training Loss: 43153, Validation Loss: 72771, L1: 72771\n",
      "Epoch 6601, Training Loss: 42882, Validation Loss: 72045, L1: 72045\n",
      "Epoch 6701, Training Loss: 42614, Validation Loss: 71122, L1: 71122\n",
      "Epoch 6801, Training Loss: 42324, Validation Loss: 70221, L1: 70221\n",
      "Epoch 6901, Training Loss: 42282, Validation Loss: 69835, L1: 69835\n",
      "Epoch 7001, Training Loss: 42072, Validation Loss: 69484, L1: 69484\n",
      "Epoch 7101, Training Loss: 41856, Validation Loss: 69072, L1: 69072\n",
      "Epoch 7201, Training Loss: 41631, Validation Loss: 68819, L1: 68819\n",
      "Epoch 7301, Training Loss: 41364, Validation Loss: 68169, L1: 68169\n",
      "Epoch 7401, Training Loss: 41197, Validation Loss: 67846, L1: 67846\n",
      "Epoch 7501, Training Loss: 41047, Validation Loss: 67503, L1: 67503\n",
      "Epoch 7601, Training Loss: 40891, Validation Loss: 67383, L1: 67383\n",
      "Epoch 7701, Training Loss: 40724, Validation Loss: 67226, L1: 67226\n",
      "Epoch 7801, Training Loss: 40562, Validation Loss: 67107, L1: 67107\n",
      "Epoch 7901, Training Loss: 40461, Validation Loss: 66809, L1: 66809\n",
      "Epoch 8001, Training Loss: 40280, Validation Loss: 66631, L1: 66631\n",
      "Epoch 8101, Training Loss: 40125, Validation Loss: 66473, L1: 66473\n",
      "Epoch 8201, Training Loss: 40018, Validation Loss: 66457, L1: 66457\n",
      "Epoch 8301, Training Loss: 39880, Validation Loss: 66157, L1: 66157\n",
      "Epoch 8401, Training Loss: 39742, Validation Loss: 65961, L1: 65961\n",
      "Epoch 8501, Training Loss: 39639, Validation Loss: 66065, L1: 66065\n",
      "Epoch 8601, Training Loss: 39467, Validation Loss: 66048, L1: 66048\n",
      "Epoch 8701, Training Loss: 39409, Validation Loss: 65882, L1: 65882\n",
      "Epoch 8801, Training Loss: 39268, Validation Loss: 65666, L1: 65666\n",
      "Epoch 8901, Training Loss: 39257, Validation Loss: 65874, L1: 65874\n",
      "Epoch 9001, Training Loss: 39130, Validation Loss: 65877, L1: 65877\n",
      "Epoch 9101, Training Loss: 38999, Validation Loss: 66004, L1: 66004\n",
      "Epoch 9201, Training Loss: 38920, Validation Loss: 66110, L1: 66110\n",
      "Epoch 9301, Training Loss: 38818, Validation Loss: 66043, L1: 66043\n",
      "Epoch 9401, Training Loss: 38619, Validation Loss: 65938, L1: 65938\n",
      "Epoch 9501, Training Loss: 38569, Validation Loss: 65856, L1: 65856\n",
      "Epoch 9601, Training Loss: 38336, Validation Loss: 65760, L1: 65760\n",
      "Epoch 9701, Training Loss: 38267, Validation Loss: 65860, L1: 65860\n",
      "Epoch 9801, Training Loss: 38174, Validation Loss: 65831, L1: 65831\n",
      "Epoch 9901, Training Loss: 38118, Validation Loss: 65697, L1: 65697\n",
      "Epoch 10001, Training Loss: 37978, Validation Loss: 66017, L1: 66017\n",
      "Epoch 10101, Training Loss: 37818, Validation Loss: 65810, L1: 65810\n",
      "Epoch 10201, Training Loss: 37793, Validation Loss: 65771, L1: 65771\n",
      "Epoch 10301, Training Loss: 37694, Validation Loss: 66053, L1: 66053\n",
      "Epoch 10401, Training Loss: 37717, Validation Loss: 65918, L1: 65918\n",
      "Epoch 10501, Training Loss: 37473, Validation Loss: 66102, L1: 66102\n",
      "Epoch 10601, Training Loss: 37439, Validation Loss: 66082, L1: 66082\n",
      "Epoch 10701, Training Loss: 37540, Validation Loss: 66030, L1: 66030\n",
      "Epoch 10801, Training Loss: 37293, Validation Loss: 66219, L1: 66219\n",
      "Epoch 10901, Training Loss: 37287, Validation Loss: 66277, L1: 66277\n",
      "Epoch 11001, Training Loss: 37168, Validation Loss: 66307, L1: 66307\n",
      "Epoch 11101, Training Loss: 37188, Validation Loss: 66103, L1: 66103\n",
      "Epoch 11201, Training Loss: 37054, Validation Loss: 66402, L1: 66402\n",
      "Epoch 11301, Training Loss: 37117, Validation Loss: 66416, L1: 66416\n",
      "Epoch 11401, Training Loss: 37164, Validation Loss: 66513, L1: 66513\n",
      "Epoch 11501, Training Loss: 36960, Validation Loss: 66660, L1: 66660\n",
      "Epoch 11601, Training Loss: 36881, Validation Loss: 66399, L1: 66399\n",
      "Epoch 11701, Training Loss: 36782, Validation Loss: 66405, L1: 66405\n",
      "Epoch 11801, Training Loss: 36755, Validation Loss: 66387, L1: 66387\n",
      "Epoch 11901, Training Loss: 36909, Validation Loss: 66561, L1: 66561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12001, Training Loss: 36641, Validation Loss: 66440, L1: 66440\n",
      "Epoch 12101, Training Loss: 36656, Validation Loss: 66468, L1: 66468\n",
      "Epoch 12201, Training Loss: 36626, Validation Loss: 66516, L1: 66516\n",
      "Epoch 12301, Training Loss: 36530, Validation Loss: 66557, L1: 66557\n",
      "Epoch 12401, Training Loss: 36462, Validation Loss: 66662, L1: 66662\n",
      "Epoch 12501, Training Loss: 36463, Validation Loss: 66620, L1: 66620\n",
      "Epoch 12601, Training Loss: 36407, Validation Loss: 66564, L1: 66564\n",
      "Epoch 12701, Training Loss: 36604, Validation Loss: 66393, L1: 66393\n",
      "Epoch 12801, Training Loss: 36329, Validation Loss: 66721, L1: 66721\n",
      "Epoch 12901, Training Loss: 36249, Validation Loss: 66553, L1: 66553\n",
      "Epoch 13001, Training Loss: 36187, Validation Loss: 66672, L1: 66672\n",
      "Epoch 13101, Training Loss: 36259, Validation Loss: 66632, L1: 66632\n",
      "Epoch 13201, Training Loss: 36201, Validation Loss: 66527, L1: 66527\n",
      "Epoch 13301, Training Loss: 36064, Validation Loss: 66460, L1: 66460\n",
      "Epoch 13401, Training Loss: 36047, Validation Loss: 66463, L1: 66463\n",
      "Epoch 13501, Training Loss: 35982, Validation Loss: 66635, L1: 66635\n",
      "Epoch 13601, Training Loss: 35901, Validation Loss: 66624, L1: 66624\n",
      "Epoch 13701, Training Loss: 35870, Validation Loss: 66686, L1: 66686\n",
      "Epoch 13801, Training Loss: 35831, Validation Loss: 66592, L1: 66592\n",
      "Epoch 13901, Training Loss: 35785, Validation Loss: 66666, L1: 66666\n",
      "Epoch 14001, Training Loss: 35718, Validation Loss: 66655, L1: 66655\n",
      "Epoch 14101, Training Loss: 35712, Validation Loss: 66467, L1: 66467\n",
      "Epoch 14201, Training Loss: 35639, Validation Loss: 66780, L1: 66780\n",
      "Epoch 14301, Training Loss: 35587, Validation Loss: 66711, L1: 66711\n",
      "Epoch 14401, Training Loss: 35582, Validation Loss: 66834, L1: 66834\n",
      "Epoch 14501, Training Loss: 35517, Validation Loss: 66548, L1: 66548\n",
      "Epoch 14601, Training Loss: 35626, Validation Loss: 66786, L1: 66786\n",
      "Epoch 14701, Training Loss: 35423, Validation Loss: 66640, L1: 66640\n",
      "Epoch 14801, Training Loss: 35363, Validation Loss: 66762, L1: 66762\n",
      "Epoch 14901, Training Loss: 35380, Validation Loss: 66684, L1: 66684\n",
      "Epoch 15001, Training Loss: 35447, Validation Loss: 66956, L1: 66956\n",
      "Epoch 15101, Training Loss: 35295, Validation Loss: 66645, L1: 66645\n",
      "Epoch 15201, Training Loss: 35224, Validation Loss: 66681, L1: 66681\n",
      "Epoch 15301, Training Loss: 35144, Validation Loss: 66851, L1: 66851\n",
      "Epoch 15401, Training Loss: 35164, Validation Loss: 66660, L1: 66660\n",
      "Epoch 15501, Training Loss: 35259, Validation Loss: 66483, L1: 66483\n",
      "Epoch 15601, Training Loss: 35070, Validation Loss: 66436, L1: 66436\n",
      "Epoch 15701, Training Loss: 35069, Validation Loss: 66773, L1: 66773\n",
      "Epoch 15801, Training Loss: 35108, Validation Loss: 66629, L1: 66629\n",
      "Epoch 15901, Training Loss: 35007, Validation Loss: 66646, L1: 66646\n",
      "Epoch 16001, Training Loss: 34944, Validation Loss: 66682, L1: 66682\n",
      "Epoch 16101, Training Loss: 34973, Validation Loss: 66620, L1: 66620\n",
      "Epoch 16201, Training Loss: 34821, Validation Loss: 66862, L1: 66862\n",
      "Epoch 16301, Training Loss: 34817, Validation Loss: 66706, L1: 66706\n",
      "Epoch 16401, Training Loss: 34750, Validation Loss: 66988, L1: 66988\n",
      "Epoch 16501, Training Loss: 34710, Validation Loss: 66719, L1: 66719\n",
      "Epoch 16601, Training Loss: 34702, Validation Loss: 66587, L1: 66587\n",
      "Epoch 16701, Training Loss: 34657, Validation Loss: 66616, L1: 66616\n",
      "Epoch 16801, Training Loss: 34679, Validation Loss: 66765, L1: 66765\n",
      "Epoch 16901, Training Loss: 34571, Validation Loss: 66634, L1: 66634\n",
      "Epoch 17001, Training Loss: 34563, Validation Loss: 66789, L1: 66789\n",
      "Epoch 17101, Training Loss: 34578, Validation Loss: 66744, L1: 66744\n",
      "Epoch 17201, Training Loss: 34460, Validation Loss: 66750, L1: 66750\n",
      "Epoch 17301, Training Loss: 34433, Validation Loss: 66962, L1: 66962\n",
      "Epoch 17401, Training Loss: 34391, Validation Loss: 66759, L1: 66759\n",
      "Epoch 17501, Training Loss: 34362, Validation Loss: 66777, L1: 66777\n",
      "Epoch 17601, Training Loss: 34360, Validation Loss: 66976, L1: 66976\n",
      "Epoch 17701, Training Loss: 34264, Validation Loss: 66892, L1: 66892\n",
      "Epoch 17801, Training Loss: 34389, Validation Loss: 66844, L1: 66844\n",
      "Epoch 17901, Training Loss: 34256, Validation Loss: 67019, L1: 67019\n",
      "Epoch 18001, Training Loss: 34209, Validation Loss: 67111, L1: 67111\n",
      "Epoch 18101, Training Loss: 34153, Validation Loss: 67085, L1: 67085\n",
      "Epoch 18201, Training Loss: 34143, Validation Loss: 67171, L1: 67171\n",
      "Epoch 18301, Training Loss: 34130, Validation Loss: 66815, L1: 66815\n",
      "Epoch 18401, Training Loss: 34248, Validation Loss: 66860, L1: 66860\n",
      "Epoch 18501, Training Loss: 34098, Validation Loss: 67060, L1: 67060\n",
      "Epoch 18601, Training Loss: 33968, Validation Loss: 66962, L1: 66962\n",
      "Epoch 18701, Training Loss: 33942, Validation Loss: 66916, L1: 66916\n",
      "Epoch 18801, Training Loss: 34020, Validation Loss: 66910, L1: 66910\n",
      "Epoch 18901, Training Loss: 33866, Validation Loss: 67138, L1: 67138\n",
      "Epoch 19001, Training Loss: 33819, Validation Loss: 67069, L1: 67069\n",
      "Epoch 19101, Training Loss: 33770, Validation Loss: 66967, L1: 66967\n",
      "Epoch 19201, Training Loss: 33867, Validation Loss: 67081, L1: 67081\n",
      "Epoch 19301, Training Loss: 33768, Validation Loss: 67040, L1: 67040\n",
      "Epoch 19401, Training Loss: 33732, Validation Loss: 67471, L1: 67471\n",
      "Epoch 19501, Training Loss: 33662, Validation Loss: 67147, L1: 67147\n",
      "Epoch 19601, Training Loss: 33626, Validation Loss: 67253, L1: 67253\n",
      "Epoch 19701, Training Loss: 33645, Validation Loss: 67181, L1: 67181\n",
      "Epoch 19801, Training Loss: 33601, Validation Loss: 67346, L1: 67346\n",
      "Epoch 19901, Training Loss: 33612, Validation Loss: 67382, L1: 67382\n",
      "Epoch 20001, Training Loss: 33447, Validation Loss: 67267, L1: 67267\n",
      "Epoch 20101, Training Loss: 33401, Validation Loss: 67311, L1: 67311\n",
      "Epoch 20201, Training Loss: 33598, Validation Loss: 67549, L1: 67549\n",
      "Epoch 20301, Training Loss: 33366, Validation Loss: 67391, L1: 67391\n",
      "Epoch 20401, Training Loss: 33449, Validation Loss: 67171, L1: 67171\n",
      "Epoch 20501, Training Loss: 33544, Validation Loss: 67419, L1: 67419\n",
      "Epoch 20601, Training Loss: 33208, Validation Loss: 67215, L1: 67215\n",
      "Epoch 20701, Training Loss: 33341, Validation Loss: 67364, L1: 67364\n",
      "Epoch 20801, Training Loss: 33150, Validation Loss: 67301, L1: 67301\n",
      "Epoch 20901, Training Loss: 33117, Validation Loss: 67269, L1: 67269\n",
      "Epoch 21001, Training Loss: 33051, Validation Loss: 67505, L1: 67505\n",
      "Epoch 21101, Training Loss: 33026, Validation Loss: 67650, L1: 67650\n",
      "Epoch 21201, Training Loss: 32990, Validation Loss: 67428, L1: 67428\n",
      "Epoch 21301, Training Loss: 33148, Validation Loss: 67653, L1: 67653\n",
      "Epoch 21401, Training Loss: 32848, Validation Loss: 67596, L1: 67596\n",
      "Epoch 21501, Training Loss: 32814, Validation Loss: 67729, L1: 67729\n",
      "Epoch 21601, Training Loss: 32778, Validation Loss: 67758, L1: 67758\n",
      "Epoch 21701, Training Loss: 32708, Validation Loss: 68007, L1: 68007\n",
      "Epoch 21801, Training Loss: 32944, Validation Loss: 67695, L1: 67695\n",
      "Epoch 21901, Training Loss: 32680, Validation Loss: 67864, L1: 67864\n",
      "Epoch 22001, Training Loss: 32552, Validation Loss: 68192, L1: 68192\n",
      "Epoch 22101, Training Loss: 32553, Validation Loss: 67952, L1: 67952\n",
      "Epoch 22201, Training Loss: 32463, Validation Loss: 67964, L1: 67964\n",
      "Epoch 22301, Training Loss: 32546, Validation Loss: 67979, L1: 67979\n",
      "Epoch 22401, Training Loss: 32445, Validation Loss: 68328, L1: 68328\n",
      "Epoch 22501, Training Loss: 32394, Validation Loss: 67870, L1: 67870\n",
      "Epoch 22601, Training Loss: 32306, Validation Loss: 68166, L1: 68166\n",
      "Epoch 22701, Training Loss: 32241, Validation Loss: 68301, L1: 68301\n",
      "Epoch 22801, Training Loss: 32241, Validation Loss: 68131, L1: 68131\n",
      "Epoch 22901, Training Loss: 32403, Validation Loss: 68026, L1: 68026\n",
      "Epoch 23001, Training Loss: 32100, Validation Loss: 68184, L1: 68184\n",
      "Epoch 23101, Training Loss: 32115, Validation Loss: 68334, L1: 68334\n",
      "Epoch 23201, Training Loss: 32318, Validation Loss: 68025, L1: 68025\n",
      "Epoch 23301, Training Loss: 31984, Validation Loss: 68213, L1: 68213\n",
      "Epoch 23401, Training Loss: 31969, Validation Loss: 68174, L1: 68174\n",
      "Epoch 23501, Training Loss: 31943, Validation Loss: 68282, L1: 68282\n",
      "Epoch 23601, Training Loss: 31953, Validation Loss: 68308, L1: 68308\n",
      "Epoch 23701, Training Loss: 31817, Validation Loss: 68125, L1: 68125\n",
      "Epoch 23801, Training Loss: 31891, Validation Loss: 67975, L1: 67975\n",
      "Epoch 23901, Training Loss: 31814, Validation Loss: 68282, L1: 68282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24001, Training Loss: 31661, Validation Loss: 68299, L1: 68299\n",
      "Epoch 24101, Training Loss: 31636, Validation Loss: 68408, L1: 68408\n",
      "Epoch 24201, Training Loss: 31636, Validation Loss: 68197, L1: 68197\n",
      "Epoch 24301, Training Loss: 31624, Validation Loss: 68089, L1: 68089\n",
      "Epoch 24401, Training Loss: 31505, Validation Loss: 68219, L1: 68219\n",
      "Epoch 24501, Training Loss: 31659, Validation Loss: 68298, L1: 68298\n",
      "Epoch 24601, Training Loss: 31706, Validation Loss: 68116, L1: 68116\n",
      "Epoch 24701, Training Loss: 31523, Validation Loss: 68101, L1: 68101\n",
      "Epoch 24801, Training Loss: 31536, Validation Loss: 68077, L1: 68077\n",
      "Epoch 24901, Training Loss: 31362, Validation Loss: 68413, L1: 68413\n",
      "Epoch 25001, Training Loss: 31394, Validation Loss: 68259, L1: 68259\n",
      "Epoch 25101, Training Loss: 31170, Validation Loss: 68314, L1: 68314\n",
      "Epoch 25201, Training Loss: 31144, Validation Loss: 68372, L1: 68372\n",
      "Epoch 25301, Training Loss: 31165, Validation Loss: 68772, L1: 68772\n",
      "Epoch 25401, Training Loss: 31377, Validation Loss: 68628, L1: 68628\n",
      "Epoch 25501, Training Loss: 31009, Validation Loss: 68717, L1: 68717\n",
      "Epoch 25601, Training Loss: 30999, Validation Loss: 68288, L1: 68288\n",
      "Epoch 25701, Training Loss: 30908, Validation Loss: 68325, L1: 68325\n",
      "Epoch 25801, Training Loss: 30923, Validation Loss: 68654, L1: 68654\n",
      "Epoch 25901, Training Loss: 30861, Validation Loss: 68688, L1: 68688\n",
      "Epoch 26001, Training Loss: 30877, Validation Loss: 68328, L1: 68328\n",
      "Epoch 26101, Training Loss: 30775, Validation Loss: 68818, L1: 68818\n",
      "Epoch 26201, Training Loss: 30861, Validation Loss: 68626, L1: 68626\n",
      "Epoch 26301, Training Loss: 30994, Validation Loss: 68640, L1: 68640\n",
      "Epoch 26401, Training Loss: 30779, Validation Loss: 68735, L1: 68735\n",
      "Epoch 26501, Training Loss: 30568, Validation Loss: 68927, L1: 68927\n",
      "Epoch 26601, Training Loss: 30673, Validation Loss: 68998, L1: 68998\n",
      "Epoch 26701, Training Loss: 30620, Validation Loss: 68803, L1: 68803\n",
      "Epoch 26801, Training Loss: 30477, Validation Loss: 68637, L1: 68637\n",
      "Epoch 26901, Training Loss: 30566, Validation Loss: 68957, L1: 68957\n",
      "Epoch 27001, Training Loss: 30422, Validation Loss: 68985, L1: 68985\n",
      "Epoch 27101, Training Loss: 30346, Validation Loss: 68976, L1: 68976\n",
      "Epoch 27201, Training Loss: 30353, Validation Loss: 68992, L1: 68992\n",
      "Epoch 27301, Training Loss: 30685, Validation Loss: 68898, L1: 68898\n",
      "Epoch 27401, Training Loss: 30291, Validation Loss: 68846, L1: 68846\n",
      "Epoch 27501, Training Loss: 30385, Validation Loss: 69004, L1: 69004\n",
      "Epoch 27601, Training Loss: 30156, Validation Loss: 69231, L1: 69231\n",
      "Epoch 27701, Training Loss: 30112, Validation Loss: 69271, L1: 69271\n",
      "Epoch 27801, Training Loss: 30208, Validation Loss: 69113, L1: 69113\n",
      "Epoch 27901, Training Loss: 30120, Validation Loss: 69206, L1: 69206\n",
      "Epoch 28001, Training Loss: 30087, Validation Loss: 69206, L1: 69206\n",
      "Epoch 28101, Training Loss: 30179, Validation Loss: 69235, L1: 69235\n",
      "Epoch 28201, Training Loss: 29906, Validation Loss: 69381, L1: 69381\n",
      "Epoch 28301, Training Loss: 29858, Validation Loss: 69361, L1: 69361\n",
      "Epoch 28401, Training Loss: 30106, Validation Loss: 69258, L1: 69258\n",
      "Epoch 28501, Training Loss: 29815, Validation Loss: 69454, L1: 69454\n",
      "Epoch 28601, Training Loss: 29835, Validation Loss: 69344, L1: 69344\n",
      "Epoch 28701, Training Loss: 29692, Validation Loss: 69497, L1: 69497\n",
      "Epoch 28801, Training Loss: 29799, Validation Loss: 69499, L1: 69499\n",
      "Epoch 28901, Training Loss: 29618, Validation Loss: 69611, L1: 69611\n",
      "Epoch 29001, Training Loss: 29572, Validation Loss: 69544, L1: 69544\n",
      "Epoch 29101, Training Loss: 29693, Validation Loss: 69739, L1: 69739\n",
      "Epoch 29201, Training Loss: 29781, Validation Loss: 69495, L1: 69495\n",
      "Epoch 29301, Training Loss: 29806, Validation Loss: 69747, L1: 69747\n",
      "Epoch 29401, Training Loss: 29428, Validation Loss: 69781, L1: 69781\n",
      "Epoch 29501, Training Loss: 29510, Validation Loss: 69868, L1: 69868\n",
      "Epoch 29601, Training Loss: 29373, Validation Loss: 69974, L1: 69974\n",
      "Epoch 29701, Training Loss: 29321, Validation Loss: 69903, L1: 69903\n",
      "Epoch 29801, Training Loss: 29264, Validation Loss: 70102, L1: 70102\n",
      "Epoch 29901, Training Loss: 29256, Validation Loss: 70084, L1: 70084\n",
      "Epoch 30001, Training Loss: 29343, Validation Loss: 70122, L1: 70122\n",
      "Epoch 30101, Training Loss: 29288, Validation Loss: 70095, L1: 70095\n",
      "Epoch 30201, Training Loss: 29124, Validation Loss: 70229, L1: 70229\n",
      "Epoch 30301, Training Loss: 29035, Validation Loss: 70242, L1: 70242\n",
      "Epoch 30401, Training Loss: 29254, Validation Loss: 70324, L1: 70324\n",
      "Epoch 30501, Training Loss: 29070, Validation Loss: 70336, L1: 70336\n",
      "Epoch 30601, Training Loss: 29180, Validation Loss: 70200, L1: 70200\n",
      "Epoch 30701, Training Loss: 28959, Validation Loss: 70168, L1: 70168\n",
      "Epoch 30801, Training Loss: 28895, Validation Loss: 70320, L1: 70320\n",
      "Epoch 30901, Training Loss: 28867, Validation Loss: 70519, L1: 70519\n",
      "Epoch 31001, Training Loss: 28848, Validation Loss: 70488, L1: 70488\n",
      "Epoch 31101, Training Loss: 28725, Validation Loss: 70579, L1: 70579\n",
      "Epoch 31201, Training Loss: 28752, Validation Loss: 70519, L1: 70519\n",
      "Epoch 31301, Training Loss: 28623, Validation Loss: 70743, L1: 70743\n",
      "Epoch 31401, Training Loss: 28920, Validation Loss: 70639, L1: 70639\n",
      "Epoch 31501, Training Loss: 28562, Validation Loss: 70732, L1: 70732\n",
      "Epoch 31601, Training Loss: 28524, Validation Loss: 70634, L1: 70634\n",
      "Epoch 31701, Training Loss: 28641, Validation Loss: 70760, L1: 70760\n",
      "Epoch 31801, Training Loss: 28447, Validation Loss: 70767, L1: 70767\n",
      "Epoch 31901, Training Loss: 28436, Validation Loss: 70930, L1: 70930\n",
      "Epoch 32001, Training Loss: 28429, Validation Loss: 70859, L1: 70859\n",
      "Epoch 32101, Training Loss: 28375, Validation Loss: 71079, L1: 71079\n",
      "Epoch 32201, Training Loss: 28295, Validation Loss: 71147, L1: 71147\n",
      "Epoch 32301, Training Loss: 28458, Validation Loss: 70839, L1: 70839\n",
      "Epoch 32401, Training Loss: 28422, Validation Loss: 71061, L1: 71061\n",
      "Epoch 32501, Training Loss: 28190, Validation Loss: 70915, L1: 70915\n",
      "Epoch 32601, Training Loss: 28164, Validation Loss: 70876, L1: 70876\n",
      "Epoch 32701, Training Loss: 28143, Validation Loss: 70843, L1: 70843\n",
      "Epoch 32801, Training Loss: 28388, Validation Loss: 70693, L1: 70693\n",
      "Epoch 32901, Training Loss: 28630, Validation Loss: 70919, L1: 70919\n",
      "Epoch 33001, Training Loss: 28155, Validation Loss: 70987, L1: 70987\n",
      "Epoch 33101, Training Loss: 28011, Validation Loss: 70975, L1: 70975\n",
      "Epoch 33201, Training Loss: 28289, Validation Loss: 70889, L1: 70889\n",
      "Epoch 33301, Training Loss: 27956, Validation Loss: 70905, L1: 70905\n",
      "Epoch 33401, Training Loss: 28089, Validation Loss: 70841, L1: 70841\n",
      "Epoch 33501, Training Loss: 27890, Validation Loss: 70732, L1: 70732\n",
      "Epoch 33601, Training Loss: 28101, Validation Loss: 70865, L1: 70865\n",
      "Epoch 33701, Training Loss: 27872, Validation Loss: 70805, L1: 70805\n",
      "Epoch 33801, Training Loss: 27800, Validation Loss: 70692, L1: 70692\n",
      "Epoch 33901, Training Loss: 27854, Validation Loss: 70557, L1: 70557\n",
      "Epoch 34001, Training Loss: 27735, Validation Loss: 70580, L1: 70580\n",
      "Epoch 34101, Training Loss: 27790, Validation Loss: 70625, L1: 70625\n",
      "Epoch 34201, Training Loss: 28344, Validation Loss: 70673, L1: 70673\n",
      "Epoch 34301, Training Loss: 27775, Validation Loss: 70408, L1: 70408\n",
      "Epoch 34401, Training Loss: 27825, Validation Loss: 70455, L1: 70455\n",
      "Epoch 34501, Training Loss: 27556, Validation Loss: 70333, L1: 70333\n",
      "Epoch 34601, Training Loss: 27633, Validation Loss: 70255, L1: 70255\n",
      "Epoch 34701, Training Loss: 27613, Validation Loss: 70292, L1: 70292\n",
      "Epoch 34801, Training Loss: 27568, Validation Loss: 70438, L1: 70438\n",
      "Epoch 34901, Training Loss: 27604, Validation Loss: 70223, L1: 70223\n",
      "Epoch 35001, Training Loss: 27504, Validation Loss: 70149, L1: 70149\n",
      "Epoch 35101, Training Loss: 27454, Validation Loss: 70199, L1: 70199\n",
      "Epoch 35201, Training Loss: 27393, Validation Loss: 70111, L1: 70111\n",
      "Epoch 35301, Training Loss: 27455, Validation Loss: 70149, L1: 70149\n",
      "Epoch 35401, Training Loss: 27288, Validation Loss: 69948, L1: 69948\n",
      "Epoch 35501, Training Loss: 27408, Validation Loss: 69964, L1: 69964\n",
      "Epoch 35601, Training Loss: 27159, Validation Loss: 69971, L1: 69971\n",
      "Epoch 35701, Training Loss: 27306, Validation Loss: 69940, L1: 69940\n",
      "Epoch 35801, Training Loss: 27358, Validation Loss: 69971, L1: 69971\n",
      "Epoch 35901, Training Loss: 27093, Validation Loss: 69817, L1: 69817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36001, Training Loss: 27264, Validation Loss: 69955, L1: 69955\n",
      "Epoch 36101, Training Loss: 27297, Validation Loss: 69802, L1: 69802\n",
      "Epoch 36201, Training Loss: 27257, Validation Loss: 69934, L1: 69934\n",
      "Epoch 36301, Training Loss: 27373, Validation Loss: 69692, L1: 69692\n",
      "Epoch 36401, Training Loss: 27069, Validation Loss: 69684, L1: 69684\n",
      "Epoch 36501, Training Loss: 27314, Validation Loss: 69781, L1: 69781\n",
      "Epoch 36601, Training Loss: 27435, Validation Loss: 69721, L1: 69721\n",
      "Epoch 36701, Training Loss: 26942, Validation Loss: 69879, L1: 69879\n",
      "Epoch 36801, Training Loss: 26971, Validation Loss: 69770, L1: 69770\n",
      "Epoch 36901, Training Loss: 26939, Validation Loss: 69640, L1: 69640\n",
      "Epoch 37001, Training Loss: 27131, Validation Loss: 69790, L1: 69790\n",
      "Epoch 37101, Training Loss: 27023, Validation Loss: 69740, L1: 69740\n",
      "Epoch 37201, Training Loss: 26839, Validation Loss: 69648, L1: 69648\n",
      "Epoch 37301, Training Loss: 26836, Validation Loss: 69758, L1: 69758\n",
      "Epoch 37401, Training Loss: 26800, Validation Loss: 69785, L1: 69785\n",
      "Epoch 37501, Training Loss: 26953, Validation Loss: 69842, L1: 69842\n",
      "Epoch 37601, Training Loss: 26607, Validation Loss: 69746, L1: 69746\n",
      "Epoch 37701, Training Loss: 26841, Validation Loss: 69764, L1: 69764\n",
      "Epoch 37801, Training Loss: 26630, Validation Loss: 69738, L1: 69738\n",
      "Epoch 37901, Training Loss: 26889, Validation Loss: 69651, L1: 69651\n",
      "Epoch 38001, Training Loss: 27086, Validation Loss: 69552, L1: 69552\n",
      "Epoch 38101, Training Loss: 26480, Validation Loss: 69734, L1: 69734\n",
      "Epoch 38201, Training Loss: 26641, Validation Loss: 69654, L1: 69654\n",
      "Epoch 38301, Training Loss: 26849, Validation Loss: 69779, L1: 69779\n",
      "Epoch 38401, Training Loss: 26456, Validation Loss: 69663, L1: 69663\n",
      "Epoch 38501, Training Loss: 26351, Validation Loss: 69573, L1: 69573\n",
      "Epoch 38601, Training Loss: 26309, Validation Loss: 69533, L1: 69533\n",
      "Epoch 38701, Training Loss: 26445, Validation Loss: 69874, L1: 69874\n",
      "Epoch 38801, Training Loss: 26606, Validation Loss: 69830, L1: 69830\n",
      "Epoch 38901, Training Loss: 26223, Validation Loss: 69632, L1: 69632\n",
      "Epoch 39001, Training Loss: 26190, Validation Loss: 69579, L1: 69579\n",
      "Epoch 39101, Training Loss: 26175, Validation Loss: 69492, L1: 69492\n",
      "Epoch 39201, Training Loss: 26123, Validation Loss: 69547, L1: 69547\n",
      "Epoch 39301, Training Loss: 26504, Validation Loss: 69724, L1: 69724\n",
      "Epoch 39401, Training Loss: 26109, Validation Loss: 69493, L1: 69493\n",
      "Epoch 39501, Training Loss: 26507, Validation Loss: 69533, L1: 69533\n",
      "Epoch 39601, Training Loss: 26169, Validation Loss: 69450, L1: 69450\n",
      "Epoch 39701, Training Loss: 26351, Validation Loss: 69779, L1: 69779\n",
      "Epoch 39801, Training Loss: 26332, Validation Loss: 69579, L1: 69579\n",
      "Epoch 39901, Training Loss: 26476, Validation Loss: 69632, L1: 69632\n",
      "Epoch 40001, Training Loss: 26031, Validation Loss: 69472, L1: 69472\n",
      "Epoch 40101, Training Loss: 25949, Validation Loss: 69424, L1: 69424\n",
      "Epoch 40201, Training Loss: 25943, Validation Loss: 69639, L1: 69639\n",
      "Epoch 40301, Training Loss: 26090, Validation Loss: 69455, L1: 69455\n",
      "Epoch 40401, Training Loss: 25974, Validation Loss: 69346, L1: 69346\n",
      "Epoch 40501, Training Loss: 26067, Validation Loss: 69626, L1: 69626\n",
      "Epoch 40601, Training Loss: 25875, Validation Loss: 69576, L1: 69576\n",
      "Epoch 40701, Training Loss: 25909, Validation Loss: 69451, L1: 69451\n",
      "Epoch 40801, Training Loss: 25866, Validation Loss: 69532, L1: 69532\n",
      "Epoch 40901, Training Loss: 25787, Validation Loss: 69513, L1: 69513\n",
      "Epoch 41001, Training Loss: 25754, Validation Loss: 69516, L1: 69516\n",
      "Epoch 41101, Training Loss: 25801, Validation Loss: 69564, L1: 69564\n",
      "Epoch 41201, Training Loss: 25691, Validation Loss: 69557, L1: 69557\n",
      "Epoch 41301, Training Loss: 25570, Validation Loss: 69498, L1: 69498\n",
      "Epoch 41401, Training Loss: 25764, Validation Loss: 69596, L1: 69596\n",
      "Epoch 41501, Training Loss: 25553, Validation Loss: 69481, L1: 69481\n",
      "Epoch 41601, Training Loss: 25678, Validation Loss: 69432, L1: 69432\n",
      "Epoch 41701, Training Loss: 25752, Validation Loss: 69578, L1: 69578\n",
      "Epoch 41801, Training Loss: 25690, Validation Loss: 69440, L1: 69440\n",
      "Epoch 41901, Training Loss: 26167, Validation Loss: 69189, L1: 69189\n",
      "Epoch 42001, Training Loss: 25398, Validation Loss: 69385, L1: 69385\n",
      "Epoch 42101, Training Loss: 25669, Validation Loss: 69378, L1: 69378\n",
      "Epoch 42201, Training Loss: 26891, Validation Loss: 69169, L1: 69169\n",
      "Epoch 42301, Training Loss: 25409, Validation Loss: 69350, L1: 69350\n",
      "Epoch 42401, Training Loss: 25360, Validation Loss: 69422, L1: 69422\n",
      "Epoch 42501, Training Loss: 25692, Validation Loss: 69398, L1: 69398\n",
      "Epoch 42601, Training Loss: 25760, Validation Loss: 69284, L1: 69284\n",
      "Epoch 42701, Training Loss: 25567, Validation Loss: 69411, L1: 69411\n",
      "Epoch 42801, Training Loss: 25259, Validation Loss: 69256, L1: 69256\n",
      "Epoch 42901, Training Loss: 25191, Validation Loss: 69287, L1: 69287\n",
      "Epoch 43001, Training Loss: 25351, Validation Loss: 69233, L1: 69233\n",
      "Epoch 43101, Training Loss: 25203, Validation Loss: 69225, L1: 69225\n",
      "Epoch 43201, Training Loss: 25403, Validation Loss: 69436, L1: 69436\n",
      "Epoch 43301, Training Loss: 25255, Validation Loss: 69317, L1: 69317\n",
      "Epoch 43401, Training Loss: 25192, Validation Loss: 69206, L1: 69206\n",
      "Epoch 43501, Training Loss: 25097, Validation Loss: 69302, L1: 69302\n",
      "Epoch 43601, Training Loss: 25305, Validation Loss: 69025, L1: 69025\n",
      "Epoch 43701, Training Loss: 25722, Validation Loss: 68914, L1: 68914\n",
      "Epoch 43801, Training Loss: 25241, Validation Loss: 69185, L1: 69185\n",
      "Epoch 43901, Training Loss: 25278, Validation Loss: 69145, L1: 69145\n",
      "Epoch 44001, Training Loss: 25136, Validation Loss: 69295, L1: 69295\n",
      "Epoch 44101, Training Loss: 25068, Validation Loss: 69248, L1: 69248\n",
      "Epoch 44201, Training Loss: 25238, Validation Loss: 69148, L1: 69148\n",
      "Epoch 44301, Training Loss: 25043, Validation Loss: 69230, L1: 69230\n",
      "Epoch 44401, Training Loss: 25029, Validation Loss: 69066, L1: 69066\n",
      "Epoch 44501, Training Loss: 24989, Validation Loss: 69384, L1: 69384\n",
      "Epoch 44601, Training Loss: 24930, Validation Loss: 69198, L1: 69198\n",
      "Epoch 44701, Training Loss: 24913, Validation Loss: 69099, L1: 69099\n",
      "Epoch 44801, Training Loss: 25405, Validation Loss: 69118, L1: 69118\n",
      "Epoch 44901, Training Loss: 24912, Validation Loss: 69183, L1: 69183\n",
      "Epoch 45001, Training Loss: 24815, Validation Loss: 69253, L1: 69253\n",
      "Epoch 45101, Training Loss: 24965, Validation Loss: 69211, L1: 69211\n",
      "Epoch 45201, Training Loss: 24830, Validation Loss: 69062, L1: 69062\n",
      "Epoch 45301, Training Loss: 24977, Validation Loss: 69165, L1: 69165\n",
      "Epoch 45401, Training Loss: 24800, Validation Loss: 69137, L1: 69137\n",
      "Epoch 45501, Training Loss: 24707, Validation Loss: 69153, L1: 69153\n",
      "Epoch 45601, Training Loss: 24861, Validation Loss: 69167, L1: 69167\n",
      "Epoch 45701, Training Loss: 24912, Validation Loss: 69107, L1: 69107\n",
      "Epoch 45801, Training Loss: 24890, Validation Loss: 69129, L1: 69129\n",
      "Epoch 45901, Training Loss: 25402, Validation Loss: 68956, L1: 68956\n",
      "Epoch 46001, Training Loss: 25064, Validation Loss: 69268, L1: 69268\n",
      "Epoch 46101, Training Loss: 24622, Validation Loss: 69099, L1: 69099\n",
      "Epoch 46201, Training Loss: 24820, Validation Loss: 69256, L1: 69256\n",
      "Epoch 46301, Training Loss: 24549, Validation Loss: 69144, L1: 69144\n",
      "Epoch 46401, Training Loss: 24826, Validation Loss: 68993, L1: 68993\n",
      "Epoch 46501, Training Loss: 24607, Validation Loss: 69088, L1: 69088\n",
      "Epoch 46601, Training Loss: 25059, Validation Loss: 68909, L1: 68909\n",
      "Epoch 46701, Training Loss: 24856, Validation Loss: 69081, L1: 69081\n",
      "Epoch 46801, Training Loss: 24650, Validation Loss: 69192, L1: 69192\n",
      "Epoch 46901, Training Loss: 24521, Validation Loss: 69024, L1: 69024\n",
      "Epoch 47001, Training Loss: 24504, Validation Loss: 69006, L1: 69006\n",
      "Epoch 47101, Training Loss: 24412, Validation Loss: 69045, L1: 69045\n",
      "Epoch 47201, Training Loss: 24517, Validation Loss: 69200, L1: 69200\n",
      "Epoch 47301, Training Loss: 24815, Validation Loss: 68872, L1: 68872\n",
      "Epoch 47401, Training Loss: 24335, Validation Loss: 69138, L1: 69138\n",
      "Epoch 47501, Training Loss: 24464, Validation Loss: 69059, L1: 69059\n",
      "Epoch 47601, Training Loss: 24361, Validation Loss: 69002, L1: 69002\n",
      "Epoch 47701, Training Loss: 24679, Validation Loss: 69342, L1: 69342\n",
      "Epoch 47801, Training Loss: 24325, Validation Loss: 69178, L1: 69178\n",
      "Epoch 47901, Training Loss: 24242, Validation Loss: 69179, L1: 69179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48001, Training Loss: 24255, Validation Loss: 69175, L1: 69175\n",
      "Epoch 48101, Training Loss: 24536, Validation Loss: 69253, L1: 69253\n",
      "Epoch 48201, Training Loss: 24360, Validation Loss: 69256, L1: 69256\n",
      "Epoch 48301, Training Loss: 24366, Validation Loss: 69012, L1: 69012\n",
      "Epoch 48401, Training Loss: 24802, Validation Loss: 68985, L1: 68985\n",
      "Epoch 48501, Training Loss: 24252, Validation Loss: 69176, L1: 69176\n",
      "Epoch 48601, Training Loss: 24252, Validation Loss: 68967, L1: 68967\n",
      "Epoch 48701, Training Loss: 24084, Validation Loss: 69181, L1: 69181\n",
      "Epoch 48801, Training Loss: 24256, Validation Loss: 69132, L1: 69132\n",
      "Epoch 48901, Training Loss: 24014, Validation Loss: 69088, L1: 69088\n",
      "Epoch 49001, Training Loss: 24097, Validation Loss: 69189, L1: 69189\n",
      "Epoch 49101, Training Loss: 23988, Validation Loss: 69108, L1: 69108\n",
      "Epoch 49201, Training Loss: 24336, Validation Loss: 69331, L1: 69331\n",
      "Epoch 49301, Training Loss: 24300, Validation Loss: 69157, L1: 69157\n",
      "Epoch 49401, Training Loss: 23941, Validation Loss: 69103, L1: 69103\n",
      "Epoch 49501, Training Loss: 24447, Validation Loss: 68970, L1: 68970\n",
      "Epoch 49601, Training Loss: 24065, Validation Loss: 69169, L1: 69169\n",
      "Epoch 49701, Training Loss: 23841, Validation Loss: 69147, L1: 69147\n",
      "Epoch 49801, Training Loss: 23823, Validation Loss: 69340, L1: 69340\n",
      "Epoch 49901, Training Loss: 23999, Validation Loss: 69183, L1: 69183\n",
      "Epoch 50001, Training Loss: 23965, Validation Loss: 69221, L1: 69221\n",
      "Epoch 50101, Training Loss: 24019, Validation Loss: 69060, L1: 69060\n",
      "Epoch 50201, Training Loss: 23801, Validation Loss: 69311, L1: 69311\n",
      "Epoch 50301, Training Loss: 23776, Validation Loss: 69282, L1: 69282\n",
      "Epoch 50401, Training Loss: 23975, Validation Loss: 69360, L1: 69360\n",
      "Epoch 50501, Training Loss: 23761, Validation Loss: 69041, L1: 69041\n",
      "Epoch 50601, Training Loss: 23947, Validation Loss: 69352, L1: 69352\n",
      "Epoch 50701, Training Loss: 23929, Validation Loss: 69351, L1: 69351\n",
      "Epoch 50801, Training Loss: 23659, Validation Loss: 69355, L1: 69355\n",
      "Epoch 50901, Training Loss: 23748, Validation Loss: 69299, L1: 69299\n",
      "Epoch 51001, Training Loss: 23549, Validation Loss: 69217, L1: 69217\n",
      "Epoch 51101, Training Loss: 23661, Validation Loss: 69168, L1: 69168\n",
      "Epoch 51201, Training Loss: 23734, Validation Loss: 69266, L1: 69266\n",
      "Epoch 51301, Training Loss: 23883, Validation Loss: 69269, L1: 69269\n",
      "Epoch 51401, Training Loss: 23641, Validation Loss: 69327, L1: 69327\n",
      "Epoch 51501, Training Loss: 23400, Validation Loss: 69375, L1: 69375\n",
      "Epoch 51601, Training Loss: 23544, Validation Loss: 69208, L1: 69208\n",
      "Epoch 51701, Training Loss: 23415, Validation Loss: 69432, L1: 69432\n",
      "Epoch 51801, Training Loss: 23556, Validation Loss: 69359, L1: 69359\n",
      "Epoch 51901, Training Loss: 23448, Validation Loss: 69343, L1: 69343\n",
      "Epoch 52001, Training Loss: 23684, Validation Loss: 69393, L1: 69393\n",
      "Epoch 52101, Training Loss: 23581, Validation Loss: 69316, L1: 69316\n",
      "Epoch 52201, Training Loss: 23577, Validation Loss: 69254, L1: 69254\n",
      "Epoch 52301, Training Loss: 23322, Validation Loss: 69257, L1: 69257\n",
      "Epoch 52401, Training Loss: 23321, Validation Loss: 69334, L1: 69334\n",
      "Epoch 52501, Training Loss: 23190, Validation Loss: 69294, L1: 69294\n",
      "Epoch 52601, Training Loss: 23257, Validation Loss: 69344, L1: 69344\n",
      "Epoch 52701, Training Loss: 23729, Validation Loss: 69847, L1: 69847\n",
      "Epoch 52801, Training Loss: 23433, Validation Loss: 69274, L1: 69274\n",
      "Epoch 52901, Training Loss: 23651, Validation Loss: 69510, L1: 69510\n",
      "Epoch 53001, Training Loss: 23140, Validation Loss: 69501, L1: 69501\n",
      "Epoch 53101, Training Loss: 23280, Validation Loss: 69536, L1: 69536\n",
      "Epoch 53201, Training Loss: 23161, Validation Loss: 69475, L1: 69475\n",
      "Epoch 53301, Training Loss: 23256, Validation Loss: 69330, L1: 69330\n",
      "Epoch 53401, Training Loss: 23044, Validation Loss: 69367, L1: 69367\n",
      "Epoch 53501, Training Loss: 23040, Validation Loss: 69565, L1: 69565\n",
      "Epoch 53601, Training Loss: 22874, Validation Loss: 69427, L1: 69427\n",
      "Epoch 53701, Training Loss: 23109, Validation Loss: 69504, L1: 69504\n",
      "Epoch 53801, Training Loss: 23037, Validation Loss: 69453, L1: 69453\n",
      "Epoch 53901, Training Loss: 23098, Validation Loss: 69392, L1: 69392\n",
      "Epoch 54001, Training Loss: 23222, Validation Loss: 69606, L1: 69606\n",
      "Epoch 54101, Training Loss: 23004, Validation Loss: 69446, L1: 69446\n",
      "Epoch 54201, Training Loss: 22908, Validation Loss: 69365, L1: 69365\n",
      "Epoch 54301, Training Loss: 22912, Validation Loss: 69370, L1: 69370\n",
      "Epoch 54401, Training Loss: 22847, Validation Loss: 69483, L1: 69483\n",
      "Epoch 54501, Training Loss: 22757, Validation Loss: 69365, L1: 69365\n",
      "Epoch 54601, Training Loss: 22798, Validation Loss: 69422, L1: 69422\n",
      "Epoch 54701, Training Loss: 22842, Validation Loss: 69525, L1: 69525\n",
      "Epoch 54801, Training Loss: 22758, Validation Loss: 69509, L1: 69509\n",
      "Epoch 54901, Training Loss: 22850, Validation Loss: 69550, L1: 69550\n",
      "Epoch 55001, Training Loss: 22752, Validation Loss: 69385, L1: 69385\n",
      "Epoch 55101, Training Loss: 22779, Validation Loss: 69487, L1: 69487\n",
      "Epoch 55201, Training Loss: 22765, Validation Loss: 69583, L1: 69583\n",
      "Epoch 55301, Training Loss: 22750, Validation Loss: 69580, L1: 69580\n",
      "Epoch 55401, Training Loss: 22475, Validation Loss: 69630, L1: 69630\n",
      "Epoch 55501, Training Loss: 22418, Validation Loss: 69419, L1: 69419\n",
      "Epoch 55601, Training Loss: 22643, Validation Loss: 69519, L1: 69519\n",
      "Epoch 55701, Training Loss: 22430, Validation Loss: 69498, L1: 69498\n",
      "Epoch 55801, Training Loss: 22731, Validation Loss: 69442, L1: 69442\n",
      "Epoch 55901, Training Loss: 22581, Validation Loss: 69407, L1: 69407\n",
      "Epoch 56001, Training Loss: 22596, Validation Loss: 69608, L1: 69608\n",
      "Epoch 56101, Training Loss: 22323, Validation Loss: 69684, L1: 69684\n",
      "Epoch 56201, Training Loss: 22341, Validation Loss: 69377, L1: 69377\n",
      "Epoch 56301, Training Loss: 22275, Validation Loss: 69378, L1: 69378\n",
      "Epoch 56401, Training Loss: 22260, Validation Loss: 69528, L1: 69528\n",
      "Epoch 56501, Training Loss: 22191, Validation Loss: 69454, L1: 69454\n",
      "Epoch 56601, Training Loss: 22273, Validation Loss: 69505, L1: 69505\n",
      "Epoch 56701, Training Loss: 22371, Validation Loss: 69606, L1: 69606\n",
      "Epoch 56801, Training Loss: 22409, Validation Loss: 69746, L1: 69746\n",
      "Epoch 56901, Training Loss: 22072, Validation Loss: 69415, L1: 69415\n",
      "Epoch 57001, Training Loss: 22243, Validation Loss: 69672, L1: 69672\n",
      "Epoch 57101, Training Loss: 22120, Validation Loss: 69721, L1: 69721\n",
      "Epoch 57201, Training Loss: 22133, Validation Loss: 69745, L1: 69745\n",
      "Epoch 57301, Training Loss: 22267, Validation Loss: 69631, L1: 69631\n",
      "Epoch 57401, Training Loss: 22028, Validation Loss: 69656, L1: 69656\n",
      "Epoch 57501, Training Loss: 22494, Validation Loss: 69982, L1: 69982\n",
      "Epoch 57601, Training Loss: 21869, Validation Loss: 69512, L1: 69512\n",
      "Epoch 57701, Training Loss: 22335, Validation Loss: 69910, L1: 69910\n",
      "Epoch 57801, Training Loss: 21809, Validation Loss: 69809, L1: 69809\n",
      "Epoch 57901, Training Loss: 21951, Validation Loss: 69528, L1: 69528\n",
      "Epoch 58001, Training Loss: 21771, Validation Loss: 69504, L1: 69504\n",
      "Epoch 58101, Training Loss: 21765, Validation Loss: 69746, L1: 69746\n",
      "Epoch 58201, Training Loss: 21923, Validation Loss: 69776, L1: 69776\n",
      "Epoch 58301, Training Loss: 21821, Validation Loss: 69645, L1: 69645\n",
      "Epoch 58401, Training Loss: 22345, Validation Loss: 70087, L1: 70087\n",
      "Epoch 58501, Training Loss: 21680, Validation Loss: 69448, L1: 69448\n",
      "Epoch 58601, Training Loss: 21674, Validation Loss: 69796, L1: 69796\n",
      "Epoch 58701, Training Loss: 21936, Validation Loss: 69655, L1: 69655\n",
      "Epoch 58801, Training Loss: 21533, Validation Loss: 69745, L1: 69745\n",
      "Epoch 58901, Training Loss: 21560, Validation Loss: 69872, L1: 69872\n",
      "Epoch 59001, Training Loss: 21788, Validation Loss: 69820, L1: 69820\n",
      "Epoch 59101, Training Loss: 21481, Validation Loss: 69927, L1: 69927\n",
      "Epoch 59201, Training Loss: 21493, Validation Loss: 69885, L1: 69885\n",
      "Epoch 59301, Training Loss: 21546, Validation Loss: 69808, L1: 69808\n",
      "Epoch 59401, Training Loss: 21489, Validation Loss: 69823, L1: 69823\n",
      "Epoch 59501, Training Loss: 21346, Validation Loss: 70109, L1: 70109\n",
      "Epoch 59601, Training Loss: 21352, Validation Loss: 69865, L1: 69865\n",
      "Epoch 59701, Training Loss: 21276, Validation Loss: 69870, L1: 69870\n",
      "Epoch 59801, Training Loss: 21258, Validation Loss: 70168, L1: 70168\n",
      "Epoch 59901, Training Loss: 21411, Validation Loss: 70057, L1: 70057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60001, Training Loss: 21186, Validation Loss: 69939, L1: 69939\n",
      "Epoch 60101, Training Loss: 21441, Validation Loss: 69849, L1: 69849\n",
      "Epoch 60201, Training Loss: 21755, Validation Loss: 70054, L1: 70054\n",
      "Epoch 60301, Training Loss: 21174, Validation Loss: 70217, L1: 70217\n",
      "Epoch 60401, Training Loss: 21128, Validation Loss: 69901, L1: 69901\n",
      "Epoch 60501, Training Loss: 21411, Validation Loss: 70087, L1: 70087\n",
      "Epoch 60601, Training Loss: 21075, Validation Loss: 69996, L1: 69996\n",
      "Epoch 60701, Training Loss: 21205, Validation Loss: 70185, L1: 70185\n",
      "Epoch 60801, Training Loss: 21858, Validation Loss: 70110, L1: 70110\n",
      "Epoch 60901, Training Loss: 21046, Validation Loss: 70247, L1: 70247\n",
      "Epoch 61001, Training Loss: 20977, Validation Loss: 70087, L1: 70087\n",
      "Epoch 61101, Training Loss: 20905, Validation Loss: 70223, L1: 70223\n",
      "Epoch 61201, Training Loss: 21054, Validation Loss: 70141, L1: 70141\n",
      "Epoch 61301, Training Loss: 20869, Validation Loss: 70329, L1: 70329\n",
      "Epoch 61401, Training Loss: 21168, Validation Loss: 69884, L1: 69884\n",
      "Epoch 61501, Training Loss: 20825, Validation Loss: 70176, L1: 70176\n",
      "Epoch 61601, Training Loss: 20796, Validation Loss: 70254, L1: 70254\n",
      "Epoch 61701, Training Loss: 21083, Validation Loss: 70267, L1: 70267\n",
      "Epoch 61801, Training Loss: 21239, Validation Loss: 70572, L1: 70572\n",
      "Epoch 61901, Training Loss: 20872, Validation Loss: 70428, L1: 70428\n",
      "Epoch 62001, Training Loss: 21093, Validation Loss: 70634, L1: 70634\n",
      "Epoch 62101, Training Loss: 20619, Validation Loss: 70217, L1: 70217\n",
      "Epoch 62201, Training Loss: 20734, Validation Loss: 70444, L1: 70444\n",
      "Epoch 62301, Training Loss: 20788, Validation Loss: 70393, L1: 70393\n",
      "Epoch 62401, Training Loss: 21039, Validation Loss: 70225, L1: 70225\n",
      "Epoch 62501, Training Loss: 20506, Validation Loss: 70322, L1: 70322\n",
      "Epoch 62601, Training Loss: 20564, Validation Loss: 70657, L1: 70657\n",
      "Epoch 62701, Training Loss: 21017, Validation Loss: 70708, L1: 70708\n",
      "Epoch 62801, Training Loss: 20660, Validation Loss: 70517, L1: 70517\n",
      "Epoch 62901, Training Loss: 20475, Validation Loss: 70472, L1: 70472\n",
      "Epoch 63001, Training Loss: 20926, Validation Loss: 70599, L1: 70599\n",
      "Epoch 63101, Training Loss: 20559, Validation Loss: 70842, L1: 70842\n",
      "Epoch 63201, Training Loss: 20567, Validation Loss: 70624, L1: 70624\n",
      "Epoch 63301, Training Loss: 20347, Validation Loss: 70533, L1: 70533\n",
      "Epoch 63401, Training Loss: 20435, Validation Loss: 70725, L1: 70725\n",
      "Epoch 63501, Training Loss: 20218, Validation Loss: 70500, L1: 70500\n",
      "Epoch 63601, Training Loss: 20422, Validation Loss: 70849, L1: 70849\n",
      "Epoch 63701, Training Loss: 20426, Validation Loss: 70875, L1: 70875\n",
      "Epoch 63801, Training Loss: 20918, Validation Loss: 70283, L1: 70283\n",
      "Epoch 63901, Training Loss: 20204, Validation Loss: 71134, L1: 71134\n",
      "Epoch 64001, Training Loss: 20147, Validation Loss: 70474, L1: 70474\n",
      "Epoch 64101, Training Loss: 20287, Validation Loss: 70659, L1: 70659\n",
      "Epoch 64201, Training Loss: 20152, Validation Loss: 70780, L1: 70780\n",
      "Epoch 64301, Training Loss: 20050, Validation Loss: 70819, L1: 70819\n",
      "Epoch 64401, Training Loss: 20746, Validation Loss: 70572, L1: 70572\n",
      "Epoch 64501, Training Loss: 20063, Validation Loss: 70808, L1: 70808\n",
      "Epoch 64601, Training Loss: 20087, Validation Loss: 70757, L1: 70757\n",
      "Epoch 64701, Training Loss: 19938, Validation Loss: 70670, L1: 70670\n",
      "Epoch 64801, Training Loss: 19995, Validation Loss: 70528, L1: 70528\n",
      "Epoch 64901, Training Loss: 19992, Validation Loss: 70520, L1: 70520\n",
      "Epoch 65001, Training Loss: 19950, Validation Loss: 70710, L1: 70710\n",
      "Epoch 65101, Training Loss: 19836, Validation Loss: 70579, L1: 70579\n",
      "Epoch 65201, Training Loss: 20206, Validation Loss: 70302, L1: 70302\n",
      "Epoch 65301, Training Loss: 20338, Validation Loss: 70408, L1: 70408\n",
      "Epoch 65401, Training Loss: 19759, Validation Loss: 70372, L1: 70372\n",
      "Epoch 65501, Training Loss: 19635, Validation Loss: 70739, L1: 70739\n",
      "Epoch 65601, Training Loss: 20041, Validation Loss: 70817, L1: 70817\n",
      "Epoch 65701, Training Loss: 19652, Validation Loss: 70647, L1: 70647\n",
      "Epoch 65801, Training Loss: 19929, Validation Loss: 70595, L1: 70595\n",
      "Epoch 65901, Training Loss: 20322, Validation Loss: 70158, L1: 70158\n",
      "Epoch 66001, Training Loss: 19990, Validation Loss: 70909, L1: 70909\n",
      "Epoch 66101, Training Loss: 19703, Validation Loss: 70813, L1: 70813\n",
      "Epoch 66201, Training Loss: 19866, Validation Loss: 70511, L1: 70511\n",
      "Epoch 66301, Training Loss: 19424, Validation Loss: 70691, L1: 70691\n",
      "Epoch 66401, Training Loss: 19548, Validation Loss: 70514, L1: 70514\n",
      "Epoch 66501, Training Loss: 19335, Validation Loss: 70608, L1: 70608\n",
      "Epoch 66601, Training Loss: 19444, Validation Loss: 70604, L1: 70604\n",
      "Epoch 66701, Training Loss: 19380, Validation Loss: 70561, L1: 70561\n",
      "Epoch 66801, Training Loss: 19243, Validation Loss: 70516, L1: 70516\n",
      "Epoch 66901, Training Loss: 19396, Validation Loss: 70576, L1: 70576\n",
      "Epoch 67001, Training Loss: 19433, Validation Loss: 70809, L1: 70809\n",
      "Epoch 67101, Training Loss: 19159, Validation Loss: 70759, L1: 70759\n",
      "Epoch 67201, Training Loss: 19342, Validation Loss: 70600, L1: 70600\n",
      "Epoch 67301, Training Loss: 19073, Validation Loss: 70529, L1: 70529\n",
      "Epoch 67401, Training Loss: 19089, Validation Loss: 70263, L1: 70263\n",
      "Epoch 67501, Training Loss: 19211, Validation Loss: 70531, L1: 70531\n",
      "Epoch 67601, Training Loss: 19180, Validation Loss: 70125, L1: 70125\n",
      "Epoch 67701, Training Loss: 19006, Validation Loss: 70600, L1: 70600\n",
      "Epoch 67801, Training Loss: 19304, Validation Loss: 70223, L1: 70223\n",
      "Epoch 67901, Training Loss: 19332, Validation Loss: 70672, L1: 70672\n",
      "Epoch 68001, Training Loss: 19059, Validation Loss: 70539, L1: 70539\n",
      "Epoch 68101, Training Loss: 19249, Validation Loss: 70180, L1: 70180\n",
      "Epoch 68201, Training Loss: 18950, Validation Loss: 70293, L1: 70293\n",
      "Epoch 68301, Training Loss: 18771, Validation Loss: 70347, L1: 70347\n",
      "Epoch 68401, Training Loss: 18738, Validation Loss: 70359, L1: 70359\n",
      "Epoch 68501, Training Loss: 18845, Validation Loss: 70204, L1: 70204\n",
      "Epoch 68601, Training Loss: 18787, Validation Loss: 70117, L1: 70117\n",
      "Epoch 68701, Training Loss: 18642, Validation Loss: 70215, L1: 70215\n",
      "Epoch 68801, Training Loss: 18596, Validation Loss: 70081, L1: 70081\n",
      "Epoch 68901, Training Loss: 18641, Validation Loss: 70228, L1: 70228\n",
      "Epoch 69001, Training Loss: 18552, Validation Loss: 70271, L1: 70271\n",
      "Epoch 69101, Training Loss: 18613, Validation Loss: 70037, L1: 70037\n",
      "Epoch 69201, Training Loss: 18636, Validation Loss: 70440, L1: 70440\n",
      "Epoch 69301, Training Loss: 18696, Validation Loss: 70197, L1: 70197\n",
      "Epoch 69401, Training Loss: 18781, Validation Loss: 70219, L1: 70219\n",
      "Epoch 69501, Training Loss: 18516, Validation Loss: 70084, L1: 70084\n",
      "Epoch 69601, Training Loss: 18522, Validation Loss: 70043, L1: 70043\n",
      "Epoch 69701, Training Loss: 18470, Validation Loss: 70216, L1: 70216\n",
      "Epoch 69801, Training Loss: 18698, Validation Loss: 70090, L1: 70090\n",
      "Epoch 69901, Training Loss: 18539, Validation Loss: 69714, L1: 69714\n",
      "Epoch 70001, Training Loss: 18576, Validation Loss: 69936, L1: 69936\n",
      "Epoch 70101, Training Loss: 18254, Validation Loss: 69745, L1: 69745\n",
      "Epoch 70201, Training Loss: 18317, Validation Loss: 69654, L1: 69654\n",
      "Epoch 70301, Training Loss: 18217, Validation Loss: 69956, L1: 69956\n",
      "Epoch 70401, Training Loss: 18572, Validation Loss: 69459, L1: 69459\n",
      "Epoch 70501, Training Loss: 18116, Validation Loss: 69785, L1: 69785\n",
      "Epoch 70601, Training Loss: 18184, Validation Loss: 69525, L1: 69525\n",
      "Epoch 70701, Training Loss: 18398, Validation Loss: 69423, L1: 69423\n",
      "Epoch 70801, Training Loss: 18152, Validation Loss: 69340, L1: 69340\n",
      "Epoch 70901, Training Loss: 18002, Validation Loss: 69342, L1: 69342\n",
      "Epoch 71001, Training Loss: 18074, Validation Loss: 69651, L1: 69651\n",
      "Epoch 71101, Training Loss: 18063, Validation Loss: 69898, L1: 69898\n",
      "Epoch 71201, Training Loss: 18120, Validation Loss: 69334, L1: 69334\n",
      "Epoch 71301, Training Loss: 17928, Validation Loss: 69247, L1: 69247\n",
      "Epoch 71401, Training Loss: 17934, Validation Loss: 69189, L1: 69189\n",
      "Epoch 71501, Training Loss: 17851, Validation Loss: 69163, L1: 69163\n",
      "Epoch 71601, Training Loss: 17791, Validation Loss: 69365, L1: 69365\n",
      "Epoch 71701, Training Loss: 17753, Validation Loss: 68811, L1: 68811\n",
      "Epoch 71801, Training Loss: 17925, Validation Loss: 68902, L1: 68902\n",
      "Epoch 71901, Training Loss: 17864, Validation Loss: 68994, L1: 68994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72001, Training Loss: 17627, Validation Loss: 69048, L1: 69048\n",
      "Epoch 72101, Training Loss: 17992, Validation Loss: 69248, L1: 69248\n",
      "Epoch 72201, Training Loss: 17677, Validation Loss: 69098, L1: 69098\n",
      "Epoch 72301, Training Loss: 17875, Validation Loss: 69089, L1: 69089\n",
      "Epoch 72401, Training Loss: 17538, Validation Loss: 69030, L1: 69030\n",
      "Epoch 72501, Training Loss: 17428, Validation Loss: 69055, L1: 69055\n",
      "Epoch 72601, Training Loss: 17798, Validation Loss: 69238, L1: 69238\n",
      "Epoch 72701, Training Loss: 17621, Validation Loss: 69157, L1: 69157\n",
      "Epoch 72801, Training Loss: 17802, Validation Loss: 69204, L1: 69204\n",
      "Epoch 72901, Training Loss: 17813, Validation Loss: 69001, L1: 69001\n",
      "Epoch 73001, Training Loss: 17606, Validation Loss: 68802, L1: 68802\n",
      "Epoch 73101, Training Loss: 17379, Validation Loss: 68894, L1: 68894\n",
      "Epoch 73201, Training Loss: 17258, Validation Loss: 68570, L1: 68570\n",
      "Epoch 73301, Training Loss: 17709, Validation Loss: 69061, L1: 69061\n",
      "Epoch 73401, Training Loss: 17188, Validation Loss: 68637, L1: 68637\n",
      "Epoch 73501, Training Loss: 17151, Validation Loss: 68744, L1: 68744\n",
      "Epoch 73601, Training Loss: 17469, Validation Loss: 68980, L1: 68980\n",
      "Epoch 73701, Training Loss: 17110, Validation Loss: 68698, L1: 68698\n",
      "Epoch 73801, Training Loss: 17290, Validation Loss: 68948, L1: 68948\n",
      "Epoch 73901, Training Loss: 17094, Validation Loss: 68494, L1: 68494\n",
      "Epoch 74001, Training Loss: 17080, Validation Loss: 68518, L1: 68518\n",
      "Epoch 74101, Training Loss: 17607, Validation Loss: 68715, L1: 68715\n",
      "Epoch 74201, Training Loss: 17018, Validation Loss: 68449, L1: 68449\n",
      "Epoch 74301, Training Loss: 16907, Validation Loss: 68729, L1: 68729\n",
      "Epoch 74401, Training Loss: 16850, Validation Loss: 68372, L1: 68372\n",
      "Epoch 74501, Training Loss: 17459, Validation Loss: 68664, L1: 68664\n",
      "Epoch 74601, Training Loss: 17155, Validation Loss: 69098, L1: 69098\n",
      "Epoch 74701, Training Loss: 17172, Validation Loss: 68380, L1: 68380\n",
      "Epoch 74801, Training Loss: 16772, Validation Loss: 68490, L1: 68490\n",
      "Epoch 74901, Training Loss: 16870, Validation Loss: 68445, L1: 68445\n",
      "Epoch 75001, Training Loss: 17159, Validation Loss: 68292, L1: 68292\n",
      "Epoch 75101, Training Loss: 16962, Validation Loss: 68521, L1: 68521\n",
      "Epoch 75201, Training Loss: 17254, Validation Loss: 68685, L1: 68685\n",
      "Epoch 75301, Training Loss: 16792, Validation Loss: 68471, L1: 68471\n",
      "Epoch 75401, Training Loss: 16634, Validation Loss: 68290, L1: 68290\n",
      "Epoch 75501, Training Loss: 16790, Validation Loss: 68131, L1: 68131\n",
      "Epoch 75601, Training Loss: 16507, Validation Loss: 68120, L1: 68120\n",
      "Epoch 75701, Training Loss: 16647, Validation Loss: 68341, L1: 68341\n",
      "Epoch 75801, Training Loss: 16781, Validation Loss: 68337, L1: 68337\n",
      "Epoch 75901, Training Loss: 17026, Validation Loss: 68212, L1: 68212\n",
      "Epoch 76001, Training Loss: 16481, Validation Loss: 68470, L1: 68470\n",
      "Epoch 76101, Training Loss: 16373, Validation Loss: 68005, L1: 68005\n",
      "Epoch 76201, Training Loss: 16869, Validation Loss: 68065, L1: 68065\n",
      "Epoch 76301, Training Loss: 16361, Validation Loss: 68028, L1: 68028\n",
      "Epoch 76401, Training Loss: 16366, Validation Loss: 68069, L1: 68069\n",
      "Epoch 76501, Training Loss: 16478, Validation Loss: 68050, L1: 68050\n",
      "Epoch 76601, Training Loss: 16347, Validation Loss: 68595, L1: 68595\n",
      "Epoch 76701, Training Loss: 16307, Validation Loss: 68245, L1: 68245\n",
      "Epoch 76801, Training Loss: 16519, Validation Loss: 68263, L1: 68263\n",
      "Epoch 76901, Training Loss: 16401, Validation Loss: 68109, L1: 68109\n",
      "Epoch 77001, Training Loss: 16221, Validation Loss: 67998, L1: 67998\n",
      "Epoch 77101, Training Loss: 16151, Validation Loss: 68037, L1: 68037\n",
      "Epoch 77201, Training Loss: 16057, Validation Loss: 68041, L1: 68041\n",
      "Epoch 77301, Training Loss: 16297, Validation Loss: 67917, L1: 67917\n",
      "Epoch 77401, Training Loss: 16393, Validation Loss: 68127, L1: 68127\n",
      "Epoch 77501, Training Loss: 15964, Validation Loss: 67875, L1: 67875\n",
      "Epoch 77601, Training Loss: 16018, Validation Loss: 68235, L1: 68235\n",
      "Epoch 77701, Training Loss: 16112, Validation Loss: 67955, L1: 67955\n",
      "Epoch 77801, Training Loss: 16266, Validation Loss: 68009, L1: 68009\n",
      "Epoch 77901, Training Loss: 16064, Validation Loss: 67923, L1: 67923\n",
      "Epoch 78001, Training Loss: 15749, Validation Loss: 68012, L1: 68012\n",
      "Epoch 78101, Training Loss: 15688, Validation Loss: 67927, L1: 67927\n",
      "Epoch 78201, Training Loss: 15994, Validation Loss: 67955, L1: 67955\n",
      "Epoch 78301, Training Loss: 15813, Validation Loss: 68097, L1: 68097\n",
      "Epoch 78401, Training Loss: 15943, Validation Loss: 67940, L1: 67940\n",
      "Epoch 78501, Training Loss: 15636, Validation Loss: 68014, L1: 68014\n",
      "Epoch 78601, Training Loss: 15985, Validation Loss: 68258, L1: 68258\n",
      "Epoch 78701, Training Loss: 15839, Validation Loss: 67777, L1: 67777\n",
      "Epoch 78801, Training Loss: 16400, Validation Loss: 67902, L1: 67902\n",
      "Epoch 78901, Training Loss: 15635, Validation Loss: 67834, L1: 67834\n",
      "Epoch 79001, Training Loss: 15404, Validation Loss: 67706, L1: 67706\n",
      "Epoch 79101, Training Loss: 15853, Validation Loss: 67635, L1: 67635\n",
      "Epoch 79201, Training Loss: 15357, Validation Loss: 67785, L1: 67785\n",
      "Epoch 79301, Training Loss: 15768, Validation Loss: 67754, L1: 67754\n",
      "Epoch 79401, Training Loss: 16012, Validation Loss: 67930, L1: 67930\n",
      "Epoch 79501, Training Loss: 15512, Validation Loss: 67725, L1: 67725\n",
      "Epoch 79601, Training Loss: 15879, Validation Loss: 67871, L1: 67871\n",
      "Epoch 79701, Training Loss: 15526, Validation Loss: 67946, L1: 67946\n",
      "Epoch 79801, Training Loss: 15938, Validation Loss: 68076, L1: 68076\n",
      "Epoch 79901, Training Loss: 15379, Validation Loss: 67834, L1: 67834\n",
      "Epoch 80001, Training Loss: 15262, Validation Loss: 67774, L1: 67774\n",
      "Epoch 80101, Training Loss: 15557, Validation Loss: 67715, L1: 67715\n",
      "Epoch 80201, Training Loss: 15208, Validation Loss: 67797, L1: 67797\n",
      "Epoch 80301, Training Loss: 15345, Validation Loss: 67846, L1: 67846\n",
      "Epoch 80401, Training Loss: 15176, Validation Loss: 67876, L1: 67876\n",
      "Epoch 80501, Training Loss: 15683, Validation Loss: 68017, L1: 68017\n",
      "Epoch 80601, Training Loss: 15753, Validation Loss: 67783, L1: 67783\n",
      "Epoch 80701, Training Loss: 15269, Validation Loss: 67685, L1: 67685\n",
      "Epoch 80801, Training Loss: 15429, Validation Loss: 67884, L1: 67884\n",
      "Epoch 80901, Training Loss: 15166, Validation Loss: 67684, L1: 67684\n",
      "Epoch 81001, Training Loss: 15782, Validation Loss: 68202, L1: 68202\n",
      "Epoch 81101, Training Loss: 16170, Validation Loss: 68203, L1: 68203\n",
      "Epoch 81201, Training Loss: 15807, Validation Loss: 67847, L1: 67847\n",
      "Epoch 81301, Training Loss: 15100, Validation Loss: 67789, L1: 67789\n",
      "Epoch 81401, Training Loss: 15271, Validation Loss: 67805, L1: 67805\n",
      "Epoch 81501, Training Loss: 15213, Validation Loss: 67711, L1: 67711\n",
      "Epoch 81601, Training Loss: 15018, Validation Loss: 67854, L1: 67854\n",
      "Epoch 81701, Training Loss: 15378, Validation Loss: 67792, L1: 67792\n",
      "Epoch 81801, Training Loss: 15355, Validation Loss: 67640, L1: 67640\n",
      "Epoch 81901, Training Loss: 14967, Validation Loss: 67755, L1: 67755\n",
      "Epoch 82001, Training Loss: 15116, Validation Loss: 67750, L1: 67750\n",
      "Epoch 82101, Training Loss: 15070, Validation Loss: 67828, L1: 67828\n",
      "Epoch 82201, Training Loss: 15097, Validation Loss: 67843, L1: 67843\n",
      "Epoch 82301, Training Loss: 15092, Validation Loss: 67757, L1: 67757\n",
      "Epoch 82401, Training Loss: 15059, Validation Loss: 67765, L1: 67765\n",
      "Epoch 82501, Training Loss: 14822, Validation Loss: 67648, L1: 67648\n",
      "Epoch 82601, Training Loss: 15496, Validation Loss: 68152, L1: 68152\n",
      "Epoch 82701, Training Loss: 15021, Validation Loss: 67866, L1: 67866\n",
      "Epoch 82801, Training Loss: 15107, Validation Loss: 67901, L1: 67901\n",
      "Epoch 82901, Training Loss: 14779, Validation Loss: 67778, L1: 67778\n",
      "Epoch 83001, Training Loss: 14897, Validation Loss: 67571, L1: 67571\n",
      "Epoch 83101, Training Loss: 14852, Validation Loss: 67646, L1: 67646\n",
      "Epoch 83201, Training Loss: 14884, Validation Loss: 67634, L1: 67634\n",
      "Epoch 83301, Training Loss: 15309, Validation Loss: 67898, L1: 67898\n",
      "Epoch 83401, Training Loss: 15003, Validation Loss: 67815, L1: 67815\n",
      "Epoch 83501, Training Loss: 15256, Validation Loss: 67820, L1: 67820\n",
      "Epoch 83601, Training Loss: 14804, Validation Loss: 67723, L1: 67723\n",
      "Epoch 83701, Training Loss: 15004, Validation Loss: 68078, L1: 68078\n",
      "Epoch 83801, Training Loss: 14772, Validation Loss: 67806, L1: 67806\n",
      "Epoch 83901, Training Loss: 14796, Validation Loss: 67580, L1: 67580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84001, Training Loss: 14746, Validation Loss: 67937, L1: 67937\n",
      "Epoch 84101, Training Loss: 14803, Validation Loss: 67983, L1: 67983\n",
      "Epoch 84201, Training Loss: 15147, Validation Loss: 68096, L1: 68096\n",
      "Epoch 84301, Training Loss: 14768, Validation Loss: 67749, L1: 67749\n",
      "Epoch 84401, Training Loss: 15177, Validation Loss: 67924, L1: 67924\n",
      "Epoch 84501, Training Loss: 14666, Validation Loss: 67834, L1: 67834\n",
      "Epoch 84601, Training Loss: 14659, Validation Loss: 67796, L1: 67796\n",
      "Epoch 84701, Training Loss: 15250, Validation Loss: 68000, L1: 68000\n",
      "Epoch 84801, Training Loss: 15240, Validation Loss: 67663, L1: 67663\n",
      "Epoch 84901, Training Loss: 14749, Validation Loss: 67808, L1: 67808\n",
      "Epoch 85001, Training Loss: 14541, Validation Loss: 67735, L1: 67735\n",
      "Epoch 85101, Training Loss: 14497, Validation Loss: 67646, L1: 67646\n",
      "Epoch 85201, Training Loss: 14789, Validation Loss: 67572, L1: 67572\n",
      "Epoch 85301, Training Loss: 15138, Validation Loss: 67736, L1: 67736\n",
      "Epoch 85401, Training Loss: 14529, Validation Loss: 67613, L1: 67613\n",
      "Epoch 85501, Training Loss: 14857, Validation Loss: 67471, L1: 67471\n",
      "Epoch 85601, Training Loss: 14661, Validation Loss: 67725, L1: 67725\n",
      "Epoch 85701, Training Loss: 14512, Validation Loss: 67772, L1: 67772\n",
      "Epoch 85801, Training Loss: 15942, Validation Loss: 67765, L1: 67765\n",
      "Epoch 85901, Training Loss: 14416, Validation Loss: 67517, L1: 67517\n",
      "Epoch 86001, Training Loss: 14571, Validation Loss: 67560, L1: 67560\n",
      "Epoch 86101, Training Loss: 14713, Validation Loss: 67654, L1: 67654\n",
      "Epoch 86201, Training Loss: 14409, Validation Loss: 67550, L1: 67550\n",
      "Epoch 86301, Training Loss: 14432, Validation Loss: 67538, L1: 67538\n",
      "Epoch 86401, Training Loss: 14289, Validation Loss: 67578, L1: 67578\n",
      "Epoch 86501, Training Loss: 14851, Validation Loss: 67712, L1: 67712\n",
      "Epoch 86601, Training Loss: 14711, Validation Loss: 67609, L1: 67609\n",
      "Epoch 86701, Training Loss: 14923, Validation Loss: 67627, L1: 67627\n",
      "Epoch 86801, Training Loss: 14344, Validation Loss: 67445, L1: 67445\n",
      "Epoch 86901, Training Loss: 14512, Validation Loss: 67442, L1: 67442\n",
      "Epoch 87001, Training Loss: 14234, Validation Loss: 67383, L1: 67383\n",
      "Epoch 87101, Training Loss: 14254, Validation Loss: 67595, L1: 67595\n",
      "Epoch 87201, Training Loss: 14639, Validation Loss: 67607, L1: 67607\n",
      "Epoch 87301, Training Loss: 14279, Validation Loss: 67365, L1: 67365\n",
      "Epoch 87401, Training Loss: 14330, Validation Loss: 67377, L1: 67377\n",
      "Epoch 87501, Training Loss: 14314, Validation Loss: 67360, L1: 67360\n",
      "Epoch 87601, Training Loss: 14274, Validation Loss: 67432, L1: 67432\n",
      "Epoch 87701, Training Loss: 14217, Validation Loss: 67423, L1: 67423\n",
      "Epoch 87801, Training Loss: 14254, Validation Loss: 67456, L1: 67456\n",
      "Epoch 87901, Training Loss: 14140, Validation Loss: 67439, L1: 67439\n",
      "Epoch 88001, Training Loss: 14291, Validation Loss: 67459, L1: 67459\n",
      "Epoch 88101, Training Loss: 14181, Validation Loss: 67442, L1: 67442\n",
      "Epoch 88201, Training Loss: 14494, Validation Loss: 67452, L1: 67452\n",
      "Epoch 88301, Training Loss: 15012, Validation Loss: 67408, L1: 67408\n",
      "Epoch 88401, Training Loss: 14407, Validation Loss: 67412, L1: 67412\n",
      "Epoch 88501, Training Loss: 14200, Validation Loss: 67563, L1: 67563\n",
      "Epoch 88601, Training Loss: 14455, Validation Loss: 67458, L1: 67458\n",
      "Epoch 88701, Training Loss: 14734, Validation Loss: 67550, L1: 67550\n",
      "Epoch 88801, Training Loss: 14535, Validation Loss: 67695, L1: 67695\n",
      "Epoch 88901, Training Loss: 14312, Validation Loss: 67375, L1: 67375\n",
      "Epoch 89001, Training Loss: 14348, Validation Loss: 67329, L1: 67329\n",
      "Epoch 89101, Training Loss: 14070, Validation Loss: 67482, L1: 67482\n",
      "Epoch 89201, Training Loss: 14100, Validation Loss: 67634, L1: 67634\n",
      "Epoch 89301, Training Loss: 14269, Validation Loss: 67652, L1: 67652\n",
      "Epoch 89401, Training Loss: 14541, Validation Loss: 67676, L1: 67676\n",
      "Epoch 89501, Training Loss: 14589, Validation Loss: 67671, L1: 67671\n",
      "Epoch 89601, Training Loss: 14855, Validation Loss: 67371, L1: 67371\n",
      "Epoch 89701, Training Loss: 13921, Validation Loss: 67383, L1: 67383\n",
      "Epoch 89801, Training Loss: 13972, Validation Loss: 67553, L1: 67553\n",
      "Epoch 89901, Training Loss: 14427, Validation Loss: 67595, L1: 67595\n",
      "Epoch 90001, Training Loss: 14097, Validation Loss: 67227, L1: 67227\n",
      "Epoch 90101, Training Loss: 14019, Validation Loss: 67292, L1: 67292\n",
      "Epoch 90201, Training Loss: 13975, Validation Loss: 67176, L1: 67176\n",
      "Epoch 90301, Training Loss: 13944, Validation Loss: 67520, L1: 67520\n",
      "Epoch 90401, Training Loss: 13838, Validation Loss: 67313, L1: 67313\n",
      "Epoch 90501, Training Loss: 13879, Validation Loss: 67491, L1: 67491\n",
      "Epoch 90601, Training Loss: 13833, Validation Loss: 67230, L1: 67230\n",
      "Epoch 90701, Training Loss: 13810, Validation Loss: 67153, L1: 67153\n",
      "Epoch 90801, Training Loss: 14012, Validation Loss: 67133, L1: 67133\n",
      "Epoch 90901, Training Loss: 13720, Validation Loss: 67221, L1: 67221\n",
      "Epoch 91001, Training Loss: 13921, Validation Loss: 67294, L1: 67294\n",
      "Epoch 91101, Training Loss: 14031, Validation Loss: 67613, L1: 67613\n",
      "Epoch 91201, Training Loss: 14142, Validation Loss: 67284, L1: 67284\n",
      "Epoch 91301, Training Loss: 13828, Validation Loss: 67216, L1: 67216\n",
      "Epoch 91401, Training Loss: 13676, Validation Loss: 67154, L1: 67154\n",
      "Epoch 91501, Training Loss: 13775, Validation Loss: 67215, L1: 67215\n",
      "Epoch 91601, Training Loss: 13671, Validation Loss: 67338, L1: 67338\n",
      "Epoch 91701, Training Loss: 13637, Validation Loss: 67169, L1: 67169\n",
      "Epoch 91801, Training Loss: 14278, Validation Loss: 67385, L1: 67385\n",
      "Epoch 91901, Training Loss: 14150, Validation Loss: 67351, L1: 67351\n",
      "Epoch 92001, Training Loss: 13715, Validation Loss: 67589, L1: 67589\n",
      "Epoch 92101, Training Loss: 13700, Validation Loss: 67409, L1: 67409\n",
      "Epoch 92201, Training Loss: 13685, Validation Loss: 67327, L1: 67327\n",
      "Epoch 92301, Training Loss: 13832, Validation Loss: 67231, L1: 67231\n",
      "Epoch 92401, Training Loss: 14068, Validation Loss: 67446, L1: 67446\n",
      "Epoch 92501, Training Loss: 13506, Validation Loss: 67177, L1: 67177\n",
      "Epoch 92601, Training Loss: 14061, Validation Loss: 67155, L1: 67155\n",
      "Epoch 92701, Training Loss: 13745, Validation Loss: 67076, L1: 67076\n",
      "Epoch 92801, Training Loss: 14129, Validation Loss: 67336, L1: 67336\n",
      "Epoch 92901, Training Loss: 14448, Validation Loss: 67547, L1: 67547\n",
      "Epoch 93001, Training Loss: 13436, Validation Loss: 67135, L1: 67135\n",
      "Epoch 93101, Training Loss: 13677, Validation Loss: 67184, L1: 67184\n",
      "Epoch 93201, Training Loss: 13915, Validation Loss: 67562, L1: 67562\n",
      "Epoch 93301, Training Loss: 13848, Validation Loss: 67243, L1: 67243\n",
      "Epoch 93401, Training Loss: 13529, Validation Loss: 67541, L1: 67541\n",
      "Epoch 93501, Training Loss: 13612, Validation Loss: 67223, L1: 67223\n",
      "Epoch 93601, Training Loss: 13532, Validation Loss: 67181, L1: 67181\n",
      "Epoch 93701, Training Loss: 13499, Validation Loss: 67047, L1: 67047\n",
      "Epoch 93801, Training Loss: 13436, Validation Loss: 67100, L1: 67100\n",
      "Epoch 93901, Training Loss: 13415, Validation Loss: 67202, L1: 67202\n",
      "Epoch 94001, Training Loss: 13478, Validation Loss: 67137, L1: 67137\n",
      "Epoch 94101, Training Loss: 14101, Validation Loss: 67400, L1: 67400\n",
      "Epoch 94201, Training Loss: 13416, Validation Loss: 67168, L1: 67168\n",
      "Epoch 94301, Training Loss: 13391, Validation Loss: 67181, L1: 67181\n",
      "Epoch 94401, Training Loss: 13484, Validation Loss: 67079, L1: 67079\n",
      "Epoch 94501, Training Loss: 13426, Validation Loss: 67210, L1: 67210\n",
      "Epoch 94601, Training Loss: 13324, Validation Loss: 66934, L1: 66934\n",
      "Epoch 94701, Training Loss: 13540, Validation Loss: 67078, L1: 67078\n",
      "Epoch 94801, Training Loss: 13450, Validation Loss: 67066, L1: 67066\n",
      "Epoch 94901, Training Loss: 13744, Validation Loss: 67353, L1: 67353\n",
      "Epoch 95001, Training Loss: 13247, Validation Loss: 66992, L1: 66992\n",
      "Epoch 95101, Training Loss: 13829, Validation Loss: 67146, L1: 67146\n",
      "Epoch 95201, Training Loss: 13912, Validation Loss: 67155, L1: 67155\n",
      "Epoch 95301, Training Loss: 13257, Validation Loss: 66903, L1: 66903\n",
      "Epoch 95401, Training Loss: 13282, Validation Loss: 67076, L1: 67076\n",
      "Epoch 95501, Training Loss: 13299, Validation Loss: 67023, L1: 67023\n",
      "Epoch 95601, Training Loss: 13926, Validation Loss: 67353, L1: 67353\n",
      "Epoch 95701, Training Loss: 13328, Validation Loss: 67024, L1: 67024\n",
      "Epoch 95801, Training Loss: 13208, Validation Loss: 66935, L1: 66935\n",
      "Epoch 95901, Training Loss: 13357, Validation Loss: 66833, L1: 66833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96001, Training Loss: 14088, Validation Loss: 67095, L1: 67095\n",
      "Epoch 96101, Training Loss: 13389, Validation Loss: 66917, L1: 66917\n",
      "Epoch 96201, Training Loss: 13438, Validation Loss: 67137, L1: 67137\n",
      "Epoch 96301, Training Loss: 13651, Validation Loss: 67164, L1: 67164\n",
      "Epoch 96401, Training Loss: 13495, Validation Loss: 67031, L1: 67031\n",
      "Epoch 96501, Training Loss: 13194, Validation Loss: 66921, L1: 66921\n",
      "Epoch 96601, Training Loss: 13150, Validation Loss: 67144, L1: 67144\n",
      "Epoch 96701, Training Loss: 13211, Validation Loss: 66969, L1: 66969\n",
      "Epoch 96801, Training Loss: 13067, Validation Loss: 66906, L1: 66906\n",
      "Epoch 96901, Training Loss: 13540, Validation Loss: 67075, L1: 67075\n",
      "Epoch 97001, Training Loss: 13367, Validation Loss: 67023, L1: 67023\n",
      "Epoch 97101, Training Loss: 13513, Validation Loss: 66770, L1: 66770\n",
      "Epoch 97201, Training Loss: 13467, Validation Loss: 67039, L1: 67039\n",
      "Epoch 97301, Training Loss: 13202, Validation Loss: 66854, L1: 66854\n",
      "Epoch 97401, Training Loss: 14013, Validation Loss: 66771, L1: 66771\n",
      "Epoch 97501, Training Loss: 13072, Validation Loss: 67071, L1: 67071\n",
      "Epoch 97601, Training Loss: 13122, Validation Loss: 67081, L1: 67081\n",
      "Epoch 97701, Training Loss: 13007, Validation Loss: 67026, L1: 67026\n",
      "Epoch 97801, Training Loss: 13021, Validation Loss: 66863, L1: 66863\n",
      "Epoch 97901, Training Loss: 13460, Validation Loss: 67226, L1: 67226\n",
      "Epoch 98001, Training Loss: 13457, Validation Loss: 66967, L1: 66967\n",
      "Epoch 98101, Training Loss: 12852, Validation Loss: 66925, L1: 66925\n",
      "Epoch 98201, Training Loss: 13501, Validation Loss: 66947, L1: 66947\n",
      "Epoch 98301, Training Loss: 12968, Validation Loss: 66947, L1: 66947\n",
      "Epoch 98401, Training Loss: 13505, Validation Loss: 66904, L1: 66904\n",
      "Epoch 98501, Training Loss: 12923, Validation Loss: 66952, L1: 66952\n",
      "Epoch 98601, Training Loss: 13819, Validation Loss: 67196, L1: 67196\n",
      "Epoch 98701, Training Loss: 13151, Validation Loss: 66868, L1: 66868\n",
      "Epoch 98801, Training Loss: 13055, Validation Loss: 66544, L1: 66544\n",
      "Epoch 98901, Training Loss: 12847, Validation Loss: 66738, L1: 66738\n",
      "Epoch 99001, Training Loss: 12861, Validation Loss: 66781, L1: 66781\n",
      "Epoch 99101, Training Loss: 12984, Validation Loss: 66690, L1: 66690\n",
      "Epoch 99201, Training Loss: 13361, Validation Loss: 66892, L1: 66892\n",
      "Epoch 99301, Training Loss: 13244, Validation Loss: 66824, L1: 66824\n",
      "Epoch 99401, Training Loss: 13090, Validation Loss: 66905, L1: 66905\n",
      "Epoch 99501, Training Loss: 12851, Validation Loss: 66713, L1: 66713\n",
      "Epoch 99601, Training Loss: 12988, Validation Loss: 66817, L1: 66817\n",
      "Epoch 99701, Training Loss: 13488, Validation Loss: 66689, L1: 66689\n",
      "Epoch 99801, Training Loss: 12993, Validation Loss: 66930, L1: 66930\n",
      "Epoch 99901, Training Loss: 12866, Validation Loss: 66734, L1: 66734\n",
      "Epoch 100001, Training Loss: 12686, Validation Loss: 66675, L1: 66675\n",
      "Epoch 100101, Training Loss: 13301, Validation Loss: 66702, L1: 66702\n",
      "Epoch 100201, Training Loss: 13184, Validation Loss: 66565, L1: 66565\n",
      "Epoch 100301, Training Loss: 12597, Validation Loss: 66640, L1: 66640\n",
      "Epoch 100401, Training Loss: 12999, Validation Loss: 66473, L1: 66473\n",
      "Epoch 100501, Training Loss: 12748, Validation Loss: 66798, L1: 66798\n",
      "Epoch 100601, Training Loss: 12666, Validation Loss: 66843, L1: 66843\n",
      "Epoch 100701, Training Loss: 13166, Validation Loss: 66775, L1: 66775\n",
      "Epoch 100801, Training Loss: 12722, Validation Loss: 66482, L1: 66482\n",
      "Epoch 100901, Training Loss: 12662, Validation Loss: 66618, L1: 66618\n",
      "Epoch 101001, Training Loss: 12647, Validation Loss: 66481, L1: 66481\n",
      "Epoch 101101, Training Loss: 12550, Validation Loss: 66624, L1: 66624\n",
      "Epoch 101201, Training Loss: 12650, Validation Loss: 66483, L1: 66483\n",
      "Epoch 101301, Training Loss: 12531, Validation Loss: 66601, L1: 66601\n",
      "Epoch 101401, Training Loss: 12639, Validation Loss: 66777, L1: 66777\n",
      "Epoch 101501, Training Loss: 12764, Validation Loss: 66901, L1: 66901\n",
      "Epoch 101601, Training Loss: 12570, Validation Loss: 66655, L1: 66655\n",
      "Epoch 101701, Training Loss: 12685, Validation Loss: 66677, L1: 66677\n",
      "Epoch 101801, Training Loss: 13110, Validation Loss: 66875, L1: 66875\n",
      "Epoch 101901, Training Loss: 12548, Validation Loss: 66543, L1: 66543\n",
      "Epoch 102001, Training Loss: 12374, Validation Loss: 66444, L1: 66444\n",
      "Epoch 102101, Training Loss: 12370, Validation Loss: 66548, L1: 66548\n",
      "Epoch 102201, Training Loss: 12352, Validation Loss: 66656, L1: 66656\n",
      "Epoch 102301, Training Loss: 12525, Validation Loss: 66604, L1: 66604\n",
      "Epoch 102401, Training Loss: 13165, Validation Loss: 66687, L1: 66687\n",
      "Epoch 102501, Training Loss: 12840, Validation Loss: 66780, L1: 66780\n",
      "Epoch 102601, Training Loss: 12528, Validation Loss: 66446, L1: 66446\n",
      "Epoch 102701, Training Loss: 12684, Validation Loss: 66512, L1: 66512\n",
      "Epoch 102801, Training Loss: 12375, Validation Loss: 66550, L1: 66550\n",
      "Epoch 102901, Training Loss: 12421, Validation Loss: 66506, L1: 66506\n",
      "Epoch 103001, Training Loss: 12364, Validation Loss: 66392, L1: 66392\n",
      "Epoch 103101, Training Loss: 12755, Validation Loss: 66683, L1: 66683\n",
      "Epoch 103201, Training Loss: 12295, Validation Loss: 66489, L1: 66489\n",
      "Epoch 103301, Training Loss: 12479, Validation Loss: 66496, L1: 66496\n",
      "Epoch 103401, Training Loss: 12560, Validation Loss: 66524, L1: 66524\n",
      "Epoch 103501, Training Loss: 12581, Validation Loss: 66602, L1: 66602\n",
      "Epoch 103601, Training Loss: 12213, Validation Loss: 66625, L1: 66625\n",
      "Epoch 103701, Training Loss: 12181, Validation Loss: 66552, L1: 66552\n",
      "Epoch 103801, Training Loss: 13248, Validation Loss: 66868, L1: 66868\n",
      "Epoch 103901, Training Loss: 12417, Validation Loss: 66468, L1: 66468\n",
      "Epoch 104001, Training Loss: 12119, Validation Loss: 66333, L1: 66333\n",
      "Epoch 104101, Training Loss: 12817, Validation Loss: 66478, L1: 66478\n",
      "Epoch 104201, Training Loss: 12859, Validation Loss: 66379, L1: 66379\n",
      "Epoch 104301, Training Loss: 12368, Validation Loss: 66520, L1: 66520\n",
      "Epoch 104401, Training Loss: 12077, Validation Loss: 66567, L1: 66567\n",
      "Epoch 104501, Training Loss: 12357, Validation Loss: 66438, L1: 66438\n",
      "Epoch 104601, Training Loss: 12648, Validation Loss: 66796, L1: 66796\n",
      "Epoch 104701, Training Loss: 12137, Validation Loss: 66653, L1: 66653\n",
      "Epoch 104801, Training Loss: 12532, Validation Loss: 66411, L1: 66411\n",
      "Epoch 104901, Training Loss: 12432, Validation Loss: 66638, L1: 66638\n",
      "Epoch 105001, Training Loss: 12742, Validation Loss: 66535, L1: 66535\n",
      "Epoch 105101, Training Loss: 12123, Validation Loss: 66807, L1: 66807\n",
      "Epoch 105201, Training Loss: 12019, Validation Loss: 66514, L1: 66514\n",
      "Epoch 105301, Training Loss: 11951, Validation Loss: 66540, L1: 66540\n",
      "Epoch 105401, Training Loss: 12267, Validation Loss: 66519, L1: 66519\n",
      "Epoch 105501, Training Loss: 12045, Validation Loss: 66574, L1: 66574\n",
      "Epoch 105601, Training Loss: 12369, Validation Loss: 66547, L1: 66547\n",
      "Epoch 105701, Training Loss: 11895, Validation Loss: 66434, L1: 66434\n",
      "Epoch 105801, Training Loss: 11976, Validation Loss: 66624, L1: 66624\n",
      "Epoch 105901, Training Loss: 12640, Validation Loss: 66751, L1: 66751\n",
      "Epoch 106001, Training Loss: 12231, Validation Loss: 66747, L1: 66747\n",
      "Epoch 106101, Training Loss: 12083, Validation Loss: 66757, L1: 66757\n",
      "Epoch 106201, Training Loss: 12073, Validation Loss: 66538, L1: 66538\n",
      "Epoch 106301, Training Loss: 11858, Validation Loss: 66483, L1: 66483\n",
      "Epoch 106401, Training Loss: 12384, Validation Loss: 66643, L1: 66643\n",
      "Epoch 106501, Training Loss: 11713, Validation Loss: 66528, L1: 66528\n",
      "Epoch 106601, Training Loss: 11699, Validation Loss: 66517, L1: 66517\n",
      "Epoch 106701, Training Loss: 12077, Validation Loss: 66919, L1: 66919\n",
      "Epoch 106801, Training Loss: 11990, Validation Loss: 66567, L1: 66567\n",
      "Epoch 106901, Training Loss: 12386, Validation Loss: 66425, L1: 66425\n",
      "Epoch 107001, Training Loss: 11681, Validation Loss: 66564, L1: 66564\n",
      "Epoch 107101, Training Loss: 11908, Validation Loss: 66670, L1: 66670\n",
      "Epoch 107201, Training Loss: 11787, Validation Loss: 66552, L1: 66552\n",
      "Epoch 107301, Training Loss: 12290, Validation Loss: 66747, L1: 66747\n",
      "Epoch 107401, Training Loss: 12037, Validation Loss: 66593, L1: 66593\n",
      "Epoch 107501, Training Loss: 11718, Validation Loss: 66751, L1: 66751\n",
      "Epoch 107601, Training Loss: 11875, Validation Loss: 66519, L1: 66519\n",
      "Epoch 107701, Training Loss: 11756, Validation Loss: 66603, L1: 66603\n",
      "Epoch 107801, Training Loss: 12474, Validation Loss: 66780, L1: 66780\n",
      "Epoch 107901, Training Loss: 11825, Validation Loss: 66614, L1: 66614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108001, Training Loss: 11702, Validation Loss: 66532, L1: 66532\n",
      "Epoch 108101, Training Loss: 11701, Validation Loss: 66600, L1: 66600\n",
      "Epoch 108201, Training Loss: 11822, Validation Loss: 66807, L1: 66807\n",
      "Epoch 108301, Training Loss: 11973, Validation Loss: 66658, L1: 66658\n",
      "Epoch 108401, Training Loss: 11989, Validation Loss: 66714, L1: 66714\n",
      "Epoch 108501, Training Loss: 11666, Validation Loss: 66700, L1: 66700\n",
      "Epoch 108601, Training Loss: 11676, Validation Loss: 66636, L1: 66636\n",
      "Epoch 108701, Training Loss: 11573, Validation Loss: 66802, L1: 66802\n",
      "Epoch 108801, Training Loss: 12128, Validation Loss: 66745, L1: 66745\n",
      "Epoch 108901, Training Loss: 11958, Validation Loss: 66897, L1: 66897\n",
      "Epoch 109001, Training Loss: 11572, Validation Loss: 66760, L1: 66760\n",
      "Epoch 109101, Training Loss: 11582, Validation Loss: 66684, L1: 66684\n",
      "Epoch 109201, Training Loss: 11740, Validation Loss: 66721, L1: 66721\n",
      "Epoch 109301, Training Loss: 11517, Validation Loss: 66733, L1: 66733\n",
      "Epoch 109401, Training Loss: 11433, Validation Loss: 66818, L1: 66818\n",
      "Epoch 109501, Training Loss: 11936, Validation Loss: 66869, L1: 66869\n",
      "Epoch 109601, Training Loss: 11456, Validation Loss: 66732, L1: 66732\n",
      "Epoch 109701, Training Loss: 11620, Validation Loss: 66754, L1: 66754\n",
      "Epoch 109801, Training Loss: 11962, Validation Loss: 66833, L1: 66833\n",
      "Epoch 109901, Training Loss: 11659, Validation Loss: 66868, L1: 66868\n",
      "Epoch 110001, Training Loss: 11681, Validation Loss: 66688, L1: 66688\n",
      "Epoch 110101, Training Loss: 11579, Validation Loss: 66950, L1: 66950\n",
      "Epoch 110201, Training Loss: 11952, Validation Loss: 67033, L1: 67033\n",
      "Epoch 110301, Training Loss: 11595, Validation Loss: 66851, L1: 66851\n",
      "Epoch 110401, Training Loss: 11760, Validation Loss: 66705, L1: 66705\n",
      "Epoch 110501, Training Loss: 11361, Validation Loss: 66892, L1: 66892\n",
      "Epoch 110601, Training Loss: 11920, Validation Loss: 66905, L1: 66905\n",
      "Epoch 110701, Training Loss: 11694, Validation Loss: 66902, L1: 66902\n",
      "Epoch 110801, Training Loss: 11731, Validation Loss: 66841, L1: 66841\n",
      "Epoch 110901, Training Loss: 11668, Validation Loss: 66877, L1: 66877\n",
      "Epoch 111001, Training Loss: 11560, Validation Loss: 66836, L1: 66836\n",
      "Epoch 111101, Training Loss: 11769, Validation Loss: 66648, L1: 66648\n",
      "Epoch 111201, Training Loss: 11500, Validation Loss: 66866, L1: 66866\n",
      "Epoch 111301, Training Loss: 11278, Validation Loss: 66892, L1: 66892\n",
      "Epoch 111401, Training Loss: 11455, Validation Loss: 66758, L1: 66758\n",
      "Epoch 111501, Training Loss: 11478, Validation Loss: 66795, L1: 66795\n",
      "Epoch 111601, Training Loss: 11272, Validation Loss: 66767, L1: 66767\n",
      "Epoch 111701, Training Loss: 11452, Validation Loss: 66827, L1: 66827\n",
      "Epoch 111801, Training Loss: 11228, Validation Loss: 66868, L1: 66868\n",
      "Epoch 111901, Training Loss: 11466, Validation Loss: 66805, L1: 66805\n",
      "Epoch 112001, Training Loss: 11334, Validation Loss: 66988, L1: 66988\n",
      "Epoch 112101, Training Loss: 11250, Validation Loss: 66947, L1: 66947\n",
      "Epoch 112201, Training Loss: 11328, Validation Loss: 66860, L1: 66860\n",
      "Epoch 112301, Training Loss: 11181, Validation Loss: 66869, L1: 66869\n",
      "Epoch 112401, Training Loss: 11410, Validation Loss: 66767, L1: 66767\n",
      "Epoch 112501, Training Loss: 11464, Validation Loss: 66799, L1: 66799\n",
      "Epoch 112601, Training Loss: 11279, Validation Loss: 66760, L1: 66760\n",
      "Epoch 112701, Training Loss: 11282, Validation Loss: 66864, L1: 66864\n",
      "Epoch 112801, Training Loss: 11274, Validation Loss: 66839, L1: 66839\n",
      "Epoch 112901, Training Loss: 11376, Validation Loss: 66974, L1: 66974\n",
      "Epoch 113001, Training Loss: 11197, Validation Loss: 66768, L1: 66768\n",
      "Epoch 113101, Training Loss: 11108, Validation Loss: 66894, L1: 66894\n",
      "Epoch 113201, Training Loss: 11377, Validation Loss: 66864, L1: 66864\n",
      "Epoch 113301, Training Loss: 11177, Validation Loss: 66987, L1: 66987\n",
      "Epoch 113401, Training Loss: 11492, Validation Loss: 67021, L1: 67021\n",
      "Epoch 113501, Training Loss: 11427, Validation Loss: 66827, L1: 66827\n",
      "Epoch 113601, Training Loss: 11423, Validation Loss: 66861, L1: 66861\n",
      "Epoch 113701, Training Loss: 11792, Validation Loss: 67171, L1: 67171\n",
      "Epoch 113801, Training Loss: 11405, Validation Loss: 66950, L1: 66950\n",
      "Epoch 113901, Training Loss: 11017, Validation Loss: 66814, L1: 66814\n",
      "Epoch 114001, Training Loss: 11626, Validation Loss: 67104, L1: 67104\n",
      "Epoch 114101, Training Loss: 12046, Validation Loss: 67112, L1: 67112\n",
      "Epoch 114201, Training Loss: 11086, Validation Loss: 67076, L1: 67076\n",
      "Epoch 114301, Training Loss: 11449, Validation Loss: 66909, L1: 66909\n",
      "Epoch 114401, Training Loss: 11613, Validation Loss: 66991, L1: 66991\n",
      "Epoch 114501, Training Loss: 11054, Validation Loss: 66943, L1: 66943\n",
      "Epoch 114601, Training Loss: 11070, Validation Loss: 67007, L1: 67007\n",
      "Epoch 114701, Training Loss: 10970, Validation Loss: 66947, L1: 66947\n",
      "Epoch 114801, Training Loss: 11039, Validation Loss: 67028, L1: 67028\n",
      "Epoch 114901, Training Loss: 10971, Validation Loss: 66918, L1: 66918\n",
      "Epoch 115001, Training Loss: 11092, Validation Loss: 66987, L1: 66987\n",
      "Epoch 115101, Training Loss: 11310, Validation Loss: 66881, L1: 66881\n",
      "Epoch 115201, Training Loss: 11169, Validation Loss: 67039, L1: 67039\n",
      "Epoch 115301, Training Loss: 11558, Validation Loss: 67056, L1: 67056\n",
      "Epoch 115401, Training Loss: 10977, Validation Loss: 66983, L1: 66983\n",
      "Epoch 115501, Training Loss: 11171, Validation Loss: 67133, L1: 67133\n",
      "Epoch 115601, Training Loss: 10911, Validation Loss: 66973, L1: 66973\n",
      "Epoch 115701, Training Loss: 10947, Validation Loss: 66910, L1: 66910\n",
      "Epoch 115801, Training Loss: 11314, Validation Loss: 66880, L1: 66880\n",
      "Epoch 115901, Training Loss: 10857, Validation Loss: 67057, L1: 67057\n",
      "Epoch 116001, Training Loss: 11731, Validation Loss: 67025, L1: 67025\n",
      "Epoch 116101, Training Loss: 11177, Validation Loss: 66960, L1: 66960\n",
      "Epoch 116201, Training Loss: 11127, Validation Loss: 67007, L1: 67007\n",
      "Epoch 116301, Training Loss: 10994, Validation Loss: 67107, L1: 67107\n",
      "Epoch 116401, Training Loss: 12101, Validation Loss: 67426, L1: 67426\n",
      "Epoch 116501, Training Loss: 11038, Validation Loss: 67399, L1: 67399\n",
      "Epoch 116601, Training Loss: 11121, Validation Loss: 67136, L1: 67136\n",
      "Epoch 116701, Training Loss: 10751, Validation Loss: 67106, L1: 67106\n",
      "Epoch 116801, Training Loss: 11105, Validation Loss: 67269, L1: 67269\n",
      "Epoch 116901, Training Loss: 11027, Validation Loss: 67057, L1: 67057\n",
      "Epoch 117001, Training Loss: 11557, Validation Loss: 67466, L1: 67466\n",
      "Epoch 117101, Training Loss: 11205, Validation Loss: 67179, L1: 67179\n",
      "Epoch 117201, Training Loss: 11473, Validation Loss: 67255, L1: 67255\n",
      "Epoch 117301, Training Loss: 10878, Validation Loss: 67301, L1: 67301\n",
      "Epoch 117401, Training Loss: 11016, Validation Loss: 67324, L1: 67324\n",
      "Epoch 117501, Training Loss: 11136, Validation Loss: 67359, L1: 67359\n",
      "Epoch 117601, Training Loss: 11157, Validation Loss: 67258, L1: 67258\n",
      "Epoch 117701, Training Loss: 10988, Validation Loss: 67080, L1: 67080\n",
      "Epoch 117801, Training Loss: 10597, Validation Loss: 67045, L1: 67045\n",
      "Epoch 117901, Training Loss: 10651, Validation Loss: 67015, L1: 67015\n",
      "Epoch 118001, Training Loss: 10698, Validation Loss: 67247, L1: 67247\n",
      "Epoch 118101, Training Loss: 10859, Validation Loss: 67144, L1: 67144\n",
      "Epoch 118201, Training Loss: 10887, Validation Loss: 66980, L1: 66980\n",
      "Epoch 118301, Training Loss: 10678, Validation Loss: 67147, L1: 67147\n",
      "Epoch 118401, Training Loss: 10799, Validation Loss: 67017, L1: 67017\n",
      "Epoch 118501, Training Loss: 11437, Validation Loss: 67411, L1: 67411\n",
      "Epoch 118601, Training Loss: 11196, Validation Loss: 67274, L1: 67274\n",
      "Epoch 118701, Training Loss: 10928, Validation Loss: 67132, L1: 67132\n",
      "Epoch 118801, Training Loss: 10769, Validation Loss: 67111, L1: 67111\n",
      "Epoch 118901, Training Loss: 10592, Validation Loss: 67122, L1: 67122\n",
      "Epoch 119001, Training Loss: 10561, Validation Loss: 67350, L1: 67350\n",
      "Epoch 119101, Training Loss: 11210, Validation Loss: 66959, L1: 66959\n",
      "Epoch 119201, Training Loss: 10727, Validation Loss: 67202, L1: 67202\n",
      "Epoch 119301, Training Loss: 10527, Validation Loss: 67134, L1: 67134\n",
      "Epoch 119401, Training Loss: 10601, Validation Loss: 67012, L1: 67012\n",
      "Epoch 119501, Training Loss: 10639, Validation Loss: 67150, L1: 67150\n",
      "Epoch 119601, Training Loss: 11457, Validation Loss: 67216, L1: 67216\n",
      "Epoch 119701, Training Loss: 11086, Validation Loss: 67306, L1: 67306\n",
      "Epoch 119801, Training Loss: 10422, Validation Loss: 67178, L1: 67178\n",
      "Epoch 119901, Training Loss: 10754, Validation Loss: 67328, L1: 67328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120001, Training Loss: 10477, Validation Loss: 67115, L1: 67115\n",
      "Epoch 120101, Training Loss: 10531, Validation Loss: 67049, L1: 67049\n",
      "Epoch 120201, Training Loss: 10443, Validation Loss: 66948, L1: 66948\n",
      "Epoch 120301, Training Loss: 10397, Validation Loss: 66997, L1: 66997\n",
      "Epoch 120401, Training Loss: 10613, Validation Loss: 67040, L1: 67040\n",
      "Epoch 120501, Training Loss: 10496, Validation Loss: 67184, L1: 67184\n",
      "Epoch 120601, Training Loss: 11259, Validation Loss: 67177, L1: 67177\n",
      "Epoch 120701, Training Loss: 10396, Validation Loss: 67029, L1: 67029\n",
      "Epoch 120801, Training Loss: 10603, Validation Loss: 66978, L1: 66978\n",
      "Epoch 120901, Training Loss: 10419, Validation Loss: 67241, L1: 67241\n",
      "Epoch 121001, Training Loss: 11174, Validation Loss: 67332, L1: 67332\n",
      "Epoch 121101, Training Loss: 11067, Validation Loss: 67129, L1: 67129\n",
      "Epoch 121201, Training Loss: 10329, Validation Loss: 67017, L1: 67017\n",
      "Epoch 121301, Training Loss: 10590, Validation Loss: 67043, L1: 67043\n",
      "Epoch 121401, Training Loss: 10666, Validation Loss: 67339, L1: 67339\n",
      "Epoch 121501, Training Loss: 11162, Validation Loss: 66947, L1: 66947\n",
      "Epoch 121601, Training Loss: 10457, Validation Loss: 67160, L1: 67160\n",
      "Epoch 121701, Training Loss: 10388, Validation Loss: 66935, L1: 66935\n",
      "Epoch 121801, Training Loss: 10560, Validation Loss: 67307, L1: 67307\n",
      "Epoch 121901, Training Loss: 10283, Validation Loss: 67024, L1: 67024\n",
      "Epoch 122001, Training Loss: 10718, Validation Loss: 67201, L1: 67201\n",
      "Epoch 122101, Training Loss: 10526, Validation Loss: 67012, L1: 67012\n",
      "Epoch 122201, Training Loss: 10354, Validation Loss: 67073, L1: 67073\n",
      "Epoch 122301, Training Loss: 10531, Validation Loss: 67014, L1: 67014\n",
      "Epoch 122401, Training Loss: 10950, Validation Loss: 67285, L1: 67285\n",
      "Epoch 122501, Training Loss: 10295, Validation Loss: 67057, L1: 67057\n",
      "Epoch 122601, Training Loss: 10318, Validation Loss: 67166, L1: 67166\n",
      "Epoch 122701, Training Loss: 11048, Validation Loss: 66895, L1: 66895\n",
      "Epoch 122801, Training Loss: 10688, Validation Loss: 67075, L1: 67075\n",
      "Epoch 122901, Training Loss: 10310, Validation Loss: 67044, L1: 67044\n",
      "Epoch 123001, Training Loss: 10446, Validation Loss: 67331, L1: 67331\n",
      "Epoch 123101, Training Loss: 10474, Validation Loss: 67070, L1: 67070\n",
      "Epoch 123201, Training Loss: 10178, Validation Loss: 67164, L1: 67164\n",
      "Epoch 123301, Training Loss: 10287, Validation Loss: 67180, L1: 67180\n",
      "Epoch 123401, Training Loss: 10445, Validation Loss: 67156, L1: 67156\n",
      "Epoch 123501, Training Loss: 10433, Validation Loss: 67109, L1: 67109\n",
      "Epoch 123601, Training Loss: 10127, Validation Loss: 67234, L1: 67234\n",
      "Epoch 123701, Training Loss: 10242, Validation Loss: 67267, L1: 67267\n",
      "Epoch 123801, Training Loss: 10511, Validation Loss: 67030, L1: 67030\n",
      "Epoch 123901, Training Loss: 10328, Validation Loss: 66935, L1: 66935\n",
      "Epoch 124001, Training Loss: 10096, Validation Loss: 67150, L1: 67150\n",
      "Epoch 124101, Training Loss: 10215, Validation Loss: 67196, L1: 67196\n",
      "Epoch 124201, Training Loss: 10695, Validation Loss: 67233, L1: 67233\n",
      "Epoch 124301, Training Loss: 10067, Validation Loss: 67131, L1: 67131\n",
      "Epoch 124401, Training Loss: 10595, Validation Loss: 67173, L1: 67173\n",
      "Epoch 124501, Training Loss: 10073, Validation Loss: 67068, L1: 67068\n",
      "Epoch 124601, Training Loss: 10705, Validation Loss: 67337, L1: 67337\n",
      "Epoch 124701, Training Loss: 10213, Validation Loss: 67202, L1: 67202\n",
      "Epoch 124801, Training Loss: 11636, Validation Loss: 66873, L1: 66873\n",
      "Epoch 124901, Training Loss: 10367, Validation Loss: 67081, L1: 67081\n",
      "Epoch 125001, Training Loss: 10043, Validation Loss: 67188, L1: 67188\n",
      "Epoch 125101, Training Loss: 10077, Validation Loss: 67221, L1: 67221\n",
      "Epoch 125201, Training Loss: 10081, Validation Loss: 67213, L1: 67213\n",
      "Epoch 125301, Training Loss: 10516, Validation Loss: 67294, L1: 67294\n",
      "Epoch 125401, Training Loss: 10055, Validation Loss: 67159, L1: 67159\n",
      "Epoch 125501, Training Loss: 10434, Validation Loss: 67253, L1: 67253\n",
      "Epoch 125601, Training Loss: 9982, Validation Loss: 67106, L1: 67106\n",
      "Epoch 125701, Training Loss: 10115, Validation Loss: 67167, L1: 67167\n",
      "Epoch 125801, Training Loss: 10745, Validation Loss: 67088, L1: 67088\n",
      "Epoch 125901, Training Loss: 10134, Validation Loss: 67098, L1: 67098\n",
      "Epoch 126001, Training Loss: 10195, Validation Loss: 67145, L1: 67145\n",
      "Epoch 126101, Training Loss: 10550, Validation Loss: 67008, L1: 67008\n",
      "Epoch 126201, Training Loss: 9993, Validation Loss: 66999, L1: 66999\n",
      "Epoch 126301, Training Loss: 10193, Validation Loss: 67029, L1: 67029\n",
      "Epoch 126401, Training Loss: 10410, Validation Loss: 67217, L1: 67217\n",
      "Epoch 126501, Training Loss: 10018, Validation Loss: 67122, L1: 67122\n",
      "Epoch 126601, Training Loss: 10199, Validation Loss: 67150, L1: 67150\n",
      "Epoch 126701, Training Loss: 10236, Validation Loss: 66906, L1: 66906\n",
      "Epoch 126801, Training Loss: 10395, Validation Loss: 67026, L1: 67026\n",
      "Epoch 126901, Training Loss: 10171, Validation Loss: 67229, L1: 67229\n",
      "Epoch 127001, Training Loss: 9885, Validation Loss: 67139, L1: 67139\n",
      "Epoch 127101, Training Loss: 9849, Validation Loss: 67138, L1: 67138\n",
      "Epoch 127201, Training Loss: 9901, Validation Loss: 67095, L1: 67095\n",
      "Epoch 127301, Training Loss: 9878, Validation Loss: 67024, L1: 67024\n",
      "Epoch 127401, Training Loss: 9914, Validation Loss: 67187, L1: 67187\n",
      "Epoch 127501, Training Loss: 9887, Validation Loss: 67220, L1: 67220\n",
      "Epoch 127601, Training Loss: 9962, Validation Loss: 67201, L1: 67201\n",
      "Epoch 127701, Training Loss: 10015, Validation Loss: 67257, L1: 67257\n",
      "Epoch 127801, Training Loss: 11060, Validation Loss: 67319, L1: 67319\n",
      "Epoch 127901, Training Loss: 9976, Validation Loss: 67127, L1: 67127\n",
      "Epoch 128001, Training Loss: 9961, Validation Loss: 66997, L1: 66997\n",
      "Epoch 128101, Training Loss: 10428, Validation Loss: 67207, L1: 67207\n",
      "Epoch 128201, Training Loss: 9858, Validation Loss: 67305, L1: 67305\n",
      "Epoch 128301, Training Loss: 10116, Validation Loss: 67384, L1: 67384\n",
      "Epoch 128401, Training Loss: 9786, Validation Loss: 67105, L1: 67105\n",
      "Epoch 128501, Training Loss: 10916, Validation Loss: 67341, L1: 67341\n",
      "Epoch 128601, Training Loss: 9997, Validation Loss: 67326, L1: 67326\n",
      "Epoch 128701, Training Loss: 10268, Validation Loss: 67259, L1: 67259\n",
      "Epoch 128801, Training Loss: 10303, Validation Loss: 67170, L1: 67170\n",
      "Epoch 128901, Training Loss: 10015, Validation Loss: 67127, L1: 67127\n",
      "Epoch 129001, Training Loss: 9863, Validation Loss: 67128, L1: 67128\n",
      "Epoch 129101, Training Loss: 9811, Validation Loss: 67080, L1: 67080\n",
      "Epoch 129201, Training Loss: 9988, Validation Loss: 67183, L1: 67183\n",
      "Epoch 129301, Training Loss: 9685, Validation Loss: 67290, L1: 67290\n",
      "Epoch 129401, Training Loss: 9766, Validation Loss: 67308, L1: 67308\n",
      "Epoch 129501, Training Loss: 9629, Validation Loss: 67186, L1: 67186\n",
      "Epoch 129601, Training Loss: 9904, Validation Loss: 67364, L1: 67364\n",
      "Epoch 129701, Training Loss: 9602, Validation Loss: 67127, L1: 67127\n",
      "Epoch 129801, Training Loss: 10182, Validation Loss: 67232, L1: 67232\n",
      "Epoch 129901, Training Loss: 9610, Validation Loss: 67167, L1: 67167\n",
      "Epoch 130001, Training Loss: 10189, Validation Loss: 67255, L1: 67255\n",
      "Epoch 130101, Training Loss: 9868, Validation Loss: 67448, L1: 67448\n",
      "Epoch 130201, Training Loss: 10383, Validation Loss: 67181, L1: 67181\n",
      "Epoch 130301, Training Loss: 9942, Validation Loss: 67325, L1: 67325\n",
      "Epoch 130401, Training Loss: 9627, Validation Loss: 67268, L1: 67268\n",
      "Epoch 130501, Training Loss: 9629, Validation Loss: 67361, L1: 67361\n",
      "Epoch 130601, Training Loss: 9602, Validation Loss: 67250, L1: 67250\n",
      "Epoch 130701, Training Loss: 9721, Validation Loss: 67297, L1: 67297\n",
      "Epoch 130801, Training Loss: 9648, Validation Loss: 67290, L1: 67290\n",
      "Epoch 130901, Training Loss: 9671, Validation Loss: 67291, L1: 67291\n",
      "Epoch 131001, Training Loss: 10010, Validation Loss: 67239, L1: 67239\n",
      "Epoch 131101, Training Loss: 9566, Validation Loss: 67290, L1: 67290\n",
      "Epoch 131201, Training Loss: 9915, Validation Loss: 67320, L1: 67320\n",
      "Epoch 131301, Training Loss: 9540, Validation Loss: 67341, L1: 67341\n",
      "Epoch 131401, Training Loss: 9570, Validation Loss: 67210, L1: 67210\n",
      "Epoch 131501, Training Loss: 9573, Validation Loss: 67103, L1: 67103\n",
      "Epoch 131601, Training Loss: 9601, Validation Loss: 67341, L1: 67341\n",
      "Epoch 131701, Training Loss: 10134, Validation Loss: 67310, L1: 67310\n",
      "Epoch 131801, Training Loss: 9727, Validation Loss: 67296, L1: 67296\n",
      "Epoch 131901, Training Loss: 9711, Validation Loss: 67376, L1: 67376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132001, Training Loss: 9682, Validation Loss: 67445, L1: 67445\n",
      "Epoch 132101, Training Loss: 9935, Validation Loss: 67353, L1: 67353\n",
      "Epoch 132201, Training Loss: 9852, Validation Loss: 67381, L1: 67381\n",
      "Epoch 132301, Training Loss: 9740, Validation Loss: 67479, L1: 67479\n",
      "Epoch 132401, Training Loss: 9701, Validation Loss: 67526, L1: 67526\n",
      "Epoch 132501, Training Loss: 10207, Validation Loss: 67346, L1: 67346\n",
      "Epoch 132601, Training Loss: 9680, Validation Loss: 67344, L1: 67344\n",
      "Epoch 132701, Training Loss: 9770, Validation Loss: 67170, L1: 67170\n",
      "Epoch 132801, Training Loss: 10993, Validation Loss: 67150, L1: 67150\n",
      "Epoch 132901, Training Loss: 10037, Validation Loss: 67364, L1: 67364\n",
      "Epoch 133001, Training Loss: 9836, Validation Loss: 67372, L1: 67372\n",
      "Epoch 133101, Training Loss: 9515, Validation Loss: 67278, L1: 67278\n",
      "Epoch 133201, Training Loss: 9788, Validation Loss: 67581, L1: 67581\n",
      "Epoch 133301, Training Loss: 9462, Validation Loss: 67180, L1: 67180\n",
      "Epoch 133401, Training Loss: 9954, Validation Loss: 67480, L1: 67480\n",
      "Epoch 133501, Training Loss: 9356, Validation Loss: 67332, L1: 67332\n",
      "Epoch 133601, Training Loss: 10123, Validation Loss: 67490, L1: 67490\n",
      "Epoch 133701, Training Loss: 9441, Validation Loss: 67479, L1: 67479\n",
      "Epoch 133801, Training Loss: 9982, Validation Loss: 67234, L1: 67234\n",
      "Epoch 133901, Training Loss: 9697, Validation Loss: 67624, L1: 67624\n",
      "Epoch 134001, Training Loss: 9456, Validation Loss: 67344, L1: 67344\n",
      "Epoch 134101, Training Loss: 9649, Validation Loss: 67280, L1: 67280\n",
      "Epoch 134201, Training Loss: 9441, Validation Loss: 67336, L1: 67336\n",
      "Epoch 134301, Training Loss: 10054, Validation Loss: 67237, L1: 67237\n",
      "Epoch 134401, Training Loss: 10073, Validation Loss: 67391, L1: 67391\n",
      "Epoch 134501, Training Loss: 9547, Validation Loss: 67453, L1: 67453\n",
      "Epoch 134601, Training Loss: 9638, Validation Loss: 67510, L1: 67510\n",
      "Epoch 134701, Training Loss: 10169, Validation Loss: 67535, L1: 67535\n",
      "Epoch 134801, Training Loss: 9352, Validation Loss: 67425, L1: 67425\n",
      "Epoch 134901, Training Loss: 9821, Validation Loss: 67325, L1: 67325\n",
      "Epoch 135001, Training Loss: 9304, Validation Loss: 67339, L1: 67339\n",
      "Epoch 135101, Training Loss: 10173, Validation Loss: 67361, L1: 67361\n",
      "Epoch 135201, Training Loss: 10190, Validation Loss: 67562, L1: 67562\n",
      "Epoch 135301, Training Loss: 9472, Validation Loss: 67395, L1: 67395\n",
      "Epoch 135401, Training Loss: 9428, Validation Loss: 67265, L1: 67265\n",
      "Epoch 135501, Training Loss: 9554, Validation Loss: 67408, L1: 67408\n",
      "Epoch 135601, Training Loss: 9298, Validation Loss: 67417, L1: 67417\n",
      "Epoch 135701, Training Loss: 9301, Validation Loss: 67255, L1: 67255\n",
      "Epoch 135801, Training Loss: 9443, Validation Loss: 67444, L1: 67444\n",
      "Epoch 135901, Training Loss: 9999, Validation Loss: 67634, L1: 67634\n",
      "Epoch 136001, Training Loss: 9144, Validation Loss: 67392, L1: 67392\n",
      "Epoch 136101, Training Loss: 9566, Validation Loss: 67360, L1: 67360\n",
      "Epoch 136201, Training Loss: 9374, Validation Loss: 67441, L1: 67441\n",
      "Epoch 136301, Training Loss: 9527, Validation Loss: 67231, L1: 67231\n",
      "Epoch 136401, Training Loss: 9263, Validation Loss: 67452, L1: 67452\n",
      "Epoch 136501, Training Loss: 9791, Validation Loss: 67476, L1: 67476\n",
      "Epoch 136601, Training Loss: 9566, Validation Loss: 67319, L1: 67319\n",
      "Epoch 136701, Training Loss: 9179, Validation Loss: 67566, L1: 67566\n",
      "Epoch 136801, Training Loss: 9303, Validation Loss: 67414, L1: 67414\n",
      "Epoch 136901, Training Loss: 9177, Validation Loss: 67531, L1: 67531\n",
      "Epoch 137001, Training Loss: 9403, Validation Loss: 67408, L1: 67408\n",
      "Epoch 137101, Training Loss: 9135, Validation Loss: 67461, L1: 67461\n",
      "Epoch 137201, Training Loss: 9249, Validation Loss: 67472, L1: 67472\n",
      "Epoch 137301, Training Loss: 9611, Validation Loss: 67516, L1: 67516\n",
      "Epoch 137401, Training Loss: 9203, Validation Loss: 67393, L1: 67393\n",
      "Epoch 137501, Training Loss: 9060, Validation Loss: 67450, L1: 67450\n",
      "Epoch 137601, Training Loss: 9162, Validation Loss: 67426, L1: 67426\n",
      "Epoch 137701, Training Loss: 9294, Validation Loss: 67317, L1: 67317\n",
      "Epoch 137801, Training Loss: 9271, Validation Loss: 67520, L1: 67520\n",
      "Epoch 137901, Training Loss: 9101, Validation Loss: 67270, L1: 67270\n",
      "Epoch 138001, Training Loss: 9843, Validation Loss: 67527, L1: 67527\n",
      "Epoch 138101, Training Loss: 10283, Validation Loss: 67512, L1: 67512\n",
      "Epoch 138201, Training Loss: 9041, Validation Loss: 67405, L1: 67405\n",
      "Epoch 138301, Training Loss: 9337, Validation Loss: 67479, L1: 67479\n",
      "Epoch 138401, Training Loss: 9648, Validation Loss: 67414, L1: 67414\n",
      "Epoch 138501, Training Loss: 9409, Validation Loss: 67429, L1: 67429\n",
      "Epoch 138601, Training Loss: 9053, Validation Loss: 67497, L1: 67497\n",
      "Epoch 138701, Training Loss: 9023, Validation Loss: 67382, L1: 67382\n",
      "Epoch 138801, Training Loss: 9036, Validation Loss: 67416, L1: 67416\n",
      "Epoch 138901, Training Loss: 9486, Validation Loss: 67593, L1: 67593\n",
      "Epoch 139001, Training Loss: 9177, Validation Loss: 67555, L1: 67555\n",
      "Epoch 139101, Training Loss: 10013, Validation Loss: 67550, L1: 67550\n",
      "Epoch 139201, Training Loss: 10418, Validation Loss: 67458, L1: 67458\n",
      "Epoch 139301, Training Loss: 9101, Validation Loss: 67504, L1: 67504\n",
      "Epoch 139401, Training Loss: 9296, Validation Loss: 67409, L1: 67409\n",
      "Epoch 139501, Training Loss: 9201, Validation Loss: 67656, L1: 67656\n",
      "Epoch 139601, Training Loss: 9022, Validation Loss: 67484, L1: 67484\n",
      "Epoch 139701, Training Loss: 9393, Validation Loss: 67648, L1: 67648\n",
      "Epoch 139801, Training Loss: 8936, Validation Loss: 67448, L1: 67448\n",
      "Epoch 139901, Training Loss: 10158, Validation Loss: 67243, L1: 67243\n",
      "Epoch 140001, Training Loss: 9528, Validation Loss: 67331, L1: 67331\n",
      "Epoch 140101, Training Loss: 9007, Validation Loss: 67495, L1: 67495\n",
      "Epoch 140201, Training Loss: 9097, Validation Loss: 67378, L1: 67378\n",
      "Epoch 140301, Training Loss: 9195, Validation Loss: 67476, L1: 67476\n",
      "Epoch 140401, Training Loss: 9014, Validation Loss: 67412, L1: 67412\n",
      "Epoch 140501, Training Loss: 9510, Validation Loss: 67338, L1: 67338\n",
      "Epoch 140601, Training Loss: 9618, Validation Loss: 67280, L1: 67280\n",
      "Epoch 140701, Training Loss: 8934, Validation Loss: 67345, L1: 67345\n",
      "Epoch 140801, Training Loss: 9146, Validation Loss: 67508, L1: 67508\n",
      "Epoch 140901, Training Loss: 9115, Validation Loss: 67298, L1: 67298\n",
      "Epoch 141001, Training Loss: 9367, Validation Loss: 67327, L1: 67327\n",
      "Epoch 141101, Training Loss: 9483, Validation Loss: 67299, L1: 67299\n",
      "Epoch 141201, Training Loss: 9387, Validation Loss: 67269, L1: 67269\n",
      "Epoch 141301, Training Loss: 9034, Validation Loss: 67413, L1: 67413\n",
      "Epoch 141401, Training Loss: 8848, Validation Loss: 67388, L1: 67388\n",
      "Epoch 141501, Training Loss: 9310, Validation Loss: 67341, L1: 67341\n",
      "Epoch 141601, Training Loss: 8809, Validation Loss: 67406, L1: 67406\n",
      "Epoch 141701, Training Loss: 9744, Validation Loss: 67586, L1: 67586\n",
      "Epoch 141801, Training Loss: 8800, Validation Loss: 67440, L1: 67440\n",
      "Epoch 141901, Training Loss: 8981, Validation Loss: 67302, L1: 67302\n",
      "Epoch 142001, Training Loss: 9251, Validation Loss: 67177, L1: 67177\n",
      "Epoch 142101, Training Loss: 9519, Validation Loss: 67402, L1: 67402\n",
      "Epoch 142201, Training Loss: 9409, Validation Loss: 67567, L1: 67567\n",
      "Epoch 142301, Training Loss: 9139, Validation Loss: 67365, L1: 67365\n",
      "Epoch 142401, Training Loss: 9539, Validation Loss: 67602, L1: 67602\n",
      "Epoch 142501, Training Loss: 8878, Validation Loss: 67340, L1: 67340\n",
      "Epoch 142601, Training Loss: 9228, Validation Loss: 67356, L1: 67356\n",
      "Epoch 142701, Training Loss: 8786, Validation Loss: 67347, L1: 67347\n",
      "Epoch 142801, Training Loss: 9088, Validation Loss: 67261, L1: 67261\n",
      "Epoch 142901, Training Loss: 9703, Validation Loss: 67558, L1: 67558\n",
      "Epoch 143001, Training Loss: 9066, Validation Loss: 67424, L1: 67424\n",
      "Epoch 143101, Training Loss: 9519, Validation Loss: 67483, L1: 67483\n",
      "Epoch 143201, Training Loss: 8821, Validation Loss: 67328, L1: 67328\n",
      "Epoch 143301, Training Loss: 8968, Validation Loss: 67277, L1: 67277\n",
      "Epoch 143401, Training Loss: 8818, Validation Loss: 67234, L1: 67234\n",
      "Epoch 143501, Training Loss: 8823, Validation Loss: 67405, L1: 67405\n",
      "Epoch 143601, Training Loss: 9181, Validation Loss: 67296, L1: 67296\n",
      "Epoch 143701, Training Loss: 9411, Validation Loss: 67612, L1: 67612\n",
      "Epoch 143801, Training Loss: 8963, Validation Loss: 67333, L1: 67333\n",
      "Epoch 143901, Training Loss: 8941, Validation Loss: 67336, L1: 67336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144001, Training Loss: 8890, Validation Loss: 67446, L1: 67446\n",
      "Epoch 144101, Training Loss: 9516, Validation Loss: 67223, L1: 67223\n",
      "Epoch 144201, Training Loss: 8789, Validation Loss: 67270, L1: 67270\n",
      "Epoch 144301, Training Loss: 9345, Validation Loss: 67351, L1: 67351\n",
      "Epoch 144401, Training Loss: 9229, Validation Loss: 67256, L1: 67256\n",
      "Epoch 144501, Training Loss: 8677, Validation Loss: 67104, L1: 67104\n",
      "Epoch 144601, Training Loss: 8849, Validation Loss: 67412, L1: 67412\n",
      "Epoch 144701, Training Loss: 8797, Validation Loss: 67298, L1: 67298\n",
      "Epoch 144801, Training Loss: 8729, Validation Loss: 67541, L1: 67541\n",
      "Epoch 144901, Training Loss: 8671, Validation Loss: 67266, L1: 67266\n",
      "Epoch 145001, Training Loss: 8946, Validation Loss: 67238, L1: 67238\n",
      "Epoch 145101, Training Loss: 9740, Validation Loss: 67190, L1: 67190\n",
      "Epoch 145201, Training Loss: 8820, Validation Loss: 67130, L1: 67130\n",
      "Epoch 145301, Training Loss: 9201, Validation Loss: 67187, L1: 67187\n",
      "Epoch 145401, Training Loss: 8688, Validation Loss: 67402, L1: 67402\n",
      "Epoch 145501, Training Loss: 8786, Validation Loss: 67375, L1: 67375\n",
      "Epoch 145601, Training Loss: 8947, Validation Loss: 67135, L1: 67135\n",
      "Epoch 145701, Training Loss: 8997, Validation Loss: 67368, L1: 67368\n",
      "Epoch 145801, Training Loss: 8715, Validation Loss: 67201, L1: 67201\n",
      "Epoch 145901, Training Loss: 9223, Validation Loss: 67489, L1: 67489\n",
      "Epoch 146001, Training Loss: 8746, Validation Loss: 67253, L1: 67253\n",
      "Epoch 146101, Training Loss: 8907, Validation Loss: 67269, L1: 67269\n",
      "Epoch 146201, Training Loss: 9497, Validation Loss: 67236, L1: 67236\n",
      "Epoch 146301, Training Loss: 9099, Validation Loss: 67021, L1: 67021\n",
      "Epoch 146401, Training Loss: 8897, Validation Loss: 67200, L1: 67200\n",
      "Epoch 146501, Training Loss: 9459, Validation Loss: 67387, L1: 67387\n",
      "Epoch 146601, Training Loss: 8673, Validation Loss: 67309, L1: 67309\n",
      "Epoch 146701, Training Loss: 9003, Validation Loss: 67332, L1: 67332\n",
      "Epoch 146801, Training Loss: 8898, Validation Loss: 67168, L1: 67168\n",
      "Epoch 146901, Training Loss: 8586, Validation Loss: 67098, L1: 67098\n",
      "Epoch 147001, Training Loss: 8960, Validation Loss: 67244, L1: 67244\n",
      "Epoch 147101, Training Loss: 8654, Validation Loss: 67318, L1: 67318\n",
      "Epoch 147201, Training Loss: 9574, Validation Loss: 67151, L1: 67151\n",
      "Epoch 147301, Training Loss: 10291, Validation Loss: 66961, L1: 66961\n",
      "Epoch 147401, Training Loss: 9126, Validation Loss: 67239, L1: 67239\n",
      "Epoch 147501, Training Loss: 8735, Validation Loss: 67168, L1: 67168\n",
      "Epoch 147601, Training Loss: 8917, Validation Loss: 67158, L1: 67158\n",
      "Epoch 147701, Training Loss: 8503, Validation Loss: 67099, L1: 67099\n",
      "Epoch 147801, Training Loss: 8788, Validation Loss: 67371, L1: 67371\n",
      "Epoch 147901, Training Loss: 8958, Validation Loss: 67224, L1: 67224\n",
      "Epoch 148001, Training Loss: 8510, Validation Loss: 67300, L1: 67300\n",
      "Epoch 148101, Training Loss: 8485, Validation Loss: 67107, L1: 67107\n",
      "Epoch 148201, Training Loss: 9374, Validation Loss: 67056, L1: 67056\n",
      "Epoch 148301, Training Loss: 8514, Validation Loss: 67243, L1: 67243\n",
      "Epoch 148401, Training Loss: 9084, Validation Loss: 67363, L1: 67363\n",
      "Epoch 148501, Training Loss: 8599, Validation Loss: 67265, L1: 67265\n",
      "Epoch 148601, Training Loss: 9666, Validation Loss: 67124, L1: 67124\n",
      "Epoch 148701, Training Loss: 8540, Validation Loss: 67223, L1: 67223\n",
      "Epoch 148801, Training Loss: 9239, Validation Loss: 67298, L1: 67298\n",
      "Epoch 148901, Training Loss: 8494, Validation Loss: 67421, L1: 67421\n",
      "Epoch 149001, Training Loss: 8986, Validation Loss: 67084, L1: 67084\n",
      "Epoch 149101, Training Loss: 8668, Validation Loss: 67200, L1: 67200\n",
      "Epoch 149201, Training Loss: 8947, Validation Loss: 67068, L1: 67068\n",
      "Epoch 149301, Training Loss: 8397, Validation Loss: 67223, L1: 67223\n",
      "Epoch 149401, Training Loss: 8774, Validation Loss: 67204, L1: 67204\n",
      "Epoch 149501, Training Loss: 8653, Validation Loss: 66987, L1: 66987\n",
      "Epoch 149601, Training Loss: 9314, Validation Loss: 67332, L1: 67332\n",
      "Epoch 149701, Training Loss: 10174, Validation Loss: 67295, L1: 67295\n",
      "Epoch 149801, Training Loss: 8693, Validation Loss: 67340, L1: 67340\n",
      "Epoch 149901, Training Loss: 9686, Validation Loss: 67594, L1: 67594\n",
      "Epoch 150001, Training Loss: 8406, Validation Loss: 67264, L1: 67264\n",
      "Epoch 150101, Training Loss: 8522, Validation Loss: 67210, L1: 67210\n",
      "Epoch 150201, Training Loss: 8833, Validation Loss: 67270, L1: 67270\n",
      "Epoch 150301, Training Loss: 9121, Validation Loss: 67288, L1: 67288\n",
      "Epoch 150401, Training Loss: 8903, Validation Loss: 67053, L1: 67053\n",
      "Epoch 150501, Training Loss: 8446, Validation Loss: 67387, L1: 67387\n",
      "Epoch 150601, Training Loss: 8503, Validation Loss: 66895, L1: 66895\n",
      "Epoch 150701, Training Loss: 8437, Validation Loss: 67197, L1: 67197\n",
      "Epoch 150801, Training Loss: 8387, Validation Loss: 67310, L1: 67310\n",
      "Epoch 150901, Training Loss: 8510, Validation Loss: 67127, L1: 67127\n",
      "Epoch 151001, Training Loss: 8536, Validation Loss: 67147, L1: 67147\n",
      "Epoch 151101, Training Loss: 8318, Validation Loss: 67263, L1: 67263\n",
      "Epoch 151201, Training Loss: 8321, Validation Loss: 67193, L1: 67193\n",
      "Epoch 151301, Training Loss: 8573, Validation Loss: 67247, L1: 67247\n",
      "Epoch 151401, Training Loss: 8279, Validation Loss: 67155, L1: 67155\n",
      "Epoch 151501, Training Loss: 8417, Validation Loss: 67219, L1: 67219\n",
      "Epoch 151601, Training Loss: 9944, Validation Loss: 67274, L1: 67274\n",
      "Epoch 151701, Training Loss: 9472, Validation Loss: 67291, L1: 67291\n",
      "Epoch 151801, Training Loss: 8392, Validation Loss: 67226, L1: 67226\n",
      "Epoch 151901, Training Loss: 8787, Validation Loss: 67094, L1: 67094\n",
      "Epoch 152001, Training Loss: 8330, Validation Loss: 67023, L1: 67023\n",
      "Epoch 152101, Training Loss: 8350, Validation Loss: 67210, L1: 67210\n",
      "Epoch 152201, Training Loss: 9003, Validation Loss: 67306, L1: 67306\n",
      "Epoch 152301, Training Loss: 8389, Validation Loss: 67115, L1: 67115\n",
      "Epoch 152401, Training Loss: 9642, Validation Loss: 67376, L1: 67376\n",
      "Epoch 152501, Training Loss: 8856, Validation Loss: 67482, L1: 67482\n",
      "Epoch 152601, Training Loss: 8374, Validation Loss: 67109, L1: 67109\n",
      "Epoch 152701, Training Loss: 8931, Validation Loss: 67149, L1: 67149\n",
      "Epoch 152801, Training Loss: 8337, Validation Loss: 67268, L1: 67268\n",
      "Epoch 152901, Training Loss: 8372, Validation Loss: 67287, L1: 67287\n",
      "Epoch 153001, Training Loss: 8378, Validation Loss: 67109, L1: 67109\n",
      "Epoch 153101, Training Loss: 8251, Validation Loss: 67327, L1: 67327\n",
      "Epoch 153201, Training Loss: 10247, Validation Loss: 67363, L1: 67363\n",
      "Epoch 153301, Training Loss: 10626, Validation Loss: 67487, L1: 67487\n",
      "Epoch 153401, Training Loss: 8218, Validation Loss: 67201, L1: 67201\n",
      "Epoch 153501, Training Loss: 10098, Validation Loss: 67260, L1: 67260\n",
      "Epoch 153601, Training Loss: 8218, Validation Loss: 67277, L1: 67277\n",
      "Epoch 153701, Training Loss: 8285, Validation Loss: 67276, L1: 67276\n",
      "Epoch 153801, Training Loss: 8822, Validation Loss: 67007, L1: 67007\n",
      "Epoch 153901, Training Loss: 8594, Validation Loss: 67230, L1: 67230\n",
      "Epoch 154001, Training Loss: 8859, Validation Loss: 67131, L1: 67131\n",
      "Epoch 154101, Training Loss: 8687, Validation Loss: 67098, L1: 67098\n",
      "Epoch 154201, Training Loss: 8384, Validation Loss: 67228, L1: 67228\n",
      "Epoch 154301, Training Loss: 8964, Validation Loss: 67326, L1: 67326\n",
      "Epoch 154401, Training Loss: 8520, Validation Loss: 66998, L1: 66998\n",
      "Epoch 154501, Training Loss: 8339, Validation Loss: 67109, L1: 67109\n",
      "Epoch 154601, Training Loss: 8813, Validation Loss: 67236, L1: 67236\n",
      "Epoch 154701, Training Loss: 8235, Validation Loss: 67201, L1: 67201\n",
      "Epoch 154801, Training Loss: 8158, Validation Loss: 67177, L1: 67177\n",
      "Epoch 154901, Training Loss: 8875, Validation Loss: 67309, L1: 67309\n",
      "Epoch 155001, Training Loss: 8224, Validation Loss: 67120, L1: 67120\n",
      "Epoch 155101, Training Loss: 8349, Validation Loss: 67059, L1: 67059\n",
      "Epoch 155201, Training Loss: 8854, Validation Loss: 67301, L1: 67301\n",
      "Epoch 155301, Training Loss: 9120, Validation Loss: 67266, L1: 67266\n",
      "Epoch 155401, Training Loss: 8310, Validation Loss: 67147, L1: 67147\n",
      "Epoch 155501, Training Loss: 8410, Validation Loss: 67067, L1: 67067\n",
      "Epoch 155601, Training Loss: 8216, Validation Loss: 67095, L1: 67095\n",
      "Epoch 155701, Training Loss: 8107, Validation Loss: 67222, L1: 67222\n",
      "Epoch 155801, Training Loss: 8168, Validation Loss: 67039, L1: 67039\n",
      "Epoch 155901, Training Loss: 8303, Validation Loss: 67012, L1: 67012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156001, Training Loss: 8414, Validation Loss: 67250, L1: 67250\n",
      "Epoch 156101, Training Loss: 8637, Validation Loss: 66973, L1: 66973\n",
      "Epoch 156201, Training Loss: 8071, Validation Loss: 67168, L1: 67168\n",
      "Epoch 156301, Training Loss: 8596, Validation Loss: 67130, L1: 67130\n",
      "Epoch 156401, Training Loss: 9502, Validation Loss: 67194, L1: 67194\n",
      "Epoch 156501, Training Loss: 8230, Validation Loss: 66847, L1: 66847\n",
      "Epoch 156601, Training Loss: 8254, Validation Loss: 66966, L1: 66966\n",
      "Epoch 156701, Training Loss: 8113, Validation Loss: 67086, L1: 67086\n",
      "Epoch 156801, Training Loss: 8265, Validation Loss: 67266, L1: 67266\n",
      "Epoch 156901, Training Loss: 8166, Validation Loss: 67176, L1: 67176\n",
      "Epoch 157001, Training Loss: 8486, Validation Loss: 67130, L1: 67130\n",
      "Epoch 157101, Training Loss: 8499, Validation Loss: 67145, L1: 67145\n",
      "Epoch 157201, Training Loss: 8074, Validation Loss: 67318, L1: 67318\n",
      "Epoch 157301, Training Loss: 8203, Validation Loss: 67181, L1: 67181\n",
      "Epoch 157401, Training Loss: 8206, Validation Loss: 67211, L1: 67211\n",
      "Epoch 157501, Training Loss: 8165, Validation Loss: 66954, L1: 66954\n",
      "Epoch 157601, Training Loss: 8247, Validation Loss: 67118, L1: 67118\n",
      "Epoch 157701, Training Loss: 8258, Validation Loss: 66941, L1: 66941\n",
      "Epoch 157801, Training Loss: 8626, Validation Loss: 67201, L1: 67201\n",
      "Epoch 157901, Training Loss: 8275, Validation Loss: 67149, L1: 67149\n",
      "Epoch 158001, Training Loss: 8602, Validation Loss: 67168, L1: 67168\n",
      "Epoch 158101, Training Loss: 8094, Validation Loss: 66989, L1: 66989\n",
      "Epoch 158201, Training Loss: 8169, Validation Loss: 66923, L1: 66923\n",
      "Epoch 158301, Training Loss: 8347, Validation Loss: 67054, L1: 67054\n",
      "Epoch 158401, Training Loss: 8009, Validation Loss: 67288, L1: 67288\n",
      "Epoch 158501, Training Loss: 8157, Validation Loss: 67151, L1: 67151\n",
      "Epoch 158601, Training Loss: 8228, Validation Loss: 67060, L1: 67060\n",
      "Epoch 158701, Training Loss: 8489, Validation Loss: 67098, L1: 67098\n",
      "Epoch 158801, Training Loss: 9146, Validation Loss: 66862, L1: 66862\n",
      "Epoch 158901, Training Loss: 8255, Validation Loss: 67106, L1: 67106\n",
      "Epoch 159001, Training Loss: 7867, Validation Loss: 67012, L1: 67012\n",
      "Epoch 159101, Training Loss: 8903, Validation Loss: 67182, L1: 67182\n",
      "Epoch 159201, Training Loss: 8542, Validation Loss: 67012, L1: 67012\n",
      "Epoch 159301, Training Loss: 8240, Validation Loss: 67039, L1: 67039\n",
      "Epoch 159401, Training Loss: 8123, Validation Loss: 66943, L1: 66943\n",
      "Epoch 159501, Training Loss: 8375, Validation Loss: 67127, L1: 67127\n",
      "Epoch 159601, Training Loss: 8737, Validation Loss: 67081, L1: 67081\n",
      "Epoch 159701, Training Loss: 7873, Validation Loss: 67117, L1: 67117\n",
      "Epoch 159801, Training Loss: 9334, Validation Loss: 67173, L1: 67173\n",
      "Epoch 159901, Training Loss: 8568, Validation Loss: 66897, L1: 66897\n",
      "Epoch 160001, Training Loss: 8325, Validation Loss: 66939, L1: 66939\n",
      "Epoch 160101, Training Loss: 8118, Validation Loss: 67265, L1: 67265\n",
      "Epoch 160201, Training Loss: 8063, Validation Loss: 66999, L1: 66999\n",
      "Epoch 160301, Training Loss: 7910, Validation Loss: 67068, L1: 67068\n",
      "Epoch 160401, Training Loss: 8151, Validation Loss: 67075, L1: 67075\n",
      "Epoch 160501, Training Loss: 7988, Validation Loss: 67181, L1: 67181\n",
      "Epoch 160601, Training Loss: 8213, Validation Loss: 67091, L1: 67091\n",
      "Epoch 160701, Training Loss: 8123, Validation Loss: 67085, L1: 67085\n",
      "Epoch 160801, Training Loss: 7870, Validation Loss: 67179, L1: 67179\n",
      "Epoch 160901, Training Loss: 8338, Validation Loss: 67191, L1: 67191\n",
      "Epoch 161001, Training Loss: 8089, Validation Loss: 67088, L1: 67088\n",
      "Epoch 161101, Training Loss: 8013, Validation Loss: 67157, L1: 67157\n",
      "Epoch 161201, Training Loss: 7974, Validation Loss: 67048, L1: 67048\n",
      "Epoch 161301, Training Loss: 8358, Validation Loss: 67196, L1: 67196\n",
      "Epoch 161401, Training Loss: 8100, Validation Loss: 66915, L1: 66915\n",
      "Epoch 161501, Training Loss: 7852, Validation Loss: 67110, L1: 67110\n",
      "Epoch 161601, Training Loss: 7881, Validation Loss: 67098, L1: 67098\n",
      "Epoch 161701, Training Loss: 7801, Validation Loss: 67308, L1: 67308\n",
      "Epoch 161801, Training Loss: 10390, Validation Loss: 67475, L1: 67475\n",
      "Epoch 161901, Training Loss: 8112, Validation Loss: 67272, L1: 67272\n",
      "Epoch 162001, Training Loss: 8459, Validation Loss: 67076, L1: 67076\n",
      "Epoch 162101, Training Loss: 8595, Validation Loss: 66867, L1: 66867\n",
      "Epoch 162201, Training Loss: 8198, Validation Loss: 66951, L1: 66951\n",
      "Epoch 162301, Training Loss: 7847, Validation Loss: 67185, L1: 67185\n",
      "Epoch 162401, Training Loss: 7754, Validation Loss: 67168, L1: 67168\n",
      "Epoch 162501, Training Loss: 8406, Validation Loss: 67140, L1: 67140\n",
      "Epoch 162601, Training Loss: 7746, Validation Loss: 67079, L1: 67079\n",
      "Epoch 162701, Training Loss: 8030, Validation Loss: 66875, L1: 66875\n",
      "Epoch 162801, Training Loss: 7946, Validation Loss: 66997, L1: 66997\n",
      "Epoch 162901, Training Loss: 7976, Validation Loss: 67087, L1: 67087\n",
      "Epoch 163001, Training Loss: 7886, Validation Loss: 67100, L1: 67100\n",
      "Epoch 163101, Training Loss: 8509, Validation Loss: 67273, L1: 67273\n",
      "Epoch 163201, Training Loss: 8288, Validation Loss: 67162, L1: 67162\n",
      "Epoch 163301, Training Loss: 7911, Validation Loss: 67009, L1: 67009\n",
      "Epoch 163401, Training Loss: 7645, Validation Loss: 67028, L1: 67028\n",
      "Epoch 163501, Training Loss: 7751, Validation Loss: 66970, L1: 66970\n",
      "Epoch 163601, Training Loss: 7775, Validation Loss: 67082, L1: 67082\n",
      "Epoch 163701, Training Loss: 8044, Validation Loss: 66987, L1: 66987\n",
      "Epoch 163801, Training Loss: 7677, Validation Loss: 67181, L1: 67181\n",
      "Epoch 163901, Training Loss: 8004, Validation Loss: 66853, L1: 66853\n",
      "Epoch 164001, Training Loss: 8456, Validation Loss: 67277, L1: 67277\n",
      "Epoch 164101, Training Loss: 7807, Validation Loss: 66958, L1: 66958\n",
      "Epoch 164201, Training Loss: 7714, Validation Loss: 67024, L1: 67024\n",
      "Epoch 164301, Training Loss: 7693, Validation Loss: 67082, L1: 67082\n",
      "Epoch 164401, Training Loss: 7650, Validation Loss: 67095, L1: 67095\n",
      "Epoch 164501, Training Loss: 7711, Validation Loss: 66993, L1: 66993\n",
      "Epoch 164601, Training Loss: 8020, Validation Loss: 67064, L1: 67064\n",
      "Epoch 164701, Training Loss: 7646, Validation Loss: 66960, L1: 66960\n",
      "Epoch 164801, Training Loss: 7727, Validation Loss: 67052, L1: 67052\n",
      "Epoch 164901, Training Loss: 7919, Validation Loss: 66952, L1: 66952\n",
      "Epoch 165001, Training Loss: 7682, Validation Loss: 67136, L1: 67136\n",
      "Epoch 165101, Training Loss: 8555, Validation Loss: 66973, L1: 66973\n",
      "Epoch 165201, Training Loss: 7659, Validation Loss: 67026, L1: 67026\n",
      "Epoch 165301, Training Loss: 8121, Validation Loss: 67035, L1: 67035\n",
      "Epoch 165401, Training Loss: 8338, Validation Loss: 67022, L1: 67022\n",
      "Epoch 165501, Training Loss: 8348, Validation Loss: 67068, L1: 67068\n",
      "Epoch 165601, Training Loss: 7554, Validation Loss: 67068, L1: 67068\n",
      "Epoch 165701, Training Loss: 8327, Validation Loss: 66872, L1: 66872\n",
      "Epoch 165801, Training Loss: 7802, Validation Loss: 67013, L1: 67013\n",
      "Epoch 165901, Training Loss: 8448, Validation Loss: 66879, L1: 66879\n",
      "Epoch 166001, Training Loss: 7657, Validation Loss: 67114, L1: 67114\n",
      "Epoch 166101, Training Loss: 7786, Validation Loss: 66939, L1: 66939\n",
      "Epoch 166201, Training Loss: 8247, Validation Loss: 67147, L1: 67147\n",
      "Epoch 166301, Training Loss: 7500, Validation Loss: 66852, L1: 66852\n",
      "Epoch 166401, Training Loss: 7700, Validation Loss: 66981, L1: 66981\n",
      "Epoch 166501, Training Loss: 8095, Validation Loss: 66964, L1: 66964\n",
      "Epoch 166601, Training Loss: 7501, Validation Loss: 67019, L1: 67019\n",
      "Epoch 166701, Training Loss: 7532, Validation Loss: 66964, L1: 66964\n",
      "Epoch 166801, Training Loss: 8019, Validation Loss: 67047, L1: 67047\n",
      "Epoch 166901, Training Loss: 7897, Validation Loss: 67112, L1: 67112\n",
      "Epoch 167001, Training Loss: 7530, Validation Loss: 67094, L1: 67094\n",
      "Epoch 167101, Training Loss: 7836, Validation Loss: 67061, L1: 67061\n",
      "Epoch 167201, Training Loss: 8222, Validation Loss: 67094, L1: 67094\n",
      "Epoch 167301, Training Loss: 8747, Validation Loss: 67140, L1: 67140\n",
      "Epoch 167401, Training Loss: 7639, Validation Loss: 67205, L1: 67205\n",
      "Epoch 167501, Training Loss: 8335, Validation Loss: 67138, L1: 67138\n",
      "Epoch 167601, Training Loss: 8707, Validation Loss: 67101, L1: 67101\n",
      "Epoch 167701, Training Loss: 7726, Validation Loss: 67015, L1: 67015\n",
      "Epoch 167801, Training Loss: 7769, Validation Loss: 67143, L1: 67143\n",
      "Epoch 167901, Training Loss: 7756, Validation Loss: 66951, L1: 66951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168001, Training Loss: 7446, Validation Loss: 67161, L1: 67161\n",
      "Epoch 168101, Training Loss: 8494, Validation Loss: 67258, L1: 67258\n",
      "Epoch 168201, Training Loss: 8090, Validation Loss: 67008, L1: 67008\n",
      "Epoch 168301, Training Loss: 7707, Validation Loss: 67020, L1: 67020\n",
      "Epoch 168401, Training Loss: 7552, Validation Loss: 66851, L1: 66851\n",
      "Epoch 168501, Training Loss: 7482, Validation Loss: 67191, L1: 67191\n",
      "Epoch 168601, Training Loss: 8597, Validation Loss: 67342, L1: 67342\n",
      "Epoch 168701, Training Loss: 7805, Validation Loss: 66669, L1: 66669\n",
      "Epoch 168801, Training Loss: 8240, Validation Loss: 66860, L1: 66860\n",
      "Epoch 168901, Training Loss: 7451, Validation Loss: 67069, L1: 67069\n",
      "Epoch 169001, Training Loss: 7315, Validation Loss: 66993, L1: 66993\n",
      "Epoch 169101, Training Loss: 7541, Validation Loss: 66961, L1: 66961\n",
      "Epoch 169201, Training Loss: 7379, Validation Loss: 66959, L1: 66959\n",
      "Epoch 169301, Training Loss: 7341, Validation Loss: 67155, L1: 67155\n",
      "Epoch 169401, Training Loss: 8324, Validation Loss: 67258, L1: 67258\n",
      "Epoch 169501, Training Loss: 7694, Validation Loss: 67135, L1: 67135\n",
      "Epoch 169601, Training Loss: 7537, Validation Loss: 66846, L1: 66846\n",
      "Epoch 169701, Training Loss: 7602, Validation Loss: 66880, L1: 66880\n",
      "Epoch 169801, Training Loss: 7604, Validation Loss: 67077, L1: 67077\n",
      "Epoch 169901, Training Loss: 8099, Validation Loss: 66943, L1: 66943\n",
      "Epoch 170001, Training Loss: 7318, Validation Loss: 67050, L1: 67050\n",
      "Epoch 170101, Training Loss: 7894, Validation Loss: 66950, L1: 66950\n",
      "Epoch 170201, Training Loss: 8108, Validation Loss: 67006, L1: 67006\n",
      "Epoch 170301, Training Loss: 7882, Validation Loss: 67260, L1: 67260\n",
      "Epoch 170401, Training Loss: 7431, Validation Loss: 66923, L1: 66923\n",
      "Epoch 170501, Training Loss: 7744, Validation Loss: 67067, L1: 67067\n",
      "Epoch 170601, Training Loss: 8324, Validation Loss: 67129, L1: 67129\n",
      "Epoch 170701, Training Loss: 7352, Validation Loss: 66945, L1: 66945\n",
      "Epoch 170801, Training Loss: 7550, Validation Loss: 66988, L1: 66988\n",
      "Epoch 170901, Training Loss: 7400, Validation Loss: 66909, L1: 66909\n",
      "Epoch 171001, Training Loss: 7562, Validation Loss: 66989, L1: 66989\n",
      "Epoch 171101, Training Loss: 7928, Validation Loss: 66870, L1: 66870\n",
      "Epoch 171201, Training Loss: 7298, Validation Loss: 66922, L1: 66922\n",
      "Epoch 171301, Training Loss: 7498, Validation Loss: 66917, L1: 66917\n",
      "Epoch 171401, Training Loss: 7191, Validation Loss: 66946, L1: 66946\n",
      "Epoch 171501, Training Loss: 7627, Validation Loss: 67016, L1: 67016\n",
      "Epoch 171601, Training Loss: 7642, Validation Loss: 67189, L1: 67189\n",
      "Epoch 171701, Training Loss: 7333, Validation Loss: 66736, L1: 66736\n",
      "Epoch 171801, Training Loss: 8637, Validation Loss: 66991, L1: 66991\n",
      "Epoch 171901, Training Loss: 7292, Validation Loss: 66978, L1: 66978\n",
      "Epoch 172001, Training Loss: 7258, Validation Loss: 66931, L1: 66931\n",
      "Epoch 172101, Training Loss: 7264, Validation Loss: 66615, L1: 66615\n",
      "Epoch 172201, Training Loss: 7388, Validation Loss: 67205, L1: 67205\n",
      "Epoch 172301, Training Loss: 9210, Validation Loss: 66892, L1: 66892\n",
      "Epoch 172401, Training Loss: 7869, Validation Loss: 67280, L1: 67280\n",
      "Epoch 172501, Training Loss: 8181, Validation Loss: 66838, L1: 66838\n",
      "Epoch 172601, Training Loss: 7353, Validation Loss: 66886, L1: 66886\n",
      "Epoch 172701, Training Loss: 7952, Validation Loss: 66665, L1: 66665\n",
      "Epoch 172801, Training Loss: 7444, Validation Loss: 67354, L1: 67354\n",
      "Epoch 172901, Training Loss: 7776, Validation Loss: 66648, L1: 66648\n",
      "Epoch 173001, Training Loss: 7182, Validation Loss: 66876, L1: 66876\n",
      "Epoch 173101, Training Loss: 7097, Validation Loss: 66823, L1: 66823\n",
      "Epoch 173201, Training Loss: 7226, Validation Loss: 67208, L1: 67208\n",
      "Epoch 173301, Training Loss: 7161, Validation Loss: 67151, L1: 67151\n",
      "Epoch 173401, Training Loss: 7156, Validation Loss: 66646, L1: 66646\n",
      "Epoch 173501, Training Loss: 7252, Validation Loss: 66758, L1: 66758\n",
      "Epoch 173601, Training Loss: 7427, Validation Loss: 66923, L1: 66923\n",
      "Epoch 173701, Training Loss: 7130, Validation Loss: 67003, L1: 67003\n",
      "Epoch 173801, Training Loss: 7333, Validation Loss: 66794, L1: 66794\n",
      "Epoch 173901, Training Loss: 7301, Validation Loss: 66927, L1: 66927\n",
      "Epoch 174001, Training Loss: 7226, Validation Loss: 67058, L1: 67058\n",
      "Epoch 174101, Training Loss: 7114, Validation Loss: 66715, L1: 66715\n",
      "Epoch 174201, Training Loss: 8572, Validation Loss: 66862, L1: 66862\n",
      "Epoch 174301, Training Loss: 7440, Validation Loss: 66916, L1: 66916\n",
      "Epoch 174401, Training Loss: 7384, Validation Loss: 66701, L1: 66701\n",
      "Epoch 174501, Training Loss: 7071, Validation Loss: 66852, L1: 66852\n",
      "Epoch 174601, Training Loss: 7163, Validation Loss: 66512, L1: 66512\n",
      "Epoch 174701, Training Loss: 7308, Validation Loss: 66843, L1: 66843\n",
      "Epoch 174801, Training Loss: 7166, Validation Loss: 67078, L1: 67078\n",
      "Epoch 174901, Training Loss: 7071, Validation Loss: 66968, L1: 66968\n",
      "Epoch 175001, Training Loss: 8098, Validation Loss: 66926, L1: 66926\n",
      "Epoch 175101, Training Loss: 7293, Validation Loss: 67127, L1: 67127\n",
      "Epoch 175201, Training Loss: 7556, Validation Loss: 66715, L1: 66715\n",
      "Epoch 175301, Training Loss: 7888, Validation Loss: 66672, L1: 66672\n",
      "Epoch 175401, Training Loss: 7002, Validation Loss: 66770, L1: 66770\n",
      "Epoch 175501, Training Loss: 7277, Validation Loss: 66782, L1: 66782\n",
      "Epoch 175601, Training Loss: 7289, Validation Loss: 67089, L1: 67089\n",
      "Epoch 175701, Training Loss: 7056, Validation Loss: 67116, L1: 67116\n",
      "Epoch 175801, Training Loss: 7713, Validation Loss: 67034, L1: 67034\n",
      "Epoch 175901, Training Loss: 7122, Validation Loss: 66633, L1: 66633\n",
      "Epoch 176001, Training Loss: 7564, Validation Loss: 66781, L1: 66781\n",
      "Epoch 176101, Training Loss: 7320, Validation Loss: 66810, L1: 66810\n",
      "Epoch 176201, Training Loss: 6926, Validation Loss: 66679, L1: 66679\n",
      "Epoch 176301, Training Loss: 6935, Validation Loss: 66742, L1: 66742\n",
      "Epoch 176401, Training Loss: 6971, Validation Loss: 66690, L1: 66690\n",
      "Epoch 176501, Training Loss: 6981, Validation Loss: 66775, L1: 66775\n",
      "Epoch 176601, Training Loss: 7286, Validation Loss: 67014, L1: 67014\n",
      "Epoch 176701, Training Loss: 7487, Validation Loss: 66959, L1: 66959\n",
      "Epoch 176801, Training Loss: 6978, Validation Loss: 67009, L1: 67009\n",
      "Epoch 176901, Training Loss: 7434, Validation Loss: 66880, L1: 66880\n",
      "Epoch 177001, Training Loss: 7495, Validation Loss: 66383, L1: 66383\n",
      "Epoch 177101, Training Loss: 7107, Validation Loss: 66482, L1: 66482\n",
      "Epoch 177201, Training Loss: 6948, Validation Loss: 66972, L1: 66972\n",
      "Epoch 177301, Training Loss: 7114, Validation Loss: 66787, L1: 66787\n",
      "Epoch 177401, Training Loss: 6835, Validation Loss: 66863, L1: 66863\n",
      "Epoch 177501, Training Loss: 7005, Validation Loss: 67048, L1: 67048\n",
      "Epoch 177601, Training Loss: 7060, Validation Loss: 66820, L1: 66820\n",
      "Epoch 177701, Training Loss: 7077, Validation Loss: 67187, L1: 67187\n",
      "Epoch 177801, Training Loss: 7050, Validation Loss: 67024, L1: 67024\n",
      "Epoch 177901, Training Loss: 6981, Validation Loss: 66680, L1: 66680\n",
      "Epoch 178001, Training Loss: 7849, Validation Loss: 66793, L1: 66793\n",
      "Epoch 178101, Training Loss: 6809, Validation Loss: 66911, L1: 66911\n",
      "Epoch 178201, Training Loss: 7160, Validation Loss: 66863, L1: 66863\n",
      "Epoch 178301, Training Loss: 7139, Validation Loss: 66927, L1: 66927\n",
      "Epoch 178401, Training Loss: 7251, Validation Loss: 66932, L1: 66932\n",
      "Epoch 178501, Training Loss: 6825, Validation Loss: 66911, L1: 66911\n",
      "Epoch 178601, Training Loss: 7240, Validation Loss: 66706, L1: 66706\n",
      "Epoch 178701, Training Loss: 7666, Validation Loss: 67027, L1: 67027\n",
      "Epoch 178801, Training Loss: 6996, Validation Loss: 67023, L1: 67023\n",
      "Epoch 178901, Training Loss: 7023, Validation Loss: 66585, L1: 66585\n",
      "Epoch 179001, Training Loss: 7743, Validation Loss: 66878, L1: 66878\n",
      "Epoch 179101, Training Loss: 7493, Validation Loss: 66826, L1: 66826\n",
      "Epoch 179201, Training Loss: 8858, Validation Loss: 67067, L1: 67067\n",
      "Epoch 179301, Training Loss: 7120, Validation Loss: 67391, L1: 67391\n",
      "Epoch 179401, Training Loss: 7560, Validation Loss: 66238, L1: 66238\n",
      "Epoch 179501, Training Loss: 8623, Validation Loss: 66997, L1: 66997\n",
      "Epoch 179601, Training Loss: 6998, Validation Loss: 66613, L1: 66613\n",
      "Epoch 179701, Training Loss: 6763, Validation Loss: 66864, L1: 66864\n",
      "Epoch 179801, Training Loss: 7130, Validation Loss: 66919, L1: 66919\n",
      "Epoch 179901, Training Loss: 7294, Validation Loss: 66480, L1: 66480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180001, Training Loss: 7364, Validation Loss: 66801, L1: 66801\n",
      "Epoch 180101, Training Loss: 6876, Validation Loss: 66577, L1: 66577\n",
      "Epoch 180201, Training Loss: 6828, Validation Loss: 66547, L1: 66547\n",
      "Epoch 180301, Training Loss: 7270, Validation Loss: 66337, L1: 66337\n",
      "Epoch 180401, Training Loss: 7255, Validation Loss: 66334, L1: 66334\n",
      "Epoch 180501, Training Loss: 6765, Validation Loss: 66592, L1: 66592\n",
      "Epoch 180601, Training Loss: 7221, Validation Loss: 66379, L1: 66379\n",
      "Epoch 180701, Training Loss: 6894, Validation Loss: 66727, L1: 66727\n",
      "Epoch 180801, Training Loss: 7076, Validation Loss: 67099, L1: 67099\n",
      "Epoch 180901, Training Loss: 7905, Validation Loss: 65939, L1: 65939\n",
      "Epoch 181001, Training Loss: 6844, Validation Loss: 66887, L1: 66887\n",
      "Epoch 181101, Training Loss: 6829, Validation Loss: 66796, L1: 66796\n",
      "Epoch 181201, Training Loss: 6842, Validation Loss: 66781, L1: 66781\n",
      "Epoch 181301, Training Loss: 6740, Validation Loss: 66853, L1: 66853\n",
      "Epoch 181401, Training Loss: 6820, Validation Loss: 66623, L1: 66623\n",
      "Epoch 181501, Training Loss: 6672, Validation Loss: 66384, L1: 66384\n",
      "Epoch 181601, Training Loss: 7255, Validation Loss: 66812, L1: 66812\n",
      "Epoch 181701, Training Loss: 7340, Validation Loss: 66879, L1: 66879\n",
      "Epoch 181801, Training Loss: 6855, Validation Loss: 66267, L1: 66267\n",
      "Epoch 181901, Training Loss: 6957, Validation Loss: 66896, L1: 66896\n",
      "Epoch 182001, Training Loss: 6956, Validation Loss: 66578, L1: 66578\n",
      "Epoch 182101, Training Loss: 6872, Validation Loss: 66652, L1: 66652\n",
      "Epoch 182201, Training Loss: 7337, Validation Loss: 66982, L1: 66982\n",
      "Epoch 182301, Training Loss: 7273, Validation Loss: 66614, L1: 66614\n",
      "Epoch 182401, Training Loss: 8615, Validation Loss: 66350, L1: 66350\n",
      "Epoch 182501, Training Loss: 7104, Validation Loss: 66134, L1: 66134\n",
      "Epoch 182601, Training Loss: 6907, Validation Loss: 66687, L1: 66687\n",
      "Epoch 182701, Training Loss: 6947, Validation Loss: 67019, L1: 67019\n",
      "Epoch 182801, Training Loss: 6713, Validation Loss: 66915, L1: 66915\n",
      "Epoch 182901, Training Loss: 6660, Validation Loss: 66789, L1: 66789\n",
      "Epoch 183001, Training Loss: 7442, Validation Loss: 66783, L1: 66783\n",
      "Epoch 183101, Training Loss: 6939, Validation Loss: 66970, L1: 66970\n",
      "Epoch 183201, Training Loss: 7092, Validation Loss: 66967, L1: 66967\n",
      "Epoch 183301, Training Loss: 6610, Validation Loss: 66852, L1: 66852\n",
      "Epoch 183401, Training Loss: 7508, Validation Loss: 66729, L1: 66729\n",
      "Epoch 183501, Training Loss: 6619, Validation Loss: 66716, L1: 66716\n",
      "Epoch 183601, Training Loss: 6655, Validation Loss: 66334, L1: 66334\n",
      "Epoch 183701, Training Loss: 6644, Validation Loss: 66557, L1: 66557\n",
      "Epoch 183801, Training Loss: 6823, Validation Loss: 66802, L1: 66802\n",
      "Epoch 183901, Training Loss: 6748, Validation Loss: 66505, L1: 66505\n",
      "Epoch 184001, Training Loss: 7781, Validation Loss: 66002, L1: 66002\n",
      "Epoch 184101, Training Loss: 6599, Validation Loss: 66189, L1: 66189\n",
      "Epoch 184201, Training Loss: 6687, Validation Loss: 66603, L1: 66603\n",
      "Epoch 184301, Training Loss: 7120, Validation Loss: 66500, L1: 66500\n",
      "Epoch 184401, Training Loss: 6684, Validation Loss: 66846, L1: 66846\n",
      "Epoch 184501, Training Loss: 6864, Validation Loss: 66807, L1: 66807\n",
      "Epoch 184601, Training Loss: 6614, Validation Loss: 66787, L1: 66787\n",
      "Epoch 184701, Training Loss: 6968, Validation Loss: 67019, L1: 67019\n",
      "Epoch 184801, Training Loss: 7649, Validation Loss: 66563, L1: 66563\n",
      "Epoch 184901, Training Loss: 8121, Validation Loss: 66308, L1: 66308\n",
      "Epoch 185001, Training Loss: 7479, Validation Loss: 66083, L1: 66083\n",
      "Epoch 185101, Training Loss: 6885, Validation Loss: 66755, L1: 66755\n",
      "Epoch 185201, Training Loss: 7154, Validation Loss: 66942, L1: 66942\n",
      "Epoch 185301, Training Loss: 6869, Validation Loss: 66608, L1: 66608\n",
      "Epoch 185401, Training Loss: 6739, Validation Loss: 66710, L1: 66710\n",
      "Epoch 185501, Training Loss: 6576, Validation Loss: 66548, L1: 66548\n",
      "Epoch 185601, Training Loss: 7087, Validation Loss: 66785, L1: 66785\n",
      "Epoch 185701, Training Loss: 6485, Validation Loss: 66408, L1: 66408\n",
      "Epoch 185801, Training Loss: 7136, Validation Loss: 66688, L1: 66688\n",
      "Epoch 185901, Training Loss: 7444, Validation Loss: 67063, L1: 67063\n",
      "Epoch 186001, Training Loss: 6631, Validation Loss: 66372, L1: 66372\n",
      "Epoch 186101, Training Loss: 7388, Validation Loss: 66846, L1: 66846\n",
      "Epoch 186201, Training Loss: 6604, Validation Loss: 66883, L1: 66883\n",
      "Epoch 186301, Training Loss: 6602, Validation Loss: 66834, L1: 66834\n",
      "Epoch 186401, Training Loss: 6741, Validation Loss: 67127, L1: 67127\n",
      "Epoch 186501, Training Loss: 7772, Validation Loss: 65991, L1: 65991\n",
      "Epoch 186601, Training Loss: 7140, Validation Loss: 66432, L1: 66432\n",
      "Epoch 186701, Training Loss: 6629, Validation Loss: 67033, L1: 67033\n",
      "Epoch 186801, Training Loss: 6704, Validation Loss: 66644, L1: 66644\n",
      "Epoch 186901, Training Loss: 6450, Validation Loss: 66914, L1: 66914\n",
      "Epoch 187001, Training Loss: 6602, Validation Loss: 66574, L1: 66574\n",
      "Epoch 187101, Training Loss: 6688, Validation Loss: 66644, L1: 66644\n",
      "Epoch 187201, Training Loss: 7099, Validation Loss: 66412, L1: 66412\n",
      "Epoch 187301, Training Loss: 7273, Validation Loss: 65876, L1: 65876\n",
      "Epoch 187401, Training Loss: 8163, Validation Loss: 66737, L1: 66737\n",
      "Epoch 187501, Training Loss: 6885, Validation Loss: 66944, L1: 66944\n",
      "Epoch 187601, Training Loss: 7386, Validation Loss: 66185, L1: 66185\n",
      "Epoch 187701, Training Loss: 7453, Validation Loss: 66123, L1: 66123\n",
      "Epoch 187801, Training Loss: 6675, Validation Loss: 66808, L1: 66808\n",
      "Epoch 187901, Training Loss: 6336, Validation Loss: 66861, L1: 66861\n",
      "Epoch 188001, Training Loss: 6592, Validation Loss: 66603, L1: 66603\n",
      "Epoch 188101, Training Loss: 7546, Validation Loss: 66740, L1: 66740\n",
      "Epoch 188201, Training Loss: 6634, Validation Loss: 66763, L1: 66763\n",
      "Epoch 188301, Training Loss: 7617, Validation Loss: 65937, L1: 65937\n",
      "Epoch 188401, Training Loss: 6393, Validation Loss: 66614, L1: 66614\n",
      "Epoch 188501, Training Loss: 7152, Validation Loss: 66363, L1: 66363\n",
      "Epoch 188601, Training Loss: 6834, Validation Loss: 66347, L1: 66347\n",
      "Epoch 188701, Training Loss: 7024, Validation Loss: 66821, L1: 66821\n",
      "Epoch 188801, Training Loss: 7788, Validation Loss: 65823, L1: 65823\n",
      "Epoch 188901, Training Loss: 6500, Validation Loss: 66643, L1: 66643\n",
      "Epoch 189001, Training Loss: 6801, Validation Loss: 66641, L1: 66641\n",
      "Epoch 189101, Training Loss: 6803, Validation Loss: 66465, L1: 66465\n",
      "Epoch 189201, Training Loss: 6562, Validation Loss: 66556, L1: 66556\n",
      "Epoch 189301, Training Loss: 6441, Validation Loss: 67075, L1: 67075\n",
      "Epoch 189401, Training Loss: 6564, Validation Loss: 66882, L1: 66882\n",
      "Epoch 189501, Training Loss: 6398, Validation Loss: 66610, L1: 66610\n",
      "Epoch 189601, Training Loss: 6246, Validation Loss: 66799, L1: 66799\n",
      "Epoch 189701, Training Loss: 7154, Validation Loss: 66354, L1: 66354\n",
      "Epoch 189801, Training Loss: 6401, Validation Loss: 66865, L1: 66865\n",
      "Epoch 189901, Training Loss: 6592, Validation Loss: 66928, L1: 66928\n",
      "Epoch 190001, Training Loss: 6773, Validation Loss: 66783, L1: 66783\n",
      "Epoch 190101, Training Loss: 6670, Validation Loss: 66911, L1: 66911\n",
      "Epoch 190201, Training Loss: 7300, Validation Loss: 67035, L1: 67035\n",
      "Epoch 190301, Training Loss: 7996, Validation Loss: 66714, L1: 66714\n",
      "Epoch 190401, Training Loss: 6967, Validation Loss: 66740, L1: 66740\n",
      "Epoch 190501, Training Loss: 7020, Validation Loss: 66768, L1: 66768\n",
      "Epoch 190601, Training Loss: 6242, Validation Loss: 66639, L1: 66639\n",
      "Epoch 190701, Training Loss: 6748, Validation Loss: 66828, L1: 66828\n",
      "Epoch 190801, Training Loss: 7834, Validation Loss: 67251, L1: 67251\n",
      "Epoch 190901, Training Loss: 6881, Validation Loss: 66683, L1: 66683\n",
      "Epoch 191001, Training Loss: 6858, Validation Loss: 65959, L1: 65959\n",
      "Epoch 191101, Training Loss: 6526, Validation Loss: 66342, L1: 66342\n",
      "Epoch 191201, Training Loss: 6740, Validation Loss: 66779, L1: 66779\n",
      "Epoch 191301, Training Loss: 6490, Validation Loss: 67132, L1: 67132\n",
      "Epoch 191401, Training Loss: 7348, Validation Loss: 66660, L1: 66660\n",
      "Epoch 191501, Training Loss: 6339, Validation Loss: 66615, L1: 66615\n",
      "Epoch 191601, Training Loss: 6377, Validation Loss: 66263, L1: 66263\n",
      "Epoch 191701, Training Loss: 7380, Validation Loss: 66681, L1: 66681\n",
      "Epoch 191801, Training Loss: 6643, Validation Loss: 66451, L1: 66451\n",
      "Epoch 191901, Training Loss: 6382, Validation Loss: 66812, L1: 66812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192001, Training Loss: 6364, Validation Loss: 66773, L1: 66773\n",
      "Epoch 192101, Training Loss: 7307, Validation Loss: 66502, L1: 66502\n",
      "Epoch 192201, Training Loss: 6791, Validation Loss: 66414, L1: 66414\n",
      "Epoch 192301, Training Loss: 6512, Validation Loss: 66488, L1: 66488\n",
      "Epoch 192401, Training Loss: 6796, Validation Loss: 66299, L1: 66299\n",
      "Epoch 192501, Training Loss: 7122, Validation Loss: 66435, L1: 66435\n",
      "Epoch 192601, Training Loss: 6440, Validation Loss: 66959, L1: 66959\n",
      "Epoch 192701, Training Loss: 6418, Validation Loss: 66268, L1: 66268\n",
      "Epoch 192801, Training Loss: 6242, Validation Loss: 66721, L1: 66721\n",
      "Epoch 192901, Training Loss: 6615, Validation Loss: 66444, L1: 66444\n",
      "Epoch 193001, Training Loss: 6485, Validation Loss: 66448, L1: 66448\n",
      "Epoch 193101, Training Loss: 6524, Validation Loss: 66357, L1: 66357\n",
      "Epoch 193201, Training Loss: 6798, Validation Loss: 66807, L1: 66807\n",
      "Epoch 193301, Training Loss: 6525, Validation Loss: 66858, L1: 66858\n",
      "Epoch 193401, Training Loss: 6293, Validation Loss: 66473, L1: 66473\n",
      "Epoch 193501, Training Loss: 6676, Validation Loss: 66773, L1: 66773\n",
      "Epoch 193601, Training Loss: 6642, Validation Loss: 66852, L1: 66852\n",
      "Epoch 193701, Training Loss: 6767, Validation Loss: 66019, L1: 66019\n",
      "Epoch 193801, Training Loss: 6848, Validation Loss: 66976, L1: 66976\n",
      "Epoch 193901, Training Loss: 6904, Validation Loss: 66862, L1: 66862\n",
      "Epoch 194001, Training Loss: 7654, Validation Loss: 66482, L1: 66482\n",
      "Epoch 194101, Training Loss: 6345, Validation Loss: 66259, L1: 66259\n",
      "Epoch 194201, Training Loss: 6624, Validation Loss: 66458, L1: 66458\n",
      "Epoch 194301, Training Loss: 7252, Validation Loss: 66151, L1: 66151\n",
      "Epoch 194401, Training Loss: 7002, Validation Loss: 66261, L1: 66261\n",
      "Epoch 194501, Training Loss: 7030, Validation Loss: 66464, L1: 66464\n",
      "Epoch 194601, Training Loss: 6201, Validation Loss: 66822, L1: 66822\n",
      "Epoch 194701, Training Loss: 6266, Validation Loss: 66770, L1: 66770\n",
      "Epoch 194801, Training Loss: 7260, Validation Loss: 66398, L1: 66398\n",
      "Epoch 194901, Training Loss: 6478, Validation Loss: 66755, L1: 66755\n",
      "Epoch 195001, Training Loss: 7039, Validation Loss: 66717, L1: 66717\n",
      "Epoch 195101, Training Loss: 6524, Validation Loss: 66849, L1: 66849\n",
      "Epoch 195201, Training Loss: 6379, Validation Loss: 66637, L1: 66637\n",
      "Epoch 195301, Training Loss: 7815, Validation Loss: 66612, L1: 66612\n",
      "Epoch 195401, Training Loss: 6698, Validation Loss: 66569, L1: 66569\n",
      "Epoch 195501, Training Loss: 6245, Validation Loss: 66868, L1: 66868\n",
      "Epoch 195601, Training Loss: 6073, Validation Loss: 66653, L1: 66653\n",
      "Epoch 195701, Training Loss: 6544, Validation Loss: 66358, L1: 66358\n",
      "Epoch 195801, Training Loss: 6804, Validation Loss: 66550, L1: 66550\n",
      "Epoch 195901, Training Loss: 6097, Validation Loss: 66450, L1: 66450\n",
      "Epoch 196001, Training Loss: 6518, Validation Loss: 66700, L1: 66700\n",
      "Epoch 196101, Training Loss: 6159, Validation Loss: 66725, L1: 66725\n",
      "Epoch 196201, Training Loss: 6426, Validation Loss: 66694, L1: 66694\n",
      "Epoch 196301, Training Loss: 6856, Validation Loss: 66643, L1: 66643\n",
      "Epoch 196401, Training Loss: 6292, Validation Loss: 66440, L1: 66440\n",
      "Epoch 196501, Training Loss: 6109, Validation Loss: 66701, L1: 66701\n",
      "Epoch 196601, Training Loss: 6104, Validation Loss: 66751, L1: 66751\n",
      "Epoch 196701, Training Loss: 6305, Validation Loss: 67059, L1: 67059\n",
      "Epoch 196801, Training Loss: 6082, Validation Loss: 66667, L1: 66667\n",
      "Epoch 196901, Training Loss: 6718, Validation Loss: 66482, L1: 66482\n",
      "Epoch 197001, Training Loss: 6395, Validation Loss: 66468, L1: 66468\n",
      "Epoch 197101, Training Loss: 6180, Validation Loss: 66847, L1: 66847\n",
      "Epoch 197201, Training Loss: 6655, Validation Loss: 66257, L1: 66257\n",
      "Epoch 197301, Training Loss: 6473, Validation Loss: 67279, L1: 67279\n",
      "Epoch 197401, Training Loss: 6072, Validation Loss: 66563, L1: 66563\n",
      "Epoch 197501, Training Loss: 6393, Validation Loss: 66631, L1: 66631\n",
      "Epoch 197601, Training Loss: 6176, Validation Loss: 66691, L1: 66691\n",
      "Epoch 197701, Training Loss: 6361, Validation Loss: 66861, L1: 66861\n",
      "Epoch 197801, Training Loss: 7061, Validation Loss: 66438, L1: 66438\n",
      "Epoch 197901, Training Loss: 6498, Validation Loss: 66883, L1: 66883\n",
      "Epoch 198001, Training Loss: 6524, Validation Loss: 66887, L1: 66887\n",
      "Epoch 198101, Training Loss: 8138, Validation Loss: 66080, L1: 66080\n",
      "Epoch 198201, Training Loss: 6281, Validation Loss: 66682, L1: 66682\n",
      "Epoch 198301, Training Loss: 6327, Validation Loss: 66418, L1: 66418\n",
      "Epoch 198401, Training Loss: 6540, Validation Loss: 66501, L1: 66501\n",
      "Epoch 198501, Training Loss: 6139, Validation Loss: 66403, L1: 66403\n",
      "Epoch 198601, Training Loss: 5995, Validation Loss: 66675, L1: 66675\n",
      "Epoch 198701, Training Loss: 6169, Validation Loss: 66587, L1: 66587\n",
      "Epoch 198801, Training Loss: 5971, Validation Loss: 66709, L1: 66709\n",
      "Epoch 198901, Training Loss: 6054, Validation Loss: 66489, L1: 66489\n",
      "Epoch 199001, Training Loss: 6450, Validation Loss: 66665, L1: 66665\n",
      "Epoch 199101, Training Loss: 6266, Validation Loss: 66700, L1: 66700\n",
      "Epoch 199201, Training Loss: 7298, Validation Loss: 65841, L1: 65841\n",
      "Epoch 199301, Training Loss: 7522, Validation Loss: 66793, L1: 66793\n",
      "Epoch 199401, Training Loss: 6262, Validation Loss: 66585, L1: 66585\n",
      "Epoch 199501, Training Loss: 6506, Validation Loss: 66582, L1: 66582\n",
      "Epoch 199601, Training Loss: 6103, Validation Loss: 66803, L1: 66803\n",
      "Epoch 199701, Training Loss: 6755, Validation Loss: 66891, L1: 66891\n",
      "Epoch 199801, Training Loss: 6105, Validation Loss: 66584, L1: 66584\n",
      "Epoch 199901, Training Loss: 7191, Validation Loss: 66445, L1: 66445\n",
      "Epoch 200001, Training Loss: 5958, Validation Loss: 66786, L1: 66786\n",
      "Epoch 200101, Training Loss: 6236, Validation Loss: 66227, L1: 66227\n",
      "Epoch 200201, Training Loss: 6642, Validation Loss: 66345, L1: 66345\n",
      "Epoch 200301, Training Loss: 5977, Validation Loss: 66966, L1: 66966\n",
      "Epoch 200401, Training Loss: 6258, Validation Loss: 66523, L1: 66523\n",
      "Epoch 200501, Training Loss: 6085, Validation Loss: 66245, L1: 66245\n",
      "Epoch 200601, Training Loss: 7083, Validation Loss: 66605, L1: 66605\n",
      "Epoch 200701, Training Loss: 6793, Validation Loss: 66467, L1: 66467\n",
      "Epoch 200801, Training Loss: 6311, Validation Loss: 66617, L1: 66617\n",
      "Epoch 200901, Training Loss: 6172, Validation Loss: 66825, L1: 66825\n",
      "Epoch 201001, Training Loss: 7899, Validation Loss: 67437, L1: 67437\n",
      "Epoch 201101, Training Loss: 6393, Validation Loss: 66586, L1: 66586\n",
      "Epoch 201201, Training Loss: 6602, Validation Loss: 66352, L1: 66352\n",
      "Epoch 201301, Training Loss: 6185, Validation Loss: 66871, L1: 66871\n",
      "Epoch 201401, Training Loss: 6482, Validation Loss: 66796, L1: 66796\n",
      "Epoch 201501, Training Loss: 6601, Validation Loss: 66830, L1: 66830\n",
      "Epoch 201601, Training Loss: 6124, Validation Loss: 66591, L1: 66591\n",
      "Epoch 201701, Training Loss: 6723, Validation Loss: 66462, L1: 66462\n",
      "Epoch 201801, Training Loss: 6126, Validation Loss: 66898, L1: 66898\n",
      "Epoch 201901, Training Loss: 6857, Validation Loss: 67008, L1: 67008\n",
      "Epoch 202001, Training Loss: 6188, Validation Loss: 66502, L1: 66502\n",
      "Epoch 202101, Training Loss: 6808, Validation Loss: 66816, L1: 66816\n",
      "Epoch 202201, Training Loss: 6060, Validation Loss: 66785, L1: 66785\n",
      "Epoch 202301, Training Loss: 6309, Validation Loss: 66484, L1: 66484\n",
      "Epoch 202401, Training Loss: 6016, Validation Loss: 66490, L1: 66490\n",
      "Epoch 202501, Training Loss: 7165, Validation Loss: 66595, L1: 66595\n",
      "Epoch 202601, Training Loss: 6211, Validation Loss: 66456, L1: 66456\n",
      "Epoch 202701, Training Loss: 6158, Validation Loss: 66699, L1: 66699\n",
      "Epoch 202801, Training Loss: 6797, Validation Loss: 66280, L1: 66280\n",
      "Epoch 202901, Training Loss: 5920, Validation Loss: 67149, L1: 67149\n",
      "Epoch 203001, Training Loss: 6538, Validation Loss: 66102, L1: 66102\n",
      "Epoch 203101, Training Loss: 5921, Validation Loss: 66729, L1: 66729\n",
      "Epoch 203201, Training Loss: 6238, Validation Loss: 66870, L1: 66870\n",
      "Epoch 203301, Training Loss: 6546, Validation Loss: 66770, L1: 66770\n",
      "Epoch 203401, Training Loss: 5952, Validation Loss: 67016, L1: 67016\n",
      "Epoch 203501, Training Loss: 6701, Validation Loss: 66355, L1: 66355\n",
      "Epoch 203601, Training Loss: 6359, Validation Loss: 66834, L1: 66834\n",
      "Epoch 203701, Training Loss: 6927, Validation Loss: 66035, L1: 66035\n",
      "Epoch 203801, Training Loss: 5943, Validation Loss: 66437, L1: 66437\n",
      "Epoch 203901, Training Loss: 6801, Validation Loss: 66909, L1: 66909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204001, Training Loss: 6307, Validation Loss: 66145, L1: 66145\n",
      "Epoch 204101, Training Loss: 6327, Validation Loss: 66511, L1: 66511\n",
      "Epoch 204201, Training Loss: 6165, Validation Loss: 67067, L1: 67067\n",
      "Epoch 204301, Training Loss: 6535, Validation Loss: 66383, L1: 66383\n",
      "Epoch 204401, Training Loss: 6726, Validation Loss: 66323, L1: 66323\n",
      "Epoch 204501, Training Loss: 6882, Validation Loss: 66437, L1: 66437\n",
      "Epoch 204601, Training Loss: 6487, Validation Loss: 66858, L1: 66858\n",
      "Epoch 204701, Training Loss: 6148, Validation Loss: 66712, L1: 66712\n",
      "Epoch 204801, Training Loss: 5917, Validation Loss: 66762, L1: 66762\n",
      "Epoch 204901, Training Loss: 6347, Validation Loss: 66880, L1: 66880\n",
      "Epoch 205001, Training Loss: 6437, Validation Loss: 66664, L1: 66664\n",
      "Epoch 205101, Training Loss: 6231, Validation Loss: 66655, L1: 66655\n",
      "Epoch 205201, Training Loss: 5896, Validation Loss: 66887, L1: 66887\n",
      "Epoch 205301, Training Loss: 6322, Validation Loss: 66830, L1: 66830\n",
      "Epoch 205401, Training Loss: 9079, Validation Loss: 66407, L1: 66407\n",
      "Epoch 205501, Training Loss: 6067, Validation Loss: 66466, L1: 66466\n",
      "Epoch 205601, Training Loss: 5856, Validation Loss: 66818, L1: 66818\n",
      "Epoch 205701, Training Loss: 5807, Validation Loss: 67024, L1: 67024\n",
      "Epoch 205801, Training Loss: 6347, Validation Loss: 66516, L1: 66516\n",
      "Epoch 205901, Training Loss: 6463, Validation Loss: 66878, L1: 66878\n",
      "Epoch 206001, Training Loss: 6211, Validation Loss: 66538, L1: 66538\n",
      "Epoch 206101, Training Loss: 6362, Validation Loss: 66529, L1: 66529\n",
      "Epoch 206201, Training Loss: 6248, Validation Loss: 66315, L1: 66315\n",
      "Epoch 206301, Training Loss: 6496, Validation Loss: 66852, L1: 66852\n",
      "Epoch 206401, Training Loss: 6614, Validation Loss: 66900, L1: 66900\n",
      "Epoch 206501, Training Loss: 6721, Validation Loss: 66735, L1: 66735\n",
      "Epoch 206601, Training Loss: 6722, Validation Loss: 66640, L1: 66640\n",
      "Epoch 206701, Training Loss: 7886, Validation Loss: 66330, L1: 66330\n",
      "Epoch 206801, Training Loss: 8136, Validation Loss: 66326, L1: 66326\n",
      "Epoch 206901, Training Loss: 6965, Validation Loss: 66719, L1: 66719\n",
      "Epoch 207001, Training Loss: 5932, Validation Loss: 66674, L1: 66674\n",
      "Epoch 207101, Training Loss: 6432, Validation Loss: 66436, L1: 66436\n",
      "Epoch 207201, Training Loss: 6480, Validation Loss: 66807, L1: 66807\n",
      "Epoch 207301, Training Loss: 5822, Validation Loss: 66682, L1: 66682\n",
      "Epoch 207401, Training Loss: 6673, Validation Loss: 67008, L1: 67008\n",
      "Epoch 207501, Training Loss: 5958, Validation Loss: 66634, L1: 66634\n",
      "Epoch 207601, Training Loss: 5821, Validation Loss: 66677, L1: 66677\n",
      "Epoch 207701, Training Loss: 5788, Validation Loss: 67252, L1: 67252\n",
      "Epoch 207801, Training Loss: 6445, Validation Loss: 67056, L1: 67056\n",
      "Epoch 207901, Training Loss: 5750, Validation Loss: 66667, L1: 66667\n",
      "Epoch 208001, Training Loss: 5919, Validation Loss: 66422, L1: 66422\n",
      "Epoch 208101, Training Loss: 6138, Validation Loss: 66576, L1: 66576\n",
      "Epoch 208201, Training Loss: 6087, Validation Loss: 66829, L1: 66829\n",
      "Epoch 208301, Training Loss: 5827, Validation Loss: 66704, L1: 66704\n",
      "Epoch 208401, Training Loss: 6082, Validation Loss: 66760, L1: 66760\n",
      "Epoch 208501, Training Loss: 7972, Validation Loss: 66936, L1: 66936\n",
      "Epoch 208601, Training Loss: 5747, Validation Loss: 66933, L1: 66933\n",
      "Epoch 208701, Training Loss: 5974, Validation Loss: 67007, L1: 67007\n",
      "Epoch 208801, Training Loss: 7366, Validation Loss: 67262, L1: 67262\n",
      "Epoch 208901, Training Loss: 6678, Validation Loss: 66338, L1: 66338\n",
      "Epoch 209001, Training Loss: 6344, Validation Loss: 66931, L1: 66931\n",
      "Epoch 209101, Training Loss: 6248, Validation Loss: 66634, L1: 66634\n",
      "Epoch 209201, Training Loss: 6355, Validation Loss: 66493, L1: 66493\n",
      "Epoch 209301, Training Loss: 6443, Validation Loss: 66808, L1: 66808\n",
      "Epoch 209401, Training Loss: 6299, Validation Loss: 66429, L1: 66429\n",
      "Epoch 209501, Training Loss: 6967, Validation Loss: 66288, L1: 66288\n",
      "Epoch 209601, Training Loss: 5773, Validation Loss: 66801, L1: 66801\n",
      "Epoch 209701, Training Loss: 6016, Validation Loss: 67036, L1: 67036\n",
      "Epoch 209801, Training Loss: 5903, Validation Loss: 66472, L1: 66472\n",
      "Epoch 209901, Training Loss: 5879, Validation Loss: 67036, L1: 67036\n",
      "Epoch 210001, Training Loss: 7897, Validation Loss: 66289, L1: 66289\n",
      "Epoch 210101, Training Loss: 5857, Validation Loss: 66995, L1: 66995\n",
      "Epoch 210201, Training Loss: 6218, Validation Loss: 66878, L1: 66878\n",
      "Epoch 210301, Training Loss: 6014, Validation Loss: 66907, L1: 66907\n",
      "Epoch 210401, Training Loss: 5935, Validation Loss: 66912, L1: 66912\n",
      "Epoch 210501, Training Loss: 5624, Validation Loss: 66744, L1: 66744\n",
      "Epoch 210601, Training Loss: 8433, Validation Loss: 66944, L1: 66944\n",
      "Epoch 210701, Training Loss: 7731, Validation Loss: 67456, L1: 67456\n",
      "Epoch 210801, Training Loss: 7755, Validation Loss: 66388, L1: 66388\n",
      "Epoch 210901, Training Loss: 6270, Validation Loss: 66636, L1: 66636\n",
      "Epoch 211001, Training Loss: 5748, Validation Loss: 66469, L1: 66469\n",
      "Epoch 211101, Training Loss: 5669, Validation Loss: 66878, L1: 66878\n",
      "Epoch 211201, Training Loss: 5563, Validation Loss: 66948, L1: 66948\n",
      "Epoch 211301, Training Loss: 6638, Validation Loss: 66739, L1: 66739\n",
      "Epoch 211401, Training Loss: 5616, Validation Loss: 66736, L1: 66736\n",
      "Epoch 211501, Training Loss: 5733, Validation Loss: 66984, L1: 66984\n",
      "Epoch 211601, Training Loss: 5883, Validation Loss: 66882, L1: 66882\n",
      "Epoch 211701, Training Loss: 5604, Validation Loss: 66478, L1: 66478\n",
      "Epoch 211801, Training Loss: 7099, Validation Loss: 66583, L1: 66583\n",
      "Epoch 211901, Training Loss: 6444, Validation Loss: 66795, L1: 66795\n",
      "Epoch 212001, Training Loss: 6745, Validation Loss: 66080, L1: 66080\n",
      "Epoch 212101, Training Loss: 6461, Validation Loss: 66924, L1: 66924\n",
      "Epoch 212201, Training Loss: 6075, Validation Loss: 66852, L1: 66852\n",
      "Epoch 212301, Training Loss: 6178, Validation Loss: 66706, L1: 66706\n",
      "Epoch 212401, Training Loss: 7293, Validation Loss: 66913, L1: 66913\n",
      "Epoch 212501, Training Loss: 5761, Validation Loss: 66847, L1: 66847\n",
      "Epoch 212601, Training Loss: 5686, Validation Loss: 66538, L1: 66538\n",
      "Epoch 212701, Training Loss: 5755, Validation Loss: 66649, L1: 66649\n",
      "Epoch 212801, Training Loss: 6040, Validation Loss: 66921, L1: 66921\n",
      "Epoch 212901, Training Loss: 6015, Validation Loss: 66560, L1: 66560\n",
      "Epoch 213001, Training Loss: 5569, Validation Loss: 66694, L1: 66694\n",
      "Epoch 213101, Training Loss: 5965, Validation Loss: 66605, L1: 66605\n",
      "Epoch 213201, Training Loss: 5533, Validation Loss: 66905, L1: 66905\n",
      "Epoch 213301, Training Loss: 6283, Validation Loss: 66544, L1: 66544\n",
      "Epoch 213401, Training Loss: 6023, Validation Loss: 66912, L1: 66912\n",
      "Epoch 213501, Training Loss: 6025, Validation Loss: 66476, L1: 66476\n",
      "Epoch 213601, Training Loss: 5634, Validation Loss: 66834, L1: 66834\n",
      "Epoch 213701, Training Loss: 6428, Validation Loss: 66905, L1: 66905\n",
      "Epoch 213801, Training Loss: 5653, Validation Loss: 66678, L1: 66678\n",
      "Epoch 213901, Training Loss: 6638, Validation Loss: 66947, L1: 66947\n",
      "Epoch 214001, Training Loss: 6248, Validation Loss: 66822, L1: 66822\n",
      "Epoch 214101, Training Loss: 6020, Validation Loss: 66828, L1: 66828\n",
      "Epoch 214201, Training Loss: 5797, Validation Loss: 66636, L1: 66636\n",
      "Epoch 214301, Training Loss: 5604, Validation Loss: 66623, L1: 66623\n",
      "Epoch 214401, Training Loss: 5848, Validation Loss: 66724, L1: 66724\n",
      "Epoch 214501, Training Loss: 6339, Validation Loss: 66923, L1: 66923\n",
      "Epoch 214601, Training Loss: 5701, Validation Loss: 66519, L1: 66519\n",
      "Epoch 214701, Training Loss: 5622, Validation Loss: 66493, L1: 66493\n",
      "Epoch 214801, Training Loss: 6024, Validation Loss: 66595, L1: 66595\n",
      "Epoch 214901, Training Loss: 6695, Validation Loss: 66802, L1: 66802\n",
      "Epoch 215001, Training Loss: 5742, Validation Loss: 66554, L1: 66554\n",
      "Epoch 215101, Training Loss: 5645, Validation Loss: 66791, L1: 66791\n",
      "Epoch 215201, Training Loss: 5849, Validation Loss: 66143, L1: 66143\n",
      "Epoch 215301, Training Loss: 5523, Validation Loss: 66703, L1: 66703\n",
      "Epoch 215401, Training Loss: 9014, Validation Loss: 65875, L1: 65875\n",
      "Epoch 215501, Training Loss: 5556, Validation Loss: 66355, L1: 66355\n",
      "Epoch 215601, Training Loss: 5825, Validation Loss: 66859, L1: 66859\n",
      "Epoch 215701, Training Loss: 5947, Validation Loss: 66771, L1: 66771\n",
      "Epoch 215801, Training Loss: 6096, Validation Loss: 66602, L1: 66602\n",
      "Epoch 215901, Training Loss: 6025, Validation Loss: 66661, L1: 66661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216001, Training Loss: 5592, Validation Loss: 67146, L1: 67146\n",
      "Epoch 216101, Training Loss: 6021, Validation Loss: 66371, L1: 66371\n",
      "Epoch 216201, Training Loss: 5597, Validation Loss: 66851, L1: 66851\n",
      "Epoch 216301, Training Loss: 6468, Validation Loss: 66475, L1: 66475\n",
      "Epoch 216401, Training Loss: 5809, Validation Loss: 66761, L1: 66761\n",
      "Epoch 216501, Training Loss: 6534, Validation Loss: 66514, L1: 66514\n",
      "Epoch 216601, Training Loss: 5506, Validation Loss: 66736, L1: 66736\n",
      "Epoch 216701, Training Loss: 5567, Validation Loss: 66868, L1: 66868\n",
      "Epoch 216801, Training Loss: 6156, Validation Loss: 66892, L1: 66892\n",
      "Epoch 216901, Training Loss: 5462, Validation Loss: 66679, L1: 66679\n",
      "Epoch 217001, Training Loss: 6280, Validation Loss: 66989, L1: 66989\n",
      "Epoch 217101, Training Loss: 7435, Validation Loss: 67401, L1: 67401\n",
      "Epoch 217201, Training Loss: 5742, Validation Loss: 66455, L1: 66455\n",
      "Epoch 217301, Training Loss: 6024, Validation Loss: 66433, L1: 66433\n",
      "Epoch 217401, Training Loss: 5758, Validation Loss: 66516, L1: 66516\n",
      "Epoch 217501, Training Loss: 5463, Validation Loss: 66887, L1: 66887\n",
      "Epoch 217601, Training Loss: 5512, Validation Loss: 66656, L1: 66656\n",
      "Epoch 217701, Training Loss: 5892, Validation Loss: 66808, L1: 66808\n",
      "Epoch 217801, Training Loss: 5372, Validation Loss: 66638, L1: 66638\n",
      "Epoch 217901, Training Loss: 5648, Validation Loss: 66938, L1: 66938\n",
      "Epoch 218001, Training Loss: 6186, Validation Loss: 66525, L1: 66525\n",
      "Epoch 218101, Training Loss: 6354, Validation Loss: 66509, L1: 66509\n",
      "Epoch 218201, Training Loss: 5982, Validation Loss: 66588, L1: 66588\n",
      "Epoch 218301, Training Loss: 6320, Validation Loss: 67069, L1: 67069\n",
      "Epoch 218401, Training Loss: 5519, Validation Loss: 66540, L1: 66540\n",
      "Epoch 218501, Training Loss: 6214, Validation Loss: 66523, L1: 66523\n",
      "Epoch 218601, Training Loss: 5426, Validation Loss: 66770, L1: 66770\n",
      "Epoch 218701, Training Loss: 6357, Validation Loss: 66837, L1: 66837\n",
      "Epoch 218801, Training Loss: 6568, Validation Loss: 66437, L1: 66437\n",
      "Epoch 218901, Training Loss: 6049, Validation Loss: 66385, L1: 66385\n",
      "Epoch 219001, Training Loss: 6638, Validation Loss: 67188, L1: 67188\n",
      "Epoch 219101, Training Loss: 5957, Validation Loss: 66861, L1: 66861\n",
      "Epoch 219201, Training Loss: 5321, Validation Loss: 66839, L1: 66839\n",
      "Epoch 219301, Training Loss: 5374, Validation Loss: 66927, L1: 66927\n",
      "Epoch 219401, Training Loss: 5472, Validation Loss: 66998, L1: 66998\n",
      "Epoch 219501, Training Loss: 6460, Validation Loss: 66630, L1: 66630\n",
      "Epoch 219601, Training Loss: 5558, Validation Loss: 66686, L1: 66686\n",
      "Epoch 219701, Training Loss: 6041, Validation Loss: 67042, L1: 67042\n",
      "Epoch 219801, Training Loss: 6017, Validation Loss: 66897, L1: 66897\n",
      "Epoch 219901, Training Loss: 7002, Validation Loss: 66453, L1: 66453\n",
      "Epoch 220001, Training Loss: 5880, Validation Loss: 66846, L1: 66846\n",
      "Epoch 220101, Training Loss: 5837, Validation Loss: 66976, L1: 66976\n",
      "Epoch 220201, Training Loss: 5589, Validation Loss: 66729, L1: 66729\n",
      "Epoch 220301, Training Loss: 5978, Validation Loss: 66825, L1: 66825\n",
      "Epoch 220401, Training Loss: 7274, Validation Loss: 66829, L1: 66829\n",
      "Epoch 220501, Training Loss: 6256, Validation Loss: 66937, L1: 66937\n",
      "Epoch 220601, Training Loss: 5459, Validation Loss: 67002, L1: 67002\n",
      "Epoch 220701, Training Loss: 5279, Validation Loss: 66953, L1: 66953\n",
      "Epoch 220801, Training Loss: 5358, Validation Loss: 67219, L1: 67219\n",
      "Epoch 220901, Training Loss: 5806, Validation Loss: 67215, L1: 67215\n",
      "Epoch 221001, Training Loss: 5987, Validation Loss: 66807, L1: 66807\n",
      "Epoch 221101, Training Loss: 5355, Validation Loss: 67087, L1: 67087\n",
      "Epoch 221201, Training Loss: 5403, Validation Loss: 67105, L1: 67105\n",
      "Epoch 221301, Training Loss: 5368, Validation Loss: 67125, L1: 67125\n",
      "Epoch 221401, Training Loss: 5652, Validation Loss: 66926, L1: 66926\n",
      "Epoch 221501, Training Loss: 5345, Validation Loss: 66854, L1: 66854\n",
      "Epoch 221601, Training Loss: 5433, Validation Loss: 66960, L1: 66960\n",
      "Epoch 221701, Training Loss: 5922, Validation Loss: 66890, L1: 66890\n",
      "Epoch 221801, Training Loss: 5808, Validation Loss: 66768, L1: 66768\n",
      "Epoch 221901, Training Loss: 6545, Validation Loss: 66994, L1: 66994\n",
      "Epoch 222001, Training Loss: 5859, Validation Loss: 66546, L1: 66546\n",
      "Epoch 222101, Training Loss: 6345, Validation Loss: 67151, L1: 67151\n",
      "Epoch 222201, Training Loss: 6172, Validation Loss: 67166, L1: 67166\n",
      "Epoch 222301, Training Loss: 5542, Validation Loss: 67079, L1: 67079\n",
      "Epoch 222401, Training Loss: 6335, Validation Loss: 66869, L1: 66869\n",
      "Epoch 222501, Training Loss: 5245, Validation Loss: 66945, L1: 66945\n",
      "Epoch 222601, Training Loss: 5437, Validation Loss: 67387, L1: 67387\n",
      "Epoch 222701, Training Loss: 5420, Validation Loss: 67018, L1: 67018\n",
      "Epoch 222801, Training Loss: 5441, Validation Loss: 66847, L1: 66847\n",
      "Epoch 222901, Training Loss: 5954, Validation Loss: 66988, L1: 66988\n",
      "Epoch 223001, Training Loss: 6100, Validation Loss: 66733, L1: 66733\n",
      "Epoch 223101, Training Loss: 6823, Validation Loss: 66731, L1: 66731\n",
      "Epoch 223201, Training Loss: 5361, Validation Loss: 67121, L1: 67121\n",
      "Epoch 223301, Training Loss: 5366, Validation Loss: 66854, L1: 66854\n",
      "Epoch 223401, Training Loss: 5377, Validation Loss: 66937, L1: 66937\n",
      "Epoch 223501, Training Loss: 5463, Validation Loss: 67110, L1: 67110\n",
      "Epoch 223601, Training Loss: 5643, Validation Loss: 67032, L1: 67032\n",
      "Epoch 223701, Training Loss: 5799, Validation Loss: 66660, L1: 66660\n",
      "Epoch 223801, Training Loss: 5338, Validation Loss: 66830, L1: 66830\n",
      "Epoch 223901, Training Loss: 5583, Validation Loss: 66848, L1: 66848\n",
      "Epoch 224001, Training Loss: 5799, Validation Loss: 66814, L1: 66814\n",
      "Epoch 224101, Training Loss: 6443, Validation Loss: 67123, L1: 67123\n",
      "Epoch 224201, Training Loss: 5828, Validation Loss: 67052, L1: 67052\n",
      "Epoch 224301, Training Loss: 6411, Validation Loss: 67195, L1: 67195\n",
      "Epoch 224401, Training Loss: 5456, Validation Loss: 67426, L1: 67426\n",
      "Epoch 224501, Training Loss: 5188, Validation Loss: 67122, L1: 67122\n",
      "Epoch 224601, Training Loss: 5217, Validation Loss: 67261, L1: 67261\n",
      "Epoch 224701, Training Loss: 5365, Validation Loss: 66957, L1: 66957\n",
      "Epoch 224801, Training Loss: 6959, Validation Loss: 66269, L1: 66269\n",
      "Epoch 224901, Training Loss: 5200, Validation Loss: 67007, L1: 67007\n",
      "Epoch 225001, Training Loss: 5211, Validation Loss: 66991, L1: 66991\n",
      "Epoch 225101, Training Loss: 5890, Validation Loss: 66978, L1: 66978\n",
      "Epoch 225201, Training Loss: 5442, Validation Loss: 67203, L1: 67203\n",
      "Epoch 225301, Training Loss: 5988, Validation Loss: 66582, L1: 66582\n",
      "Epoch 225401, Training Loss: 5254, Validation Loss: 66922, L1: 66922\n",
      "Epoch 225501, Training Loss: 5513, Validation Loss: 67034, L1: 67034\n",
      "Epoch 225601, Training Loss: 5160, Validation Loss: 66895, L1: 66895\n",
      "Epoch 225701, Training Loss: 5894, Validation Loss: 67234, L1: 67234\n",
      "Epoch 225801, Training Loss: 5635, Validation Loss: 66763, L1: 66763\n",
      "Epoch 225901, Training Loss: 5699, Validation Loss: 66686, L1: 66686\n",
      "Epoch 226001, Training Loss: 7020, Validation Loss: 67554, L1: 67554\n",
      "Epoch 226101, Training Loss: 5224, Validation Loss: 67399, L1: 67399\n",
      "Epoch 226201, Training Loss: 5143, Validation Loss: 67326, L1: 67326\n",
      "Epoch 226301, Training Loss: 5817, Validation Loss: 66769, L1: 66769\n",
      "Epoch 226401, Training Loss: 5458, Validation Loss: 67050, L1: 67050\n",
      "Epoch 226501, Training Loss: 5712, Validation Loss: 67016, L1: 67016\n",
      "Epoch 226601, Training Loss: 5179, Validation Loss: 67113, L1: 67113\n",
      "Epoch 226701, Training Loss: 5358, Validation Loss: 66978, L1: 66978\n",
      "Epoch 226801, Training Loss: 5504, Validation Loss: 67202, L1: 67202\n",
      "Epoch 226901, Training Loss: 5523, Validation Loss: 67090, L1: 67090\n",
      "Epoch 227001, Training Loss: 5163, Validation Loss: 67351, L1: 67351\n",
      "Epoch 227101, Training Loss: 6204, Validation Loss: 67286, L1: 67286\n",
      "Epoch 227201, Training Loss: 5618, Validation Loss: 67229, L1: 67229\n",
      "Epoch 227301, Training Loss: 5184, Validation Loss: 66981, L1: 66981\n",
      "Epoch 227401, Training Loss: 6104, Validation Loss: 67275, L1: 67275\n",
      "Epoch 227501, Training Loss: 5194, Validation Loss: 67006, L1: 67006\n",
      "Epoch 227601, Training Loss: 5584, Validation Loss: 67038, L1: 67038\n",
      "Epoch 227701, Training Loss: 6175, Validation Loss: 67151, L1: 67151\n",
      "Epoch 227801, Training Loss: 5409, Validation Loss: 67208, L1: 67208\n",
      "Epoch 227901, Training Loss: 5665, Validation Loss: 66697, L1: 66697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228001, Training Loss: 5377, Validation Loss: 67087, L1: 67087\n",
      "Epoch 228101, Training Loss: 5165, Validation Loss: 67146, L1: 67146\n",
      "Epoch 228201, Training Loss: 5965, Validation Loss: 66796, L1: 66796\n",
      "Epoch 228301, Training Loss: 5636, Validation Loss: 67418, L1: 67418\n",
      "Epoch 228401, Training Loss: 6187, Validation Loss: 67324, L1: 67324\n",
      "Epoch 228501, Training Loss: 5468, Validation Loss: 67062, L1: 67062\n",
      "Epoch 228601, Training Loss: 5091, Validation Loss: 67257, L1: 67257\n",
      "Epoch 228701, Training Loss: 5077, Validation Loss: 67299, L1: 67299\n",
      "Epoch 228801, Training Loss: 5022, Validation Loss: 67036, L1: 67036\n",
      "Epoch 228901, Training Loss: 5246, Validation Loss: 66927, L1: 66927\n",
      "Epoch 229001, Training Loss: 5430, Validation Loss: 67276, L1: 67276\n",
      "Epoch 229101, Training Loss: 5798, Validation Loss: 67318, L1: 67318\n",
      "Epoch 229201, Training Loss: 5096, Validation Loss: 67313, L1: 67313\n",
      "Epoch 229301, Training Loss: 6069, Validation Loss: 67455, L1: 67455\n",
      "Epoch 229401, Training Loss: 5213, Validation Loss: 67095, L1: 67095\n",
      "Epoch 229501, Training Loss: 6435, Validation Loss: 67361, L1: 67361\n",
      "Epoch 229601, Training Loss: 6071, Validation Loss: 67287, L1: 67287\n",
      "Epoch 229701, Training Loss: 5126, Validation Loss: 67204, L1: 67204\n",
      "Epoch 229801, Training Loss: 5049, Validation Loss: 67485, L1: 67485\n",
      "Epoch 229901, Training Loss: 5176, Validation Loss: 67405, L1: 67405\n",
      "Epoch 230001, Training Loss: 6202, Validation Loss: 67373, L1: 67373\n",
      "Epoch 230101, Training Loss: 5980, Validation Loss: 67263, L1: 67263\n",
      "Epoch 230201, Training Loss: 5277, Validation Loss: 67096, L1: 67096\n",
      "Epoch 230301, Training Loss: 5224, Validation Loss: 67566, L1: 67566\n",
      "Epoch 230401, Training Loss: 5278, Validation Loss: 67213, L1: 67213\n",
      "Epoch 230501, Training Loss: 4966, Validation Loss: 67404, L1: 67404\n",
      "Epoch 230601, Training Loss: 5265, Validation Loss: 67241, L1: 67241\n",
      "Epoch 230701, Training Loss: 4985, Validation Loss: 67224, L1: 67224\n",
      "Epoch 230801, Training Loss: 5132, Validation Loss: 67317, L1: 67317\n",
      "Epoch 230901, Training Loss: 8638, Validation Loss: 66612, L1: 66612\n",
      "Epoch 231001, Training Loss: 6349, Validation Loss: 67004, L1: 67004\n",
      "Epoch 231101, Training Loss: 6034, Validation Loss: 66750, L1: 66750\n",
      "Epoch 231201, Training Loss: 5786, Validation Loss: 66985, L1: 66985\n",
      "Epoch 231301, Training Loss: 5048, Validation Loss: 67474, L1: 67474\n",
      "Epoch 231401, Training Loss: 6655, Validation Loss: 66990, L1: 66990\n",
      "Epoch 231501, Training Loss: 5061, Validation Loss: 67233, L1: 67233\n",
      "Epoch 231601, Training Loss: 5128, Validation Loss: 67349, L1: 67349\n",
      "Epoch 231701, Training Loss: 5211, Validation Loss: 67463, L1: 67463\n",
      "Epoch 231801, Training Loss: 5061, Validation Loss: 66707, L1: 66707\n",
      "Epoch 231901, Training Loss: 4960, Validation Loss: 67434, L1: 67434\n",
      "Epoch 232001, Training Loss: 5600, Validation Loss: 67136, L1: 67136\n",
      "Epoch 232101, Training Loss: 5566, Validation Loss: 67042, L1: 67042\n",
      "Epoch 232201, Training Loss: 6300, Validation Loss: 67558, L1: 67558\n",
      "Epoch 232301, Training Loss: 5342, Validation Loss: 67396, L1: 67396\n",
      "Epoch 232401, Training Loss: 5986, Validation Loss: 67021, L1: 67021\n",
      "Epoch 232501, Training Loss: 6387, Validation Loss: 66620, L1: 66620\n",
      "Epoch 232601, Training Loss: 5257, Validation Loss: 67203, L1: 67203\n",
      "Epoch 232701, Training Loss: 5300, Validation Loss: 67157, L1: 67157\n",
      "Epoch 232801, Training Loss: 6356, Validation Loss: 67593, L1: 67593\n",
      "Epoch 232901, Training Loss: 6682, Validation Loss: 67100, L1: 67100\n",
      "Epoch 233001, Training Loss: 6474, Validation Loss: 66818, L1: 66818\n",
      "Epoch 233101, Training Loss: 4874, Validation Loss: 67366, L1: 67366\n",
      "Epoch 233201, Training Loss: 5548, Validation Loss: 67245, L1: 67245\n",
      "Epoch 233301, Training Loss: 5958, Validation Loss: 67412, L1: 67412\n",
      "Epoch 233401, Training Loss: 6003, Validation Loss: 67510, L1: 67510\n",
      "Epoch 233501, Training Loss: 5048, Validation Loss: 67152, L1: 67152\n",
      "Epoch 233601, Training Loss: 4907, Validation Loss: 67371, L1: 67371\n",
      "Epoch 233701, Training Loss: 4944, Validation Loss: 67284, L1: 67284\n",
      "Epoch 233801, Training Loss: 4827, Validation Loss: 67508, L1: 67508\n",
      "Epoch 233901, Training Loss: 6367, Validation Loss: 67068, L1: 67068\n",
      "Epoch 234001, Training Loss: 5899, Validation Loss: 67029, L1: 67029\n",
      "Epoch 234101, Training Loss: 5339, Validation Loss: 66980, L1: 66980\n",
      "Epoch 234201, Training Loss: 5665, Validation Loss: 66923, L1: 66923\n",
      "Epoch 234301, Training Loss: 5129, Validation Loss: 67479, L1: 67479\n",
      "Epoch 234401, Training Loss: 5077, Validation Loss: 66793, L1: 66793\n",
      "Epoch 234501, Training Loss: 5178, Validation Loss: 67114, L1: 67114\n",
      "Epoch 234601, Training Loss: 6227, Validation Loss: 66674, L1: 66674\n",
      "Epoch 234701, Training Loss: 6678, Validation Loss: 67403, L1: 67403\n",
      "Epoch 234801, Training Loss: 5036, Validation Loss: 67313, L1: 67313\n",
      "Epoch 234901, Training Loss: 5285, Validation Loss: 67064, L1: 67064\n",
      "Epoch 235001, Training Loss: 4982, Validation Loss: 67354, L1: 67354\n",
      "Epoch 235101, Training Loss: 4961, Validation Loss: 67296, L1: 67296\n",
      "Epoch 235201, Training Loss: 4928, Validation Loss: 67146, L1: 67146\n",
      "Epoch 235301, Training Loss: 5808, Validation Loss: 67350, L1: 67350\n",
      "Epoch 235401, Training Loss: 4860, Validation Loss: 67267, L1: 67267\n",
      "Epoch 235501, Training Loss: 5280, Validation Loss: 66952, L1: 66952\n",
      "Epoch 235601, Training Loss: 4848, Validation Loss: 67257, L1: 67257\n",
      "Epoch 235701, Training Loss: 5010, Validation Loss: 67084, L1: 67084\n",
      "Epoch 235801, Training Loss: 5335, Validation Loss: 67459, L1: 67459\n",
      "Epoch 235901, Training Loss: 4940, Validation Loss: 67577, L1: 67577\n",
      "Epoch 236001, Training Loss: 4902, Validation Loss: 67432, L1: 67432\n",
      "Epoch 236101, Training Loss: 5333, Validation Loss: 67291, L1: 67291\n",
      "Epoch 236201, Training Loss: 5354, Validation Loss: 67457, L1: 67457\n",
      "Epoch 236301, Training Loss: 5046, Validation Loss: 67639, L1: 67639\n",
      "Epoch 236401, Training Loss: 5050, Validation Loss: 67539, L1: 67539\n",
      "Epoch 236501, Training Loss: 5185, Validation Loss: 67480, L1: 67480\n",
      "Epoch 236601, Training Loss: 5057, Validation Loss: 67472, L1: 67472\n",
      "Epoch 236701, Training Loss: 6213, Validation Loss: 67120, L1: 67120\n",
      "Epoch 236801, Training Loss: 5817, Validation Loss: 67423, L1: 67423\n",
      "Epoch 236901, Training Loss: 5375, Validation Loss: 67266, L1: 67266\n",
      "Epoch 237001, Training Loss: 5318, Validation Loss: 67285, L1: 67285\n",
      "Epoch 237101, Training Loss: 5279, Validation Loss: 67417, L1: 67417\n",
      "Epoch 237201, Training Loss: 5581, Validation Loss: 67665, L1: 67665\n",
      "Epoch 237301, Training Loss: 5020, Validation Loss: 67044, L1: 67044\n",
      "Epoch 237401, Training Loss: 5123, Validation Loss: 67465, L1: 67465\n",
      "Epoch 237501, Training Loss: 5827, Validation Loss: 67690, L1: 67690\n",
      "Epoch 237601, Training Loss: 7097, Validation Loss: 67014, L1: 67014\n",
      "Epoch 237701, Training Loss: 5515, Validation Loss: 66922, L1: 66922\n",
      "Epoch 237801, Training Loss: 6234, Validation Loss: 67832, L1: 67832\n",
      "Epoch 237901, Training Loss: 5586, Validation Loss: 67474, L1: 67474\n",
      "Epoch 238001, Training Loss: 4832, Validation Loss: 67598, L1: 67598\n",
      "Epoch 238101, Training Loss: 5286, Validation Loss: 67554, L1: 67554\n",
      "Epoch 238201, Training Loss: 5239, Validation Loss: 67731, L1: 67731\n",
      "Epoch 238301, Training Loss: 4920, Validation Loss: 67186, L1: 67186\n",
      "Epoch 238401, Training Loss: 5036, Validation Loss: 67239, L1: 67239\n",
      "Epoch 238501, Training Loss: 5162, Validation Loss: 67364, L1: 67364\n",
      "Epoch 238601, Training Loss: 5345, Validation Loss: 67175, L1: 67175\n",
      "Epoch 238701, Training Loss: 5457, Validation Loss: 67307, L1: 67307\n",
      "Epoch 238801, Training Loss: 5373, Validation Loss: 67399, L1: 67399\n",
      "Epoch 238901, Training Loss: 5701, Validation Loss: 67300, L1: 67300\n",
      "Epoch 239001, Training Loss: 5291, Validation Loss: 67078, L1: 67078\n",
      "Epoch 239101, Training Loss: 5198, Validation Loss: 67539, L1: 67539\n",
      "Epoch 239201, Training Loss: 4811, Validation Loss: 67651, L1: 67651\n",
      "Epoch 239301, Training Loss: 5153, Validation Loss: 67755, L1: 67755\n",
      "Epoch 239401, Training Loss: 4892, Validation Loss: 67379, L1: 67379\n",
      "Epoch 239501, Training Loss: 5879, Validation Loss: 67205, L1: 67205\n",
      "Epoch 239601, Training Loss: 4780, Validation Loss: 67430, L1: 67430\n",
      "Epoch 239701, Training Loss: 4926, Validation Loss: 67342, L1: 67342\n",
      "Epoch 239801, Training Loss: 5834, Validation Loss: 67059, L1: 67059\n",
      "Epoch 239901, Training Loss: 4909, Validation Loss: 67321, L1: 67321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240001, Training Loss: 4813, Validation Loss: 67426, L1: 67426\n",
      "Epoch 240101, Training Loss: 7283, Validation Loss: 67094, L1: 67094\n",
      "Epoch 240201, Training Loss: 5584, Validation Loss: 67104, L1: 67104\n",
      "Epoch 240301, Training Loss: 5552, Validation Loss: 67181, L1: 67181\n",
      "Epoch 240401, Training Loss: 4809, Validation Loss: 67484, L1: 67484\n",
      "Epoch 240501, Training Loss: 5225, Validation Loss: 67172, L1: 67172\n",
      "Epoch 240601, Training Loss: 4813, Validation Loss: 67721, L1: 67721\n",
      "Epoch 240701, Training Loss: 5081, Validation Loss: 67617, L1: 67617\n",
      "Epoch 240801, Training Loss: 4795, Validation Loss: 67632, L1: 67632\n",
      "Epoch 240901, Training Loss: 5455, Validation Loss: 67328, L1: 67328\n",
      "Epoch 241001, Training Loss: 5386, Validation Loss: 67335, L1: 67335\n",
      "Epoch 241101, Training Loss: 6204, Validation Loss: 67474, L1: 67474\n",
      "Epoch 241201, Training Loss: 4755, Validation Loss: 67315, L1: 67315\n",
      "Epoch 241301, Training Loss: 4719, Validation Loss: 67402, L1: 67402\n",
      "Epoch 241401, Training Loss: 5160, Validation Loss: 67728, L1: 67728\n",
      "Epoch 241501, Training Loss: 5386, Validation Loss: 67704, L1: 67704\n",
      "Epoch 241601, Training Loss: 5334, Validation Loss: 67666, L1: 67666\n",
      "Epoch 241701, Training Loss: 5373, Validation Loss: 67474, L1: 67474\n",
      "Epoch 241801, Training Loss: 4709, Validation Loss: 67300, L1: 67300\n",
      "Epoch 241901, Training Loss: 5555, Validation Loss: 67247, L1: 67247\n",
      "Epoch 242001, Training Loss: 5374, Validation Loss: 67381, L1: 67381\n",
      "Epoch 242101, Training Loss: 5869, Validation Loss: 67199, L1: 67199\n",
      "Epoch 242201, Training Loss: 6544, Validation Loss: 67635, L1: 67635\n",
      "Epoch 242301, Training Loss: 5374, Validation Loss: 67874, L1: 67874\n",
      "Epoch 242401, Training Loss: 4623, Validation Loss: 67617, L1: 67617\n",
      "Epoch 242501, Training Loss: 4770, Validation Loss: 67541, L1: 67541\n",
      "Epoch 242601, Training Loss: 4806, Validation Loss: 67602, L1: 67602\n",
      "Epoch 242701, Training Loss: 5044, Validation Loss: 67477, L1: 67477\n",
      "Epoch 242801, Training Loss: 4669, Validation Loss: 67770, L1: 67770\n",
      "Epoch 242901, Training Loss: 4800, Validation Loss: 67677, L1: 67677\n",
      "Epoch 243001, Training Loss: 5090, Validation Loss: 67760, L1: 67760\n",
      "Epoch 243101, Training Loss: 5142, Validation Loss: 67527, L1: 67527\n",
      "Epoch 243201, Training Loss: 5256, Validation Loss: 67660, L1: 67660\n",
      "Epoch 243301, Training Loss: 4697, Validation Loss: 67240, L1: 67240\n",
      "Epoch 243401, Training Loss: 5210, Validation Loss: 67375, L1: 67375\n",
      "Epoch 243501, Training Loss: 4557, Validation Loss: 67552, L1: 67552\n",
      "Epoch 243601, Training Loss: 4617, Validation Loss: 67501, L1: 67501\n",
      "Epoch 243701, Training Loss: 5076, Validation Loss: 67498, L1: 67498\n",
      "Epoch 243801, Training Loss: 4766, Validation Loss: 67643, L1: 67643\n",
      "Epoch 243901, Training Loss: 5189, Validation Loss: 67640, L1: 67640\n",
      "Epoch 244001, Training Loss: 4728, Validation Loss: 67927, L1: 67927\n",
      "Epoch 244101, Training Loss: 4889, Validation Loss: 67770, L1: 67770\n",
      "Epoch 244201, Training Loss: 6028, Validation Loss: 67492, L1: 67492\n",
      "Epoch 244301, Training Loss: 5151, Validation Loss: 67474, L1: 67474\n",
      "Epoch 244401, Training Loss: 5121, Validation Loss: 67663, L1: 67663\n",
      "Epoch 244501, Training Loss: 4708, Validation Loss: 67468, L1: 67468\n",
      "Epoch 244601, Training Loss: 5269, Validation Loss: 67486, L1: 67486\n",
      "Epoch 244701, Training Loss: 4727, Validation Loss: 67697, L1: 67697\n",
      "Epoch 244801, Training Loss: 6404, Validation Loss: 67510, L1: 67510\n",
      "Epoch 244901, Training Loss: 4774, Validation Loss: 67558, L1: 67558\n",
      "Epoch 245001, Training Loss: 5180, Validation Loss: 67695, L1: 67695\n",
      "Epoch 245101, Training Loss: 5044, Validation Loss: 67663, L1: 67663\n",
      "Epoch 245201, Training Loss: 4629, Validation Loss: 67747, L1: 67747\n",
      "Epoch 245301, Training Loss: 4871, Validation Loss: 67516, L1: 67516\n",
      "Epoch 245401, Training Loss: 5230, Validation Loss: 67681, L1: 67681\n",
      "Epoch 245501, Training Loss: 5375, Validation Loss: 67742, L1: 67742\n",
      "Epoch 245601, Training Loss: 4712, Validation Loss: 67536, L1: 67536\n",
      "Epoch 245701, Training Loss: 4640, Validation Loss: 67589, L1: 67589\n",
      "Epoch 245801, Training Loss: 4857, Validation Loss: 67688, L1: 67688\n",
      "Epoch 245901, Training Loss: 4683, Validation Loss: 67340, L1: 67340\n",
      "Epoch 246001, Training Loss: 4796, Validation Loss: 67508, L1: 67508\n",
      "Epoch 246101, Training Loss: 4986, Validation Loss: 67623, L1: 67623\n",
      "Epoch 246201, Training Loss: 5991, Validation Loss: 67228, L1: 67228\n",
      "Epoch 246301, Training Loss: 6569, Validation Loss: 67749, L1: 67749\n",
      "Epoch 246401, Training Loss: 5462, Validation Loss: 67959, L1: 67959\n",
      "Epoch 246501, Training Loss: 4555, Validation Loss: 67635, L1: 67635\n",
      "Epoch 246601, Training Loss: 4606, Validation Loss: 67457, L1: 67457\n",
      "Epoch 246701, Training Loss: 5928, Validation Loss: 68049, L1: 68049\n",
      "Epoch 246801, Training Loss: 6569, Validation Loss: 67580, L1: 67580\n",
      "Epoch 246901, Training Loss: 5269, Validation Loss: 67523, L1: 67523\n",
      "Epoch 247001, Training Loss: 6118, Validation Loss: 68216, L1: 68216\n",
      "Epoch 247101, Training Loss: 5225, Validation Loss: 67763, L1: 67763\n",
      "Epoch 247201, Training Loss: 4799, Validation Loss: 67654, L1: 67654\n",
      "Epoch 247301, Training Loss: 4822, Validation Loss: 67520, L1: 67520\n",
      "Epoch 247401, Training Loss: 4811, Validation Loss: 67624, L1: 67624\n",
      "Epoch 247501, Training Loss: 5379, Validation Loss: 67631, L1: 67631\n",
      "Epoch 247601, Training Loss: 5158, Validation Loss: 67822, L1: 67822\n",
      "Epoch 247701, Training Loss: 4536, Validation Loss: 67684, L1: 67684\n",
      "Epoch 247801, Training Loss: 5083, Validation Loss: 67592, L1: 67592\n",
      "Epoch 247901, Training Loss: 5898, Validation Loss: 67423, L1: 67423\n",
      "Epoch 248001, Training Loss: 5854, Validation Loss: 67367, L1: 67367\n",
      "Epoch 248101, Training Loss: 6617, Validation Loss: 67849, L1: 67849\n",
      "Epoch 248201, Training Loss: 5394, Validation Loss: 67974, L1: 67974\n",
      "Epoch 248301, Training Loss: 5660, Validation Loss: 68012, L1: 68012\n",
      "Epoch 248401, Training Loss: 4831, Validation Loss: 67869, L1: 67869\n",
      "Epoch 248501, Training Loss: 4598, Validation Loss: 67641, L1: 67641\n",
      "Epoch 248601, Training Loss: 4822, Validation Loss: 67580, L1: 67580\n",
      "Epoch 248701, Training Loss: 5017, Validation Loss: 67661, L1: 67661\n",
      "Epoch 248801, Training Loss: 5029, Validation Loss: 67852, L1: 67852\n",
      "Epoch 248901, Training Loss: 5456, Validation Loss: 67578, L1: 67578\n",
      "Epoch 249001, Training Loss: 4672, Validation Loss: 67672, L1: 67672\n",
      "Epoch 249101, Training Loss: 5053, Validation Loss: 67654, L1: 67654\n",
      "Epoch 249201, Training Loss: 4613, Validation Loss: 67640, L1: 67640\n",
      "Epoch 249301, Training Loss: 5099, Validation Loss: 67818, L1: 67818\n",
      "Epoch 249401, Training Loss: 4980, Validation Loss: 67810, L1: 67810\n",
      "Epoch 249501, Training Loss: 4718, Validation Loss: 68239, L1: 68239\n",
      "Epoch 249601, Training Loss: 4706, Validation Loss: 67532, L1: 67532\n",
      "Epoch 249701, Training Loss: 4871, Validation Loss: 67862, L1: 67862\n",
      "Epoch 249801, Training Loss: 5179, Validation Loss: 67852, L1: 67852\n",
      "Epoch 249901, Training Loss: 6384, Validation Loss: 67795, L1: 67795\n",
      "Epoch 250001, Training Loss: 5170, Validation Loss: 67966, L1: 67966\n",
      "Epoch 250101, Training Loss: 5592, Validation Loss: 67636, L1: 67636\n",
      "Epoch 250201, Training Loss: 4938, Validation Loss: 67969, L1: 67969\n",
      "Epoch 250301, Training Loss: 4659, Validation Loss: 67898, L1: 67898\n",
      "Epoch 250401, Training Loss: 4920, Validation Loss: 67962, L1: 67962\n",
      "Epoch 250501, Training Loss: 5646, Validation Loss: 67958, L1: 67958\n",
      "Epoch 250601, Training Loss: 4611, Validation Loss: 67706, L1: 67706\n",
      "Epoch 250701, Training Loss: 4498, Validation Loss: 67872, L1: 67872\n",
      "Epoch 250801, Training Loss: 4539, Validation Loss: 67655, L1: 67655\n",
      "Epoch 250901, Training Loss: 4635, Validation Loss: 67959, L1: 67959\n",
      "Epoch 251001, Training Loss: 4584, Validation Loss: 67885, L1: 67885\n",
      "Epoch 251101, Training Loss: 4588, Validation Loss: 67935, L1: 67935\n",
      "Epoch 251201, Training Loss: 4876, Validation Loss: 67698, L1: 67698\n",
      "Epoch 251301, Training Loss: 4499, Validation Loss: 67642, L1: 67642\n",
      "Epoch 251401, Training Loss: 4525, Validation Loss: 67649, L1: 67649\n",
      "Epoch 251501, Training Loss: 4469, Validation Loss: 67656, L1: 67656\n",
      "Epoch 251601, Training Loss: 4505, Validation Loss: 67783, L1: 67783\n",
      "Epoch 251701, Training Loss: 5083, Validation Loss: 67632, L1: 67632\n",
      "Epoch 251801, Training Loss: 5851, Validation Loss: 67548, L1: 67548\n",
      "Epoch 251901, Training Loss: 5180, Validation Loss: 67665, L1: 67665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252001, Training Loss: 5306, Validation Loss: 67717, L1: 67717\n",
      "Epoch 252101, Training Loss: 4771, Validation Loss: 67735, L1: 67735\n",
      "Epoch 252201, Training Loss: 4371, Validation Loss: 67810, L1: 67810\n",
      "Epoch 252301, Training Loss: 4482, Validation Loss: 67749, L1: 67749\n",
      "Epoch 252401, Training Loss: 4941, Validation Loss: 68066, L1: 68066\n",
      "Epoch 252501, Training Loss: 4597, Validation Loss: 67931, L1: 67931\n",
      "Epoch 252601, Training Loss: 4442, Validation Loss: 67908, L1: 67908\n",
      "Epoch 252701, Training Loss: 4594, Validation Loss: 67872, L1: 67872\n",
      "Epoch 252801, Training Loss: 5353, Validation Loss: 67918, L1: 67918\n",
      "Epoch 252901, Training Loss: 5140, Validation Loss: 67695, L1: 67695\n",
      "Epoch 253001, Training Loss: 4791, Validation Loss: 67949, L1: 67949\n",
      "Epoch 253101, Training Loss: 4671, Validation Loss: 67786, L1: 67786\n",
      "Epoch 253201, Training Loss: 5648, Validation Loss: 67655, L1: 67655\n",
      "Epoch 253301, Training Loss: 5683, Validation Loss: 67584, L1: 67584\n",
      "Epoch 253401, Training Loss: 5186, Validation Loss: 67815, L1: 67815\n",
      "Epoch 253501, Training Loss: 4387, Validation Loss: 67853, L1: 67853\n",
      "Epoch 253601, Training Loss: 6425, Validation Loss: 67960, L1: 67960\n",
      "Epoch 253701, Training Loss: 4826, Validation Loss: 68131, L1: 68131\n",
      "Epoch 253801, Training Loss: 5814, Validation Loss: 67710, L1: 67710\n",
      "Epoch 253901, Training Loss: 6206, Validation Loss: 68013, L1: 68013\n",
      "Epoch 254001, Training Loss: 4703, Validation Loss: 68281, L1: 68281\n",
      "Epoch 254101, Training Loss: 5299, Validation Loss: 67951, L1: 67951\n",
      "Epoch 254201, Training Loss: 4710, Validation Loss: 67944, L1: 67944\n",
      "Epoch 254301, Training Loss: 4445, Validation Loss: 67996, L1: 67996\n",
      "Epoch 254401, Training Loss: 4970, Validation Loss: 67853, L1: 67853\n",
      "Epoch 254501, Training Loss: 5036, Validation Loss: 67857, L1: 67857\n",
      "Epoch 254601, Training Loss: 4405, Validation Loss: 67906, L1: 67906\n",
      "Epoch 254701, Training Loss: 4444, Validation Loss: 68032, L1: 68032\n",
      "Epoch 254801, Training Loss: 4750, Validation Loss: 67991, L1: 67991\n",
      "Epoch 254901, Training Loss: 4278, Validation Loss: 68012, L1: 68012\n",
      "Epoch 255001, Training Loss: 4629, Validation Loss: 67887, L1: 67887\n",
      "Epoch 255101, Training Loss: 5387, Validation Loss: 67923, L1: 67923\n",
      "Epoch 255201, Training Loss: 4726, Validation Loss: 67648, L1: 67648\n",
      "Epoch 255301, Training Loss: 5066, Validation Loss: 68161, L1: 68161\n",
      "Epoch 255401, Training Loss: 5765, Validation Loss: 67754, L1: 67754\n",
      "Epoch 255501, Training Loss: 6406, Validation Loss: 68033, L1: 68033\n",
      "Epoch 255601, Training Loss: 4360, Validation Loss: 67976, L1: 67976\n",
      "Epoch 255701, Training Loss: 4618, Validation Loss: 67965, L1: 67965\n",
      "Epoch 255801, Training Loss: 4340, Validation Loss: 67803, L1: 67803\n",
      "Epoch 255901, Training Loss: 4465, Validation Loss: 67855, L1: 67855\n",
      "Epoch 256001, Training Loss: 4469, Validation Loss: 68060, L1: 68060\n",
      "Epoch 256101, Training Loss: 4433, Validation Loss: 67839, L1: 67839\n",
      "Epoch 256201, Training Loss: 4841, Validation Loss: 68171, L1: 68171\n",
      "Epoch 256301, Training Loss: 5016, Validation Loss: 67990, L1: 67990\n",
      "Epoch 256401, Training Loss: 4717, Validation Loss: 68063, L1: 68063\n",
      "Epoch 256501, Training Loss: 4669, Validation Loss: 68200, L1: 68200\n",
      "Epoch 256601, Training Loss: 4979, Validation Loss: 67916, L1: 67916\n",
      "Epoch 256701, Training Loss: 5144, Validation Loss: 68041, L1: 68041\n",
      "Epoch 256801, Training Loss: 4684, Validation Loss: 67884, L1: 67884\n",
      "Epoch 256901, Training Loss: 4772, Validation Loss: 67990, L1: 67990\n",
      "Epoch 257001, Training Loss: 4526, Validation Loss: 67832, L1: 67832\n",
      "Epoch 257101, Training Loss: 4753, Validation Loss: 67981, L1: 67981\n",
      "Epoch 257201, Training Loss: 4539, Validation Loss: 68026, L1: 68026\n",
      "Epoch 257301, Training Loss: 4615, Validation Loss: 67976, L1: 67976\n",
      "Epoch 257401, Training Loss: 4584, Validation Loss: 67893, L1: 67893\n",
      "Epoch 257501, Training Loss: 4288, Validation Loss: 67908, L1: 67908\n",
      "Epoch 257601, Training Loss: 4550, Validation Loss: 68062, L1: 68062\n",
      "Epoch 257701, Training Loss: 4318, Validation Loss: 67936, L1: 67936\n",
      "Epoch 257801, Training Loss: 4435, Validation Loss: 68266, L1: 68266\n",
      "Epoch 257901, Training Loss: 4244, Validation Loss: 67999, L1: 67999\n",
      "Epoch 258001, Training Loss: 4602, Validation Loss: 68185, L1: 68185\n",
      "Epoch 258101, Training Loss: 4969, Validation Loss: 67894, L1: 67894\n",
      "Epoch 258201, Training Loss: 5544, Validation Loss: 67859, L1: 67859\n",
      "Epoch 258301, Training Loss: 4530, Validation Loss: 68141, L1: 68141\n",
      "Epoch 258401, Training Loss: 5087, Validation Loss: 67936, L1: 67936\n",
      "Epoch 258501, Training Loss: 5668, Validation Loss: 68315, L1: 68315\n",
      "Epoch 258601, Training Loss: 6517, Validation Loss: 67974, L1: 67974\n",
      "Epoch 258701, Training Loss: 4328, Validation Loss: 67945, L1: 67945\n",
      "Epoch 258801, Training Loss: 4599, Validation Loss: 68050, L1: 68050\n",
      "Epoch 258901, Training Loss: 7660, Validation Loss: 68544, L1: 68544\n",
      "Epoch 259001, Training Loss: 4677, Validation Loss: 68055, L1: 68055\n",
      "Epoch 259101, Training Loss: 4935, Validation Loss: 68063, L1: 68063\n",
      "Epoch 259201, Training Loss: 4990, Validation Loss: 68165, L1: 68165\n",
      "Epoch 259301, Training Loss: 4290, Validation Loss: 68006, L1: 68006\n",
      "Epoch 259401, Training Loss: 4934, Validation Loss: 67979, L1: 67979\n",
      "Epoch 259501, Training Loss: 4424, Validation Loss: 67884, L1: 67884\n",
      "Epoch 259601, Training Loss: 4736, Validation Loss: 68044, L1: 68044\n",
      "Epoch 259701, Training Loss: 4736, Validation Loss: 68019, L1: 68019\n",
      "Epoch 259801, Training Loss: 4889, Validation Loss: 68230, L1: 68230\n",
      "Epoch 259901, Training Loss: 4143, Validation Loss: 68185, L1: 68185\n",
      "Epoch 260001, Training Loss: 4457, Validation Loss: 68125, L1: 68125\n",
      "Epoch 260101, Training Loss: 4428, Validation Loss: 68014, L1: 68014\n",
      "Epoch 260201, Training Loss: 5848, Validation Loss: 68019, L1: 68019\n",
      "Epoch 260301, Training Loss: 5633, Validation Loss: 68098, L1: 68098\n",
      "Epoch 260401, Training Loss: 4322, Validation Loss: 68261, L1: 68261\n",
      "Epoch 260501, Training Loss: 4363, Validation Loss: 68115, L1: 68115\n",
      "Epoch 260601, Training Loss: 4326, Validation Loss: 68046, L1: 68046\n",
      "Epoch 260701, Training Loss: 4558, Validation Loss: 68060, L1: 68060\n",
      "Epoch 260801, Training Loss: 5346, Validation Loss: 68103, L1: 68103\n",
      "Epoch 260901, Training Loss: 4270, Validation Loss: 68090, L1: 68090\n",
      "Epoch 261001, Training Loss: 4583, Validation Loss: 68105, L1: 68105\n",
      "Epoch 261101, Training Loss: 4951, Validation Loss: 68045, L1: 68045\n",
      "Epoch 261201, Training Loss: 5228, Validation Loss: 67770, L1: 67770\n",
      "Epoch 261301, Training Loss: 4304, Validation Loss: 68141, L1: 68141\n",
      "Epoch 261401, Training Loss: 4616, Validation Loss: 67938, L1: 67938\n",
      "Epoch 261501, Training Loss: 4812, Validation Loss: 67980, L1: 67980\n",
      "Epoch 261601, Training Loss: 5074, Validation Loss: 68179, L1: 68179\n",
      "Epoch 261701, Training Loss: 4767, Validation Loss: 68284, L1: 68284\n",
      "Epoch 261801, Training Loss: 4801, Validation Loss: 67975, L1: 67975\n",
      "Epoch 261901, Training Loss: 5002, Validation Loss: 68158, L1: 68158\n",
      "Epoch 262001, Training Loss: 4371, Validation Loss: 68315, L1: 68315\n",
      "Epoch 262101, Training Loss: 4608, Validation Loss: 68026, L1: 68026\n",
      "Epoch 262201, Training Loss: 4441, Validation Loss: 68103, L1: 68103\n",
      "Epoch 262301, Training Loss: 4538, Validation Loss: 68127, L1: 68127\n",
      "Epoch 262401, Training Loss: 4529, Validation Loss: 68185, L1: 68185\n",
      "Epoch 262501, Training Loss: 4672, Validation Loss: 68226, L1: 68226\n",
      "Epoch 262601, Training Loss: 4759, Validation Loss: 68401, L1: 68401\n",
      "Epoch 262701, Training Loss: 4323, Validation Loss: 68142, L1: 68142\n",
      "Epoch 262801, Training Loss: 4222, Validation Loss: 68236, L1: 68236\n",
      "Epoch 262901, Training Loss: 5254, Validation Loss: 68304, L1: 68304\n",
      "Epoch 263001, Training Loss: 5194, Validation Loss: 68149, L1: 68149\n",
      "Epoch 263101, Training Loss: 4245, Validation Loss: 68300, L1: 68300\n",
      "Epoch 263201, Training Loss: 4095, Validation Loss: 68324, L1: 68324\n",
      "Epoch 263301, Training Loss: 4364, Validation Loss: 68350, L1: 68350\n",
      "Epoch 263401, Training Loss: 4325, Validation Loss: 68189, L1: 68189\n",
      "Epoch 263501, Training Loss: 4242, Validation Loss: 68230, L1: 68230\n",
      "Epoch 263601, Training Loss: 4150, Validation Loss: 68216, L1: 68216\n",
      "Epoch 263701, Training Loss: 4228, Validation Loss: 68188, L1: 68188\n",
      "Epoch 263801, Training Loss: 4677, Validation Loss: 68389, L1: 68389\n",
      "Epoch 263901, Training Loss: 5571, Validation Loss: 68235, L1: 68235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264001, Training Loss: 4849, Validation Loss: 68383, L1: 68383\n",
      "Epoch 264101, Training Loss: 5585, Validation Loss: 68120, L1: 68120\n",
      "Epoch 264201, Training Loss: 6559, Validation Loss: 68506, L1: 68506\n",
      "Epoch 264301, Training Loss: 4484, Validation Loss: 68277, L1: 68277\n",
      "Epoch 264401, Training Loss: 4404, Validation Loss: 68241, L1: 68241\n",
      "Epoch 264501, Training Loss: 4372, Validation Loss: 68266, L1: 68266\n",
      "Epoch 264601, Training Loss: 5255, Validation Loss: 68240, L1: 68240\n",
      "Epoch 264701, Training Loss: 4480, Validation Loss: 68124, L1: 68124\n",
      "Epoch 264801, Training Loss: 4236, Validation Loss: 68226, L1: 68226\n",
      "Epoch 264901, Training Loss: 4235, Validation Loss: 68325, L1: 68325\n",
      "Epoch 265001, Training Loss: 4805, Validation Loss: 68190, L1: 68190\n",
      "Epoch 265101, Training Loss: 4066, Validation Loss: 68386, L1: 68386\n",
      "Epoch 265201, Training Loss: 5045, Validation Loss: 68561, L1: 68561\n",
      "Epoch 265301, Training Loss: 4413, Validation Loss: 68305, L1: 68305\n",
      "Epoch 265401, Training Loss: 4856, Validation Loss: 68221, L1: 68221\n",
      "Epoch 265501, Training Loss: 4742, Validation Loss: 68242, L1: 68242\n",
      "Epoch 265601, Training Loss: 4676, Validation Loss: 68290, L1: 68290\n",
      "Epoch 265701, Training Loss: 4636, Validation Loss: 68346, L1: 68346\n",
      "Epoch 265801, Training Loss: 4716, Validation Loss: 68271, L1: 68271\n",
      "Epoch 265901, Training Loss: 4260, Validation Loss: 68510, L1: 68510\n",
      "Epoch 266001, Training Loss: 5138, Validation Loss: 68201, L1: 68201\n",
      "Epoch 266101, Training Loss: 4481, Validation Loss: 68287, L1: 68287\n",
      "Epoch 266201, Training Loss: 4639, Validation Loss: 68407, L1: 68407\n",
      "Epoch 266301, Training Loss: 4735, Validation Loss: 68206, L1: 68206\n",
      "Epoch 266401, Training Loss: 4589, Validation Loss: 68332, L1: 68332\n",
      "Epoch 266501, Training Loss: 4914, Validation Loss: 68347, L1: 68347\n",
      "Epoch 266601, Training Loss: 4963, Validation Loss: 68267, L1: 68267\n",
      "Epoch 266701, Training Loss: 4344, Validation Loss: 68432, L1: 68432\n",
      "Epoch 266801, Training Loss: 5043, Validation Loss: 68302, L1: 68302\n",
      "Epoch 266901, Training Loss: 5406, Validation Loss: 68656, L1: 68656\n",
      "Epoch 267001, Training Loss: 6291, Validation Loss: 68377, L1: 68377\n",
      "Epoch 267101, Training Loss: 4644, Validation Loss: 68458, L1: 68458\n",
      "Epoch 267201, Training Loss: 4254, Validation Loss: 68542, L1: 68542\n",
      "Epoch 267301, Training Loss: 4007, Validation Loss: 68492, L1: 68492\n",
      "Epoch 267401, Training Loss: 4519, Validation Loss: 68455, L1: 68455\n",
      "Epoch 267501, Training Loss: 4539, Validation Loss: 68560, L1: 68560\n",
      "Epoch 267601, Training Loss: 4740, Validation Loss: 68405, L1: 68405\n",
      "Epoch 267701, Training Loss: 4448, Validation Loss: 68508, L1: 68508\n",
      "Epoch 267801, Training Loss: 4649, Validation Loss: 68565, L1: 68565\n",
      "Epoch 267901, Training Loss: 4684, Validation Loss: 68622, L1: 68622\n",
      "Epoch 268001, Training Loss: 4459, Validation Loss: 68511, L1: 68511\n",
      "Epoch 268101, Training Loss: 4165, Validation Loss: 68478, L1: 68478\n",
      "Epoch 268201, Training Loss: 4906, Validation Loss: 68291, L1: 68291\n",
      "Epoch 268301, Training Loss: 4261, Validation Loss: 68484, L1: 68484\n",
      "Epoch 268401, Training Loss: 4086, Validation Loss: 68400, L1: 68400\n",
      "Epoch 268501, Training Loss: 4137, Validation Loss: 68356, L1: 68356\n",
      "Epoch 268601, Training Loss: 4246, Validation Loss: 68479, L1: 68479\n",
      "Epoch 268701, Training Loss: 4430, Validation Loss: 68382, L1: 68382\n",
      "Epoch 268801, Training Loss: 4459, Validation Loss: 68293, L1: 68293\n",
      "Epoch 268901, Training Loss: 4940, Validation Loss: 68377, L1: 68377\n",
      "Epoch 269001, Training Loss: 4466, Validation Loss: 68442, L1: 68442\n",
      "Epoch 269101, Training Loss: 4597, Validation Loss: 68596, L1: 68596\n",
      "Epoch 269201, Training Loss: 4673, Validation Loss: 68751, L1: 68751\n",
      "Epoch 269301, Training Loss: 4701, Validation Loss: 68624, L1: 68624\n",
      "Epoch 269401, Training Loss: 4006, Validation Loss: 68522, L1: 68522\n",
      "Epoch 269501, Training Loss: 5494, Validation Loss: 68342, L1: 68342\n",
      "Epoch 269601, Training Loss: 6301, Validation Loss: 68680, L1: 68680\n",
      "Epoch 269701, Training Loss: 4794, Validation Loss: 68759, L1: 68759\n",
      "Epoch 269801, Training Loss: 4862, Validation Loss: 68692, L1: 68692\n",
      "Epoch 269901, Training Loss: 4576, Validation Loss: 68596, L1: 68596\n",
      "Epoch 270001, Training Loss: 4135, Validation Loss: 68557, L1: 68557\n",
      "Epoch 270101, Training Loss: 4760, Validation Loss: 68516, L1: 68516\n",
      "Epoch 270201, Training Loss: 4314, Validation Loss: 68552, L1: 68552\n",
      "Epoch 270301, Training Loss: 4274, Validation Loss: 68629, L1: 68629\n",
      "Epoch 270401, Training Loss: 3918, Validation Loss: 68486, L1: 68486\n",
      "Epoch 270501, Training Loss: 5431, Validation Loss: 68728, L1: 68728\n",
      "Epoch 270601, Training Loss: 6525, Validation Loss: 68462, L1: 68462\n",
      "Epoch 270701, Training Loss: 4000, Validation Loss: 68479, L1: 68479\n",
      "Epoch 270801, Training Loss: 4169, Validation Loss: 68594, L1: 68594\n",
      "Epoch 270901, Training Loss: 4121, Validation Loss: 68778, L1: 68778\n",
      "Epoch 271001, Training Loss: 4023, Validation Loss: 68523, L1: 68523\n",
      "Epoch 271101, Training Loss: 4561, Validation Loss: 68782, L1: 68782\n",
      "Epoch 271201, Training Loss: 4412, Validation Loss: 68514, L1: 68514\n",
      "Epoch 271301, Training Loss: 5007, Validation Loss: 68973, L1: 68973\n",
      "Epoch 271401, Training Loss: 4159, Validation Loss: 68644, L1: 68644\n",
      "Epoch 271501, Training Loss: 4273, Validation Loss: 68529, L1: 68529\n",
      "Epoch 271601, Training Loss: 4326, Validation Loss: 68551, L1: 68551\n",
      "Epoch 271701, Training Loss: 4008, Validation Loss: 68577, L1: 68577\n",
      "Epoch 271801, Training Loss: 4756, Validation Loss: 68571, L1: 68571\n",
      "Epoch 271901, Training Loss: 4211, Validation Loss: 68528, L1: 68528\n",
      "Epoch 272001, Training Loss: 4189, Validation Loss: 68566, L1: 68566\n",
      "Epoch 272101, Training Loss: 4261, Validation Loss: 68577, L1: 68577\n",
      "Epoch 272201, Training Loss: 4526, Validation Loss: 68474, L1: 68474\n",
      "Epoch 272301, Training Loss: 3942, Validation Loss: 68541, L1: 68541\n",
      "Epoch 272401, Training Loss: 4172, Validation Loss: 68557, L1: 68557\n",
      "Epoch 272501, Training Loss: 4188, Validation Loss: 68791, L1: 68791\n",
      "Epoch 272601, Training Loss: 4410, Validation Loss: 68692, L1: 68692\n",
      "Epoch 272701, Training Loss: 4573, Validation Loss: 68751, L1: 68751\n",
      "Epoch 272801, Training Loss: 4385, Validation Loss: 68544, L1: 68544\n",
      "Epoch 272901, Training Loss: 4282, Validation Loss: 68643, L1: 68643\n",
      "Epoch 273001, Training Loss: 4076, Validation Loss: 68777, L1: 68777\n",
      "Epoch 273101, Training Loss: 3944, Validation Loss: 68734, L1: 68734\n",
      "Epoch 273201, Training Loss: 3807, Validation Loss: 68626, L1: 68626\n",
      "Epoch 273301, Training Loss: 4248, Validation Loss: 68740, L1: 68740\n",
      "Epoch 273401, Training Loss: 4670, Validation Loss: 68485, L1: 68485\n",
      "Epoch 273501, Training Loss: 3852, Validation Loss: 68563, L1: 68563\n",
      "Epoch 273601, Training Loss: 3848, Validation Loss: 68686, L1: 68686\n",
      "Epoch 273701, Training Loss: 4277, Validation Loss: 68638, L1: 68638\n",
      "Epoch 273801, Training Loss: 4526, Validation Loss: 68802, L1: 68802\n",
      "Epoch 273901, Training Loss: 4406, Validation Loss: 68798, L1: 68798\n",
      "Epoch 274001, Training Loss: 4479, Validation Loss: 68880, L1: 68880\n",
      "Epoch 274101, Training Loss: 4561, Validation Loss: 68815, L1: 68815\n",
      "Epoch 274201, Training Loss: 4562, Validation Loss: 68857, L1: 68857\n",
      "Epoch 274301, Training Loss: 3954, Validation Loss: 68880, L1: 68880\n",
      "Epoch 274401, Training Loss: 4519, Validation Loss: 68789, L1: 68789\n",
      "Epoch 274501, Training Loss: 4499, Validation Loss: 68826, L1: 68826\n",
      "Epoch 274601, Training Loss: 4425, Validation Loss: 68767, L1: 68767\n",
      "Epoch 274701, Training Loss: 4133, Validation Loss: 68857, L1: 68857\n",
      "Epoch 274801, Training Loss: 4121, Validation Loss: 68812, L1: 68812\n",
      "Epoch 274901, Training Loss: 4041, Validation Loss: 68720, L1: 68720\n",
      "Epoch 275001, Training Loss: 4250, Validation Loss: 68693, L1: 68693\n",
      "Epoch 275101, Training Loss: 4026, Validation Loss: 68683, L1: 68683\n",
      "Epoch 275201, Training Loss: 4018, Validation Loss: 68717, L1: 68717\n",
      "Epoch 275301, Training Loss: 4023, Validation Loss: 68817, L1: 68817\n",
      "Epoch 275401, Training Loss: 4691, Validation Loss: 68560, L1: 68560\n",
      "Epoch 275501, Training Loss: 4479, Validation Loss: 68658, L1: 68658\n",
      "Epoch 275601, Training Loss: 4205, Validation Loss: 68685, L1: 68685\n",
      "Epoch 275701, Training Loss: 4524, Validation Loss: 68660, L1: 68660\n",
      "Epoch 275801, Training Loss: 4163, Validation Loss: 68704, L1: 68704\n",
      "Epoch 275901, Training Loss: 3907, Validation Loss: 68728, L1: 68728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276001, Training Loss: 4500, Validation Loss: 68861, L1: 68861\n",
      "Epoch 276101, Training Loss: 3854, Validation Loss: 68795, L1: 68795\n",
      "Epoch 276201, Training Loss: 4021, Validation Loss: 68670, L1: 68670\n",
      "Epoch 276301, Training Loss: 3897, Validation Loss: 68679, L1: 68679\n",
      "Epoch 276401, Training Loss: 4339, Validation Loss: 68671, L1: 68671\n",
      "Epoch 276501, Training Loss: 4526, Validation Loss: 68921, L1: 68921\n",
      "Epoch 276601, Training Loss: 4354, Validation Loss: 68775, L1: 68775\n",
      "Epoch 276701, Training Loss: 4302, Validation Loss: 68710, L1: 68710\n",
      "Epoch 276801, Training Loss: 4322, Validation Loss: 68802, L1: 68802\n",
      "Epoch 276901, Training Loss: 4195, Validation Loss: 68812, L1: 68812\n",
      "Epoch 277001, Training Loss: 4258, Validation Loss: 68822, L1: 68822\n",
      "Epoch 277101, Training Loss: 3724, Validation Loss: 68984, L1: 68984\n",
      "Epoch 277201, Training Loss: 3803, Validation Loss: 68976, L1: 68976\n",
      "Epoch 277301, Training Loss: 3744, Validation Loss: 68845, L1: 68845\n",
      "Epoch 277401, Training Loss: 3828, Validation Loss: 68870, L1: 68870\n",
      "Epoch 277501, Training Loss: 4024, Validation Loss: 68727, L1: 68727\n",
      "Epoch 277601, Training Loss: 3934, Validation Loss: 68728, L1: 68728\n",
      "Epoch 277701, Training Loss: 3776, Validation Loss: 68816, L1: 68816\n",
      "Epoch 277801, Training Loss: 4113, Validation Loss: 68954, L1: 68954\n",
      "Epoch 277901, Training Loss: 4096, Validation Loss: 68968, L1: 68968\n",
      "Epoch 278001, Training Loss: 3832, Validation Loss: 68834, L1: 68834\n",
      "Epoch 278101, Training Loss: 3876, Validation Loss: 68976, L1: 68976\n",
      "Epoch 278201, Training Loss: 3709, Validation Loss: 68896, L1: 68896\n",
      "Epoch 278301, Training Loss: 6165, Validation Loss: 68909, L1: 68909\n",
      "Epoch 278401, Training Loss: 4358, Validation Loss: 68941, L1: 68941\n",
      "Epoch 278501, Training Loss: 4016, Validation Loss: 69007, L1: 69007\n",
      "Epoch 278601, Training Loss: 3819, Validation Loss: 68767, L1: 68767\n",
      "Epoch 278701, Training Loss: 3758, Validation Loss: 68954, L1: 68954\n",
      "Epoch 278801, Training Loss: 3905, Validation Loss: 68872, L1: 68872\n",
      "Epoch 278901, Training Loss: 4402, Validation Loss: 68989, L1: 68989\n",
      "Epoch 279001, Training Loss: 6293, Validation Loss: 69077, L1: 69077\n",
      "Epoch 279101, Training Loss: 4237, Validation Loss: 69399, L1: 69399\n",
      "Epoch 279201, Training Loss: 5100, Validation Loss: 69154, L1: 69154\n",
      "Epoch 279301, Training Loss: 4171, Validation Loss: 69397, L1: 69397\n",
      "Epoch 279401, Training Loss: 4088, Validation Loss: 68984, L1: 68984\n",
      "Epoch 279501, Training Loss: 3878, Validation Loss: 68893, L1: 68893\n",
      "Epoch 279601, Training Loss: 4374, Validation Loss: 68957, L1: 68957\n",
      "Epoch 279701, Training Loss: 3676, Validation Loss: 68908, L1: 68908\n",
      "Epoch 279801, Training Loss: 3816, Validation Loss: 68852, L1: 68852\n",
      "Epoch 279901, Training Loss: 4277, Validation Loss: 69008, L1: 69008\n",
      "Epoch 280001, Training Loss: 4585, Validation Loss: 69265, L1: 69265\n",
      "Epoch 280101, Training Loss: 3909, Validation Loss: 68861, L1: 68861\n",
      "Epoch 280201, Training Loss: 3859, Validation Loss: 68990, L1: 68990\n",
      "Epoch 280301, Training Loss: 4241, Validation Loss: 68892, L1: 68892\n",
      "Epoch 280401, Training Loss: 4066, Validation Loss: 68992, L1: 68992\n",
      "Epoch 280501, Training Loss: 3883, Validation Loss: 69049, L1: 69049\n",
      "Epoch 280601, Training Loss: 4128, Validation Loss: 69193, L1: 69193\n",
      "Epoch 280701, Training Loss: 3988, Validation Loss: 69020, L1: 69020\n",
      "Epoch 280801, Training Loss: 3990, Validation Loss: 69117, L1: 69117\n",
      "Epoch 280901, Training Loss: 3868, Validation Loss: 69208, L1: 69208\n",
      "Epoch 281001, Training Loss: 3850, Validation Loss: 69018, L1: 69018\n",
      "Epoch 281101, Training Loss: 3892, Validation Loss: 69052, L1: 69052\n",
      "Epoch 281201, Training Loss: 3891, Validation Loss: 69177, L1: 69177\n",
      "Epoch 281301, Training Loss: 3975, Validation Loss: 69012, L1: 69012\n",
      "Epoch 281401, Training Loss: 4292, Validation Loss: 69035, L1: 69035\n",
      "Epoch 281501, Training Loss: 4761, Validation Loss: 69002, L1: 69002\n",
      "Epoch 281601, Training Loss: 4177, Validation Loss: 69132, L1: 69132\n",
      "Epoch 281701, Training Loss: 4312, Validation Loss: 69115, L1: 69115\n",
      "Epoch 281801, Training Loss: 4272, Validation Loss: 69098, L1: 69098\n",
      "Epoch 281901, Training Loss: 3890, Validation Loss: 69147, L1: 69147\n",
      "Epoch 282001, Training Loss: 3957, Validation Loss: 69198, L1: 69198\n",
      "Epoch 282101, Training Loss: 4349, Validation Loss: 69140, L1: 69140\n",
      "Epoch 282201, Training Loss: 3655, Validation Loss: 69088, L1: 69088\n",
      "Epoch 282301, Training Loss: 5218, Validation Loss: 69093, L1: 69093\n",
      "Epoch 282401, Training Loss: 6090, Validation Loss: 69264, L1: 69264\n",
      "Epoch 282501, Training Loss: 4463, Validation Loss: 69485, L1: 69485\n",
      "Epoch 282601, Training Loss: 4935, Validation Loss: 69135, L1: 69135\n",
      "Epoch 282701, Training Loss: 5480, Validation Loss: 69520, L1: 69520\n",
      "Epoch 282801, Training Loss: 4514, Validation Loss: 69315, L1: 69315\n",
      "Epoch 282901, Training Loss: 3815, Validation Loss: 69342, L1: 69342\n",
      "Epoch 283001, Training Loss: 3717, Validation Loss: 69347, L1: 69347\n",
      "Epoch 283101, Training Loss: 4356, Validation Loss: 69372, L1: 69372\n",
      "Epoch 283201, Training Loss: 4607, Validation Loss: 69103, L1: 69103\n",
      "Epoch 283301, Training Loss: 4417, Validation Loss: 69145, L1: 69145\n",
      "Epoch 283401, Training Loss: 4427, Validation Loss: 69136, L1: 69136\n",
      "Epoch 283501, Training Loss: 3939, Validation Loss: 69231, L1: 69231\n",
      "Epoch 283601, Training Loss: 4259, Validation Loss: 69478, L1: 69478\n",
      "Epoch 283701, Training Loss: 3862, Validation Loss: 69231, L1: 69231\n",
      "Epoch 283801, Training Loss: 3655, Validation Loss: 69345, L1: 69345\n",
      "Epoch 283901, Training Loss: 3661, Validation Loss: 69322, L1: 69322\n",
      "Epoch 284001, Training Loss: 3518, Validation Loss: 69181, L1: 69181\n",
      "Epoch 284101, Training Loss: 3650, Validation Loss: 69403, L1: 69403\n",
      "Epoch 284201, Training Loss: 3780, Validation Loss: 69039, L1: 69039\n",
      "Epoch 284301, Training Loss: 4763, Validation Loss: 69190, L1: 69190\n",
      "Epoch 284401, Training Loss: 3911, Validation Loss: 69224, L1: 69224\n",
      "Epoch 284501, Training Loss: 4257, Validation Loss: 69194, L1: 69194\n",
      "Epoch 284601, Training Loss: 4165, Validation Loss: 69272, L1: 69272\n",
      "Epoch 284701, Training Loss: 3626, Validation Loss: 69300, L1: 69300\n",
      "Epoch 284801, Training Loss: 4389, Validation Loss: 69125, L1: 69125\n",
      "Epoch 284901, Training Loss: 4397, Validation Loss: 69222, L1: 69222\n",
      "Epoch 285001, Training Loss: 3868, Validation Loss: 69313, L1: 69313\n",
      "Epoch 285101, Training Loss: 4531, Validation Loss: 69182, L1: 69182\n",
      "Epoch 285201, Training Loss: 3957, Validation Loss: 69371, L1: 69371\n",
      "Epoch 285301, Training Loss: 3603, Validation Loss: 69379, L1: 69379\n",
      "Epoch 285401, Training Loss: 3861, Validation Loss: 69341, L1: 69341\n",
      "Epoch 285501, Training Loss: 3755, Validation Loss: 69423, L1: 69423\n",
      "Epoch 285601, Training Loss: 3666, Validation Loss: 69206, L1: 69206\n",
      "Epoch 285701, Training Loss: 3522, Validation Loss: 69208, L1: 69208\n",
      "Epoch 285801, Training Loss: 5095, Validation Loss: 69397, L1: 69397\n",
      "Epoch 285901, Training Loss: 3930, Validation Loss: 69395, L1: 69395\n",
      "Epoch 286001, Training Loss: 3857, Validation Loss: 69359, L1: 69359\n",
      "Epoch 286101, Training Loss: 4665, Validation Loss: 69491, L1: 69491\n",
      "Epoch 286201, Training Loss: 4106, Validation Loss: 69567, L1: 69567\n",
      "Epoch 286301, Training Loss: 4405, Validation Loss: 69502, L1: 69502\n",
      "Epoch 286401, Training Loss: 3969, Validation Loss: 69468, L1: 69468\n",
      "Epoch 286501, Training Loss: 3801, Validation Loss: 69490, L1: 69490\n",
      "Epoch 286601, Training Loss: 4257, Validation Loss: 69565, L1: 69565\n",
      "Epoch 286701, Training Loss: 4030, Validation Loss: 69600, L1: 69600\n",
      "Epoch 286801, Training Loss: 3741, Validation Loss: 69386, L1: 69386\n",
      "Epoch 286901, Training Loss: 3878, Validation Loss: 69455, L1: 69455\n",
      "Epoch 287001, Training Loss: 3597, Validation Loss: 69488, L1: 69488\n",
      "Epoch 287101, Training Loss: 3700, Validation Loss: 69362, L1: 69362\n",
      "Epoch 287201, Training Loss: 3509, Validation Loss: 69361, L1: 69361\n",
      "Epoch 287301, Training Loss: 3409, Validation Loss: 69494, L1: 69494\n",
      "Epoch 287401, Training Loss: 3775, Validation Loss: 69466, L1: 69466\n",
      "Epoch 287501, Training Loss: 3721, Validation Loss: 69420, L1: 69420\n",
      "Epoch 287601, Training Loss: 4150, Validation Loss: 69573, L1: 69573\n",
      "Epoch 287701, Training Loss: 3726, Validation Loss: 69577, L1: 69577\n",
      "Epoch 287801, Training Loss: 4103, Validation Loss: 69339, L1: 69339\n",
      "Epoch 287901, Training Loss: 3771, Validation Loss: 69511, L1: 69511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288001, Training Loss: 3941, Validation Loss: 69521, L1: 69521\n",
      "Epoch 288101, Training Loss: 3773, Validation Loss: 69467, L1: 69467\n",
      "Epoch 288201, Training Loss: 3720, Validation Loss: 69461, L1: 69461\n",
      "Epoch 288301, Training Loss: 3576, Validation Loss: 69548, L1: 69548\n",
      "Epoch 288401, Training Loss: 3606, Validation Loss: 69701, L1: 69701\n",
      "Epoch 288501, Training Loss: 5015, Validation Loss: 69272, L1: 69272\n",
      "Epoch 288601, Training Loss: 3608, Validation Loss: 69539, L1: 69539\n",
      "Epoch 288701, Training Loss: 3651, Validation Loss: 69600, L1: 69600\n",
      "Epoch 288801, Training Loss: 3322, Validation Loss: 69535, L1: 69535\n",
      "Epoch 288901, Training Loss: 3510, Validation Loss: 69578, L1: 69578\n",
      "Epoch 289001, Training Loss: 3472, Validation Loss: 69498, L1: 69498\n",
      "Epoch 289101, Training Loss: 3481, Validation Loss: 69350, L1: 69350\n",
      "Epoch 289201, Training Loss: 4578, Validation Loss: 69461, L1: 69461\n",
      "Epoch 289301, Training Loss: 5486, Validation Loss: 69956, L1: 69956\n",
      "Epoch 289401, Training Loss: 3415, Validation Loss: 69496, L1: 69496\n",
      "Epoch 289501, Training Loss: 4864, Validation Loss: 69795, L1: 69795\n",
      "Epoch 289601, Training Loss: 3366, Validation Loss: 69497, L1: 69497\n",
      "Epoch 289701, Training Loss: 3613, Validation Loss: 69643, L1: 69643\n",
      "Epoch 289801, Training Loss: 3536, Validation Loss: 69521, L1: 69521\n",
      "Epoch 289901, Training Loss: 3797, Validation Loss: 69750, L1: 69750\n",
      "Epoch 290001, Training Loss: 3543, Validation Loss: 69631, L1: 69631\n",
      "Epoch 290101, Training Loss: 4932, Validation Loss: 69737, L1: 69737\n",
      "Epoch 290201, Training Loss: 3733, Validation Loss: 69737, L1: 69737\n",
      "Epoch 290301, Training Loss: 3588, Validation Loss: 69597, L1: 69597\n",
      "Epoch 290401, Training Loss: 3724, Validation Loss: 69663, L1: 69663\n",
      "Epoch 290501, Training Loss: 3752, Validation Loss: 69449, L1: 69449\n",
      "Epoch 290601, Training Loss: 3923, Validation Loss: 69632, L1: 69632\n",
      "Epoch 290701, Training Loss: 3421, Validation Loss: 69707, L1: 69707\n",
      "Epoch 290801, Training Loss: 4798, Validation Loss: 69964, L1: 69964\n",
      "Epoch 290901, Training Loss: 3321, Validation Loss: 69709, L1: 69709\n",
      "Epoch 291001, Training Loss: 4184, Validation Loss: 69668, L1: 69668\n",
      "Epoch 291101, Training Loss: 3494, Validation Loss: 69623, L1: 69623\n",
      "Epoch 291201, Training Loss: 3417, Validation Loss: 69646, L1: 69646\n",
      "Epoch 291301, Training Loss: 3955, Validation Loss: 69677, L1: 69677\n",
      "Epoch 291401, Training Loss: 4063, Validation Loss: 69673, L1: 69673\n",
      "Epoch 291501, Training Loss: 3721, Validation Loss: 69872, L1: 69872\n",
      "Epoch 291601, Training Loss: 3497, Validation Loss: 69661, L1: 69661\n",
      "Epoch 291701, Training Loss: 3668, Validation Loss: 69608, L1: 69608\n",
      "Epoch 291801, Training Loss: 3483, Validation Loss: 69434, L1: 69434\n",
      "Epoch 291901, Training Loss: 3785, Validation Loss: 69692, L1: 69692\n",
      "Epoch 292001, Training Loss: 3271, Validation Loss: 69689, L1: 69689\n",
      "Epoch 292101, Training Loss: 3369, Validation Loss: 69603, L1: 69603\n",
      "Epoch 292201, Training Loss: 3747, Validation Loss: 69544, L1: 69544\n",
      "Epoch 292301, Training Loss: 3664, Validation Loss: 69754, L1: 69754\n",
      "Epoch 292401, Training Loss: 4039, Validation Loss: 69562, L1: 69562\n",
      "Epoch 292501, Training Loss: 5895, Validation Loss: 69936, L1: 69936\n",
      "Epoch 292601, Training Loss: 3540, Validation Loss: 69945, L1: 69945\n",
      "Epoch 292701, Training Loss: 4693, Validation Loss: 69689, L1: 69689\n",
      "Epoch 292801, Training Loss: 5373, Validation Loss: 70004, L1: 70004\n",
      "Epoch 292901, Training Loss: 3725, Validation Loss: 69905, L1: 69905\n",
      "Epoch 293001, Training Loss: 3538, Validation Loss: 69755, L1: 69755\n",
      "Epoch 293101, Training Loss: 3830, Validation Loss: 69917, L1: 69917\n",
      "Epoch 293201, Training Loss: 3994, Validation Loss: 69885, L1: 69885\n",
      "Epoch 293301, Training Loss: 3304, Validation Loss: 69848, L1: 69848\n",
      "Epoch 293401, Training Loss: 3502, Validation Loss: 69836, L1: 69836\n",
      "Epoch 293501, Training Loss: 3729, Validation Loss: 69983, L1: 69983\n",
      "Epoch 293601, Training Loss: 4348, Validation Loss: 69615, L1: 69615\n",
      "Epoch 293701, Training Loss: 3477, Validation Loss: 69925, L1: 69925\n",
      "Epoch 293801, Training Loss: 3847, Validation Loss: 69892, L1: 69892\n",
      "Epoch 293901, Training Loss: 3476, Validation Loss: 69932, L1: 69932\n",
      "Epoch 294001, Training Loss: 3337, Validation Loss: 69787, L1: 69787\n",
      "Epoch 294101, Training Loss: 3603, Validation Loss: 69735, L1: 69735\n",
      "Epoch 294201, Training Loss: 3699, Validation Loss: 69841, L1: 69841\n",
      "Epoch 294301, Training Loss: 3967, Validation Loss: 69806, L1: 69806\n",
      "Epoch 294401, Training Loss: 4014, Validation Loss: 69828, L1: 69828\n",
      "Epoch 294501, Training Loss: 4285, Validation Loss: 69969, L1: 69969\n",
      "Epoch 294601, Training Loss: 3656, Validation Loss: 69846, L1: 69846\n",
      "Epoch 294701, Training Loss: 3653, Validation Loss: 69700, L1: 69700\n",
      "Epoch 294801, Training Loss: 3729, Validation Loss: 69869, L1: 69869\n",
      "Epoch 294901, Training Loss: 3787, Validation Loss: 69916, L1: 69916\n",
      "Epoch 295001, Training Loss: 4070, Validation Loss: 69813, L1: 69813\n",
      "Epoch 295101, Training Loss: 3854, Validation Loss: 69870, L1: 69870\n",
      "Epoch 295201, Training Loss: 3784, Validation Loss: 69895, L1: 69895\n",
      "Epoch 295301, Training Loss: 4000, Validation Loss: 70085, L1: 70085\n",
      "Epoch 295401, Training Loss: 3344, Validation Loss: 69929, L1: 69929\n",
      "Epoch 295501, Training Loss: 4173, Validation Loss: 69992, L1: 69992\n",
      "Epoch 295601, Training Loss: 3343, Validation Loss: 69994, L1: 69994\n",
      "Epoch 295701, Training Loss: 4016, Validation Loss: 69793, L1: 69793\n",
      "Epoch 295801, Training Loss: 4312, Validation Loss: 70004, L1: 70004\n",
      "Epoch 295901, Training Loss: 3160, Validation Loss: 69839, L1: 69839\n",
      "Epoch 296001, Training Loss: 4080, Validation Loss: 69863, L1: 69863\n",
      "Epoch 296101, Training Loss: 3746, Validation Loss: 69807, L1: 69807\n",
      "Epoch 296201, Training Loss: 4078, Validation Loss: 69805, L1: 69805\n",
      "Epoch 296301, Training Loss: 3345, Validation Loss: 69917, L1: 69917\n",
      "Epoch 296401, Training Loss: 4289, Validation Loss: 70113, L1: 70113\n",
      "Epoch 296501, Training Loss: 3347, Validation Loss: 70050, L1: 70050\n",
      "Epoch 296601, Training Loss: 3678, Validation Loss: 70050, L1: 70050\n",
      "Epoch 296701, Training Loss: 3659, Validation Loss: 70113, L1: 70113\n",
      "Epoch 296801, Training Loss: 3501, Validation Loss: 70163, L1: 70163\n",
      "Epoch 296901, Training Loss: 4236, Validation Loss: 69861, L1: 69861\n",
      "Epoch 297001, Training Loss: 3741, Validation Loss: 70061, L1: 70061\n",
      "Epoch 297101, Training Loss: 3756, Validation Loss: 70080, L1: 70080\n",
      "Epoch 297201, Training Loss: 3805, Validation Loss: 70019, L1: 70019\n",
      "Epoch 297301, Training Loss: 3844, Validation Loss: 70001, L1: 70001\n",
      "Epoch 297401, Training Loss: 3662, Validation Loss: 70018, L1: 70018\n",
      "Epoch 297501, Training Loss: 3174, Validation Loss: 69957, L1: 69957\n",
      "Epoch 297601, Training Loss: 3820, Validation Loss: 70090, L1: 70090\n",
      "Epoch 297701, Training Loss: 3382, Validation Loss: 69969, L1: 69969\n",
      "Epoch 297801, Training Loss: 3201, Validation Loss: 70005, L1: 70005\n",
      "Epoch 297901, Training Loss: 4017, Validation Loss: 69945, L1: 69945\n",
      "Epoch 298001, Training Loss: 4181, Validation Loss: 69854, L1: 69854\n",
      "Epoch 298101, Training Loss: 3931, Validation Loss: 70101, L1: 70101\n",
      "Epoch 298201, Training Loss: 3620, Validation Loss: 70206, L1: 70206\n",
      "Epoch 298301, Training Loss: 3580, Validation Loss: 70303, L1: 70303\n",
      "Epoch 298401, Training Loss: 3886, Validation Loss: 70380, L1: 70380\n",
      "Epoch 298501, Training Loss: 3371, Validation Loss: 70206, L1: 70206\n",
      "Epoch 298601, Training Loss: 3758, Validation Loss: 70183, L1: 70183\n",
      "Epoch 298701, Training Loss: 3835, Validation Loss: 70207, L1: 70207\n",
      "Epoch 298801, Training Loss: 3324, Validation Loss: 70049, L1: 70049\n",
      "Epoch 298901, Training Loss: 3785, Validation Loss: 70008, L1: 70008\n",
      "Epoch 299001, Training Loss: 3457, Validation Loss: 70179, L1: 70179\n",
      "Epoch 299101, Training Loss: 3301, Validation Loss: 70220, L1: 70220\n",
      "Epoch 299201, Training Loss: 3342, Validation Loss: 70046, L1: 70046\n",
      "Epoch 299301, Training Loss: 3846, Validation Loss: 70321, L1: 70321\n",
      "Epoch 299401, Training Loss: 4083, Validation Loss: 70160, L1: 70160\n",
      "Epoch 299501, Training Loss: 3761, Validation Loss: 70153, L1: 70153\n",
      "Epoch 299601, Training Loss: 3360, Validation Loss: 70217, L1: 70217\n",
      "Epoch 299701, Training Loss: 3495, Validation Loss: 70181, L1: 70181\n",
      "Epoch 299801, Training Loss: 3279, Validation Loss: 70145, L1: 70145\n",
      "Epoch 299901, Training Loss: 3333, Validation Loss: 70109, L1: 70109\n"
     ]
    }
   ],
   "source": [
    "base_neuron_count = 32\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(n_features, n_features)\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.fc(x) + x)\n",
    "\n",
    "class TabularFFNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TabularFFNN, self).__init__()\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, base_neuron_count),\n",
    "            nn.ELU(),\n",
    "            nn.LayerNorm(base_neuron_count),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count * 2),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(base_neuron_count * 2, base_neuron_count * 2),\n",
    "            nn.ELU(),\n",
    "            nn.LayerNorm(base_neuron_count * 2),\n",
    "            nn.Linear(base_neuron_count * 2, base_neuron_count // 2),\n",
    "            ResidualBlock(base_neuron_count // 2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(base_neuron_count // 2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "class TabularFFNNOLD(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNOLD, self).__init__()\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, base_neuron_count), \n",
    "            nn.BatchNorm1d(base_neuron_count),  # Ensure the input here has 512 features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count), # 16\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.4):\n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(input_size, base_neuron_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.Linear(base_neuron_count, base_neuron_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_neuron_count, output_size)\n",
    "        )\n",
    "        \n",
    "        for m in self.ffnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize the features\n",
    "device = torch.device(\"cpu\")\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "# Split the data into training and combined validation and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor, test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the combined validation and testing sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if the dimensions match the expected input size for the model\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Output\n",
    "# input_size, train_loader, test_loader\n",
    "\n",
    "model = TabularFFNNSimple(\n",
    "    input_size = input_size,\n",
    "    output_size = 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 300000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_suc = [] # to have a reference to it\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=7e-4,\n",
    "    weight_decay=0\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_abs = torch.nn.L1Loss()\n",
    "criterion = criterion_abs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.999999, \n",
    "    patience=10, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    l1_losses = []\n",
    "    for tuple_ in train_loader:\n",
    "        datas, prices = tuple_\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        prices_viewed = prices.view(-1, 1).float()\n",
    "        loss = criterion(outputs, prices_viewed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tuple_ in val_loader:\n",
    "            datas, prices = tuple_\n",
    "            outputs = model(datas)  # Forward pass\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "    l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "    # Print epoch's summary\n",
    "    epochs_suc.append(epoch)\n",
    "    scheduler.step(val_losses[-1])\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {int(train_losses[-1])}, Validation Loss: {int(val_losses[-1])}, L1: {int(l1_mean_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a1315f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833308141276423"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "model.eval()\n",
    "r2_score(model(X_train).detach().numpy(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff15514f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_sampled = train_losses[::10000]  # Select every 1000th value\n",
    "val_losses_sampled = val_losses[::10000]      # Select every 1000th value\n",
    "\n",
    "# Generate corresponding epoch numbers, assuming epochs_suc is your list of epoch numbers\n",
    "epochs_sampled = epochs_suc[::10000]\n",
    "\n",
    "plt.title(\"Overfitted model evaluation\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Use sampled data for plotting\n",
    "plt.plot(epochs_sampled, train_losses_sampled, label='Training')\n",
    "plt.plot(epochs_sampled, val_losses_sampled, label='Validation')\n",
    "\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale('log')\n",
    "plt.xticks(\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    range(1, epochs_sampled[-1], int(epochs_sampled[-1] / 8)),\n",
    "    rotation = 25\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"../visualizations/overfit_model_evaluation.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFFNNSimple(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 hidden_layer_size,\n",
    "                 hidden_layer_count,\n",
    "                 output_size = 1):\n",
    "        \n",
    "        super(TabularFFNNSimple, self).__init__()\n",
    "        \n",
    "        layers = [nn.Linear(input_size, hidden_layer_size), nn.ReLU()]\n",
    "        \n",
    "        for _ in range(hidden_layer_count - 1):\n",
    "            layers.append(nn.Linear(hidden_layer_size, hidden_layer_size))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            \n",
    "        layers.append(nn.Linear(hidden_layer_size, output_size))\n",
    "        \n",
    "        self.ffnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # print(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "\n",
    "def train(batch_size, hidden_layer_size, hidden_layer_count):\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = data.drop('price', axis=1)\n",
    "    y = data['price']\n",
    "\n",
    "    # Standardize the features\n",
    "    device = torch.device(\"cpu\")\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32, device = device)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.float32, device = device)\n",
    "\n",
    "\n",
    "    # Split the data into training and combined validation and testing sets\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(X_tensor, y_tensor,\n",
    "                                                                test_size=0.4, random_state=42)\n",
    "\n",
    "    # Split the combined validation and testing sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Create DataLoader for training, validation, and testing\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Check if the dimensions match the expected input size for the model\n",
    "    input_size = X_train.shape[1]\n",
    "\n",
    "    # Output\n",
    "    # input_size, train_loader, test_loader\n",
    "\n",
    "    model = TabularFFNNSimple(\n",
    "        input_size = input_size,\n",
    "        hidden_layer_size = hidden_layer_size,\n",
    "        hidden_layer_count = hidden_layer_count,\n",
    "        output_size = 1\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    num_epochs = 3000\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    epochs_suc = [] # to have a reference to it\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=8e-4,\n",
    "        weight_decay=5e-4\n",
    "    )\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    criterion_abs = torch.nn.L1Loss()\n",
    "    criterion = criterion_abs\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.999999, \n",
    "        patience=10, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        l1_losses = []\n",
    "        for tuple_ in train_loader:\n",
    "            datas, prices = tuple_\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(datas)\n",
    "            prices_viewed = prices.view(-1, 1).float()\n",
    "            loss = criterion(outputs, prices_viewed)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))  # Average loss for this epoch\n",
    "\n",
    "        # Validation\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for tuple_ in val_loader:\n",
    "                datas, prices = tuple_\n",
    "                outputs = model(datas)  # Forward pass\n",
    "                prices_viewed = prices.view(-1, 1).float()\n",
    "                loss = criterion(outputs, prices_viewed)  # Compute loss\n",
    "                val_loss += loss.item()  # Accumulate the loss\n",
    "                l1_losses.append(criterion_abs(outputs, prices_viewed))\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))  # Average loss for this epoch\n",
    "        l1_mean_loss = sum(l1_losses) / len(l1_losses)\n",
    "\n",
    "        epochs_suc.append(epoch)\n",
    "        scheduler.step(val_losses[-1])\n",
    "        print(f'Epoch {epoch+1}, \\\n",
    "                Training Loss: {int(train_losses[-1])}, \\\n",
    "                Validation Loss: {int(val_losses[-1])}, L1: {int(l1_mean_loss)}')\n",
    "\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92989393",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(64, 32, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b572ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tuples = [\n",
    "    (64, 32, 4),\n",
    "    \n",
    "    (64, 40, 6),\n",
    "    (64, 48, 6),\n",
    "    (64, 54, 6),\n",
    "    (64, 64, 6),\n",
    "    (64, 84, 6),\n",
    "    \n",
    "    (128, 64, 16),\n",
    "    (128, 64, 16),\n",
    "    (128, 64, 16),\n",
    "    (128, 64, 16),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for tuple_ in training_tuples:\n",
    "    train_losses, val_losses = train(tuple_[0], tuple_[1], tuple_[2])\n",
    "    results[tuple_] = (train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69febe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_SECOND_BATCH = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b83fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tuples = [\n",
    "    (128, 64, 2),\n",
    "    (128, 64, 3),\n",
    "    (128, 64, 4),\n",
    "    (128, 64, 5),\n",
    "    (128, 64, 6),\n",
    "    (128, 64, 7),\n",
    "    (128, 64, 8),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for tuple_ in training_tuples:\n",
    "    train_losses, val_losses = train(tuple_[0], tuple_[1], tuple_[2])\n",
    "    results[tuple_] = (train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a016399",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_THIRD_BATCH = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c508fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tuples = [\n",
    "    (128, 32, 7),\n",
    "    (128, 32, 8),\n",
    "    (128, 32, 8),\n",
    "    (128, 16, 8),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for tuple_ in training_tuples:\n",
    "    train_losses, val_losses = train(tuple_[0], tuple_[1], tuple_[2])\n",
    "    results[tuple_] = (train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f13d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_FORTH_BATCH = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = results_FIRST_BATCH | results_SECOND_BATCH | results_THIRD_BATCH | results_FORTH_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da548ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "del results_all[(64, 32, 4)]\n",
    "del results_all[(64, 32, 6)]\n",
    "del results_all[(64, 32, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'results' is your dictionary with training tuples as keys and (train_losses, val_losses) as values\n",
    "\n",
    "# Define the size of the grid\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(nrows=4, ncols=5, figsize=(20, 20))  # Adjust the size as needed\n",
    "plt.style.use('ggplot')\n",
    "# Iterate over the results and create a subplot for each\n",
    "for i, (params, (train_losses, val_losses)) in enumerate(results_all.items()):\n",
    "    row = i // 5\n",
    "    col = i % 5 \n",
    "    \n",
    "    ax = axs[row, col]\n",
    "    \n",
    "    ax.plot(train_losses, label='Training Loss', color = \"b\")\n",
    "    ax.plot(val_losses, label='Validation Loss', color = \"skyblue\")\n",
    "    ax.set_title(f'Layer size: {params[1]}, Layer count: {params[2]}')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_ylim(20000, 80000)\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../visualizations/DL_no_images_comparison.png\", dpi=800)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744f263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
